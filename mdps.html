<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Markov Decision Processes – CS 1840: Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./control.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="site_libs/quarto-diagram/mermaid.min.js"></script>
<script src="site_libs/quarto-diagram/mermaid-init.js"></script>
<link href="site_libs/quarto-diagram/mermaid.css" rel="stylesheet">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./mdps.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CS 1840: Introduction to Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1.1</span> Introduction</a></li>
  <li><a href="#sec-finite-horizon-mdps" id="toc-sec-finite-horizon-mdps" class="nav-link" data-scroll-target="#sec-finite-horizon-mdps"><span class="header-section-number">1.2</span> Finite-horizon Markov decision processes</a>
  <ul class="collapse">
  <li><a href="#policies" id="toc-policies" class="nav-link" data-scroll-target="#policies"><span class="header-section-number">1.2.1</span> Policies</a></li>
  <li><a href="#sec-trajectories" id="toc-sec-trajectories" class="nav-link" data-scroll-target="#sec-trajectories"><span class="header-section-number">1.2.2</span> Trajectories</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions"><span class="header-section-number">1.2.3</span> Value functions</a></li>
  <li><a href="#the-one-step-bellman-consistency-equation" id="toc-the-one-step-bellman-consistency-equation" class="nav-link" data-scroll-target="#the-one-step-bellman-consistency-equation"><span class="header-section-number">1.2.4</span> The one-step (Bellman) consistency equation</a></li>
  <li><a href="#sec-eval-dp" id="toc-sec-eval-dp" class="nav-link" data-scroll-target="#sec-eval-dp"><span class="header-section-number">1.2.5</span> Policy evaluation in finite-horizon MDPs</a></li>
  <li><a href="#dynamic-programming" id="toc-dynamic-programming" class="nav-link" data-scroll-target="#dynamic-programming"><span class="header-section-number">1.2.6</span> Dynamic programming</a></li>
  <li><a href="#sec-opt-dynamic-programming" id="toc-sec-opt-dynamic-programming" class="nav-link" data-scroll-target="#sec-opt-dynamic-programming"><span class="header-section-number">1.2.7</span> Optimal policies in finite-horizon MDPs</a></li>
  </ul></li>
  <li><a href="#sec-infinite-horizon-mdps" id="toc-sec-infinite-horizon-mdps" class="nav-link" data-scroll-target="#sec-infinite-horizon-mdps"><span class="header-section-number">1.3</span> Infinite-horizon MDPs</a>
  <ul class="collapse">
  <li><a href="#discounted-rewards" id="toc-discounted-rewards" class="nav-link" data-scroll-target="#discounted-rewards"><span class="header-section-number">1.3.1</span> Discounted rewards</a></li>
  <li><a href="#sec-stationary-policies" id="toc-sec-stationary-policies" class="nav-link" data-scroll-target="#sec-stationary-policies"><span class="header-section-number">1.3.2</span> Stationary policies</a></li>
  <li><a href="#value-functions-and-bellman-consistency" id="toc-value-functions-and-bellman-consistency" class="nav-link" data-scroll-target="#value-functions-and-bellman-consistency"><span class="header-section-number">1.3.3</span> Value functions and Bellman consistency</a></li>
  <li><a href="#contraction-mappings" id="toc-contraction-mappings" class="nav-link" data-scroll-target="#contraction-mappings"><span class="header-section-number">1.3.4</span> Contraction mappings</a></li>
  <li><a href="#policy-evaluation-in-infinite-horizon-mdps" id="toc-policy-evaluation-in-infinite-horizon-mdps" class="nav-link" data-scroll-target="#policy-evaluation-in-infinite-horizon-mdps"><span class="header-section-number">1.3.5</span> Policy evaluation in infinite-horizon MDPs</a></li>
  <li><a href="#matrix-inversion-for-deterministic-policies" id="toc-matrix-inversion-for-deterministic-policies" class="nav-link" data-scroll-target="#matrix-inversion-for-deterministic-policies"><span class="header-section-number">1.3.6</span> Matrix inversion for deterministic policies</a></li>
  <li><a href="#optimal-policies-in-infinite-horizon-mdps" id="toc-optimal-policies-in-infinite-horizon-mdps" class="nav-link" data-scroll-target="#optimal-policies-in-infinite-horizon-mdps"><span class="header-section-number">1.3.7</span> Optimal policies in infinite-horizon MDPs</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">1.4</span> Summary</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references"><span class="header-section-number">1.5</span> References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-mdps" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<hr>
<hr>
<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p><strong>Machine learning</strong> studies algorithms that learn to solve a task “on their own”, without needing a programmer to implement handwritten “if statements”. <strong>Reinforcement learning (RL)</strong> is a branch of machine learning that focuses on <strong>decision problems</strong> like the following:</p>
<p><span class="theorem-title"><strong>Example 1.1 (Decision problems)</strong></span> &nbsp;</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="3">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/chess-pieces.jpg" class="img-fluid figure-img"></p>
<figcaption>Board games and video games, such as a game of chess, where a player character takes actions that update the state of the game. <span class="citation" data-cites="guy_chess_2006">(<a href="references.html#ref-guy_chess_2006" role="doc-biblioref">Guy 2006</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/cargo-ship.jpg" class="img-fluid figure-img"></p>
<figcaption>Inventory management, where a company must efficiently move resources from producers to consumers. <span class="citation" data-cites="frans_berkelaar_container_2009">(<a href="references.html#ref-frans_berkelaar_container_2009" role="doc-biblioref">Frans Berkelaar 2009</a>)</span></figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/robot-arm.jpg" class="img-fluid figure-img"></p>
<figcaption>Robotic control, where a robot can move and interact with the real world to complete some task. <span class="citation" data-cites="gpa_photo_archive_robotic_2017">(<a href="references.html#ref-gpa_photo_archive_robotic_2017" role="doc-biblioref">GPA Photo Archive 2017</a>)</span></figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>All of these tasks involve taking a sequence of <strong>actions</strong> in an interactive environment. This <strong>interaction loop</strong> can be summarized in the following diagram:</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-rl-interaction" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rl-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-rl-interaction">graph LR
    Agent --"takes *action*"--&gt; Environment
    Environment --"observes *state*, *reward*"--&gt; Agent
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rl-interaction-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.1: The RL interaction loop. The agent chooses an <strong>action</strong> that affects the environment. The environment updates according to the <strong>state transitions</strong>. Then the agent observes the updated environment and a <strong>reward signal</strong> that says how well the agent behaved.
</figcaption>
</figure>
</div>
</div>
</div>
<p>The strategy for choosing actions is called the <em>policy</em>. The goal of an RL algorithm is to find a policy that achieves the maximum total reward. This is a very general problem! How can we describe such tasks using mathematics in a way that is <em>general</em> yet also <em>structured</em> enough for us to analyze?</p>
<p>This chapter will introduce the most popular formalization for such tasks: <strong>Markov decision processes (MDPs)</strong>. We will study <strong>dynamic programming (DP)</strong> algorithms for solving <em>fully specified</em> tasks in which we completely understand the environment. We’ll describe how to <em>evaluate</em> different policies and how to compute (or approximate) the <strong>optimal policy</strong> for a given MDP. We’ll introduce the <strong>Bellman consistency condition</strong>, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.</p>
<div id="rem-complications" class="proof remark">
<p><span class="proof-title"><em>Remark 1.1</em> (Further generalizations). </span>In the real world, we often <em>don’t</em> know how the environment works, or it is too hard to represent the environment on a computer. We will study such tasks in future chapters. Additionally, in many tasks, only a <em>subset</em> of the state is visible to the observer. We won’t discuss such <em>partially observed</em> environments in this course.</p>
</div>
<div id="6cf0cbab" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> NamedTuple, Float, Array, partial, jax, jnp, latexify, latex</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> tabulate <span class="im">import</span> tabulate</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> Markdown, display</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-finite-horizon-mdps" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="sec-finite-horizon-mdps"><span class="header-section-number">1.2</span> Finite-horizon Markov decision processes</h2>
<p>To gain a better grasp of decision processes, let us frame the <em>robotic control</em> task in <a href="#exm-decision-problems" class="quarto-xref">Example&nbsp;<span>1.1</span></a> as a decision problem.</p>
<div id="exm-mdp-robotics" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Robotic control as a decision problem)</strong></span> Suppose the goal is to move the robot to a certain position.</p>
<ul>
<li>The <strong>state</strong> consists of the positions and velocities of the robot’s joints.</li>
<li>The <strong>action</strong> consists of the forces to apply to the robot’s motors.</li>
<li>The <strong>state transitions</strong> are essentially the rules of physics: after applying a certain force to the joints, their positions and velocities would change according to physical law.</li>
<li>We <strong>reward</strong> positions that are closer to the desired position. To be more energy-efficient, we could also deduct reward for applying a lot of force, or add other terms that describe the ideal behavior.</li>
</ul>
</div>
<div id="exr-decision-problems" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.1 (Identifying decision problems)</strong></span> For each of the other examples in <a href="#exm-decision-problems" class="quarto-xref">Example&nbsp;<span>1.1</span></a>, what information should the <em>state</em> include? What might the valid set of <em>actions</em> be? Describe the <em>state transitions</em> heuristically, and a <em>reward function</em> that describes how the task should be solved.</p>
</div>
<p>In many decision problems, the state transitions only depend on the <em>current</em> state and action. For example, in <a href="#exm-mdp-robotics" class="quarto-xref">Example&nbsp;<span>1.2</span></a>, if we use Newton’s laws of physics to compute the state transitions, then just knowing the current positions and velocities is enough to calculate the next positions and velocities, since Newton’s laws are second-order. We say that such state transitions satisfy the <strong>Markov property</strong>, which we will formally define in <a href="#def-markov-property" class="quarto-xref">Definition&nbsp;<span>1.2</span></a> after introducing some notation.</p>
<div id="exr-understanding-mdps" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.2</strong></span> Look back at the state transitions you described in <a href="#exr-decision-problems" class="quarto-xref">Exercise&nbsp;<span>1.1</span></a>. Do they satisfy the Markov property? (For chess, consider the threefold repetition rule: if the same position occurs three times during the game, either player may claim a draw.)</p>
</div>
<p><strong>Markov decision processes</strong> (MDPs), which satisfy the Markov property, are the most common formalization of decision problems. MDPs can be classified as <strong>finite-horizon</strong>, where the interactions end after some finite number of time steps, or <strong>infinite-horizon</strong>, where the interactions are allowed to continue indefinitely. We’ll begin with the finite-horizon case and discuss the infinite-horizon case in <a href="#sec-infinite-horizon-mdps" class="quarto-xref"><span>Section 1.3</span></a>.</p>
<div id="def-finite-horizon-mdp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Finite-horizon Markov decision process)</strong></span> The components of a finite-horizon Markov decision process are:</p>
<ol type="1">
<li><p>The <strong>state</strong> that the agent interacts with. We use <span class="math inline">\(\mathcal{S}\)</span> to denote the set of possible states, called the <strong>state space</strong>.</p></li>
<li><p>The <strong>actions</strong> that the agent can take. We use <span class="math inline">\(\mathcal{A}\)</span> to denote the set of possible actions, called the <strong>action space</strong>.</p></li>
<li><p>Some <strong>initial state distribution</strong> <span class="math inline">\(\mu \in \triangle(\mathcal{S})\)</span>.</p></li>
<li><p>The <strong>state transitions</strong> (a.k.a. <strong>dynamics</strong>) <span class="math inline">\(P : \mathcal{S} \times \mathcal{A} \to \triangle(\mathcal{S})\)</span> that describe what state the agent transitions to after taking an action. (We write <span class="math inline">\(P(s_{h+ 1} \mid s_h, a_h)\)</span> for the probability of transitioning to state <span class="math inline">\(s_{h+1}\)</span> when starting in state <span class="math inline">\(s_h\)</span> and taking action <span class="math inline">\(a_h\)</span>.)</p></li>
<li><p>The <strong>reward function</strong>. In this course, we’ll take it to be a deterministic function on state-action pairs, <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>, but in general many results will extend to a <em>stochastic</em> reward signal.</p></li>
<li><p>A time horizon <span class="math inline">\(H\in \mathbb{N}\)</span> that specifies the number of interactions in an <strong>episode</strong>.</p></li>
</ol>
<p>Combined together, these objects specify a finite-horizon Markov decision process:</p>
<p><span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mu, P, r, H).
\]</span></p>
</div>
<p>When there are <strong>finitely</strong> many states and actions, i.e. <span class="math inline">\(|\mathcal{S}|, |\mathcal{A}| &lt; \infty\)</span>, we can express the relevant quantities as vectors and matrices (i.e.&nbsp;<em>tables</em> of values):</p>
<p><span class="math display">\[
\begin{aligned}
    \mu &amp;\in [0, 1]^{|\mathcal{S}|} &amp;
    P &amp;\in [0, 1]^{(|\mathcal{S} \times \mathcal{A}|) \times |\mathcal{S}|} &amp;
    r &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}
\end{aligned}
\]</span></p>
<p>(Verify that the types and shapes provided above make sense!)</p>
<div id="bae5cdb2" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MDP(NamedTuple):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A description of a Markov decision process with finitely many states and actions."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    S: <span class="bu">int</span>  <span class="co"># number of states</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    A: <span class="bu">int</span>  <span class="co"># number of actions</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    mu: Float[Array, <span class="st">" S"</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    P: Float[Array, <span class="st">"S A S"</span>]  <span class="co"># "current" state, "current" action, "next" state</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    r: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    H: <span class="bu">int</span>  <span class="co"># the horizon</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    gamma: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># discount factor (used later)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="def-markov-property" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Markov property)</strong></span> A decision process satisfies the <strong>Markov property</strong> if the next state is independent from the past states and actions when conditioned on the current state and action:</p>
<p><span class="math display">\[
\mathbb{P}(s_{h+1} \mid s_0, a_0, \dots, s_h, a_h) = \mathbb{P}(s_{h+1} \mid s_h, a_h)
\]</span></p>
<p>By their definition, Markov decision processes satisfy the Markov property. This is because the state transitions are <em>defined</em> using the function <span class="math inline">\(P\)</span>, which only takes in the state-action pair to transition from.</p>
</div>
<div id="exr-markov-future" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.3 (Conditional independence for future states)</strong></span> Use the chain rule of probability to show that, conditioned on the current state and action <span class="math inline">\(s_h, a_h\)</span>, <em>all</em> future states <span class="math inline">\(s_{h'}\)</span> where <span class="math inline">\(h' &gt; h\)</span> are conditionally independent of the past states and actions. That is, for all <span class="math inline">\(h' &gt; h\)</span>,</p>
<p><span class="math display">\[
\mathbb{P}(s_{h'} \mid s_0, a_0, \dots, s_h, a_h) = \mathbb{P}(s_{h'} \mid s_h, a_h).
\]</span></p>
<p>We prove by induction. MDPs already satisfy the case <span class="math inline">\(h' = h+1\)</span>. Now suppose it holds for <span class="math inline">\(h' &gt; h\)</span>. Then</p>
<p><span class="math display">\[
\mathbb{P}(s_{h'+1} \mid s_0, a_0, \dots, s_h, a_h)
= \mathbb{P}(s_{h'+1} \mid s_0, a_0, \dots, s_h, a_h, \dots, s_{h'}) \mathbb{P}(s_{h'} \mid s_0, a_0, \dots, s_h, a_h, s_{h'})
= \mathbb{P}(s_)
\]</span></p>
</div>
<div id="exm-tidy-mdp" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Tidying MDP)</strong></span> Let’s consider a simple decision problem throughout this chapter: the task of keeping your room tidy.</p>
<p>Your room has the possible states <span class="math inline">\(\mathcal{S} = \{ \text{orderly}, \text{messy} \}.\)</span> You can take either of the actions <span class="math inline">\(\mathcal{A} = \{ \text{ignore}, \text{tidy} \}.\)</span> The room starts off orderly, that is, <span class="math inline">\(\mu(\text{orderly}) = 1\)</span>.</p>
<p>The <strong>state transitions</strong> are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it <em>might</em> become messy, according to the probabilities in <a href="#tbl-tidy" class="quarto-xref">Table&nbsp;<span>1.1</span></a>.</p>
<p>The <strong>rewards</strong> are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy your additional time). Tidying a messy room is a chore that gives no reward.</p>
<p>Consider a time horizon of <span class="math inline">\(H= 7\)</span> days (one interaction per day). Let <span class="math inline">\(t = 0\)</span> correspond to Monday and <span class="math inline">\(t = 6\)</span> correspond to Sunday.</p>
</div>
<div class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tidy_mdp <span class="op">=</span> MDP(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    S<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = orderly, 1 = messy</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    A<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = ignore, 1 = tidy</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    mu<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>]),  <span class="co"># start in orderly state</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    P<span class="op">=</span>jnp.array([</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.7</span>, <span class="fl">0.3</span>],  <span class="co"># orderly, ignore</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># orderly, tidy</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.0</span>, <span class="fl">1.0</span>],  <span class="co"># messy, ignore</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># messy, tidy</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span>jnp.array([</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="fl">1.0</span>,   <span class="co"># orderly, ignore</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># orderly, tidy</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># messy, ignore</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.0</span>,   <span class="co"># messy, tidy</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    H<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(<span class="op">*</span>[</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>        [<span class="st">'orderly'</span>, <span class="st">'orderly'</span>, <span class="st">'messy'</span>, <span class="st">'messy'</span>],</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>        [<span class="st">'ignore'</span>, <span class="st">'tidy'</span>, <span class="st">'ignore'</span>, <span class="st">'tidy'</span>],</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.P[:, :, <span class="dv">0</span>].flatten(),</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.P[:, :, <span class="dv">1</span>].flatten(),</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>        tidy_mdp.r.flatten(),</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>    [</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>,</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a$"</span>,</span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$P(\text</span><span class="sc">{orderly}</span><span class="vs"> \mid s, a)$"</span>,</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$P(\text</span><span class="sc">{messy}</span><span class="vs"> \mid s, a)$"</span>,</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$r(s, a)$"</span>,</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>    ]</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-tidy" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="3">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-tidy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.1: Description of the MDP in <a href="#exm-tidy-mdp" class="quarto-xref">Example&nbsp;<span>1.3</span></a>.
</figcaption>
<div aria-describedby="tbl-tidy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="3">
<table class="do-not-create-environment cell caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: left;"><span class="math inline">\(a\)</span></th>
<th style="text-align: right;"><span class="math inline">\(P(\text{orderly} \mid s, a)\)</span></th>
<th style="text-align: right;"><span class="math inline">\(P(\text{messy} \mid s, a)\)</span></th>
<th style="text-align: right;"><span class="math inline">\(r(s, a)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">orderly</td>
<td style="text-align: left;">ignore</td>
<td style="text-align: right;">0.7</td>
<td style="text-align: right;">0.3</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">orderly</td>
<td style="text-align: left;">tidy</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">-1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">messy</td>
<td style="text-align: left;">ignore</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">-1</td>
</tr>
<tr class="even">
<td style="text-align: left;">messy</td>
<td style="text-align: left;">tidy</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
<section id="policies" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="policies"><span class="header-section-number">1.2.1</span> Policies</h3>
<div id="def-policy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 (Policies)</strong></span> A <strong>policy</strong> <span class="math inline">\(\pi\)</span> describes the agent’s strategy: which actions it takes in a given situation. A key goal of RL is to find the <strong>optimal policy</strong> that maximizes the total reward on average.</p>
<p>There are three axes along which policies can vary: their outputs, inputs, and time-dependence.</p>
<ol type="1">
<li>Outputs: <strong>Deterministic or stochastic.</strong> A deterministic policy outputs actions while a stochastic policy outputs <em>distributions</em> over actions.</li>
</ol>
<div id="fig-deterministic-stochastic-policy" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-deterministic-stochastic-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/deterministic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A deterministic policy, which produces a single action</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/stochastic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A stochastic policy, which produces a distribution over possible actions</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig quarto-uncaptioned" id="fig-deterministic-stochastic-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.2
</figcaption>
</figure>
</div>
<ol start="2" type="1">
<li><p>Inputs: <strong>State-dependent or history-dependent.</strong> A state-dependent (a.k.a. “Markovian”) policy only depends on the current state, while a history-dependent policy depends on the sequence of past states, actions, and rewards. We’ll only consider state-dependent policies in this course.</p></li>
<li><p><strong>Stationary or time-dependent.</strong> A stationary (a.k.a. time-homogeneous) policy keeps the same strategy at all time steps, while a time-dependent policy can depend on the current timestep. For consistency with states and actions, we will denote the timestep as a subscript, i.e.&nbsp;<span class="math inline">\(\pi = \{ \pi_0, \dots, \pi_{H-1} \}.\)</span></p></li>
</ol>
</div>
<p>Note that for finite state and action spaces, we can represent a randomized mapping <span class="math inline">\(\mathcal{S} \to \Delta(\mathcal{A})\)</span> as a matrix <span class="math inline">\(\pi \in [0, 1]^{\mathcal{S} \times \mathcal{A}}\)</span> where each row describes the policy’s distribution over actions for the corresponding state.</p>
<div id="exm-tidy-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Policies for the tidying MDP)</strong></span> Here are some possible deterministic policies for the tidying MDP <a href="#exm-tidy-mdp" class="quarto-xref">Example&nbsp;<span>1.3</span></a>:</p>
<ul>
<li>Always tidy: <span class="math inline">\(\pi(s) = \text{tidy}\)</span>.</li>
<li>Only tidy on weekends: <span class="math inline">\(\pi_h(s) = \text{tidy}\)</span> if <span class="math inline">\(h\in \{ 5, 6 \}\)</span> and <span class="math inline">\(\pi_h(s) = \text{ignore}\)</span> otherwise.</li>
<li>Only tidy if the room is messy: <span class="math inline">\(\pi_h(\text{messy}) = \text{tidy}\)</span> and <span class="math inline">\(\pi_h(\text{orderly}) = \text{ignore}\)</span> for all <span class="math inline">\(h\)</span>.</li>
</ul>
</div>
<div id="257fa523" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrays of shape (H, S, A) represent time-dependent policies</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tidy_policy_always_tidy <span class="op">=</span> (</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    .at[:, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tidy_policy_weekends <span class="op">=</span> (</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">5</span>:<span class="dv">7</span>, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">0</span>:<span class="dv">5</span>, :, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>tidy_policy_messy_only <span class="op">=</span> (</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">1</span>, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">0</span>, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<!-- ::: {#rem-jax-immutable}
Array objects in Jax are **immutable,** that is, they cannot be _changed._
This might seem inconvenient, but in larger projects,
immutability makes code easier to reason about.
::: -->
</section>
<section id="sec-trajectories" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="sec-trajectories"><span class="header-section-number">1.2.2</span> Trajectories</h3>
<div id="def-trajectory" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4 (Trajectories)</strong></span> A sequence of states, actions, and rewards is called a <strong>trajectory</strong>:</p>
<p><span class="math display">\[
\tau = (s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1})
\]</span></p>
<p>where <span class="math inline">\(r_h= r(s_h, a_h)\)</span>. (Note that some sources omit the reward at the final time step. This is a minor detail.)</p>
</div>
<div id="1a6087fe" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transition(NamedTuple):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single state-action-reward interaction with the environment.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    A trajectory comprises a sequence of transitions.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    s: <span class="bu">int</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    a: <span class="bu">int</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    r: <span class="bu">float</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Once we’ve chosen a policy, we can sample trajectories by repeatedly choosing actions according to the policy, transitioning according to the state transitions, and observing the rewards. This generative process induces a distribution <span class="math inline">\(\rho^{\pi}\)</span> over trajectories. (We assume that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(P\)</span> are clear from context.)</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-trajectory-diagram" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-trajectory-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-trajectory-diagram">graph LR
    S0($$s_0$$) -- $$\pi_0$$ --&gt; A0{{$$a_0$$}}
    S0 &amp; A0 --&gt; R0[$$r_0$$]
    A0 &amp; S0 --&gt; S1($$s_1$$)
    S1 -- $$\pi_1$$ --&gt; A1{{$$a_1$$}}
    S1 &amp; A1 --&gt; R1[$$r_1$$]
    A1 &amp; S1 --&gt; S2($$s_2$$)
    S2 -- $$\pi_2$$ --&gt; A2{{$$a_2$$}}
    S2 &amp; A2 --&gt; R2[$$r_2$$]
    A2 &amp; S2 --&gt; S3($$s_3$$)
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-trajectory-diagram-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.3: A trajectory is generated by taking a sequence of actions according to the policy.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exm-tidy-traj" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Trajectories in the tidying environment)</strong></span> Here is a possible trajectory for the tidying example:</p>
<div class="{table}">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(h\)</span></th>
<th style="text-align: center;"><span class="math inline">\(0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(4\)</span></th>
<th style="text-align: center;"><span class="math inline">\(5\)</span></th>
<th style="text-align: center;"><span class="math inline">\(6\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(s\)</span></td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(a\)</span></td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(r\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Could any of the policies in <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.4</span></a> have generated this trajectory?</p>
</div>
<p>Note that for a state-dependent policy, using the Markov property (<a href="#def-markov-property" class="quarto-xref">Definition&nbsp;<span>1.2</span></a>), we can write down the likelihood function of this probability distribution in an <strong>autoregressive</strong> way (i.e.&nbsp;one timestep at a time):</p>
<p><span id="eq-autoregressive-trajectories"><span class="math display">\[
\rho^{\pi}(\tau) := \mu(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots P(s_{H-1} \mid s_{H-2}, a_{H-2}) \pi_{H-1}(a_{H-1} \mid s_{H-1})
\tag{1.1}\]</span></span></p>
<div id="003910f4" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trajectory_log_likelihood(</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    traj: <span class="bu">list</span>[Transition],</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    pi: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute the log-likelihood of a trajectory under a given MDP and policy."""</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initial distribution and action</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> jnp.log(mdp.mu[traj[<span class="dv">0</span>].s])</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    total <span class="op">+=</span> jnp.log(pi[traj[<span class="dv">0</span>].s, traj[<span class="dv">0</span>].a])</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remaining state transitions and actions</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, mdp.H):</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(mdp.P[traj[i <span class="op">-</span> <span class="dv">1</span>].s, traj[i <span class="op">-</span> <span class="dv">1</span>].a, traj[i].s])</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(pi[traj[i].s, traj[i].a])</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exr-stochastic-reward-trajectory-likelihood" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.4 (Trajectory distribution for stochastic reward)</strong></span> Modify <a href="#eq-autoregressive-trajectories" class="quarto-xref">Equation&nbsp;<span>1.1</span></a> for the case when the reward function is stochastic, that is, <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \triangle(\mathbb{R})\)</span>.</p>
</div>
<p>For a deterministic policy <span class="math inline">\(\pi\)</span>, we have that <span class="math inline">\(\pi_h(a \mid s) = \mathbb{I}[a = \pi_h(s)]\)</span>; that is, the probability of taking an action is <span class="math inline">\(1\)</span> if it’s the unique action prescribed by the policy for that state and <span class="math inline">\(0\)</span> otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution <span class="math inline">\(\mu\)</span> and the state transitions <span class="math inline">\(P\)</span>.</p>
</section>
<section id="value-functions" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="value-functions"><span class="header-section-number">1.2.3</span> Value functions</h3>
<p>The core goal of an RL algorithm is to find a policy that maximizes the expected total reward</p>
<p><span class="math display">\[
\mathbb{E}[r_0 + \cdots + r_{H-1}].
\]</span></p>
<p>Note that the quantity <span class="math inline">\(r_0 + \cdots + r_{H-1}\)</span> is a random variable. For a given policy, its distribution is induced by that policy’s trajectory distribution (<a href="#eq-autoregressive-trajectories" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>). We need tools for describing the expected total reward achieved by a given policy, starting in specific states and actions. This will allow us to compare the quality of differeent policies and compute the optimal policy.</p>
<div id="def-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 (Value function)</strong></span> A policy’s <strong>value function</strong> at time <span class="math inline">\(h\)</span> is its expected remaining reward, starting in a specific state:</p>
<p><span class="math display">\[
V_h^\pi(s) := \mathbb{E}_{\tau \sim \rho^\pi} [r_h+ \cdots + r_{H-1} \mid s_h= s]
\]</span></p>
</div>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-value-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-value-trajectory">graph LR
    S0($$s_h = s$$) -- $$\pi_h$$ --&gt; A0{{$$a_h$$}}
    S0 &amp; A0 --&gt; R0[$$r_h$$]
    A0 &amp; S0 --&gt; S1("$$s_{h+1}$$")
    S1 -- "$$\pi_{h+1}$$" --&gt; A1{{"$$a_{h+1}$$"}}
    S1 &amp; A1 --&gt; R1["$$r_{h+1}$$"]
    A1 &amp; S1 --&gt; S2("$$s_{h+2}$$")
    S2 -- "$$\pi_{h+2}$$" --&gt; A2{{"$$a_{h+2}$$"}}
    S2 &amp; A2 --&gt; R2["$$r_{h+2}$$"]
    A2 &amp; S2 --&gt; S3("$$s_{h+3}$$")

    class S0,R0,R1,R2 thick
    class R0,R1,R2 reward

    classDef thick stroke-width: 4px;
    classDef reward fill: lightcoral;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.4: The policy starts in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span>. Then the expected remaining reward is computed.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="def-action-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.6 (Action-value function)</strong></span> Similarly, we can define the <strong>action-value function</strong> (aka the <strong>Q-function</strong>) at time <span class="math inline">\(h\)</span> as the expected remaining reward starting in a specific state and taking a specific action:</p>
<p><span class="math display">\[
Q_h^\pi(s, a) := \mathbb{E}_{\tau \sim \rho^\pi} [r_h+ \cdots + r_{H-1} \mid s_h= s, a_h= a]
\]</span></p>
</div>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-action-value-trajectory" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-action-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-action-value-trajectory">graph LR
    S0($$s_h = s$$) ~~~ A0{{$$a_h = a$$}}
    S0 &amp; A0 --&gt; R0[$$r_h$$]
    A0 &amp; S0 --&gt; S1("$$s_{h+1}$$")
    S1 -- "$$\pi_{h+1}$$" --&gt; A1{{"$$a_{h+1}$$"}}
    S1 &amp; A1 --&gt; R1["$$r_{h+1}$$"]
    A1 &amp; S1 --&gt; S2("$$s_{h+2}$$")
    S2 -- "$$\pi_{h+2}$$" --&gt; A2{{"$$a_{h+2}$$"}}
    S2 &amp; A2 --&gt; R2["$$r_{h+2}$$"]
    A2 &amp; S2 --&gt; S3("$$s_{h+3}$$")

    class S0,A0,R0,R1,R2 thick;
    class R0,R1,R2 reward;

    classDef thick stroke-width: 4px;
    classDef reward fill: lightcoral;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-action-value-trajectory-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.5: The policy starts in state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span> and takes action <span class="math inline">\(a\)</span>. Then the expected remaining reward is computed.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="exr-relating-value-q" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.5 (Relating the value function and action-value function)</strong></span> Show that the value function is the expected action-value over actions drawn from the policy:</p>
<p><span class="math display">\[
V_h^\pi(s) = \mathbb{E}_{a \sim \pi_h(s)} [Q_h^\pi(s, a)]
\]</span></p>
<p>and the action-value is the sum of the immediate reward and the expected value of the following state:</p>
<p><span class="math display">\[
Q_h^\pi(s, a) = r(s, a) + \mathbb{E}_{s' \sim P(s, a)} [V_{h+1}^\pi(s')]
\]</span></p>
</div>
<div id="992b23be" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_v(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    q: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the value function for a given policy in a known finite MDP</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from its action-value function.</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.average(q, weights<span class="op">=</span>policy, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_q(</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    v_next: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the action-value function in a known finite MDP</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from the corresponding value function.</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the discount factor is relevant later</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v_next</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># convert a list of v functions to a list of q functions</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>v_ary_to_q_ary <span class="op">=</span> jax.vmap(v_to_q, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="the-one-step-bellman-consistency-equation" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="the-one-step-bellman-consistency-equation"><span class="header-section-number">1.2.4</span> The one-step (Bellman) consistency equation</h3>
<p>Note that by simply considering the cumulative reward as the sum of the <em>immediate</em> reward and the <em>remaining</em> reward, we can describe the value function recursively, in terms of itself. The resulting system of equations (one equation per state) is named after <strong>Richard Bellman</strong> (1920–1984), who is credited with introducing dynamic programming in 1953.</p>
<div id="thm-bellman-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Bellman consistency equations for the value function)</strong></span> For a state-dependent policy <span class="math inline">\(\pi\)</span>,</p>
<p><span id="eq-bellman-consistency"><span class="math display">\[
\forall s \in \mathcal{S},
\quad
V_h^\pi(s) = \mathbb{E}_{\substack{a \sim \pi_h(s) \\ s' \sim P(s, a)}} [r(s, a) + V_{h+1}^\pi(s')]
\tag{1.2}\]</span></span></p>
</div>
<div id="051a8f6b" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_bellman_consistency_v(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"H S A"</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    v_ary: Float[Array, <span class="st">"H S"</span>],</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Check that the given (time-dependent) "value function"</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    satisfies the Bellman consistency equation.</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        jnp.allclose(</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># lhs</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            v_ary[h],</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rhs</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>            jnp.<span class="bu">sum</span>(policy[h] <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v_ary[h <span class="op">+</span> <span class="dv">1</span>]), axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exr-prove-bellman-consistency" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.6</strong></span> Verify that this equation holds for all <span class="math inline">\(s \in \mathcal{S}\)</span> by expanding <span class="math inline">\(V_h^\pi(s)\)</span> and <span class="math inline">\(V_{h+1}^\pi(s')\)</span> and applying linearity of expectation. Where did you use the assumption that <span class="math inline">\(\pi\)</span> doesn’t depend on the past states and actions?</p>
</div>
<div id="rem-bellman-det" class="proof remark">
<p><span class="proof-title"><em>Remark 1.2</em> (The Bellman consistency equation for deterministic policies). </span>Note that for state-dependent deterministic policies, the Bellman consistency equations simplify to</p>
<p><span class="math display">\[
V_h^\pi(s) = r(s, \pi_h(s)) + \mathbb{E}_{s' \sim P(s, \pi_h(s))} [V_{h+1}^\pi(s')]
\]</span></p>
</div>
</section>
<section id="sec-eval-dp" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="sec-eval-dp"><span class="header-section-number">1.2.5</span> Policy evaluation in finite-horizon MDPs</h3>
<p>How can we actually <em>evaluate</em> a given policy, that is, compute its value function?</p>
<p>Suppose we start with some guess <span class="math inline">\(v_h: \mathcal{S} \to \mathbb{R}\)</span> for the values of each state at time <span class="math inline">\(h= 0, \dots, H-1\)</span>. We write <span class="math inline">\(v\)</span> in lowercase to indicate that it might not be the value function of a policy. How might we improve this guess?</p>
<p>Recall that the Bellman consistency equations (<a href="#eq-bellman-consistency" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>) hold for any value function. Suppose we replace <span class="math inline">\(V^\pi_{h+1}\)</span> with <span class="math inline">\(v_{h+1}\)</span> on the right-hand side. Then the new right-hand side quantity,</p>
<p><span id="eq-updated-bellman"><span class="math display">\[
\mathbb{E}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v_{h+1}(s')],
\tag{1.3}\]</span></span></p>
<p>can be thought of as follows: we take one action according to <span class="math inline">\(\pi\)</span>, observe the immediate reward, and evaluate the next state using <span class="math inline">\(v\)</span>. This gives us an <em>updated</em> estimate of the value of <span class="math inline">\(V^\pi_h(s)\)</span> that is at least as accurate as applying <span class="math inline">\(v(s)\)</span> directly.</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-updated-bellman" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-updated-bellman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-updated-bellman">graph LR
    S0($$s_h = s$$) -- $$\pi_h$$ --&gt; A0{{"$$a_h = a$$"}}
    S0 &amp; A0 --&gt; R0["$$r_h = r(s, a)$$"]
    A0 &amp; S0 --&gt; S1("$$s_{h+1} = s'$$")
    S1 --&gt; R1["$$r_{h+1} + \cdots + r_{H-1} \approx v_{h+1}(s')$$"]

    class S0,R0,R1,R2 thick
    class R0,R1,R2 reward

    classDef thick stroke-width: 4px;
    classDef reward fill: lightcoral;
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-updated-bellman-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.6: We evaluate the next state using the guess function <span class="math inline">\(v_{h+1}\)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
<p>We can treat this operation of taking <span class="math inline">\(v\)</span> to its improved version as a higher-order function <span class="math inline">\(\mathcal{J}^\pi\)</span>, known as the Bellman operator.</p>
<div id="def-bellman-operator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.7 (Bellman operator)</strong></span> Let <span class="math inline">\(\pi : \mathcal{S} \to \triangle(\mathcal{A})\)</span>. the Bellman operator <span class="math inline">\(\mathcal{J}^{\pi} : (\mathcal{S} \to \mathbb{R}) \to (\mathcal{S} \to \mathbb{R})\)</span> is the higher-order function that takes in a function <span class="math inline">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman equation with <span class="math inline">\(v\)</span> substituted in:</p>
<p><span id="eq-bellman-operator"><span class="math display">\[
\mathcal{J}^{\pi}(v) := \left(
    s \mapsto \mathbb{E}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v(s')]
\right).
\tag{1.4}\]</span></span></p>
<p>This is a crucial tool for reasoning about MDPs. Intuitively, it answers the following question: if we evaluate the <em>next</em> state using <span class="math inline">\(v\)</span>, how good is the <em>current</em> state, according to the given policy?</p>
</div>
<div id="cell-fig-bellman-pseudocode" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator_looping(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Looping definition of the Bellman operator.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Concise version is below</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    v_new <span class="op">=</span> jnp.zeros((mdp.S,))</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(mdp.A):</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> s_next <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>                v_new[s] <span class="op">+=</span> (</span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>                    policy[s, a]</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> mdp.P[s, a, s_next]</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> (mdp.r[s, a] <span class="op">+</span> mdp.gamma <span class="op">*</span> v[s_next])</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v_new</span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co"># Note that we can concisely implement this using the `q_to_v` and `v_to_q` utilities from above:</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator(</span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""For a known finite MDP, the Bellman operator can be exactly evaluated."""</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_v(policy, v_to_q(mdp, v))  <span class="co"># equivalent</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(policy <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v), axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>latex(</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>    bellman_operator_looping,</span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"v_new"</span>: <span class="vs">r"v^\text</span><span class="sc">{lhs}</span><span class="vs">"</span>, <span class="st">"policy"</span>: <span class="vs">r"\pi"</span>, <span class="st">"s_next"</span>: <span class="vs">r"s'"</span>},</span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>    trim_prefixes<span class="op">=</span>{<span class="st">"mdp"</span>},</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>    subscript_type<span class="op">=</span><span class="st">"call"</span>,</span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-bellman-pseudocode" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="9">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bellman-pseudocode-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
\begin{array}{l} \mathbf{function} \ \mathrm{bellman\_operator\_looping}(\mathrm{mdp}: \mathrm{MDP}, \pi: \mathbb{R}^{S \times A}, v: \mathbb{R}^{S}) \\ \hspace{1em} \textrm{"
    Looping definition of the Bellman operator.
    Concise version is below
    "} \\ \hspace{1em} v^\text{lhs} \gets \mathbf{0}^{S} \\ \hspace{1em} \mathbf{for} \ s \in \mathrm{range} \mathopen{}\left( S \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathbf{for} \ a \in \mathrm{range} \mathopen{}\left( A \mathclose{}\right) \ \mathbf{do} \\ \hspace{3em} \mathbf{for} \ s' \in \mathrm{range} \mathopen{}\left( S \mathclose{}\right) \ \mathbf{do} \\ \hspace{4em} v^\text{lhs}(s) \gets v^\text{lhs}(s) + \pi(s, a) P(s, a, s') \cdot \mathopen{}\left( r(s, a) + \gamma v(s') \mathclose{}\right) \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{2em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ v^\text{lhs} \\ \mathbf{end \ function} \end{array}
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bellman-pseudocode-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.7: An algorithm for computing the Bellman operator for a finite state and action space.
</figcaption>
</figure>
</div>
</div>
<p>The Bellman operator also gives us a concise way to express the Bellman consistency equations (<a href="#eq-bellman-consistency" class="quarto-xref">Equation&nbsp;<span>1.2</span></a>) for the value function:</p>
<p><span id="eq-bellman-operator-consistency"><span class="math display">\[
V_h^\pi = \mathcal{J}^{\pi}(V_{h+1}^\pi)
\tag{1.5}\]</span></span></p>
</section>
<section id="dynamic-programming" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="dynamic-programming"><span class="header-section-number">1.2.6</span> Dynamic programming</h3>
<p>The Bellman consistency equations (<a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a>) give us a convenient algorithm for evaluating stationary policies: it expresses the value function at timestep <span class="math inline">\(h\)</span> as a function of the value function at timestep <span class="math inline">\(h+1\)</span>. This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman operator to compute the value function at each time step.</p>
<div id="cell-fig-dp-eval-finite" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dp_eval_finite(mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"H S"</span>]:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a policy using dynamic programming."""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((mdp.H <span class="op">+</span> <span class="dv">1</span>, mdp.S))  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>        V[h] <span class="op">=</span> bellman_operator(mdp, policy[h], V[h <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> V[:<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>latex(</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    dp_eval_finite,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"bellman_operator"</span>: <span class="vs">r"\mathcal</span><span class="sc">{J}</span><span class="vs">"</span>, <span class="st">"policy"</span>: <span class="vs">r"\pi"</span>, <span class="st">"V"</span>: <span class="st">"V^\pi"</span>},</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-dp-eval-finite" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="10">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dp-eval-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
\begin{array}{l} \mathbf{function} \ \mathrm{dp\_eval\_finite}(\mathrm{mdp}: \mathrm{MDP}, \pi: \mathbb{R}^{S \times A}) \\ \hspace{1em} \textrm{"Evaluate a policy using dynamic programming."} \\ \hspace{1em} V^\pi \gets \mathbf{0}^{\mathrm{mdp}.H + 1 \times \mathrm{mdp}.S} \\ \hspace{1em} \mathbf{for} \ h \in \mathrm{range} \mathopen{}\left( \mathrm{mdp}.H - 1, -1, -1 \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} V^\pi_{h} \gets \mathcal{J} \mathopen{}\left( \mathrm{mdp}, \pi_{h}, V^\pi_{h + 1} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ V^\pi_{:-1} \\ \mathbf{end \ function} \end{array}
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dp-eval-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.8: A dynamic programming algorithm for evaluating a policy in a finite-horizon MDP.
</figcaption>
</figure>
</div>
</div>
<p>This runs in time <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>. Note that the implementation of the Bellman operator in <a href="#fig-bellman-pseudocode" class="quarto-xref">Figure&nbsp;<span>1.7</span></a> can be easily modified to compute <span class="math inline">\(Q^\pi\)</span> as an intermediate step. Do you see how?</p>
<div id="exm-tidy-eval-finite" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Tidying policy evaluation)</strong></span> Let’s evaluate the policy from <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.4</span></a> in the tidying MDP that tidies if and only if the room is messy. We’ll use the Bellman consistency equation to compute the value function at each time step.</p>
<p><span class="math display">\[
\begin{aligned}
V_{H-1}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) \\
&amp;= 1 \\
V_{H-1}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) \\
&amp;= 0 \\
V_{H-2}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \mathbb{E}_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-1}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1 + 0.3 \cdot 0 \\
&amp;= 1.7 \\
V_{H-2}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \mathbb{E}_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-1}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 \\
V_{H-3}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \mathbb{E}_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-2}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1.7 + 0.3 \cdot 1 \\
&amp;= 2.49 \\
V_{H-3}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \mathbb{E}_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-2}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1.7
\end{aligned}
\]</span></p>
<p>etc. You may wish to repeat this computation for the other policies to get a better sense of this algorithm.</p>
</div>
<div id="c5035437" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>V_messy <span class="op">=</span> dp_eval_finite(tidy_mdp, tidy_policy_messy_only)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>Markdown(tabulate(</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">zip</span>(<span class="op">*</span>[</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>        [<span class="st">"orderly"</span>, <span class="st">"messy"</span>],</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>        <span class="op">*</span>V_messy,</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    [<span class="st">"$s$"</span>] <span class="op">+</span> [<span class="ss">f"$h=</span><span class="sc">{</span>h<span class="sc">}</span><span class="ss">$"</span> <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(V_messy))]</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="11">
<table class="caption-top table table-sm table-striped small">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=1\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=2\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=3\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=4\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=5\)</span></th>
<th style="text-align: right;"><span class="math inline">\(h=6\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">orderly</td>
<td style="text-align: right;">5.56217</td>
<td style="text-align: right;">4.79277</td>
<td style="text-align: right;">4.0241</td>
<td style="text-align: right;">3.253</td>
<td style="text-align: right;">2.49</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="even">
<td style="text-align: left;">messy</td>
<td style="text-align: right;">4.79277</td>
<td style="text-align: right;">4.0241</td>
<td style="text-align: right;">3.253</td>
<td style="text-align: right;">2.49</td>
<td style="text-align: right;">1.7</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">0</td>
</tr>
</tbody>
</table>
</div>
</div>
</section>
<section id="sec-opt-dynamic-programming" class="level3" data-number="1.2.7">
<h3 data-number="1.2.7" class="anchored" data-anchor-id="sec-opt-dynamic-programming"><span class="header-section-number">1.2.7</span> Optimal policies in finite-horizon MDPs</h3>
<p>We’ve just seen how to <em>evaluate</em> a given policy. But how can we find the <em>best</em> policy for a given environment? We must first define what it means for a policy to be optimal. Intuitively speaking, acting according to an optimal policy should yield the highest possible expected remaining reward in any situation. In mathematical terms:</p>
<div id="def-optimal-policy-finite" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.8 (Optimal policies)</strong></span> We call the policy <span class="math inline">\(\pi^\star\)</span> optimal if, for any partial trajectory <span class="math inline">\(\tau_{\le h} = (s_0, a_0, r_0, \dots, s_{h-1}, a_{h-1}, r_{h-1}, s_{h})\)</span>, it does at least as well as any other policy <span class="math inline">\(\pi^\text{any}\)</span>:</p>
<p><span id="eq-optimal-policy-finite"><span class="math display">\[
\begin{gathered}
\forall \pi^\text{any},
\quad
\mathbb{E}\left[
    r_h+ \cdots + r_{H-1} \mid \tau_{\le h}, a_{h'} \sim \pi^\star(\tau_{\le h'})
\right] \\
\ge
\mathbb{E}\left[
    r_h+ \cdots + r_{H-1} \mid \tau_{\le h}, a_{h'} \sim \pi^\text{any}(\tau_{\le h'})
\right].
\end{gathered}
\tag{1.6}\]</span></span></p>
</div>
<div id="exm-optimal-tidy" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.7 (Optimal policy in the tidying MDP)</strong></span> For the tidying MDP (<a href="#exm-tidy-mdp" class="quarto-xref">Example&nbsp;<span>1.3</span></a>), you might guess that the optimal strategy is to only tidy your room if it is messy, since this puts the room into the “orderly” state, in which reward can be obtained, and you are penalized for tidying otherwise. Note that this policy is state-dependent and deterministic.</p>
</div>
<p>It is a stunning fact that <em>every</em> finite-horizon MDP has an optimal policy that is <strong>state-dependent</strong> and <strong>deterministic.</strong> How is this possible? For some intuition, recall that the Markov property (<a href="#def-markov-property" class="quarto-xref">Definition&nbsp;<span>1.2</span></a>) says that once we know the current state, the next state no longer depends on the past history, so history-dependent policies, intuitively speaking, don’t benefit over simply state-dependent ones. For a rigorous proof, see Chapter 1.1.3 of <span class="citation" data-cites="agarwal_reinforcement_2022">Agarwal et al. (<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">2022</a>)</span>. This theorem greatly narrows down the space of policies that we have to search through.</p>
<div id="def-optimal-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.9 (Optimal value function)</strong></span> Given a state <span class="math inline">\(s\)</span> at time <span class="math inline">\(h\)</span>, the optimal value <span class="math inline">\(V^\star_h(s)\)</span> is the <em>maximum</em> expected remaining reward achievable by any policy <span class="math inline">\(\pi^\text{any}\)</span>. Since there exists a state-dependent optimal policy (<a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>1.2</span></a>), it suffices to restrain <span class="math inline">\(\pi^\text{any}\)</span> to state-dependent policies, which have value functions:</p>
<p><span id="eq-optimal-value"><span class="math display">\[
\forall s \in \mathcal{S}, \quad V^\star_h(s) := \max_{\pi^\text{any}} V^{\pi^\text{any}}_h(s).
\tag{1.7}\]</span></span></p>
<p>We can define the optimal action-value function analogously:</p>
<p><span id="eq-optimal-action-value"><span class="math display">\[
\forall s \in \mathcal{S}, a \in \mathcal{A}, \quad Q^\star_h(s, a) := \max_{\pi^\text{any}} Q^{\pi^\text{any}}_h(s, a).
\tag{1.8}\]</span></span></p>
</div>
<p>We can construct an optimal policy by acting <em>greedily</em> with respect to the optimal action-value function:</p>
<div id="def-greedy-policy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.10 (Greedy policies)</strong></span> For any sequence of functions <span class="math inline">\(q_h : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> for <span class="math inline">\(h = 0, \dots, H-1\)</span>, we define the <strong>greedy policy</strong> <span class="math inline">\(\hat \pi^q\)</span> to be the deterministic policy that selects the action with the highest value according to <span class="math inline">\(q\)</span> at each state:</p>
<p><span id="eq-def-greedy"><span class="math display">\[
\hat \pi_h^q(s) := \arg\max_{a \in \mathcal{A}} q_h(s, a).
\tag{1.9}\]</span></span></p>
</div>
<div id="39ad0d18" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_greedy(q: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the (deterministic) greedy policy with respect to an action-value function.</span></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> q.shape[<span class="dv">1</span>]</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>    a_ary <span class="op">=</span> jnp.argmax(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.eye(A)[a_ary]</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_greedy(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the (deterministic) greedy policy with respect to a value function."""</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_greedy(v_to_q(mdp, v))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="thm-optimal-greedy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (A deterministic optimal policy)</strong></span> The greedy policy <span class="math inline">\(\hat \pi_h^{Q^\star}(s)\)</span>, where <span class="math inline">\(Q^\star\)</span> is the optimal action-value function (<a href="#eq-optimal-action-value" class="quarto-xref">Equation&nbsp;<span>1.8</span></a>), is an optimal policy (<a href="#def-optimal-policy-finite" class="quarto-xref">Definition&nbsp;<span>1.8</span></a>).</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(V^{\star}\)</span> and <span class="math inline">\(Q^{\star}\)</span> denote the optimal value and action-value functions (<a href="#def-optimal-value" class="quarto-xref">Definition&nbsp;<span>1.9</span></a>). We aim to show that the greedy policy <span class="math inline">\(\hat \pi^{Q^\star}\)</span> is optimal; that is, <span class="math inline">\(V^{\hat \pi^{Q^\star}} = V^{\star}\)</span>. For notational convenience, we abbreviate <span class="math inline">\(\hat \pi^{Q^\star}\)</span> by just <span class="math inline">\(\hat \pi\)</span>.</p>
<p>Fix an arbitrary state <span class="math inline">\(s \in \mathcal{S}\)</span> and time <span class="math inline">\(h\in [H]\)</span>.</p>
<p>By the definition of <span class="math inline">\(V^{\star}\)</span>, we already know <span class="math inline">\(V_h^{\star}(s) \ge V_h^{\hat \pi}(s)\)</span>. So for equality to hold we just need to show that <span class="math inline">\(V_h^{\star}(s) \le V_h^{\hat \pi}(s)\)</span>.</p>
<p>We’ll first show that the Bellman operator <span class="math inline">\(\mathcal{J}^{\hat \pi_h}\)</span> never decreases <span class="math inline">\(V_h^{\star}\)</span> elementwise. Then we’ll apply this result recursively to show that <span class="math inline">\(V^{\star} = V^{\hat \pi}\)</span>.</p>
<div id="lem-bellman-increases" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 1.1 (The Bellman operator never decreases the optimal value function)</strong></span> <span class="math inline">\(\mathcal{J}^{\hat \pi_h}\)</span> never decreases <span class="math inline">\(V^{\star}\)</span> elementwise:</p>
<p><span id="eq-bellman-increases"><span class="math display">\[
\forall h \in [H], \quad [\mathcal{J}^{\hat \pi_h} (V_{h+1}^{\star})](s) \ge V_h^{\star}(s).
\tag{1.10}\]</span></span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We perform some algebraic manipulations:</p>
<p><span class="math display">\[
\begin{aligned}
    V_h^{\star}(s) &amp;= \max_{\pi \in \Pi} V_h^{\pi}(s) \\
    &amp;= \max_{\pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{h+1}^\pi(s') \right] &amp;&amp; \text{Bellman consistency} \\
    &amp;\le \max_{\pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{h+1}^{\star}(s') \right] &amp;&amp; \text{definition of } V^\star \\
    &amp;= \max_{a \in \mathcal{A}} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{h+1}^{\star}(s') \right] &amp;&amp; \text{only depends on } \pi \text{ via } a \\
    &amp;= [\mathcal{J}^{\hat \pi_h}(V_{h+1}^{\star})](s).
\end{aligned}
\]</span></p>
</div>
<p>We can now apply this result recursively to get</p>
<p><span class="math display">\[
V^{\star}_h(s) \le V^{\hat \pi}_h(s)
\]</span></p>
<p>as follows. (Note that even though <span class="math inline">\(\hat \pi\)</span> is deterministic, we’ll use the <span class="math inline">\(a \sim \hat \pi(s)\)</span> notation to make it explicit that we’re sampling a trajectory from it.)</p>
<p><span class="math display">\[
\begin{aligned}
    V_{t}^{\star}(s) &amp;\le [\mathcal{J}^{\hat \pi}(V_{h+1}^{\star})](s) \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue} V_{h+1}^{\star}(s')} \right] \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue}[ \mathcal{J}^{\hat \pi} (V_{h+2}^{\star})] (s')} \right] \right] &amp;&amp; \text{above lemma} \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)}{\color{blue} \left[ \mathop{\mathbb{E}}_{a' \sim \hat \pi}  r(s', a') + \mathop{\mathbb{E}}_{s''} V_{h+2}^{\star}(s'') \right]} \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \cdots &amp;&amp; \text{apply at all timesteps} \\
    &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\hat \pi}} \left[\sum_{h'=h}^{H-1} r(s_{h'}, a_{h'}) \mid s_h= s\right] &amp;&amp; \text{rewrite expectation} \\
    &amp;= V_{h}^{\hat \pi}(s) &amp;&amp; \text{definition}
\end{aligned}
\]</span></p>
<p>And so we have <span class="math inline">\(V^{\star} = V^{\hat \pi}\)</span>, making <span class="math inline">\(\hat \pi\)</span> optimal.</p>
</div>
<p><span class="math inline">\(\hat \pi^{Q^\star}\)</span> is a policy just like any other. In particular, we can write down the corresponding set of Bellman consistency equations (<a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a>). These are often termed the “Bellman optimality equations”.</p>
<div id="thm-bellman-consistency-optimal" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.3 (Bellman optimality equations)</strong></span> The optimal value function (<a href="#def-optimal-value" class="quarto-xref">Definition&nbsp;<span>1.9</span></a>) satisfies</p>
<p><span id="eq-bellman-consistency-optimal"><span class="math display">\[
\begin{aligned}
    \forall s \in \mathcal{S}, \quad V_h^\star(s) &amp;= \max_a r(s, a) + \mathbb{E}_{s' \sim P(s, a)} [V_{h+1}^\star(s')] \\
\end{aligned}
\tag{1.11}\]</span></span></p>
<p>Furthermore, this uniquely identifies <span class="math inline">\(V^\star\)</span>: it is the <em>only</em> function that satisfies this equation. That is, if a sequence of functions <span class="math inline">\(v_h : \mathcal{S} \to \mathbb{R}\)</span> for <span class="math inline">\(h = 0, \dots, H-1\)</span> satisfies <a href="#eq-bellman-consistency-optimal" class="quarto-xref">Equation&nbsp;<span>1.11</span></a>, then we know <span class="math inline">\(v = V^\star\)</span>. We leave the proof as an exercise; a proof by contradiction is relatively straightforward.</p>
<p>Note that due to the structure of the greedy policy, these equations don’t need the letter <span class="math inline">\(\pi\)</span> at all! This will prove useful when we discuss <strong>off-policy algorithms</strong> later in the course.</p>
</div>
<p>How can we compute <span class="math inline">\(\hat \pi^{Q^\star}\)</span>? We need to compute the optimal value function and optimal policy. We can do this by working backwards in time using <strong>dynamic programming</strong> (DP).</p>
<div class="cell" data-layout-align="default">
<div class="cell-output-display">
<div id="fig-dp-optimal-finite" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-dp-optimal-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div>
<pre class="mermaid mermaid-js" data-label="fig-dp-optimal-finite">graph RL
    Q0["$$Q^\star_{H-1} = r$$"] -- "$$\max_{a \in \mathcal{A}}$$" --&gt; V0
    Q0 -- "$$\arg\max_{a \in \mathcal{A}}$$" --&gt; P0["$$\pi^\star_{H-1}$$"]
    V0["$$V^\star_{H-1}$$"] -- "Bellman equations" --&gt; Q1
    
    Q1["$$Q^\star_{H-2}$$"] -- "$$\max_{a \in \mathcal{A}}$$" --&gt; V1
    Q1 -- "$$\arg\max_{a \in \mathcal{A}}$$" --&gt; P1["$$\pi^\star_{H-2}$$"]
    V1["$$V^\star_{H-2}$$"] -- "Bellman equations" --&gt; Q2

    Q2["..."]
</pre>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-dp-optimal-finite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.9: Illustrating a dynamic programming algorithm for computing the optimal policy in a finite-horizon MDP.
</figcaption>
</figure>
</div>
</div>
</div>
<div id="def-pi-star-dp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.11 (DP algorithm to compute an optimal policy in a finite-horizon MDP)</strong></span> <strong>Base case.</strong> At the end of the episode (time step <span class="math inline">\(H-1\)</span>), we can’t take any more actions, so the <span class="math inline">\(Q\)</span>-function is simply the reward that we obtain:</p>
<p><span class="math display">\[
Q^\star_{H-1}(s, a) = r(s, a).
\]</span></p>
<p>The best thing to do at the end of the horizon, therefore, is to act greedily and get as much reward as we can!</p>
<p><span class="math display">\[
\pi^\star_{H-1}(s) = \arg\max_{a \in \mathcal{A}} Q^\star_{H-1}(s, a)
\]</span></p>
<p>Then <span class="math inline">\(V^\star_{H-1}(s)\)</span>, the optimal value of state <span class="math inline">\(s\)</span> at the end of the trajectory, is simply whatever action gives the most reward.</p>
<p><span class="math display">\[
V^\star_{H-1}(s) = \max_{a \in \mathcal{A}} Q^\star_{H-1}(s, a)
\]</span></p>
<p><strong>Recursion.</strong> Then, we can work backwards in time, starting from the end, using the Bellman optimality equations (<a href="#thm-bellman-consistency-optimal" class="quarto-xref">Theorem&nbsp;<span>1.3</span></a>)! i.e.&nbsp;for each <span class="math inline">\(h= H-2, \dots, 0\)</span>, we set</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\star_{h}(s, a) &amp;= r(s, a) + \mathbb{E}_{s' \sim P(s, a)} [V^\star_{h+1}(s')] \\
    \pi^\star_{h}(s) &amp;= \arg\max_{a \in \mathcal{A}} Q^\star_{h}(s, a) \\
    V^\star_{h}(s) &amp;= \max_{a \in \mathcal{A}} Q^\star_{h}(s, a)
\end{aligned}
\]</span></p>
</div>
<div id="cell-fig-optimal-policy" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># </span><span class="al">TODO</span><span class="co"> fix latexify bugs</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_policy(mdp: MDP):</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> np.zeros((mdp.H, mdp.S, mdp.A))</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> np.zeros((mdp.H, mdp.S, mdp.A))</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> np.zeros((mdp.H<span class="op">+</span><span class="dv">1</span>, mdp.S))  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>        Q[h] <span class="op">=</span> mdp.r <span class="op">+</span> mdp.P <span class="op">@</span> V[h <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>        policy[h] <span class="op">=</span> jnp.eye(mdp.S)[jnp.argmax(Q[h], axis<span class="op">=</span><span class="dv">1</span>)]  <span class="co"># one-hot</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>        V[h] <span class="op">=</span> jnp.<span class="bu">max</span>(Q[h], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> jnp.stack(Q)</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    policy <span class="op">=</span> jnp.stack(policy)</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> jnp.stack(V[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> policy, V, Q</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-18"><a href="#cb13-18" aria-hidden="true" tabindex="-1"></a>latex(find_optimal_policy, id_to_latex<span class="op">=</span>{<span class="st">"policy"</span>: <span class="vs">r"\pi"</span>}, trim_prefixes<span class="op">=</span>{<span class="st">"mdp"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="fig-optimal-policy" class="cell-output cell-output-display quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="13">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
\begin{array}{l} \mathbf{function} \ \mathrm{find\_optimal\_policy}(\mathrm{mdp}: \mathrm{MDP}) \\ \hspace{1em} Q \gets \mathbf{0}^{H \times S \times A} \\ \hspace{1em} \pi \gets \mathbf{0}^{H \times S \times A} \\ \hspace{1em} V \gets \mathbf{0}^{H + 1 \times S} \\ \hspace{1em} \mathbf{for} \ h \in \mathrm{range} \mathopen{}\left( H - 1, -1, -1 \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} Q_{h} \gets r + P V_{h + 1} \\ \hspace{2em} \pi_{h} \gets \mathrm{jnp}.\mathrm{eye} \mathopen{}\left( S \mathclose{}\right)_{\mathrm{jnp}.\mathrm{argmax} \mathopen{}\left( Q_{h} \mathclose{}\right)} \\ \hspace{2em} V_{h} \gets \mathrm{jnp}.\mathrm{max} \mathopen{}\left( Q_{h} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} Q \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( Q \mathclose{}\right) \\ \hspace{1em} \pi \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( \pi \mathclose{}\right) \\ \hspace{1em} V \gets \mathrm{jnp}.\mathrm{stack} \mathopen{}\left( V_{:-1} \mathclose{}\right) \\ \hspace{1em} \mathbf{return} \ \mathopen{}\left( \pi, V, Q \mathclose{}\right) \\ \mathbf{end \ function} \end{array}
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-optimal-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1.10: Pseudocode for the dynamic programming algorithm.
</figcaption>
</figure>
</div>
</div>
<p>At each of the <span class="math inline">\(H\)</span> timesteps, we must compute <span class="math inline">\(Q^{\star}_h\)</span> for each of the <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> state-action pairs. Each computation takes <span class="math inline">\(|\mathcal{S}|\)</span> operations to evaluate the average value over <span class="math inline">\(s'\)</span>. This gives a total computation time of <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>.</p>
<p>Note that this algorithm is identical to the policy evaluation algorithm (<a href="#fig-dp-eval-finite" class="quarto-xref">Figure&nbsp;<span>1.8</span></a>), but instead of <em>averaging</em> over the actions chosen by a policy, we instead take a <em>maximum</em> over the action-values. We’ll see this relationship between <strong>policy evaluation</strong> and <strong>optimal policy computation</strong> show up again in the infinite-horizon setting.</p>
<div id="71383225" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>pi_opt, V_opt, Q_opt <span class="op">=</span> find_optimal_policy(tidy_mdp)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(pi_opt, tidy_policy_messy_only)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(V_opt, V_messy)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(Q_opt[:<span class="op">-</span><span class="dv">1</span>], v_ary_to_q_ary(tidy_mdp, V_messy)[<span class="dv">1</span>:])</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">"Assertions passed (the 'tidy when messy' policy is optimal)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>"Assertions passed (the 'tidy when messy' policy is optimal)"</code></pre>
</div>
</div>
<div id="rem-why-stochastic" class="proof remark">
<p><span class="proof-title"><em>Remark 1.3</em> (Why stochastic policies). </span>Given that there exists an optimal deterministic policy, why do we try ever try to learn stochastic policies? We will see a partial answer to this when we discuss the <em>exploration-exploitation</em> tradeoff in <a href="bandits.html" class="quarto-xref"><span>Chapter 3</span></a>. So far, we’ve assumed that the environment is totally known. If it isn’t, however, we need some way to explore different possible actions, and stochastic policies provide a natural way to do this.</p>
</div>
</section>
</section>
<section id="sec-infinite-horizon-mdps" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-infinite-horizon-mdps"><span class="header-section-number">1.3</span> Infinite-horizon MDPs</h2>
<p>What happens if a trajectory is allowed to continue forever (i.e.&nbsp;<span class="math inline">\(H = \infty\)</span>)? This is the setting of <strong>infinite horizon</strong> MDPs.</p>
<p>In this chapter, we’ll describe the necessary adjustments from the finite-horizon case. We’ll show that the Bellman operator (<a href="#def-bellman-operator" class="quarto-xref">Definition&nbsp;<span>1.7</span></a>) in the discounted reward setting is a <strong>contraction mapping</strong> for any policy. We’ll discuss how to evaluate policies (i.e.&nbsp;compute their corresponding value functions). Finally, we’ll present and analyze two iterative algorithms, based on the Bellman operator, for computing the optimal policy: <strong>value iteration</strong> and <strong>policy iteration</strong>.</p>
<section id="discounted-rewards" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="discounted-rewards"><span class="header-section-number">1.3.1</span> Discounted rewards</h3>
<p>First of all, note that the total reward <span class="math inline">\(r_h+ r_{h+1} + r_{h+2} + \cdots\)</span> might blow up to infinity. To ensure that the “total reward” can remain bounded, we insert a <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1)\)</span> such that rewards become less valuable the further into the future they are:</p>
<p><span id="eq-discounted-rewards"><span class="math display">\[
r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} + \cdots = \sum_{k=0}^\infty \gamma^k r_{h+k}.
\tag{1.12}\]</span></span></p>
<p>We can think of <span class="math inline">\(\gamma\)</span> as measuring how much we care about the future: if it’s close to <span class="math inline">\(0\)</span>, we only care about the near-term rewards; if it’s close to <span class="math inline">\(1\)</span>, we put more weight into future rewards.</p>
<p>You can also analyze <span class="math inline">\(\gamma\)</span> as the probability of <em>continuing</em> the trajectory at each time step. (This is equivalent to <span class="math inline">\(H\)</span> being distributed by a First Success distribution with success probability <span class="math inline">\(\gamma\)</span>.) This accords with the above interpretation: if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(0\)</span>, the trajectory will likely be very short, while if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(1\)</span>, the trajectory will likely continue for a long time.</p>
<div id="exr-max-discounted-rewards" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.7</strong></span> Assuming that <span class="math inline">\(r_h\in [0, 1]\)</span> for all <span class="math inline">\(h\in \mathbb{N}\)</span>, what is the maximum <strong>discounted</strong> cumulative reward? You may find it useful to review geometric series.</p>
</div>
<p>The other components of the MDP remain the same:</p>
<p><span class="math display">\[
\mathcal{M} = (\mathcal{S}, \mathcal{A}, \mu, P, r, \gamma).
\]</span></p>
<div id="076152a9" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Code-wise, we can reuse the `MDP` class from before @def-finite-horizon-mdp and set `mdp.H = float('inf')`.</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>tidy_mdp_inf <span class="op">=</span> tidy_mdp._replace(H<span class="op">=</span><span class="bu">float</span>(<span class="st">"inf"</span>), gamma<span class="op">=</span><span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-stationary-policies" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="sec-stationary-policies"><span class="header-section-number">1.3.2</span> Stationary policies</h3>
<p>The time-dependent policies from the finite-horizon case become difficult to handle in the infinite-horizon case. In particular, many of the DP approaches we saw required us to start at the end of the trajectory, which is no longer possible. We’ll shift to <strong>stationary</strong> policies <span class="math inline">\(\pi : \mathcal{S} \to \mathcal{A}\)</span> (deterministic) or <span class="math inline">\(\Delta(\mathcal{A})\)</span> (stochastic).</p>
<div id="exr-tidy-stationary" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.8</strong></span> Which of the policies in <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.4</span></a> (policies in the tidying MDP) are stationary?</p>
</div>
</section>
<section id="value-functions-and-bellman-consistency" class="level3" data-number="1.3.3">
<h3 data-number="1.3.3" class="anchored" data-anchor-id="value-functions-and-bellman-consistency"><span class="header-section-number">1.3.3</span> Value functions and Bellman consistency</h3>
<p>We also consider stationary value functions <span class="math inline">\(V^\pi : \mathcal{S} \to \mathbb{R}\)</span> and <span class="math inline">\(Q^\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>. We need to insert a factor of <span class="math inline">\(\gamma\)</span> into the Bellman consistency equations (<a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a>) to account for the discounting:</p>
<p><span id="eq-bellman-consistency-infinite"><span class="math display">\[
\begin{aligned}
    V^\pi(s) &amp;= \mathbb{E}_{\tau \sim \rho^\pi} [r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} \cdots \mid s_h= s] &amp;&amp; \text{for any } h\in \mathbb{N} \\
    &amp;= \mathbb{E}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma V^\pi(s')]\\
    Q^\pi(s, a) &amp;= \mathbb{E}_{\tau \sim \rho^\pi} [r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} + \cdots \mid s_h= s, a_h= a] &amp;&amp; \text{for any } h\in \mathbb{N} \\
    &amp;= r(s, a) + \gamma \mathbb{E}_{\substack{s' \sim P(s, a) \\ a' \sim \pi(s')}} [Q^\pi(s', a')]
\end{aligned}
\tag{1.13}\]</span></span></p>
<div id="exr-infinite-no-time-step" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.9 (Time-independent)</strong></span> Heuristically speaking, why does it no longer matter which time step we condition on when defining the value function?</p>
</div>
</section>
<section id="contraction-mappings" class="level3" data-number="1.3.4">
<h3 data-number="1.3.4" class="anchored" data-anchor-id="contraction-mappings"><span class="header-section-number">1.3.4</span> Contraction mappings</h3>
<p>Recall from <a href="#def-bellman-operator" class="quarto-xref">Definition&nbsp;<span>1.7</span></a> that the Bellman operator <span class="math inline">\(\mathcal{J}^{\pi}\)</span> for a policy <span class="math inline">\(\pi\)</span> takes in a “value function” <span class="math inline">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman equation for that “value function”. In the infinite-horizon setting, this is</p>
<p><span class="math display">\[
[\mathcal{J}^{\pi}(v)](s) := \mathbb{E}_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma v(s')].
\]</span></p>
<p>The crucial property of the Bellman operator is that it is a <strong>contraction mapping</strong> for any policy. Intuitively, if we start with two “value functions” <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>, if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate.</p>
<div id="def-contraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.12 (Contraction mapping)</strong></span> Let <span class="math inline">\(X\)</span> be some space with a norm <span class="math inline">\(\|\cdot\|\)</span>. We call an operator <span class="math inline">\(f: X \to X\)</span> a <strong>contraction mapping</strong> if for any <span class="math inline">\(x, y \in X\)</span>,</p>
<p><span class="math display">\[
\|f(x) - f(y)\| \le \gamma \|x - y\|
\]</span></p>
<p>for some fixed <span class="math inline">\(\gamma \in (0, 1)\)</span>. Intuitively, this means that if two points are <span class="math inline">\(\delta\)</span> far apart, after applying the mapping,</p>
</div>
<div id="exr-contraction-shrinks" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.10</strong></span> Show that for a contraction mapping <span class="math inline">\(f\)</span> with coefficient <span class="math inline">\(\gamma\)</span>, for all <span class="math inline">\(t \in \mathbb{N}\)</span>,</p>
<p><span class="math display">\[
\|f^{(t)}(x) - f^{(t)}(y)\| \le \gamma^t \|x - y\|,
\]</span></p>
<p>i.e.&nbsp;that any two points will be pushed closer by at least a factor of <span class="math inline">\(\gamma\)</span> at each iteration.</p>
</div>
<p>It is a powerful fact (known as the <strong>Banach fixed-point theorem</strong>) that every contraction mapping has a unique <strong>fixed point</strong> <span class="math inline">\(x^\star\)</span> such that <span class="math inline">\(f(x^\star) = x^\star\)</span>. This means that if we repeatedly apply <span class="math inline">\(f\)</span> to any starting point, we will eventually converge to <span class="math inline">\(x^\star\)</span>:</p>
<p><span id="eq-contraction-convergence"><span class="math display">\[
\|f^{(t)}(x) - x^\star\| \le \gamma^t \|x - x^\star\|.
\tag{1.14}\]</span></span></p>
<p>Let’s return to the RL setting and apply this result to the Bellman operator. How can we measure the distance between two “value functions” <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>? We’ll take the <strong>supremum norm</strong> as our distance metric:</p>
<p><span class="math display">\[
\| v - u \|_{\infty} := \sup_{s \in \mathcal{S}} |v(s) - u(s)|,
\]</span></p>
<p>i.e. we compare the “value functions” on the state that causes the biggest gap between them. Then <a href="#eq-contraction-convergence" class="quarto-xref">Equation&nbsp;<span>1.14</span></a> implies that if we repeatedly apply <span class="math inline">\(\mathcal{J}^\pi\)</span> to any starting “value function”, we will eventually converge to <span class="math inline">\(V^\pi\)</span>:</p>
<p><span id="eq-bellman-convergence"><span class="math display">\[
\|(\mathcal{J}^\pi)^{(t)}(v) - V^\pi \|_{\infty} \le \gamma^{t} \| v - V^\pi\|_{\infty}.
\tag{1.15}\]</span></span></p>
<p>We’ll use this useful fact to prove the convergence of several algorithms later on.</p>
<div id="thm-bellman-contraction" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.4 (The Bellman operator is a contraction mapping)</strong></span> <span class="math display">\[
\|\mathcal{J}^{\pi} (v) - \mathcal{J}^{\pi} (u) \|_{\infty} \le \gamma \|v - u \|_{\infty}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For all states <span class="math inline">\(s \in \mathcal{S}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
|[\mathcal{J}^{\pi} (v)](s) - [\mathcal{J}^{\pi} (u)](s)|&amp;= \Big| \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right] \\
&amp;\qquad - \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} u(s') \right] \Big| \\
&amp;= \gamma \left|\mathop{\mathbb{E}}_{s' \sim P(s, a)} [v(s') - u(s')] \right| \\
&amp;\le \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)}|v(s') - u(s')| \qquad \text{(Jensen's inequality)} \\
&amp;\le \gamma \max_{s'} |v(s') - u(s')| \\
&amp;= \gamma \|v - u \|_{\infty}.
\end{aligned}
\]</span></p>
</div>
</section>
<section id="policy-evaluation-in-infinite-horizon-mdps" class="level3" data-number="1.3.5">
<h3 data-number="1.3.5" class="anchored" data-anchor-id="policy-evaluation-in-infinite-horizon-mdps"><span class="header-section-number">1.3.5</span> Policy evaluation in infinite-horizon MDPs</h3>
<p>The backwards DP technique we used in the finite-horizon case (<a href="#sec-eval-dp" class="quarto-xref"><span>Section 1.2.5</span></a>) no longer works since there is no “final timestep” to start from. We’ll need another approach to policy evaluation.</p>
<p>The Bellman consistency conditions yield a system of equations we can solve to evaluate a deterministic policy <em>exactly</em>. For a faster approximate solution, we can iterate the policy’s Bellman operator, since we know that it has a unique fixed point at the true value function.</p>
</section>
<section id="matrix-inversion-for-deterministic-policies" class="level3" data-number="1.3.6">
<h3 data-number="1.3.6" class="anchored" data-anchor-id="matrix-inversion-for-deterministic-policies"><span class="header-section-number">1.3.6</span> Matrix inversion for deterministic policies</h3>
<p>Note that when the policy <span class="math inline">\(\pi\)</span> is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:</p>
<p><span class="math display">\[
\begin{aligned}
    r^{\pi} &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; P^{\pi} &amp;\in [0, 1]^{|\mathcal{S}| \times |\mathcal{S}|} &amp; \mu &amp;\in [0, 1]^{|\mathcal{S}|} \\
    \pi &amp;\in \mathcal{A}^{|\mathcal{S}|} &amp; V^\pi &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; Q^\pi &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}.
\end{aligned}
\]</span></p>
<p>For <span class="math inline">\(P^\pi\)</span>, we’ll treat the rows as the states and the columns as the next states. Then <span class="math inline">\(P^\pi_{s, s'}\)</span> is the probability of transitioning from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span> under policy <span class="math inline">\(\pi\)</span>.</p>
<div id="exm-tidy-tabular" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.8 (Tidying MDP)</strong></span> The tabular MDP from before has <span class="math inline">\(|\mathcal{S}| = 2\)</span> and <span class="math inline">\(|\mathcal{A}| = 2\)</span>. Let’s write down the quantities for the policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy:</p>
<p><span class="math display">\[
r^{\pi} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
        P^{\pi} = \begin{bmatrix} 0.7 &amp; 0.3 \\ 1 &amp; 0 \end{bmatrix}, \quad
        \mu = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\]</span></p>
<p>We’ll see how to evaluate this policy in the next section.</p>
</div>
<p>The Bellman consistency equation for a deterministic policy can be written in tabular notation as</p>
<p><span class="math display">\[
V^\pi = r^\pi + \gamma P^\pi V^\pi.
\]</span></p>
<p>(Unfortunately, this notation doesn’t simplify the expression for <span class="math inline">\(Q^\pi\)</span>.) This system of equations can be solved with a matrix inversion:</p>
<p><span id="eq-matrix-inversion-pe"><span class="math display">\[
V^\pi = (I - \gamma P^\pi)^{-1} r^\pi.
\tag{1.16}\]</span></span></p>
<div id="exr-invertible-dynamics" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.11</strong></span> Note we’ve assumed that <span class="math inline">\(I - \gamma P^\pi\)</span> is invertible. Can you see why this is the case?</p>
<p>(Recall that a linear operator, i.e.&nbsp;a square matrix, is invertible if and only if its null space is trivial; that is, it doesn’t map any nonzero vector to zero. In this case, we can see that <span class="math inline">\(I - \gamma P^\pi\)</span> is invertible because it maps any nonzero vector to a vector with at least one nonzero element.)</p>
</div>
<div id="fb164e40" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_deterministic_infinite(</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> jnp.argmax(policy, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># un-one-hot</span></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>    P_pi <span class="op">=</span> mdp.P[jnp.arange(mdp.S), pi]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>    r_pi <span class="op">=</span> mdp.r[jnp.arange(mdp.S), pi]</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.linalg.solve(jnp.eye(mdp.S) <span class="op">-</span> mdp.gamma <span class="op">*</span> P_pi, r_pi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exm-tidy-eval-infinite" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.9 (Tidying policy evaluation)</strong></span> Let’s use the same policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy. Setting <span class="math inline">\(\gamma = 0.95\)</span>, we must invert</p>
<p><span class="math display">\[
I - \gamma P^{\pi} = \begin{bmatrix} 1 - 0.95 \times 0.7 &amp; - 0.95 \times 0.3 \\ - 0.95 \times 1 &amp; 1 - 0.95 \times 0 \end{bmatrix} = \begin{bmatrix} 0.335 &amp; -0.285 \\ -0.95 &amp; 1 \end{bmatrix}.
\]</span></p>
<p>The inverse to two decimal points is</p>
<p><span class="math display">\[
(I - \gamma P^{\pi})^{-1} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix}.
\]</span></p>
<p>Thus the value function is</p>
<p><span class="math display">\[
V^{\pi} = (I - \gamma P^{\pi})^{-1} r^{\pi} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 15.56 \\ 14.79 \end{bmatrix}.
\]</span></p>
<p>Let’s sanity-check this result. Since rewards are at most <span class="math inline">\(1\)</span>, the maximum cumulative return of a trajectory is at most <span class="math inline">\(1/(1-\gamma) = 20\)</span>. We see that the value function is indeed slightly lower than this.</p>
</div>
<div id="82b5ea66" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>eval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>Array([15.56419, 14.78598], dtype=float32)</code></pre>
</div>
</div>
<section id="sec-iterative-pe" class="level4" data-number="1.3.6.1">
<h4 data-number="1.3.6.1" class="anchored" data-anchor-id="sec-iterative-pe"><span class="header-section-number">1.3.6.1</span> Iterative policy evaluation</h4>
<p>The matrix inversion above takes roughly <span class="math inline">\(O(|\mathcal{S}|^3)\)</span> time. It also only works for deterministic policies. Can we trade off the requirement of finding the <em>exact</em> value function for a faster <em>approximate</em> algorithm that will also extend to stochastic policies?</p>
<p>Let’s use the Bellman operator to define an iterative algorithm for computing the value function. We’ll start with an initial guess <span class="math inline">\(v^{(0)}\)</span> with elements in <span class="math inline">\([0, 1/(1-\gamma)]\)</span> and then iterate the Bellman operator:</p>
<p><span class="math display">\[
v^{(t+1)} = \mathcal{J}^{\pi}(v^{(t)}),
\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(v^{(t)} = (\mathcal{J}^{\pi})^{(t)} (v^{(0)})\)</span>. Note that each iteration takes <span class="math inline">\(O(|\mathcal{S}|^2)\)</span> time for the matrix-vector multiplication.</p>
<div id="46aafec9" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> supremum_norm(v):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(jnp.<span class="bu">abs</span>(v))  <span class="co"># same as jnp.linalg.norm(v, jnp.inf)</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loop_until_convergence(op, v, ε<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Repeatedly apply op to v until convergence (in supremum norm)."""</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>        v_new <span class="op">=</span> op(v)</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> supremum_norm(v_new <span class="op">-</span> v) <span class="op">&lt;</span> ε:</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> v_new</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v_new</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> iterative_evaluation(mdp: MDP, pi: Float[Array, <span class="st">"S A"</span>], ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_operator, mdp, pi)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Then, as we showed in <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.15</span></a>, by the Banach fixed-point theorem:</p>
<p><span class="math display">\[
\|v^{(t)} - V^\pi \|_{\infty} \le \gamma^{t} \| v^{(0)} - V^\pi\|_{\infty}.
\]</span></p>
<div id="5863551a" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>iterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<div id="rem-iterations-vi" class="proof remark">
<p><span class="proof-title"><em>Remark 1.4</em> (Convergence of iterative policy evaluation). </span>How many iterations do we need for an <span class="math inline">\(\epsilon\)</span>-accurate estimate? We can work backwards to solve for <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \gamma^t \|v^{(0)} - V^\pi\|_{\infty} &amp;\le \epsilon \\
    t &amp;\ge \frac{\log (\epsilon / \|v^{(0)} - V^\pi\|_{\infty})}{\log \gamma} \\
    &amp;= \frac{\log (\|v^{(0)} - V^\pi\|_{\infty} / \epsilon)}{\log (1 / \gamma)},
\end{aligned}
\]</span></p>
<p>and so the number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
<p>Note that we’ve applied the inequalities <span class="math inline">\(\|v^{(0)} - V^\pi\|_{\infty} \le 1/(1-\gamma)\)</span> and <span class="math inline">\(\log (1/x) \ge 1-x\)</span>.</p>
</div>
</section>
</section>
<section id="optimal-policies-in-infinite-horizon-mdps" class="level3" data-number="1.3.7">
<h3 data-number="1.3.7" class="anchored" data-anchor-id="optimal-policies-in-infinite-horizon-mdps"><span class="header-section-number">1.3.7</span> Optimal policies in infinite-horizon MDPs</h3>
<p>Now let’s move on to solving for an optimal policy in the infinite-horizon case. As in <a href="#def-optimal-policy-finite" class="quarto-xref">Definition&nbsp;<span>1.8</span></a>, an <strong>optimal policy</strong> <span class="math inline">\(\pi^\star\)</span> is one that does at least as well as any other policy in all situations. That is, for all policies <span class="math inline">\(\pi\)</span>, states <span class="math inline">\(s \in \mathcal{S}\)</span>, times <span class="math inline">\(h\in \mathbb{N}\)</span>, and initial trajectories <span class="math inline">\(\tau_h= (s_0, a_0, r_0, \dots, s_h)\)</span> where <span class="math inline">\(s_h= s\)</span>,</p>
<p><span id="eq-optimal-policy-infinite"><span class="math display">\[
\begin{aligned}
    V^{\pi^\star}(s) &amp;= \mathbb{E}_{\tau \sim \rho^{\pi^{\star}}}[r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2}  + \cdots \mid s_h= s] \\
    &amp;\ge \mathbb{E}_{\tau \sim \rho^{\pi}}[r_h+ \gamma r_{h+1} + \gamma^2 r_{h+2} + \cdots \mid \tau_h]
\end{aligned}
\tag{1.17}\]</span></span></p>
<p>Once again, all optimal policies share the same <strong>optimal value function</strong> <span class="math inline">\(V^\star\)</span>, and the greedy policy with respect to this value function is optimal.</p>
<div id="exr-optimal-greedy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.12 (The greedy optimal policy)</strong></span> Verify this by modifying the proof <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>1.2</span></a> from the finite-horizon case.</p>
</div>
<p>So how can we compute such an optimal policy? We can’t use the backwards DP approach from the finite-horizon case <a href="#def-pi-star-dp" class="quarto-xref">Definition&nbsp;<span>1.11</span></a> since there’s no “final timestep” to start from. Instead, we’ll exploit the fact that the Bellman consistency equation <a href="#eq-bellman-consistency-infinite" class="quarto-xref">Equation&nbsp;<span>1.13</span></a> for the optimal value function doesn’t depend on any policy:</p>
<p><span id="eq-bellman-optimality"><span class="math display">\[
V^\star(s) = \max_a \left[ r(s, a) + \gamma \mathbb{E}_{s' \sim P(s, a)} V^\star(s'). \right]
\tag{1.18}\]</span></span></p>
<div id="exr-infinite-optimal" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 1.13</strong></span> Verify this by substituting the greedy policy into the Bellman consistency equation.</p>
</div>
<p>As before, thinking of the r.h.s. of <a href="#eq-bellman-optimality" class="quarto-xref">Equation&nbsp;<span>1.18</span></a> as an operator on value functions gives the <strong>Bellman optimality operator</strong></p>
<p><span id="eq-bellman-optimality-operator"><span class="math display">\[
[\mathcal{J}^{\star}(v)](s) = \max_a \left[ r(s, a) + \gamma \mathbb{E}_{s' \sim P(s, a)} v(s') \right]
\tag{1.19}\]</span></span></p>
<div id="8f5b9db1" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_optimality_operator(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(mdp.r <span class="op">+</span> mdp.gamma <span class="op">*</span> mdp.P <span class="op">@</span> v, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_optimal(v: Float[Array, <span class="st">" S"</span>], mdp: MDP):</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.allclose(v, bellman_optimality_operator(v, mdp))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="sec-value-iteration" class="level4" data-number="1.3.7.1">
<h4 data-number="1.3.7.1" class="anchored" data-anchor-id="sec-value-iteration"><span class="header-section-number">1.3.7.1</span> Value iteration</h4>
<p>Since the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as <strong>value iteration</strong>.</p>
<div id="1e32c844" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> value_iteration(mdp: MDP, ε: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iterate the Bellman optimality operator until convergence."""</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_optimality_operator, mdp)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="a5ae4fd7" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>value_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<p>Note that the runtime analysis for an <span class="math inline">\(\epsilon\)</span>-optimal value function is exactly the same as <a href="#sec-iterative-pe" class="quarto-xref"><span>Section 1.3.6.1</span></a>! This is because value iteration is simply the special case of applying iterative policy evaluation to the <em>optimal</em> value function.</p>
<p>As the final step of the algorithm, to return an actual policy <span class="math inline">\(\hat \pi\)</span>, we can simply act greedily with respect to the final iteration <span class="math inline">\(v^{(T)}\)</span> of our above algorithm:</p>
<p><span class="math display">\[
\hat \pi(s) = \arg\max_a \left[ r(s, a) + \gamma \mathbb{E}_{s' \sim P(s, a)} v^{(T)}(s') \right].
\]</span></p>
<p>We must be careful, though: the value function of this greedy policy, <span class="math inline">\(V^{\hat \pi}\)</span>, is <em>not</em> the same as <span class="math inline">\(v^{(T)}\)</span>, which need not even be a well-defined value function for some policy!</p>
<p>The bound on the policy’s quality is actually quite loose: if <span class="math inline">\(\|v^{(T)} - V^\star\|_{\infty} \le \epsilon\)</span>, then the greedy policy <span class="math inline">\(\hat \pi\)</span> satisfies <span class="math inline">\(\|V^{\hat \pi} - V^\star\|_{\infty} \le \frac{2\gamma}{1-\gamma} \epsilon\)</span>, which might potentially be very large.</p>
<div id="thm-greedy-worsen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.5 (Greedy policy value worsening)</strong></span> <span class="math display">\[
\|V^{\hat \pi} - V^\star \|_{\infty} \le \frac{2 \gamma}{1-\gamma} \|v - V^\star\|_{\infty}
\]</span></p>
<p>where <span class="math inline">\(\hat \pi(s) = \arg\max_a q(s, a)\)</span> is the greedy policy with respect to</p>
<p><span class="math display">\[
q(s, a) = r(s, a) + \mathbb{E}_{s' \sim P(s, a)} v(s').
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first have</p>
<p><span class="math display">\[
\begin{aligned}
        V^{\star}(s) - V^{\hat \pi}(s) &amp;= Q^{\star}(s,\pi^\star(s)) - Q^{\hat \pi}(s, \hat \pi(s))\\
        &amp;= [Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s))] + [Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))].
\end{aligned}
\]</span></p>
<p>Let us bound these two quantities separately.</p>
<p>For the first quantity, note that by the definition of <span class="math inline">\(\hat \pi\)</span>, we have</p>
<p><span class="math display">\[
q(s, \hat \pi(s)) \ge q(s,\pi^\star(s)).
\]</span></p>
<p>Let’s add <span class="math inline">\(q(s, \hat \pi(s)) - q(s,\pi^\star(s)) \ge 0\)</span> to the first term to get</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s)) &amp;\le [Q^{\star}(s,\pi^\star(s))- q(s,\pi^\star(s))] + [q(s, \hat \pi(s)) - Q^{\star}(s, \hat \pi(s))] \\
        &amp;= \gamma \mathbb{E}_{s' \sim P(s, \pi^{\star}(s))} [ V^{\star}(s') - v(s') ] + \gamma \mathbb{E}_{s' \sim P(s, \hat \pi(s))} [ v(s') - V^{\star}(s') ] \\
        &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty}.
\end{aligned}
\]</span></p>
<p>The second quantity is bounded by</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))
        &amp;=
        \gamma \mathbb{E}_{s'\sim P(s, \hat \pi(s))}\left[ V^\star(s') - V^{\hat \pi}(s') \right] \\
        &amp; \leq
        \gamma \|V^{\star} - V^{\hat \pi}\|_\infty
\end{aligned}
\]</span></p>
<p>and thus</p>
<p><span class="math display">\[
\begin{aligned}
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty} + \gamma \|V^{\star} - V^{\hat \pi}\|_\infty \\
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le \frac{2 \gamma \|v - V^{\star}\|_{\infty}}{1-\gamma}.
\end{aligned}
\]</span></p>
</div>
<p>So in order to compensate and achieve <span class="math inline">\(\|V^{\hat \pi} - V^{\star}\| \le \epsilon\)</span>, we must have</p>
<p><span class="math display">\[
\|v^{(T)} - V^\star\|_{\infty} \le \frac{1-\gamma}{2 \gamma} \epsilon.
\]</span></p>
<p>This means, using <a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>1.4</span></a>, we need to run value iteration for</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{\gamma}{\epsilon (1-\gamma)^2}\right) \right)
\]</span></p>
<p>iterations to achieve an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function.</p>
</section>
<section id="sec-policy-iteration" class="level4" data-number="1.3.7.2">
<h4 data-number="1.3.7.2" class="anchored" data-anchor-id="sec-policy-iteration"><span class="header-section-number">1.3.7.2</span> Policy iteration</h4>
<p>Can we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function <em>together</em>? This is the idea behind <strong>policy iteration</strong>. In each step, we simply set the policy to act greedily with respect to its own value function.</p>
<div id="6a0d0b9d" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_iteration(mdp: MDP, ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iteratively improve the policy and value function."""</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> op(pi):</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    pi_init <span class="op">=</span> jnp.ones((mdp.S, mdp.A)) <span class="op">/</span> mdp.A  <span class="co"># uniform random policy</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, pi_init, ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="ebe3221a" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>policy_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>Array([[1., 0.],
       [0., 1.]], dtype=float32)</code></pre>
</div>
</div>
<p>Although PI appears more complex than VI, we’ll use <a href="#thm-bellman-contraction" class="quarto-xref">Theorem&nbsp;<span>1.4</span></a> to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an <span class="math inline">\(\epsilon\)</span>-optimal value function (<a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>1.4</span></a>), although in practice, PI often converges in fewer iterations. Why so? Intuitively, by iterating through actual policies, we can “skip over” different functions that represent the same policy, which value iteration might iterate through. For a concrete example, suppose we have an MDP with three states <span class="math inline">\(\mathcal{S} = \{\text A, \text B, \text C\}\)</span>. Compare the two functions below:</p>
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>display(Markdown(tabulate(</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>: [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>],</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 0$"</span>: [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>],</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 1$"</span>: [<span class="fl">0.2</span>, <span class="fl">0.9</span>, <span class="fl">0.5</span>],</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"keys"</span></span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>)))</span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>display(Markdown(tabulate(</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>    {</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$s$"</span>: [<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>],</span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 0$"</span>: [<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>],</span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>        <span class="vs">r"$a = 1$"</span>: [<span class="fl">0.3</span>, <span class="fl">1.0</span>, <span class="fl">0.6</span>],</span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a>    },</span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>    <span class="st">"keys"</span></span>
<span id="cb30-17"><a href="#cb30-17" aria-hidden="true" tabindex="-1"></a>)))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div id="tbl-multiple-q-same-policy" class="quarto-layout-panel anchored">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-multiple-q-same-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;1.2: Two action-value functions that result in the same greedy policy
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display cell-output-markdown quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-multiple-q-same-policy" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-multiple-q-same-policy-1" class="do-not-create-environment quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-multiple-q-same-policy-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) One Q-function
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="tbl-multiple-q-same-policy-1" class="do-not-create-environment caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.2</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">0.9</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.5</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
<div class="cell-output cell-output-display cell-output-markdown quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="tbl-multiple-q-same-policy" style="flex-basis: 50.0%;justify-content: center;">
<div id="tbl-multiple-q-same-policy-2" class="do-not-create-environment quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-tbl figure">
<figcaption class="quarto-float-caption-top quarto-subfloat-caption quarto-subfloat-tbl" id="tbl-multiple-q-same-policy-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Another Q-function
</figcaption>
<div aria-describedby="tbl-multiple-q-same-policy-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<table id="tbl-multiple-q-same-policy-2" class="do-not-create-environment caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"><span class="math inline">\(s\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 0\)</span></th>
<th style="text-align: right;"><span class="math inline">\(a = 1\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">A</td>
<td style="text-align: right;">0.1</td>
<td style="text-align: right;">0.3</td>
</tr>
<tr class="even">
<td style="text-align: left;">B</td>
<td style="text-align: right;">0.8</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: left;">C</td>
<td style="text-align: right;">0.4</td>
<td style="text-align: right;">0.6</td>
</tr>
</tbody>
</table>
</div>
</figure>
</div>
</div>
</div>
</div>
</figure>
</div>
<p>These both map to the same greedy policy, so policy iteration would never iterate through both of these functions, while value iteration might.</p>
<div id="thm-pi-iter-analysis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.6 (Policy Iteration runtime and convergence)</strong></span> We aim to show that the number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
<p>This bound follows from the contraction property <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.15</span></a>:</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
<p>We’ll prove that the iterates of PI respect the contraction property by showing that the policies improve monotonically:</p>
<p><span class="math display">\[
V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s).
\]</span></p>
<p>Then we’ll use this to show <span class="math inline">\(V^{\pi^{t+1}}(s) \ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)\)</span>. Note that</p>
<p><span class="math display">\[
\begin{aligned}
(s) &amp;= \max_a \left[ r(s, a) + \gamma \mathbb{E}_{s' \sim P(s, a)} V^{\pi^{t}}(s') \right] \\
    &amp;= r(s, \pi^{t+1}(s)) + \gamma \mathbb{E}_{s' \sim P(s, \pi^{t+1}(s))} V^{\pi^{t}}(s')
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\([\mathcal{J}^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s)\)</span>, we then have</p>
<p><span id="eq-pi-iter-proof"><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) \\
    &amp;= \gamma \mathbb{E}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right].
\end{aligned}
\tag{1.20}\]</span></span></p>
<p>But note that the expression being averaged is the same as the expression on the l.h.s. with <span class="math inline">\(s\)</span> replaced by <span class="math inline">\(s'\)</span>. So we can apply the same inequality recursively to get</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge  \gamma \mathbb{E}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge \gamma^2 \mathbb{E}_{\substack{s' \sim P(s, \pi^{t+1}(s)) \\ s'' \sim P(s', \pi^{t+1}(s'))}} \left[V^{\pi^{t+1}}(s'') -  V^{\pi^{t}}(s'') \right]\\
    &amp;\ge \cdots
\end{aligned}
\]</span></p>
<p>which implies that <span class="math inline">\(V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)\)</span> for all <span class="math inline">\(s\)</span> (since the r.h.s. converges to zero). We can then plug this back into <a href="#eq-pi-iter-proof" class="quarto-xref">Equation&nbsp;<span>1.20</span></a> to get the desired result:</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) &amp;= \gamma \mathbb{E}_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge 0 \\
    V^{\pi^{t+1}}(s) &amp;\ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)
\end{aligned}
\]</span></p>
<p>This means we can now apply the Bellman convergence result <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.15</span></a> to get</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \|\mathcal{J}^{\star} (V^{\pi^{t}}) - V^{\star}\|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
</div>
</section>
</section>
</section>
<section id="summary" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="summary"><span class="header-section-number">1.4</span> Summary</h2>
<ul>
<li><p>Markov decision processes (MDPs) are a framework for sequential decision making under uncertainty. They consist of a state space <span class="math inline">\(\mathcal{S}\)</span>, an action space <span class="math inline">\(\mathcal{A}\)</span>, an initial state distribution <span class="math inline">\(\mu \in \Delta(\mathcal{S})\)</span>, a transition function <span class="math inline">\(P(s' \mid s, a)\)</span>, and a reward function <span class="math inline">\(r(s, a)\)</span>. They can be finite-horizon (ends after <span class="math inline">\(H\)</span> timesteps) or infinite-horizon (where rewards scale by <span class="math inline">\(\gamma \in (0, 1)\)</span> at each timestep).</p></li>
<li><p>Our goal is to find a policy <span class="math inline">\(\pi\)</span> that maximizes expected total reward. Policies can be <strong>deterministic</strong> or <strong>stochastic</strong>, <strong>state-dependent</strong> or <strong>history-dependent</strong>, <strong>stationary</strong> or <strong>time-dependent</strong>.</p></li>
<li><p>A policy induces a distribution over <strong>trajectories</strong>.</p></li>
<li><p>We can evaluate a policy by computing its <strong>value function</strong> <span class="math inline">\(V^\pi(s)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span>. We can also compute the <strong>state-action value function</strong> <span class="math inline">\(Q^\pi(s, a)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(\pi\)</span>. In the finite-horizon setting, these also depend on the timestep <span class="math inline">\(h\)</span>.</p></li>
<li><p>The <strong>Bellman consistency equation</strong> is an equation that the value function must satisfy. It can be used to solve for the value functions exactly. Thinking of the r.h.s. of this equation as an operator on value functions gives the <strong>Bellman operator</strong>.</p></li>
<li><p>In the finite-horizon setting, we can compute the optimal policy using <strong>dynamic programming</strong>.</p></li>
<li><p>In the infinite-horizon setting, we can compute the optimal policy using <strong>value iteration</strong> or <strong>policy iteration</strong>.</p></li>
</ul>
</section>
<section id="references" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="references"><span class="header-section-number">1.5</span> References</h2>
<p>The proof of <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>1.2</span></a> can also be found in <span class="citation" data-cites="agarwal_reinforcement_2022">Agarwal et al. (<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">2022</a>)</span>.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-agarwal_reinforcement_2022" class="csl-entry" role="listitem">
Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. <em>Reinforcement <span>Learning</span>: <span>Theory</span> and <span>Algorithms</span></em>. <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</a>.
</div>
<div id="ref-frans_berkelaar_container_2009" class="csl-entry" role="listitem">
Frans Berkelaar. 2009. <em>Container Ship <span>MSC Davos</span> - <span>Westerschelde</span> - <span>Zeeland</span></em>. <a href="https://www.flickr.com/photos/28169156@N03/52957948820/">https://www.flickr.com/photos/28169156@N03/52957948820/</a>.
</div>
<div id="ref-gpa_photo_archive_robotic_2017" class="csl-entry" role="listitem">
GPA Photo Archive. 2017. <em>Robotic Arm</em>. <a href="https://www.flickr.com/photos/iip-photo-archive/36123310136/">https://www.flickr.com/photos/iip-photo-archive/36123310136/</a>.
</div>
<div id="ref-guy_chess_2006" class="csl-entry" role="listitem">
Guy, Romain. 2006. <em>Chess</em>. <a href="https://www.flickr.com/photos/romainguy/230416692/">https://www.flickr.com/photos/romainguy/230416692/</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./control.html" class="pagination-link" aria-label="Linear Quadratic Regulators">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>