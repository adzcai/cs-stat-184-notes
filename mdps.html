<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>1&nbsp; Markov Decision Processes – rlbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./control.html" rel="next">
<link href="./index.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./mdps.html"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">rlbook</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">1.1</span> Introduction</a></li>
  <li><a href="#finite-horizon-mdps" id="toc-finite-horizon-mdps" class="nav-link" data-scroll-target="#finite-horizon-mdps"><span class="header-section-number">1.2</span> Finite-horizon MDPs</a>
  <ul class="collapse">
  <li><a href="#definition" id="toc-definition" class="nav-link" data-scroll-target="#definition"><span class="header-section-number">1.2.1</span> Definition</a></li>
  <li><a href="#policies" id="toc-policies" class="nav-link" data-scroll-target="#policies"><span class="header-section-number">1.2.2</span> Policies</a></li>
  <li><a href="#sec-trajectories" id="toc-sec-trajectories" class="nav-link" data-scroll-target="#sec-trajectories"><span class="header-section-number">1.2.3</span> Trajectories</a></li>
  <li><a href="#value-functions" id="toc-value-functions" class="nav-link" data-scroll-target="#value-functions"><span class="header-section-number">1.2.4</span> Value functions</a></li>
  <li><a href="#the-one-step-bellman-consistency-equation" id="toc-the-one-step-bellman-consistency-equation" class="nav-link" data-scroll-target="#the-one-step-bellman-consistency-equation"><span class="header-section-number">1.2.5</span> The one-step (Bellman) consistency equation</a></li>
  <li><a href="#the-one-step-bellman-operator" id="toc-the-one-step-bellman-operator" class="nav-link" data-scroll-target="#the-one-step-bellman-operator"><span class="header-section-number">1.2.6</span> The one-step Bellman operator</a></li>
  </ul></li>
  <li><a href="#sec-finite-horizon-mdps" id="toc-sec-finite-horizon-mdps" class="nav-link" data-scroll-target="#sec-finite-horizon-mdps"><span class="header-section-number">1.3</span> Solving finite-horizon MDPs</a>
  <ul class="collapse">
  <li><a href="#sec-eval-dp" id="toc-sec-eval-dp" class="nav-link" data-scroll-target="#sec-eval-dp"><span class="header-section-number">1.3.1</span> Policy evaluation in finite-horizon MDPs</a></li>
  <li><a href="#sec-opt-dynamic-programming" id="toc-sec-opt-dynamic-programming" class="nav-link" data-scroll-target="#sec-opt-dynamic-programming"><span class="header-section-number">1.3.2</span> Optimal policies in finite-horizon MDPs</a></li>
  </ul></li>
  <li><a href="#sec-infinite-horizon-mdps" id="toc-sec-infinite-horizon-mdps" class="nav-link" data-scroll-target="#sec-infinite-horizon-mdps"><span class="header-section-number">1.4</span> Infinite-horizon MDPs</a>
  <ul class="collapse">
  <li><a href="#discounted-rewards" id="toc-discounted-rewards" class="nav-link" data-scroll-target="#discounted-rewards"><span class="header-section-number">1.4.1</span> Discounted rewards</a></li>
  <li><a href="#stationary-policies" id="toc-stationary-policies" class="nav-link" data-scroll-target="#stationary-policies"><span class="header-section-number">1.4.2</span> Stationary policies</a></li>
  <li><a href="#value-functions-and-bellman-consistency" id="toc-value-functions-and-bellman-consistency" class="nav-link" data-scroll-target="#value-functions-and-bellman-consistency"><span class="header-section-number">1.4.3</span> Value functions and Bellman consistency</a></li>
  </ul></li>
  <li><a href="#solving-infinite-horizon-mdps" id="toc-solving-infinite-horizon-mdps" class="nav-link" data-scroll-target="#solving-infinite-horizon-mdps"><span class="header-section-number">1.5</span> Solving infinite-horizon MDPs</a>
  <ul class="collapse">
  <li><a href="#the-bellman-operator-is-a-contraction-mapping" id="toc-the-bellman-operator-is-a-contraction-mapping" class="nav-link" data-scroll-target="#the-bellman-operator-is-a-contraction-mapping"><span class="header-section-number">1.5.1</span> The Bellman operator is a contraction mapping</a></li>
  <li><a href="#policy-evaluation-in-infinite-horizon-mdps" id="toc-policy-evaluation-in-infinite-horizon-mdps" class="nav-link" data-scroll-target="#policy-evaluation-in-infinite-horizon-mdps"><span class="header-section-number">1.5.2</span> Policy evaluation in infinite-horizon MDPs</a></li>
  <li><a href="#optimal-policies-in-infinite-horizon-mdps" id="toc-optimal-policies-in-infinite-horizon-mdps" class="nav-link" data-scroll-target="#optimal-policies-in-infinite-horizon-mdps"><span class="header-section-number">1.5.3</span> Optimal policies in infinite-horizon MDPs</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">1.6</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-mdps" class="quarto-section-identifier"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<hr>
<hr>
<section id="introduction" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">1.1</span> Introduction</h2>
<p>The field of RL studies how an agent can learn to make sequential decisions in an interactive environment. This is a very general problem! How can we <em>formalize</em> this task in a way that is both <em>sufficiently general</em> yet also tractable enough for <em>fruitful analysis</em>?</p>
<p>Let’s consider some examples of sequential decision problems to identify the key common properties we’d like to capture:</p>
<ul>
<li><strong>Board games and video games,</strong> where a player takes actions in a virtual environment.</li>
<li><strong>Inventory management,</strong> where a company must efficiently move resources from producers to consumers.</li>
<li><strong>Robotic control</strong>, where a robot can move and interact with the real world to complete some task.</li>
</ul>
<p>In these environments and many others, the <strong>state transitions</strong>, the “rules” of the environment, only depend on the <em>most recent</em> state and action (generally speaking). For example, if you want to take a break while playing a game of chess, you could take a picture of the board, and later on reset the board to that state and continue playing; the past history of moves doesn’t matter (generally speaking). This is called the <strong>Markov property.</strong></p>
<div id="def-markov" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.1 (Markov property)</strong></span> An interactive environment satisfies the <strong>Markov property</strong> if the probability of transitioning to a new state only depends on the current state and action:</p>
<p><span class="math display">\[
\pr(s_{\hi+1} \mid s_0, a_0, \dots, s_\hi, a_\hi) = P(s_{\hi+1} \mid s_\hi, a_\hi)
\]</span></p>
<p>where <span class="math inline">\(P : \mathcal{S} \times \mathcal{A} \to \triangle(\mathcal{S})\)</span> describes the state transitions. (We’ll elaborate on this notation later in the chapter.)</p>
</div>
<p>Environments that satisfy the Markov property are called <strong>Markov decision processes</strong> (MDPs). This chapter will focus on introducing core vocabulary for MDPs that will be useful throughout the book.</p>
<div class="{attention}">
<p>What information might be encoded in the <em>state</em> for each of the above examples? What might the valid set of <em>actions</em> be? Describe the <em>state transitions</em> heuristically and verify that they satisfy the Markov property.</p>
</div>
<p>MDPs are usually classified as <strong>finite-horizon</strong>, where the interactions end after some finite number of time steps, or <strong>infinite-horizon</strong>, where the interactions can continue indefinitely. We’ll begin with the finite-horizon case and discuss the infinite-horizon case in the second half of the chapter.</p>
<p>We’ll describe how to <em>evaluate</em> different strategies, called <strong>policies,</strong> and how to compute (or approximate) the <strong>optimal policy</strong> for a given MDP. We’ll introduce the <strong>Bellman consistency condition</strong>, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.</p>
<div id="76b6488d" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> NamedTuple, Float, Array, partial, jax, jnp, latexify, latex</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="finite-horizon-mdps" class="level2" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="finite-horizon-mdps"><span class="header-section-number">1.2</span> Finite-horizon MDPs</h2>
<section id="definition" class="level3" data-number="1.2.1">
<h3 data-number="1.2.1" class="anchored" data-anchor-id="definition"><span class="header-section-number">1.2.1</span> Definition</h3>
<div id="def-finite-horizon-mdp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.2 (Finite-horizon Markov decision process)</strong></span> The components of a finite-horizon Markov decision process are:</p>
<ol type="1">
<li><p>The <strong>state</strong> that the agent interacts with. We use <span class="math inline">\(\mathcal{S}\)</span> to denote the set of possible states, called the <strong>state space</strong>.</p></li>
<li><p>The <strong>actions</strong> that the agent can take. We use <span class="math inline">\(\mathcal{A}\)</span> to denote the set of possible actions, called the <strong>action space</strong>.</p></li>
<li><p>Some <strong>initial state distribution</strong> <span class="math inline">\(\mu \in \triangle(\mathcal{S})\)</span>.</p></li>
<li><p>The <strong>state transitions</strong> (a.k.a. <strong>dynamics</strong>) <span class="math inline">\(P : \mathcal{S} \times \mathcal{A} \to \triangle(\mathcal{S})\)</span> that describe what state the agent transitions to after taking an action.</p></li>
<li><p>The <strong>reward</strong> signal. In this course we’ll take it to be a deterministic function on state-action pairs, <span class="math inline">\(r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>, but in general many results will extend to a <em>stochastic</em> reward signal.</p></li>
<li><p>A time horizon <span class="math inline">\(\hor \in \mathbb{N}\)</span> that specifies the number of interactions in an <strong>episode</strong>.</p></li>
</ol>
<p>Combined together, these objects specify a finite-horizon Markov decision process:</p>
<p><span class="math display">\[
M = (\mathcal{S}, \mathcal{A}, \mu, P, r, \hor).
\]</span></p>
<p>When there are <strong>finitely</strong> many states and actions, i.e. <span class="math inline">\(|\mathcal{S}|, |\mathcal{A}| &lt; \infty\)</span>, we can express the relevant quantities as vectors and matrices (i.e.&nbsp;<em>tables</em> of values):</p>
<p><span class="math display">\[
\begin{aligned}
    \mu &amp;\in [0, 1]^{|\mathcal{S}|} &amp;
    P &amp;\in [0, 1]^{(|\mathcal{S} \times \mathcal{A}|) \times |\mathcal{S}|} &amp;
    r &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}
\end{aligned}
\]</span></p>
</div>
<div class="{attention}">
<p>Verify that the types and shapes provided above make sense!</p>
</div>
<div id="0b39d2bc" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MDP(NamedTuple):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A description of a Markov decision process with finitely many states and actions."""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    S: <span class="bu">int</span>  <span class="co"># number of states</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>    A: <span class="bu">int</span>  <span class="co"># number of actions</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>    μ: Float[Array, <span class="st">" S"</span>]</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    P: Float[Array, <span class="st">"S A S"</span>]  <span class="co"># "current" state, "current" action, "next" state</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    r: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    H: <span class="bu">int</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>    γ: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1.0</span>  <span class="co"># discount factor (used later)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exm-tidy-mdp" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.1 (Tidying MDP)</strong></span> Let’s consider a simple decision problem throughout this chapter: the task of keeping your room tidy!</p>
<p>Your room has the possible states <span class="math inline">\(\mathcal{S} = \{ \text{orderly}, \text{messy} \}.\)</span> You can take either of the actions <span class="math inline">\(\mathcal{A} = \{ \text{ignore}, \text{tidy} \}.\)</span> The room starts off orderly.</p>
<p>The <strong>state transitions</strong> are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it <em>might</em> become messy (see table below).</p>
<p>The <strong>rewards</strong> are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy your additional time). Tidying a messy room is a chore that gives no reward.</p>
<p>These are summarized in the following table:</p>
<p><span class="math display">\[
\begin{array}{ccccc}
    s &amp; a &amp; P(\text{orderly} \mid s, a) &amp; P(\text{messy} \mid s, a) &amp; r(s, a) \\
    \text{orderly} &amp; \text{ignore} &amp; 0.7 &amp; 0.3 &amp; 1 \\
    \text{orderly} &amp; \text{tidy} &amp; 1 &amp; 0 &amp; -1 \\
    \text{messy} &amp; \text{ignore} &amp; 0 &amp; 1 &amp; -1 \\
    \text{messy} &amp; \text{tidy} &amp; 1 &amp; 0 &amp; 0 \\
\end{array}
\]</span></p>
<p>Consider a time horizon of <span class="math inline">\(\hor = 7\)</span> days (one interaction per day). Let <span class="math inline">\(t = 0\)</span> correspond to Monday and <span class="math inline">\(t = 6\)</span> correspond to Sunday.</p>
</div>
<div id="fbb296b3" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>tidy_mdp <span class="op">=</span> MDP(</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    S<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = orderly, 1 = messy</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    A<span class="op">=</span><span class="dv">2</span>,  <span class="co"># 0 = ignore, 1 = tidy</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    μ<span class="op">=</span>jnp.array([<span class="fl">1.0</span>, <span class="fl">0.0</span>]),  <span class="co"># start in orderly state</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    P<span class="op">=</span>jnp.array([</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.7</span>, <span class="fl">0.3</span>],  <span class="co"># orderly, ignore</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># orderly, tidy</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">0.0</span>, <span class="fl">1.0</span>],  <span class="co"># messy, ignore</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>            [<span class="fl">1.0</span>, <span class="fl">0.0</span>],  <span class="co"># messy, tidy</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    r<span class="op">=</span>jnp.array([</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>            <span class="fl">1.0</span>,   <span class="co"># orderly, ignore</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># orderly, tidy</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        [</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>            <span class="op">-</span><span class="fl">1.0</span>,  <span class="co"># messy, ignore</span></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>            <span class="fl">0.0</span>,   <span class="co"># messy, tidy</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>    ]),</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>    H<span class="op">=</span><span class="dv">7</span>,</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="policies" class="level3" data-number="1.2.2">
<h3 data-number="1.2.2" class="anchored" data-anchor-id="policies"><span class="header-section-number">1.2.2</span> Policies</h3>
<div id="def-policy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.3 (Policies)</strong></span> A <strong>policy</strong> <span class="math inline">\(\pi\)</span> describes the agent’s strategy: which actions it takes in a given situation. A key goal of RL is to find the <strong>optimal policy</strong> that maximizes the total reward on average.</p>
<p>There are three axes along which policies can vary: their outputs, inputs, and time-dependence.</p>
<ol type="1">
<li><strong>Deterministic or stochastic.</strong> A deterministic policy outputs actions while a stochastic policy outputs <em>distributions</em> over actions.</li>
</ol>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/deterministic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A deterministic policy</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/stochastic_policy.png" class="img-fluid figure-img"></p>
<figcaption>A stochastic policy</figcaption>
</figure>
</div>
</div>
</div>
</div>
<ol start="2" type="1">
<li><p><strong>State-dependent or history-dependent.</strong> A state-dependent (a.k.a. “Markovian”) policy only depends on the current state, while a history-dependent policy depends on the sequence of past states, actions, and rewards. We’ll only consider state-dependent policies in this course.</p></li>
<li><p><strong>Stationary or time-dependent.</strong> A stationary (a.k.a. time-homogeneous) policy remains the same function at all time steps, while a time-dependent policy can depend on the current timestep. For consistency with states and actions, we will denote the timestep as a subscript, i.e.&nbsp;<span class="math inline">\(\pi = \{ \pi_0, \dots, \pi_{\hor-1} \}.\)</span></p></li>
</ol>
</div>
<p>Note that for finite state and action spaces, we can represent a randomized mapping <span class="math inline">\(\mathcal{S} \to \Delta(\mathcal{A})\)</span> as a matrix <span class="math inline">\(\pi \in [0, 1]^{\mathcal{S} \times \mathcal{A}}\)</span> where each row describes the policy’s distribution over actions for the corresponding state.</p>
<p>A fascinating result is that every finite-horizon MDP has an optimal deterministic time-dependent policy! Intuitively, the Markov property implies that the current state contains all the information we need to make the optimal decision. We’ll prove this result constructively later in the chapter.</p>
<div id="exm-tidy-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.2 (Policies for the tidying MDP)</strong></span> Here are some possible policies for the tidying MDP <a href="#exm-tidy-mdp" class="quarto-xref">Example&nbsp;<span>1.1</span></a>:</p>
<ul>
<li><p>Always tidy: <span class="math inline">\(\pi(s) = \text{tidy}\)</span>.</p></li>
<li><p>Only tidy on weekends: <span class="math inline">\(\pi_\hi(s) = \text{tidy}\)</span> if <span class="math inline">\(\hi \in \{ 5, 6 \}\)</span> and <span class="math inline">\(\pi_\hi(s) = \text{ignore}\)</span> otherwise.</p></li>
<li><p>Only tidy if the room is messy: <span class="math inline">\(\pi_\hi(\text{messy}) = \text{tidy}\)</span> and <span class="math inline">\(\pi_\hi(\text{orderly}) = \text{ignore}\)</span> for all <span class="math inline">\(\hi\)</span>.</p></li>
</ul>
</div>
<div id="b4a4219e" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="co"># arrays of shape (H, S, A) represent time-dependent policies</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>tidy_policy_always_tidy <span class="op">=</span> (</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    .at[:, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>tidy_policy_weekends <span class="op">=</span> (</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">5</span>:<span class="dv">7</span>, :, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    .at[<span class="dv">0</span>:<span class="dv">5</span>, :, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>tidy_policy_messy_only <span class="op">=</span> (</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    jnp.zeros((<span class="dv">7</span>, <span class="dv">2</span>, <span class="dv">2</span>))</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">1</span>, <span class="dv">1</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    .at[:, <span class="dv">0</span>, <span class="dv">0</span>].<span class="bu">set</span>(<span class="fl">1.0</span>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-jax-immutable" class="proof remark">
<p><span class="proof-title"><em>Remark 1.1</em>. </span>Array objects in Jax are <strong>immutable,</strong> that is, they cannot be <em>changed.</em> This might seem inconvenient, but in larger projects, immutability makes code easier to reason about.</p>
</div>
</section>
<section id="sec-trajectories" class="level3" data-number="1.2.3">
<h3 data-number="1.2.3" class="anchored" data-anchor-id="sec-trajectories"><span class="header-section-number">1.2.3</span> Trajectories</h3>
<div id="def-trajectory" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.4 (Trajectories)</strong></span> A sequence of states, actions, and rewards is called a <strong>trajectory</strong>:</p>
<p><span class="math display">\[
\tau = (s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1})
\]</span></p>
<p>where <span class="math inline">\(r_\hi = r(s_\hi, a_\hi)\)</span>. (Note that some sources omit the reward at the final time step. This is a minor detail.)</p>
</div>
<div id="9892667d" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Transition(NamedTuple):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A single state-action-reward interaction with the environment.</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    A trajectory comprises a sequence of transitions.</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    s: <span class="bu">int</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    a: <span class="bu">int</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    r: <span class="bu">float</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Once we’ve chosen a policy, we can sample trajectories by repeatedly choosing actions according to the policy, transitioning according to the state transitions, and observing the rewards.</p>
<pre class="mermaid"><code>graph LR
    S0($$s_0$$) -- $$\pi_0$$ --&gt; A0{{$$a_0$$}}
    S0 &amp; A0 --&gt; R0[$$r_0$$]
    A0 &amp; S0 -- $$P$$ --&gt; S1($$s_1$$)
    S1 -- $$\pi_1$$ --&gt; A1{{$$a_1$$}}
    S1 &amp; A1 --&gt; R1[$$r_1$$]
    A1 &amp; S1 -- $$P$$ --&gt; S2($$s_2$$)
    S2 -- $$\pi_2$$ --&gt; A2{{$$a_2$$}}
    S2 &amp; A2 --&gt; R2[$$r_2$$]
    A2 &amp; S2 -- $$P$$ --&gt; S3($$s_3$$)</code></pre>
<p>That is, a policy induces a distribution <span class="math inline">\(\rho^{\pi}\)</span> over trajectories. (We assume that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(P\)</span> are clear from context.)</p>
<div id="exm-tidy-traj" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.3 (Trajectories in the tidying environment)</strong></span> Here is a possible trajectory for the tidying example:</p>
<div class="{table}">
<table class="caption-top table">
<colgroup>
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 13%">
<col style="width: 11%">
<col style="width: 10%">
<col style="width: 13%">
<col style="width: 13%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(\hi\)</span></th>
<th style="text-align: center;"><span class="math inline">\(0\)</span></th>
<th style="text-align: center;"><span class="math inline">\(1\)</span></th>
<th style="text-align: center;"><span class="math inline">\(2\)</span></th>
<th style="text-align: center;"><span class="math inline">\(3\)</span></th>
<th style="text-align: center;"><span class="math inline">\(4\)</span></th>
<th style="text-align: center;"><span class="math inline">\(5\)</span></th>
<th style="text-align: center;"><span class="math inline">\(6\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(s\)</span></td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">messy</td>
<td style="text-align: center;">orderly</td>
<td style="text-align: center;">orderly</td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(a\)</span></td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">tidy</td>
<td style="text-align: center;">ignore</td>
<td style="text-align: center;">ignore</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(r\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(0\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
</tr>
</tbody>
</table>
</div>
<p>Could any of the policies in <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.2</span></a> have generated this trajectory?</p>
</div>
<p>Note that for a state-dependent policy, using the Markov property <a href="#def-markov" class="quarto-xref">Definition&nbsp;<span>1.1</span></a>, we can write down the likelihood function of this probability distribution in an <strong>autoregressive</strong> way (i.e.&nbsp;one timestep at a time):</p>
<div id="def-autoregressive-trajectories" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.5 (Autoregressive trajectory distribution)</strong></span> <span class="math display">\[
\rho^{\pi}(\tau) := \mu(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots P(s_{\hor-1} \mid s_{\hor-2}, a_{\hor-2}) \pi_{\hor-1}(a_{\hor-1} \mid s_{\hor-1})
\]</span></p>
</div>
<div id="ae6dde35" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trajectory_log_likelihood(</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    τ: <span class="bu">list</span>[Transition],</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    π: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Compute the log-likelihood of a trajectory under a given MDP and policy."""</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="co"># initial distribution and action</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    total <span class="op">=</span> jnp.log(mdp.μ[τ[<span class="dv">0</span>].s])</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    total <span class="op">+=</span> jnp.log(π[τ[<span class="dv">0</span>].s, τ[<span class="dv">0</span>].a])</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># remaining state transitions and actions</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>, mdp.H):</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(mdp.P[τ[i <span class="op">-</span> <span class="dv">1</span>].s, τ[i <span class="op">-</span> <span class="dv">1</span>].a, τ[i].s])</span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        total <span class="op">+=</span> jnp.log(π[τ[i].s, τ[i].a])</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> total</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="{attention}">
<p>How would you modify this to include stochastic rewards?</p>
</div>
<p>For a deterministic policy <span class="math inline">\(\pi\)</span>, we have that <span class="math inline">\(\pi_\hi(a \mid s) = \mathbb{I}[a = \pi_\hi(s)]\)</span>; that is, the probability of taking an action is <span class="math inline">\(1\)</span> if it’s the unique action prescribed by the policy for that state and <span class="math inline">\(0\)</span> otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution <span class="math inline">\(\mu\)</span> and the state transitions <span class="math inline">\(P\)</span>.</p>
</section>
<section id="value-functions" class="level3" data-number="1.2.4">
<h3 data-number="1.2.4" class="anchored" data-anchor-id="value-functions"><span class="header-section-number">1.2.4</span> Value functions</h3>
<p>The main goal of RL is to find a policy that maximizes the expected total reward <span class="math inline">\(\E [r_0 + \cdots + r_{\hor-1}]\)</span>.</p>
<div class="{attention}">
<p>Note that <span class="math inline">\(r_0 + \cdots + r_{\hor-1}\)</span> is a random variable. What sources of randomness does it depend on? Describe the generating process.</p>
</div>
<p>Let’s introduce some notation for analyzing this quantity.</p>
<p>A policy’s <strong>value function</strong> at time <span class="math inline">\(\hi\)</span> is its expected remaining reward <em>from a given state</em>:</p>
<div id="def-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.6 (Value function)</strong></span> <span class="math display">\[
V_\hi^\pi(s) := \E_{\tau \sim \rho^\pi} [r_\hi + \cdots + r_{H-1} \mid s_\hi = s]
\]</span></p>
</div>
<p>Similarly, we can define the <strong>action-value function</strong> (aka the <strong>Q-function</strong>) at time <span class="math inline">\(h\)</span> as the expected remaining reward <em>from a given state and taking a given action</em>:</p>
<div id="def-action-value" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.7 (Action-value function)</strong></span> <span class="math display">\[
Q_\hi^\pi(s, a) := \E_{\tau \sim \rho^\pi} [r_\hi + \cdots + r_{H-1} \mid s_\hi = s, a_\hi = a]
\]</span></p>
</div>
<section id="relating-the-value-function-and-action-value-function" class="level4" data-number="1.2.4.1">
<h4 data-number="1.2.4.1" class="anchored" data-anchor-id="relating-the-value-function-and-action-value-function"><span class="header-section-number">1.2.4.1</span> Relating the value function and action-value function</h4>
<p>Note that the value function is just the expected action-value over actions drawn from the policy:</p>
<p><span class="math display">\[
V_\hi^\pi(s) = \E_{a \sim \pi_\hi(s)} [Q_\hi^\pi(s, a)]
\]</span></p>
<div id="7c949ebd" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_v(</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    q: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the value function for a given policy in a known finite MDP</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from its action-value function.</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.average(q, weights<span class="op">=</span>policy, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>and the action-value is the sum of the immediate reward and the expected value of the following state:</p>
<p><span class="math display">\[
Q_\hi^\pi(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [V_{\hi+1}^\pi(s')]
\]</span></p>
<div id="fe08ddb1" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_q(</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    v_next: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute the action-value function in a known finite MDP</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">    at a single timestep from the corresponding value function.</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># the discount factor is relevant later</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> mdp.r <span class="op">+</span> mdp.γ <span class="op">*</span> mdp.P <span class="op">@</span> v_next</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a><span class="co"># convert a list of v functions to a list of q functions</span></span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>v_ary_to_q_ary <span class="op">=</span> jax.vmap(v_to_q, in_axes<span class="op">=</span>(<span class="va">None</span>, <span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="greedy-policies" class="level4" data-number="1.2.4.2">
<h4 data-number="1.2.4.2" class="anchored" data-anchor-id="greedy-policies"><span class="header-section-number">1.2.4.2</span> Greedy policies</h4>
<p>For any given <span class="math inline">\(Q \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}\)</span>, we can define the <strong>greedy policy</strong> <span class="math inline">\(\hat \pi_Q\)</span> as the deterministic policy that selects the action with the highest <span class="math inline">\(Q\)</span>-value at each state:</p>
<p><span class="math display">\[
\hat \pi_Q(s) = \arg\max_{a} Q_{sa}
\]</span></p>
<div id="9ba80965" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> q_to_greedy(q: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Get the (deterministic) greedy policy with respect to an action-value function.</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    A <span class="op">=</span> q.shape[<span class="dv">1</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    a_ary <span class="op">=</span> jnp.argmax(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.eye(A)[a_ary]</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> v_to_greedy(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the (deterministic) greedy policy with respect to a value function."""</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_greedy(v_to_q(mdp, v))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="the-one-step-bellman-consistency-equation" class="level3" data-number="1.2.5">
<h3 data-number="1.2.5" class="anchored" data-anchor-id="the-one-step-bellman-consistency-equation"><span class="header-section-number">1.2.5</span> The one-step (Bellman) consistency equation</h3>
<p>Note that by simply considering the cumulative reward as the sum of the <em>current</em> reward and the <em>future</em> cumulative reward, we can describe the value function recursively (in terms of itself). This is named the <strong>Bellman consistency equation</strong> after <strong>Richard Bellman</strong> (1920–1984), who is credited with introducing dynamic programming in 1953.</p>
<div id="thm-bellman-consistency" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.1 (Bellman consistency equation for the value function)</strong></span> <span class="math display">\[
V_\hi^\pi(s) = \E_{\substack{a \sim \pi_\hi(s) \\ s' \sim P(s, a)}} [r(s, a) + V_{\hi+1}^\pi(s')]
\]</span></p>
</div>
<div id="79ee6b11" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_bellman_consistency_v(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"H S A"</span>],</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>    v_ary: Float[Array, <span class="st">"H S"</span>],</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> <span class="bu">bool</span>:</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Check that the given (time-dependent) "value function"</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="co">    satisfies the Bellman consistency equation.</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">all</span>(</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>        jnp.allclose(</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>            <span class="co"># lhs</span></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a>            v_ary[h],</span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a>            <span class="co"># rhs</span></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>            jnp.<span class="bu">sum</span>(policy[h] <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.γ <span class="op">*</span> mdp.P <span class="op">@</span> v_ary[h <span class="op">+</span> <span class="dv">1</span>]), axis<span class="op">=</span><span class="dv">1</span>),</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="{attention}">
<p>Verify that this equation holds by expanding <span class="math inline">\(V_\hi^\pi(s)\)</span> and <span class="math inline">\(V_{\hi+1}^\pi(s')\)</span>.</p>
</div>
<p>One can analogously derive the Bellman consistency equation for the action-value function:</p>
<div id="thm-bellman-consistency-action" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.2 (Bellman consistency equation for action-values)</strong></span> <span class="math display">\[
Q_\hi^\pi(s, a) = r(s, a) + \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi_{\hi+1}(s')}} [Q_{\hi+1}^\pi(s', a')]
\]</span></p>
</div>
<div class="{attention}">
<p>Write a <code>check_bellman_consistency_q</code> function for the action-value function.</p>
</div>
<div id="rem-bellman-det" class="proof remark">
<p><span class="proof-title"><em>Remark 1.2</em> (The Bellman consistency equation for deterministic policies). </span>Note that for deterministic policies, the Bellman consistency equation simplifies to</p>
<p><span class="math display">\[
\begin{aligned}
    V_\hi^\pi(s) &amp;= r(s, \pi_\hi(s)) + \E_{s' \sim P(s, \pi_\hi(s))} [V_{\hi+1}^\pi(s')] \\
    Q_\hi^\pi(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [Q_{\hi+1}^\pi(s', \pi_{\hi+1}(s'))]
\end{aligned}
\]</span></p>
</div>
</section>
<section id="the-one-step-bellman-operator" class="level3" data-number="1.2.6">
<h3 data-number="1.2.6" class="anchored" data-anchor-id="the-one-step-bellman-operator"><span class="header-section-number">1.2.6</span> The one-step Bellman operator</h3>
<p>Fix a policy <span class="math inline">\(\pi\)</span>. Consider the higher-order operator that takes in a “value function” <span class="math inline">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman equation for that “value function”:</p>
<div id="def-bellman-operator" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.8 (Bellman operator)</strong></span> <span class="math display">\[
[\mathcal{J}^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v(s')].
\]</span></p>
<p>This is a crucial tool for reasoning about MDPs. Intuitively, it answers the following question: if we evaluate the <em>next</em> state using <span class="math inline">\(v\)</span>, how good is the <em>current</em> state, according to the given policy?</p>
</div>
<div id="95610cd7" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator_looping(</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Looping definition of the Bellman operator.</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Concise version is below</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    v_new <span class="op">=</span> jnp.zeros(mdp.S)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> a <span class="kw">in</span> <span class="bu">range</span>(mdp.A):</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> s_next <span class="kw">in</span> <span class="bu">range</span>(mdp.S):</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>                v_new[s] <span class="op">+=</span> (</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>                    policy[s, a]</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> mdp.P[s, a, s_next]</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>                    <span class="op">*</span> (mdp.r[s, a] <span class="op">+</span> mdp.γ <span class="op">*</span> v[s_next])</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a>                )</span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> v_new</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Note that we can concisely implement this using the <code>q_to_v</code> and <code>v_to_q</code> utilities from above:</p>
<div id="d3fd8ea1" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_operator(</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP,</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    policy: Float[Array, <span class="st">"S A"</span>],</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    v: Float[Array, <span class="st">" S"</span>],</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""For a known finite MDP, the Bellman operator can be exactly evaluated."""</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> q_to_v(policy, v_to_q(mdp, v))  <span class="co"># equivalent</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">sum</span>(policy <span class="op">*</span> (mdp.r <span class="op">+</span> mdp.γ <span class="op">*</span> mdp.P <span class="op">@</span> v), axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We’ll call <span class="math inline">\(\mathcal{J}^\pi : \mathbb{R}^{\mathcal{S}} \to \mathbb{R}^{\mathcal{S}}\)</span> the <strong>Bellman operator</strong> of <span class="math inline">\(\pi\)</span>. Note that it’s defined on any “value function” mapping states to real numbers; <span class="math inline">\(v\)</span> doesn’t have to be a well-defined value function for some policy (hence the lowercase notation). The Bellman operator also gives us a concise way to express <a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a> for the value function:</p>
<p><span class="math display">\[
V_\hi^\pi = \mathcal{J}^{\pi}(V_{\hi+1}^\pi)
\]</span></p>
<p>Intuitively, the output of the Bellman operator, a new “value function”, evaluates states as follows: from a given state, take one action according to <span class="math inline">\(\pi\)</span>, observe the reward, and then evaluate the next state using the input “value function”.</p>
<p>When we discuss infinite-horizon MDPs, the Bellman operator will turn out to be more than just a notational convenience: We’ll use it to construct algorithms for computing the optimal policy.</p>
</section>
</section>
<section id="sec-finite-horizon-mdps" class="level2" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="sec-finite-horizon-mdps"><span class="header-section-number">1.3</span> Solving finite-horizon MDPs</h2>
<section id="sec-eval-dp" class="level3" data-number="1.3.1">
<h3 data-number="1.3.1" class="anchored" data-anchor-id="sec-eval-dp"><span class="header-section-number">1.3.1</span> Policy evaluation in finite-horizon MDPs</h3>
<p>How can we actually compute the value function of a given policy? This is the task of <strong>policy evaluation</strong>.</p>
<section id="dp-algorithm-to-evaluate-a-policy-in-a-finite-horizon-mdp" class="level4 {def-dp-evaluate}" data-number="1.3.1.1">
<h4 data-number="1.3.1.1" class="anchored" data-anchor-id="dp-algorithm-to-evaluate-a-policy-in-a-finite-horizon-mdp"><span class="header-section-number">1.3.1.1</span> DP algorithm to evaluate a policy in a finite-horizon MDP</h4>
<p>The Bellman consistency equation <a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a> gives us a convenient algorithm for evaluating stationary policies: it expresses the value function at timestep <span class="math inline">\(\hi\)</span> as a function of the value function at timestep <span class="math inline">\(\hi+1\)</span>. This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman consistency equation to compute the value function at each time step.</p>
</section>
<div id="8ccc5829" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> dp_eval_finite(mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">"H S"</span>]:</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Evaluate a policy using dynamic programming."""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    V_ary <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> mdp.H <span class="op">+</span> [jnp.zeros(mdp.S)]  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        V_ary[h] <span class="op">=</span> bellman_operator(mdp, policy[h], V_ary[h <span class="op">+</span> <span class="dv">1</span>])</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.stack(V_ary[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>latex(dp_eval_finite)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>&lt;latexify.ipython_wrappers.LatexifyWrapper at 0x1280c2310&gt;</code></pre>
</div>
</div>
<p>This runs in time <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span> by counting the loops.</p>
<div class="{attention}">
<p>Do you see where we compute <span class="math inline">\(Q^\pi_\hi\)</span> along the way? Make this step explicit.</p>
</div>
<div id="exm-tidy-eval-finite" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.4 (Tidying policy evaluation)</strong></span> Let’s evaluate the policy from <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.2</span></a> in the tidying MDP that tidies if and only if the room is messy. We’ll use the Bellman consistency equation to compute the value function at each time step.</p>
<p><span class="math display">\[
\begin{aligned}
V_{H-1}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) \\
&amp;= 1 \\
V_{H-1}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) \\
&amp;= 0 \\
V_{H-2}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-1}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1 + 0.3 \cdot 0 \\
&amp;= 1.7 \\
V_{H-2}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-1}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 \\
V_{H-3}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-2}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1.7 + 0.3 \cdot 1 \\
&amp;= 2.49 \\
V_{H-3}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-2}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1.7
\end{aligned}
\]</span></p>
<p>etc. You may wish to repeat this computation for the other policies to get a better sense of this algorithm.</p>
</div>
<div id="701a3928" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>V_messy <span class="op">=</span> dp_eval_finite(tidy_mdp, tidy_policy_messy_only)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>V_messy</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>Array([[5.5621696, 4.7927704],
       [4.7927704, 4.0241003],
       [4.0241003, 3.253    ],
       [3.253    , 2.49     ],
       [2.49     , 1.7      ],
       [1.7      , 1.       ],
       [1.       , 0.       ]], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="sec-opt-dynamic-programming" class="level3" data-number="1.3.2">
<h3 data-number="1.3.2" class="anchored" data-anchor-id="sec-opt-dynamic-programming"><span class="header-section-number">1.3.2</span> Optimal policies in finite-horizon MDPs</h3>
<p>We’ve just seen how to <em>evaluate</em> a given policy. But how can we find the <strong>optimal policy</strong> for a given environment?</p>
<div id="def-optimal-policy-finite" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.9 (Optimal policies)</strong></span> We call a policy optimal, and denote it by <span class="math inline">\(\pi^\star\)</span>, if it does at least as well as <em>any</em> other policy <span class="math inline">\(\pi\)</span> (including stochastic and history-dependent ones) in all situations:</p>
<p><span class="math display">\[
\begin{aligned}
    V_\hi^{\pi^\star}(s) &amp;= \E_{\tau \sim \rho^{\pi^{\star}}}[r_\hi + \cdots + r_{H-1} \mid s_\hi = s] \\
    &amp;\ge \E_{\tau \sim \rho^{\pi}}[r_\hi + \cdots + r_{H-1} \mid \tau_\hi] \quad \forall \pi, \tau_\hi, \hi \in [H]
\end{aligned}
\]</span></p>
<p>where we condition on the trajectory up to time <span class="math inline">\(\hi\)</span>, denoted <span class="math inline">\(\tau_\hi = (s_0, a_0, r_0, \dots, s_\hi)\)</span>, where <span class="math inline">\(s_\hi = s\)</span>.</p>
</div>
<p>Convince yourself that all optimal policies must have the same value function. We call this the <strong>optimal value function</strong> and denote it by <span class="math inline">\(V_\hi^\star(s)\)</span>. The same goes for the action-value function <span class="math inline">\(Q_\hi^\star(s, a)\)</span>.</p>
<p>It is a stunning fact that <strong>every finite-horizon MDP has an optimal policy that is time-dependent and deterministic.</strong> In particular, we can construct such a policy by acting <em>greedily</em> with respect to the optimal action-value function:</p>
<div id="thm-optimal-greedy" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.3 (It is optimal to be greedy with respect to the optimal value function)</strong></span> <span class="math display">\[
\pi_\hi^\star(s) = \arg\max_a Q_\hi^\star(s, a).
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(V^{\star}\)</span> and <span class="math inline">\(Q^{\star}\)</span> denote the optimal value and action-value functions. Consider the greedy policy</p>
<p><span class="math display">\[
\hat \pi_\hi(s) := \arg\max_a Q_\hi^{\star}(s, a).
\]</span></p>
<p>We aim to show that <span class="math inline">\(\hat \pi\)</span> is optimal; that is, <span class="math inline">\(V^{\hat \pi} = V^{\star}\)</span>.</p>
<p>Fix an arbitrary state <span class="math inline">\(s \in \mathcal{S}\)</span> and time <span class="math inline">\(\hi \in [H]\)</span>.</p>
<p>Firstly, by the definition of <span class="math inline">\(V^{\star}\)</span>, we already know <span class="math inline">\(V_\hi^{\star}(s) \ge V_\hi^{\hat \pi}(s)\)</span>. So for equality to hold we just need to show that <span class="math inline">\(V_\hi^{\star}(s) \le V_\hi^{\hat \pi}(s)\)</span>. We’ll first show that the Bellman operator <span class="math inline">\(\mathcal{J}^{\hat \pi}\)</span> never decreases <span class="math inline">\(V_\hi^{\star}\)</span>. Then we’ll apply this result recursively to show that <span class="math inline">\(V^{\star} = V^{\hat \pi}\)</span>.</p>
<section id="the-bellman-operator-never-decreases-the-optimal-value-function" class="level4 {lem-bellman-increases}" data-number="1.3.2.1">
<h4 data-number="1.3.2.1" class="anchored" data-anchor-id="the-bellman-operator-never-decreases-the-optimal-value-function"><span class="header-section-number">1.3.2.1</span> The Bellman operator never decreases the optimal value function</h4>
<p><span class="math inline">\(\mathcal{J}^{\hat \pi}\)</span> never decreases <span class="math inline">\(V_\hi^{\star}\)</span> (elementwise):</p>
<p><span class="math display">\[
[\mathcal{J}^{\hat \pi} (V_{\hi+1}^{\star})](s) \ge V_\hi^{\star}(s).
\]</span></p>
<p><strong>Proof:</strong></p>
<p><span class="math display">\[
\begin{aligned}
    V_\hi^{\star}(s) &amp;= \max_{\pi \in \Pi} V_\hi^{\pi}(s) \\
    &amp;= \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^\pi(s') \right] &amp;&amp; \text{Bellman consistency} \\
    &amp;\le \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^{\star}(s') \right] &amp;&amp; \text{definition of } V^\star \\
    &amp;= \max_{a} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^{\star}(s') \right] &amp;&amp; \text{only depends on } \pi \text{ via } a \\
    &amp;= [\mathcal{J}^{\hat \pi}(V_{\hi+1}^{\star})](s).    
\end{aligned}
\]</span></p>
<p>Note that the chosen action <span class="math inline">\(a \sim \pi(\dots)\)</span> above might depend on the past history; this isn’t shown in the notation and doesn’t affect our result (make sure you see why).</p>
</section>
<p>We can now apply this result recursively to get</p>
<p><span class="math display">\[
V^{\star}_t(s) \le V^{\hat \pi}_t(s)
\]</span></p>
<p>as follows. (Note that even though <span class="math inline">\(\hat \pi\)</span> is deterministic, we’ll use the <span class="math inline">\(a \sim \hat \pi(s)\)</span> notation to make it explicit that we’re sampling a trajectory from it.)</p>
<p><span class="math display">\[
\begin{aligned}
    V_{t}^{\star}(s) &amp;\le [\mathcal{J}^{\hat \pi}(V_{\hi+1}^{\star})](s) \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue} V_{\hi+1}^{\star}(s')} \right] \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue}[ \mathcal{J}^{\hat \pi} (V_{t+2}^{\star})] (s')} \right] \right] &amp;&amp; \text{above lemma} \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)}{\color{blue} \left[ \mathop{\mathbb{E}}_{a' \sim \hat \pi}  r(s', a') + \mathop{\mathbb{E}}_{s''} V_{t+2}^{\star}(s'') \right]} \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \cdots &amp;&amp; \text{apply at all timesteps} \\
    &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\hat \pi}} [G_{t} \mid s_\hi = s] &amp;&amp; \text{rewrite expectation} \\
    &amp;= V_{t}^{\hat \pi}(s) &amp;&amp; \text{definition}
\end{aligned}
\]</span></p>
<p>And so we have <span class="math inline">\(V^{\star} = V^{\hat \pi}\)</span>, making <span class="math inline">\(\hat \pi\)</span> optimal.</p>
</div>
<p>Note that this also gives simplified forms of the <a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a> for the optimal policy:</p>
<div id="thm-bellman-consistency-optimal" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.4 (Bellman consistency equations for the optimal policy)</strong></span> <span class="math display">\[
\begin{aligned}
    V_\hi^\star(s) &amp;= \max_a Q_\hi^\star(s, a) \\
    Q_\hi^\star(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [V_{\hi+1}^\star(s')]
\end{aligned}
\]</span></p>
</div>
<p>Now that we’ve shown this particular greedy policy is optimal, all we need to do is compute the optimal value function and optimal policy. We can do this by working backwards in time using <strong>dynamic programming</strong> (DP).</p>
<div id="def-pi-star-dp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.10 (DP algorithm to compute an optimal policy in a finite-horizon MDP)</strong></span> <strong>Base case.</strong> At the end of the episode (time step <span class="math inline">\(H-1\)</span>), we can’t take any more actions, so the <span class="math inline">\(Q\)</span>-function is simply the reward that we obtain:</p>
<p><span class="math display">\[
Q^\star_{H-1}(s, a) = r(s, a)
\]</span></p>
<p>so the best thing to do is just act greedily and get as much reward as we can!</p>
<p><span class="math display">\[
\pi^\star_{H-1}(s) = \arg\max_a Q^\star_{H-1}(s, a)
\]</span></p>
<p>Then <span class="math inline">\(V^\star_{H-1}(s)\)</span>, the optimal value of state <span class="math inline">\(s\)</span> at the end of the trajectory, is simply whatever action gives the most reward.</p>
<p><span class="math display">\[
V^\star_{H-1} = \max_a Q^\star_{H-1}(s, a)
\]</span></p>
<p><strong>Recursion.</strong> Then, we can work backwards in time, starting from the end, using our consistency equations! i.e.&nbsp;for each <span class="math inline">\(t = H-2, \dots, 0\)</span>, we set</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\star_{t}(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [V^\star_{\hi+1}(s')] \\
    \pi^\star_{t}(s) &amp;= \arg\max_a Q^\star_{t}(s, a) \\
    V^\star_{t}(s) &amp;= \max_a Q^\star_{t}(s, a)
\end{aligned}
\]</span></p>
</div>
<div id="33d50d6b" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> find_optimal_policy(mdp: MDP):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> mdp.H</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> mdp.H</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> [<span class="va">None</span>] <span class="op">*</span> mdp.H <span class="op">+</span> [jnp.zeros(mdp.S)]  <span class="co"># initialize to 0 at end of time horizon</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> h <span class="kw">in</span> <span class="bu">range</span>(mdp.H <span class="op">-</span> <span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        Q[h] <span class="op">=</span> mdp.r <span class="op">+</span> mdp.P <span class="op">@</span> V[h <span class="op">+</span> <span class="dv">1</span>]</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        pi[h] <span class="op">=</span> jnp.eye(mdp.S)[jnp.argmax(Q[h], axis<span class="op">=</span><span class="dv">1</span>)]  <span class="co"># one-hot</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        V[h] <span class="op">=</span> jnp.<span class="bu">max</span>(Q[h], axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    Q <span class="op">=</span> jnp.stack(Q)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> jnp.stack(pi)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    V <span class="op">=</span> jnp.stack(V[:<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pi, V, Q</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>At each of the <span class="math inline">\(H\)</span> timesteps, we must compute <span class="math inline">\(Q^{\star}\)</span> for each of the <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> state-action pairs. Each computation takes <span class="math inline">\(|\mathcal{S}|\)</span> operations to evaluate the average value over <span class="math inline">\(s'\)</span>. This gives a total computation time of <span class="math inline">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>.</p>
<p>Note that this algorithm is identical to the policy evaluation algorithm <code>dp_eval_finite</code> in <a href="#sec-eval-dp" class="quarto-xref"><span>Section 1.3.1</span></a>, but instead of <em>averaging</em> over the actions chosen by a policy, we instead simply take a <em>maximum</em> over the action-values. We’ll see this relationship between <strong>policy evaluation</strong> and <strong>optimal policy computation</strong> show up again in the infinite-horizon setting.</p>
<div id="30e20b8a" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>π_opt, V_opt, Q_opt <span class="op">=</span> find_optimal_policy(tidy_mdp)</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(π_opt, tidy_policy_messy_only)</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(V_opt, V_messy)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> jnp.allclose(Q_opt[:<span class="op">-</span><span class="dv">1</span>], v_ary_to_q_ary(tidy_mdp, V_messy)[<span class="dv">1</span>:])</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a><span class="co">"Assertions passed (the 'tidy when messy' policy is optimal)"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>"Assertions passed (the 'tidy when messy' policy is optimal)"</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-infinite-horizon-mdps" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="sec-infinite-horizon-mdps"><span class="header-section-number">1.4</span> Infinite-horizon MDPs</h2>
<p>What happens if a trajectory is allowed to continue forever (i.e. <span class="math inline">\(H = \infty\)</span>)? This is the setting of <strong>infinite horizon</strong> MDPs.</p>
<p>In this chapter, we’ll describe the necessary adjustments from the finite-horizon case to make the problem tractable. We’ll show that the <a href="#bellman_operator">Bellman operator</a> in the discounted reward setting is a <strong>contraction mapping</strong> for any policy. We’ll discuss how to evaluate policies (i.e.&nbsp;compute their corresponding value functions). Finally, we’ll present and analyze two iterative algorithms, based on the Bellman operator, for computing the optimal policy: <strong>value iteration</strong> and <strong>policy iteration</strong>.</p>
<section id="discounted-rewards" class="level3" data-number="1.4.1">
<h3 data-number="1.4.1" class="anchored" data-anchor-id="discounted-rewards"><span class="header-section-number">1.4.1</span> Discounted rewards</h3>
<p>First of all, note that maximizing the cumulative reward <span class="math inline">\(r_\hi + r_{\hi+1} + r_{\hi+2} + \cdots\)</span> is no longer a good idea since it might blow up to infinity. Instead of a time horizon <span class="math inline">\(H\)</span>, we now need a <strong>discount factor</strong> <span class="math inline">\(\gamma \in [0, 1)\)</span> such that rewards become less valuable the further into the future they are:</p>
<p><span class="math display">\[
r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots = \sum_{k=0}^\infty \gamma^k r_{\hi+k}.
\]</span></p>
<p>We can think of <span class="math inline">\(\gamma\)</span> as measuring how much we care about the future: if it’s close to <span class="math inline">\(0\)</span>, we only care about the near-term rewards; it’s close to <span class="math inline">\(1\)</span>, we put more weight into future rewards.</p>
<p>You can also analyze <span class="math inline">\(\gamma\)</span> as the probability of <em>continuing</em> the trajectory at each time step. (This is equivalent to <span class="math inline">\(H\)</span> being distributed by a First Success distribution with success probability <span class="math inline">\(\gamma\)</span>.) This accords with the above interpretation: if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(0\)</span>, the trajectory will likely be very short, while if <span class="math inline">\(\gamma\)</span> is close to <span class="math inline">\(1\)</span>, the trajectory will likely continue for a long time.</p>
<div class="{attention}">
<p>Assuming that <span class="math inline">\(r_\hi \in [0, 1]\)</span> for all <span class="math inline">\(\hi \in \mathbb{N}\)</span>, what is the maximum <strong>discounted</strong> cumulative reward? You may find it useful to review geometric series.</p>
</div>
<p>The other components of the MDP remain the same:</p>
<p><span class="math display">\[
M = (\mathcal{S}, \mathcal{A}, \mu, P, r, \gamma).
\]</span></p>
<p>Code-wise, we can reuse the <code>MDP</code> class from before <a href="#def-finite-horizon-mdp" class="quarto-xref">Definition&nbsp;<span>1.2</span></a> and set <code>mdp.H = float('inf')</code>.</p>
<div id="59da4a16" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>tidy_mdp_inf <span class="op">=</span> tidy_mdp._replace(H<span class="op">=</span><span class="bu">float</span>(<span class="st">"inf"</span>), γ<span class="op">=</span><span class="fl">0.95</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="stationary-policies" class="level3" data-number="1.4.2">
<h3 data-number="1.4.2" class="anchored" data-anchor-id="stationary-policies"><span class="header-section-number">1.4.2</span> Stationary policies</h3>
<p>The time-dependent policies from the finite-horizon case become difficult to handle in the infinite-horizon case. In particular, many of the DP approaches we saw required us to start at the end of the trajectory, which is no longer possible. We’ll shift to <strong>stationary</strong> policies <span class="math inline">\(\pi : \mathcal{S} \to \mathcal{A}\)</span> (deterministic) or <span class="math inline">\(\Delta(\mathcal{A})\)</span> (stochastic).</p>
<div class="{attention}">
<p>Which of the policies in <a href="#exm-tidy-policy" class="quarto-xref">Example&nbsp;<span>1.2</span></a> are stationary?</p>
</div>
</section>
<section id="value-functions-and-bellman-consistency" class="level3" data-number="1.4.3">
<h3 data-number="1.4.3" class="anchored" data-anchor-id="value-functions-and-bellman-consistency"><span class="header-section-number">1.4.3</span> Value functions and Bellman consistency</h3>
<p>We also consider stationary value functions <span class="math inline">\(V^\pi : \mathcal{S} \to \mathbb{R}\)</span> and <span class="math inline">\(Q^\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>. We need to insert a factor of <span class="math inline">\(\gamma\)</span> into the Bellman consistency equation <a href="#thm-bellman-consistency" class="quarto-xref">Theorem&nbsp;<span>1.1</span></a> to account for the discounting:</p>
<p><span id="eq-bellman-consistency-infinite"><span class="math display">\[
\begin{aligned}
    V^\pi(s) &amp;= \E_{\tau \sim \rho^\pi} [r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} \cdots \mid s_\hi = s] &amp;&amp; \text{for any } \hi \in \mathbb{N} \\
    &amp;= \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma V^\pi(s')]\\
    Q^\pi(s, a) &amp;= \E_{\tau \sim \rho^\pi} [r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots \mid s_\hi = s, a_\hi = a] &amp;&amp; \text{for any } \hi \in \mathbb{N} \\
    &amp;= r(s, a) + \gamma \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi(s')}} [Q^\pi(s', a')]
\end{aligned}
\tag{1.1}\]</span></span></p>
<div class="{attention}">
<p>Heuristically speaking, why does it no longer matter which time step we condition on when defining the value function?</p>
</div>
</section>
</section>
<section id="solving-infinite-horizon-mdps" class="level2" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="solving-infinite-horizon-mdps"><span class="header-section-number">1.5</span> Solving infinite-horizon MDPs</h2>
<section id="the-bellman-operator-is-a-contraction-mapping" class="level3" data-number="1.5.1">
<h3 data-number="1.5.1" class="anchored" data-anchor-id="the-bellman-operator-is-a-contraction-mapping"><span class="header-section-number">1.5.1</span> The Bellman operator is a contraction mapping</h3>
<p>Recall from <a href="#def-bellman-operator" class="quarto-xref">Definition&nbsp;<span>1.8</span></a> that the Bellman operator <span class="math inline">\(\mathcal{J}^{\pi}\)</span> for a policy <span class="math inline">\(\pi\)</span> takes in a “value function” <span class="math inline">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman equation for that “value function”. In the infinite-horizon setting, this is</p>
<p><span class="math display">\[
[\mathcal{J}^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma v(s')].
\]</span></p>
<p>The crucial property of the Bellman operator is that it is a <strong>contraction mapping</strong> for any policy. Intuitively, if we start with two “value functions” <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>, if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate.</p>
<div id="def-contraction" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 1.11 (Contraction mapping)</strong></span> Let <span class="math inline">\(X\)</span> be some space with a norm <span class="math inline">\(\|\cdot\|\)</span>. We call an operator <span class="math inline">\(f: X \to X\)</span> a <strong>contraction mapping</strong> if for any <span class="math inline">\(x, y \in X\)</span>,</p>
<p><span class="math display">\[
\|f(x) - f(y)\| \le \gamma \|x - y\|
\]</span></p>
<p>for some fixed <span class="math inline">\(\gamma \in (0, 1)\)</span>. Intuitively, this means that if two points are <span class="math inline">\(\delta\)</span> far apart, after applying the mapping,</p>
</div>
<div class="{attention}">
<p>Show that for a contraction mapping <span class="math inline">\(f\)</span> with coefficient <span class="math inline">\(\gamma\)</span>, for all <span class="math inline">\(t \in \mathbb{N}\)</span>,</p>
<p><span class="math display">\[
\|f^{(t)}(x) - f^{(t)}(y)\| \le \gamma^t \|x - y\|,
\]</span></p>
<p>i.e.&nbsp;that any two points will be pushed closer by at least a factor of <span class="math inline">\(\gamma\)</span> at each iteration.</p>
</div>
<p>It is a powerful fact (known as the <strong>Banach fixed-point theorem</strong>) that every contraction mapping has a unique <strong>fixed point</strong> <span class="math inline">\(x^\star\)</span> such that <span class="math inline">\(f(x^\star) = x^\star\)</span>. This means that if we repeatedly apply <span class="math inline">\(f\)</span> to any starting point, we will eventually converge to <span class="math inline">\(x^\star\)</span>:</p>
<p><span id="eq-contraction-convergence"><span class="math display">\[
\|f^{(t)}(x) - x^\star\| \le \gamma^t \|x - x^\star\|.
\tag{1.2}\]</span></span></p>
<p>Let’s return to the RL setting and apply this result to the Bellman operator. How can we measure the distance between two “value functions” <span class="math inline">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>? We’ll take the <strong>supremum norm</strong> as our distance metric:</p>
<p><span class="math display">\[
\| v - u \|_{\infty} := \sup_{s \in \mathcal{S}} |v(s) - u(s)|,
\]</span></p>
<p>i.e. we compare the “value functions” on the state that causes the biggest gap between them. Then <a href="#eq-contraction-convergence" class="quarto-xref">Equation&nbsp;<span>1.2</span></a> implies that if we repeatedly apply <span class="math inline">\(\mathcal{J}^\pi\)</span> to any starting “value function”, we will eventually converge to <span class="math inline">\(V^\pi\)</span>:</p>
<p><span id="eq-bellman-convergence"><span class="math display">\[
\|(\mathcal{J}^\pi)^{(t)}(v) - V^\pi \|_{\infty} \le \gamma^{t} \| v - V^\pi\|_{\infty}.
\tag{1.3}\]</span></span></p>
<p>We’ll use this useful fact to prove the convergence of several algorithms later on.</p>
<div id="thm-bellman-contraction" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.5 (The Bellman operator is a contraction mapping)</strong></span> <span class="math display">\[
\|\mathcal{J}^{\pi} (v) - \mathcal{J}^{\pi} (u) \|_{\infty} \le \gamma \|v - u \|_{\infty}.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For all states <span class="math inline">\(s \in \mathcal{S}\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
|[\mathcal{J}^{\pi} (v)](s) - [\mathcal{J}^{\pi} (u)](s)|&amp;= \Big| \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right] \\
&amp;\qquad - \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} u(s') \right] \Big| \\
&amp;= \gamma \left|\mathop{\mathbb{E}}_{s' \sim P(s, a)} [v(s') - u(s')] \right| \\
&amp;\le \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)}|v(s') - u(s')| \qquad \text{(Jensen's inequality)} \\
&amp;\le \gamma \max_{s'} |v(s') - u(s')| \\
&amp;= \gamma \|v - u \|_{\infty}.
\end{aligned}
\]</span></p>
</div>
</section>
<section id="policy-evaluation-in-infinite-horizon-mdps" class="level3" data-number="1.5.2">
<h3 data-number="1.5.2" class="anchored" data-anchor-id="policy-evaluation-in-infinite-horizon-mdps"><span class="header-section-number">1.5.2</span> Policy evaluation in infinite-horizon MDPs</h3>
<p>The backwards DP technique we used in the finite-horizon case (<a href="#sec-eval-dp" class="quarto-xref"><span>Section 1.3.1</span></a>) no longer works since there is no “final timestep” to start from. We’ll need another approach to policy evaluation.</p>
<p>The Bellman consistency conditions yield a system of equations we can solve to evaluate a deterministic policy <em>exactly</em>. For a faster approximate solution, we can iterate the policy’s Bellman operator, since we know that it has a unique fixed point at the true value function.</p>
<section id="matrix-inversion-for-deterministic-policies" class="level4" data-number="1.5.2.1">
<h4 data-number="1.5.2.1" class="anchored" data-anchor-id="matrix-inversion-for-deterministic-policies"><span class="header-section-number">1.5.2.1</span> Matrix inversion for deterministic policies</h4>
<p>Note that when the policy <span class="math inline">\(\pi\)</span> is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:</p>
<p><span class="math display">\[
\begin{aligned}
    r^{\pi} &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; P^{\pi} &amp;\in [0, 1]^{|\mathcal{S}| \times |\mathcal{S}|} &amp; \mu &amp;\in [0, 1]^{|\mathcal{S}|} \\
    \pi &amp;\in \mathcal{A}^{|\mathcal{S}|} &amp; V^\pi &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; Q^\pi &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}.
\end{aligned}
\]</span></p>
<p>For <span class="math inline">\(P^\pi\)</span>, we’ll treat the rows as the states and the columns as the next states. Then <span class="math inline">\(P^\pi_{s, s'}\)</span> is the probability of transitioning from state <span class="math inline">\(s\)</span> to state <span class="math inline">\(s'\)</span> under policy <span class="math inline">\(\pi\)</span>.</p>
<div id="exm-tidy-tabular" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.5 (Tidying MDP)</strong></span> The tabular MDP from before has <span class="math inline">\(|\mathcal{S}| = 2\)</span> and <span class="math inline">\(|\mathcal{A}| = 2\)</span>. Let’s write down the quantities for the policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy:</p>
<p><span class="math display">\[
r^{\pi} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
        P^{\pi} = \begin{bmatrix} 0.7 &amp; 0.3 \\ 1 &amp; 0 \end{bmatrix}, \quad
        \mu = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
\]</span></p>
<p>We’ll see how to evaluate this policy in the next section.</p>
</div>
<p>The Bellman consistency equation for a deterministic policy can be written in tabular notation as</p>
<p><span class="math display">\[
V^\pi = r^\pi + \gamma P^\pi V^\pi.
\]</span></p>
<p>(Unfortunately, this notation doesn’t simplify the expression for <span class="math inline">\(Q^\pi\)</span>.) This system of equations can be solved with a matrix inversion:</p>
<p><span id="eq-matrix-inversion-pe"><span class="math display">\[
V^\pi = (I - \gamma P^\pi)^{-1} r^\pi.
\tag{1.4}\]</span></span></p>
<div class="{attention}">
<p>Note we’ve assumed that <span class="math inline">\(I - \gamma P^\pi\)</span> is invertible. Can you see why this is the case?</p>
<p>(Recall that a linear operator, i.e.&nbsp;a square matrix, is invertible if and only if its null space is trivial; that is, it doesn’t map any nonzero vector to zero. In this case, we can see that <span class="math inline">\(I - \gamma P^\pi\)</span> is invertible because it maps any nonzero vector to a vector with at least one nonzero element.)</p>
</div>
<div id="5d0f3d0a" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_deterministic_infinite(</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    mdp: MDP, policy: Float[Array, <span class="st">"S A"</span>]</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> jnp.argmax(policy, axis<span class="op">=</span><span class="dv">1</span>)  <span class="co"># un-one-hot</span></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>    P_π <span class="op">=</span> mdp.P[jnp.arange(mdp.S), pi]</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    r_π <span class="op">=</span> mdp.r[jnp.arange(mdp.S), pi]</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.linalg.solve(jnp.eye(mdp.S) <span class="op">-</span> mdp.γ <span class="op">*</span> P_π, r_π)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="exm-tidy-eval-infinite" class="theorem example">
<p><span class="theorem-title"><strong>Example 1.6 (Tidying policy evaluation)</strong></span> Let’s use the same policy <span class="math inline">\(\pi\)</span> that tidies if and only if the room is messy. Setting <span class="math inline">\(\gamma = 0.95\)</span>, we must invert</p>
<p><span class="math display">\[
I - \gamma P^{\pi} = \begin{bmatrix} 1 - 0.95 \times 0.7 &amp; - 0.95 \times 0.3 \\ - 0.95 \times 1 &amp; 1 - 0.95 \times 0 \end{bmatrix} = \begin{bmatrix} 0.335 &amp; -0.285 \\ -0.95 &amp; 1 \end{bmatrix}.
\]</span></p>
<p>The inverse to two decimal points is</p>
<p><span class="math display">\[
(I - \gamma P^{\pi})^{-1} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix}.
\]</span></p>
<p>Thus the value function is</p>
<p><span class="math display">\[
V^{\pi} = (I - \gamma P^{\pi})^{-1} r^{\pi} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 15.56 \\ 14.79 \end{bmatrix}.
\]</span></p>
<p>Let’s sanity-check this result. Since rewards are at most <span class="math inline">\(1\)</span>, the maximum cumulative return of a trajectory is at most <span class="math inline">\(1/(1-\gamma) = 20\)</span>. We see that the value function is indeed slightly lower than this.</p>
</div>
<div id="066032fa" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>eval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>Array([15.56419, 14.78598], dtype=float32)</code></pre>
</div>
</div>
</section>
<section id="sec-iterative-pe" class="level4" data-number="1.5.2.2">
<h4 data-number="1.5.2.2" class="anchored" data-anchor-id="sec-iterative-pe"><span class="header-section-number">1.5.2.2</span> Iterative policy evaluation</h4>
<p>The matrix inversion above takes roughly <span class="math inline">\(O(|\mathcal{S}|^3)\)</span> time. It also only works for deterministic policies. Can we trade off the requirement of finding the <em>exact</em> value function for a faster <em>approximate</em> algorithm that will also extend to stochastic policies?</p>
<p>Let’s use the Bellman operator to define an iterative algorithm for computing the value function. We’ll start with an initial guess <span class="math inline">\(v^{(0)}\)</span> with elements in <span class="math inline">\([0, 1/(1-\gamma)]\)</span> and then iterate the Bellman operator:</p>
<p><span class="math display">\[
v^{(t+1)} = \mathcal{J}^{\pi}(v^{(t)}),
\]</span></p>
<p>i.e.&nbsp;<span class="math inline">\(v^{(t)} = (\mathcal{J}^{\pi})^{(t)} (v^{(0)})\)</span>. Note that each iteration takes <span class="math inline">\(O(|\mathcal{S}|^2)\)</span> time for the matrix-vector multiplication.</p>
<div id="87f72fc4" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> supremum_norm(v):</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(jnp.<span class="bu">abs</span>(v))  <span class="co"># same as jnp.linalg.norm(v, jnp.inf)</span></span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> loop_until_convergence(op, v, ε<span class="op">=</span><span class="fl">1e-6</span>):</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Repeatedly apply op to v until convergence (in supremum norm)."""</span></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        v_new <span class="op">=</span> op(v)</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> supremum_norm(v_new <span class="op">-</span> v) <span class="op">&lt;</span> ε:</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> v_new</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>        v <span class="op">=</span> v_new</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> iterative_evaluation(mdp: MDP, pi: Float[Array, <span class="st">"S A"</span>], ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_operator, mdp, pi)</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Then, as we showed in <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>, by the Banach fixed-point theorem:</p>
<p><span class="math display">\[
\|v^{(t)} - V^\pi \|_{\infty} \le \gamma^{t} \| v^{(0)} - V^\pi\|_{\infty}.
\]</span></p>
<div id="6c6e8947" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>iterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<div id="rem-iterations-vi" class="proof remark">
<p><span class="proof-title"><em>Remark 1.3</em> (Convergence of iterative policy evaluation). </span>How many iterations do we need for an <span class="math inline">\(\epsilon\)</span>-accurate estimate? We can work backwards to solve for <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \gamma^t \|v^{(0)} - V^\pi\|_{\infty} &amp;\le \epsilon \\
    t &amp;\ge \frac{\log (\epsilon / \|v^{(0)} - V^\pi\|_{\infty})}{\log \gamma} \\
    &amp;= \frac{\log (\|v^{(0)} - V^\pi\|_{\infty} / \epsilon)}{\log (1 / \gamma)},
\end{aligned}
\]</span></p>
<p>and so the number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
<p>Note that we’ve applied the inequalities <span class="math inline">\(\|v^{(0)} - V^\pi\|_{\infty} \le 1/(1-\gamma)\)</span> and <span class="math inline">\(\log (1/x) \ge 1-x\)</span>.</p>
</div>
</section>
</section>
<section id="optimal-policies-in-infinite-horizon-mdps" class="level3" data-number="1.5.3">
<h3 data-number="1.5.3" class="anchored" data-anchor-id="optimal-policies-in-infinite-horizon-mdps"><span class="header-section-number">1.5.3</span> Optimal policies in infinite-horizon MDPs</h3>
<p>Now let’s move on to solving for an optimal policy in the infinite-horizon case. As in <a href="#def-optimal-policy-finite" class="quarto-xref">Definition&nbsp;<span>1.9</span></a>, an <strong>optimal policy</strong> <span class="math inline">\(\pi^\star\)</span> is one that does at least as well as any other policy in all situations. That is, for all policies <span class="math inline">\(\pi\)</span>, states <span class="math inline">\(s \in \mathcal{S}\)</span>, times <span class="math inline">\(\hi \in \mathbb{N}\)</span>, and initial trajectories <span class="math inline">\(\tau_\hi = (s_0, a_0, r_0, \dots, s_\hi)\)</span> where <span class="math inline">\(s_\hi = s\)</span>,</p>
<p><span id="eq-optimal-policy-infinite"><span class="math display">\[
\begin{aligned}
    V^{\pi^\star}(s) &amp;= \E_{\tau \sim \rho^{\pi^{\star}}}[r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2}  + \cdots \mid s_\hi = s] \\
    &amp;\ge \E_{\tau \sim \rho^{\pi}}[r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots \mid \tau_\hi]
\end{aligned}
\tag{1.5}\]</span></span></p>
<p>Once again, all optimal policies share the same <strong>optimal value function</strong> <span class="math inline">\(V^\star\)</span>, and the greedy policy with respect to this value function is optimal.</p>
<div class="{attention}">
<p>Verify this by modifying the proof <a href="#thm-optimal-greedy" class="quarto-xref">Theorem&nbsp;<span>1.3</span></a> from the finite-horizon case.</p>
</div>
<p>So how can we compute such an optimal policy? We can’t use the backwards DP approach from the finite-horizon case <a href="#def-pi-star-dp" class="quarto-xref">Definition&nbsp;<span>1.10</span></a> since there’s no “final timestep” to start from. Instead, we’ll exploit the fact that the Bellman consistency equation <a href="#eq-bellman-consistency-infinite" class="quarto-xref">Equation&nbsp;<span>1.1</span></a> for the optimal value function doesn’t depend on any policy:</p>
<p><span id="eq-bellman-optimality"><span class="math display">\[
V^\star(s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^\star(s'). \right]
\tag{1.6}\]</span></span></p>
<div class="{attention}">
<p>Verify this by substituting the greedy policy into the Bellman consistency equation.</p>
</div>
<p>As before, thinking of the r.h.s. of <a href="#eq-bellman-optimality" class="quarto-xref">Equation&nbsp;<span>1.6</span></a> as an operator on value functions gives the <strong>Bellman optimality operator</strong></p>
<p><span id="eq-bellman-optimality-operator"><span class="math display">\[
[\mathcal{J}^{\star}(v)](s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v(s') \right]
\tag{1.7}\]</span></span></p>
<div id="217c0b6e" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bellman_optimality_operator(mdp: MDP, v: Float[Array, <span class="st">" S"</span>]) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.<span class="bu">max</span>(mdp.r <span class="op">+</span> mdp.γ <span class="op">*</span> mdp.P <span class="op">@</span> v, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> check_optimal(v: Float[Array, <span class="st">" S"</span>], mdp: MDP):</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> jnp.allclose(v, bellman_optimality_operator(v, mdp))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="sec-value-iteration" class="level4" data-number="1.5.3.1">
<h4 data-number="1.5.3.1" class="anchored" data-anchor-id="sec-value-iteration"><span class="header-section-number">1.5.3.1</span> Value iteration</h4>
<p>Since the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as <strong>value iteration</strong>.</p>
<div id="22d6c0bd" class="cell" data-execution_count="23">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> value_iteration(mdp: MDP, ε: <span class="bu">float</span> <span class="op">=</span> <span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" S"</span>]:</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iterate the Bellman optimality operator until convergence."""</span></span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    op <span class="op">=</span> partial(bellman_optimality_operator, mdp)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, jnp.zeros(mdp.S), ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="3acc2868" class="cell" data-execution_count="24">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>value_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="24">
<pre><code>Array([15.564166, 14.785956], dtype=float32)</code></pre>
</div>
</div>
<p>Note that the runtime analysis for an <span class="math inline">\(\epsilon\)</span>-optimal value function is exactly the same as <a href="#sec-iterative-pe" class="quarto-xref"><span>Section 1.5.2.2</span></a>! This is because value iteration is simply the special case of applying iterative policy evaluation to the <em>optimal</em> value function.</p>
<p>As the final step of the algorithm, to return an actual policy <span class="math inline">\(\hat \pi\)</span>, we can simply act greedily with respect to the final iteration <span class="math inline">\(v^{(T)}\)</span> of our above algorithm:</p>
<p><span class="math display">\[
\hat \pi(s) = \arg\max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v^{(T)}(s') \right].
\]</span></p>
<p>We must be careful, though: the value function of this greedy policy, <span class="math inline">\(V^{\hat \pi}\)</span>, is <em>not</em> the same as <span class="math inline">\(v^{(T)}\)</span>, which need not even be a well-defined value function for some policy!</p>
<p>The bound on the policy’s quality is actually quite loose: if <span class="math inline">\(\|v^{(T)} - V^\star\|_{\infty} \le \epsilon\)</span>, then the greedy policy <span class="math inline">\(\hat \pi\)</span> satisfies <span class="math inline">\(\|V^{\hat \pi} - V^\star\|_{\infty} \le \frac{2\gamma}{1-\gamma} \epsilon\)</span>, which might potentially be very large.</p>
<div id="thm-greedy-worsen" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.6 (Greedy policy value worsening)</strong></span> <span class="math display">\[
\|V^{\hat \pi} - V^\star \|_{\infty} \le \frac{2 \gamma}{1-\gamma} \|v - V^\star\|_{\infty}
\]</span></p>
<p>where <span class="math inline">\(\hat \pi(s) = \arg\max_a q(s, a)\)</span> is the greedy policy with respect to</p>
<p><span class="math display">\[
q(s, a) = r(s, a) + \E_{s' \sim P(s, a)} v(s').
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We first have</p>
<p><span class="math display">\[
\begin{aligned}
        V^{\star}(s) - V^{\hat \pi}(s) &amp;= Q^{\star}(s,\pi^\star(s)) - Q^{\hat \pi}(s, \hat \pi(s))\\
        &amp;= [Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s))] + [Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))].
\end{aligned}
\]</span></p>
<p>Let’s bound these two quantities separately.</p>
<p>For the first quantity, note that by the definition of <span class="math inline">\(\hat \pi\)</span>, we have</p>
<p><span class="math display">\[
q(s, \hat \pi(s)) \ge q(s,\pi^\star(s)).
\]</span></p>
<p>Let’s add <span class="math inline">\(q(s, \hat \pi(s)) - q(s,\pi^\star(s)) \ge 0\)</span> to the first term to get</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s)) &amp;\le [Q^{\star}(s,\pi^\star(s))- q(s,\pi^\star(s))] + [q(s, \hat \pi(s)) - Q^{\star}(s, \hat \pi(s))] \\
        &amp;= \gamma \E_{s' \sim P(s, \pi^{\star}(s))} [ V^{\star}(s') - v(s') ] + \gamma \E_{s' \sim P(s, \hat \pi(s))} [ v(s') - V^{\star}(s') ] \\
        &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty}.
\end{aligned}
\]</span></p>
<p>The second quantity is bounded by</p>
<p><span class="math display">\[
\begin{aligned}
        Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))
        &amp;=
        \gamma \E_{s'\sim P(s, \hat \pi(s))}\left[ V^\star(s') - V^{\hat \pi}(s') \right] \\
        &amp; \leq
        \gamma \|V^{\star} - V^{\hat \pi}\|_\infty
\end{aligned}
\]</span></p>
<p>and thus</p>
<p><span class="math display">\[
\begin{aligned}
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty} + \gamma \|V^{\star} - V^{\hat \pi}\|_\infty \\
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le \frac{2 \gamma \|v - V^{\star}\|_{\infty}}{1-\gamma}.
\end{aligned}
\]</span></p>
</div>
<p>So in order to compensate and achieve <span class="math inline">\(\|V^{\hat \pi} - V^{\star}\| \le \epsilon\)</span>, we must have</p>
<p><span class="math display">\[
\|v^{(T)} - V^\star\|_{\infty} \le \frac{1-\gamma}{2 \gamma} \epsilon.
\]</span></p>
<p>This means, using <a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>1.3</span></a>, we need to run value iteration for</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{\gamma}{\epsilon (1-\gamma)^2}\right) \right)
\]</span></p>
<p>iterations to achieve an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function.</p>
</section>
<section id="sec-policy-iteration" class="level4" data-number="1.5.3.2">
<h4 data-number="1.5.3.2" class="anchored" data-anchor-id="sec-policy-iteration"><span class="header-section-number">1.5.3.2</span> Policy iteration</h4>
<p>Can we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function <em>together</em>? This is the idea behind <strong>policy iteration</strong>. In each step, we simply set the policy to act greedily with respect to its own value function.</p>
<div id="df1dabcf" class="cell" data-execution_count="25">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_iteration(mdp: MDP, ε<span class="op">=</span><span class="fl">1e-6</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">"S A"</span>]:</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Iteratively improve the policy and value function."""</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> op(pi):</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    π_init <span class="op">=</span> jnp.ones((mdp.S, mdp.A)) <span class="op">/</span> mdp.A  <span class="co"># uniform random policy</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> loop_until_convergence(op, π_init, ε)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d1260735" class="cell" data-execution_count="26">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>policy_iteration(tidy_mdp_inf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>Array([[1., 0.],
       [0., 1.]], dtype=float32)</code></pre>
</div>
</div>
<p>Although PI appears more complex than VI, we’ll use <a href="#thm-bellman-contraction" class="quarto-xref">Theorem&nbsp;<span>1.5</span></a> to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an <span class="math inline">\(\epsilon\)</span>-optimal value function <a href="#rem-iterations-vi" class="quarto-xref">Remark&nbsp;<span>1.3</span></a>, although in practice, PI often converges much faster.</p>
<div id="thm-pi-iter-analysis" class="theorem">
<p><span class="theorem-title"><strong>Theorem 1.7 (Policy Iteration runtime and convergence)</strong></span> We aim to show that the number of iterations required for an <span class="math inline">\(\epsilon\)</span>-accurate estimate of the optimal value function is</p>
<p><span class="math display">\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</span></p>
<p>This bound follows from the contraction property <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.3</span></a>:</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
<p>We’ll prove that the iterates of PI respect the contraction property by showing that the policies improve monotonically:</p>
<p><span class="math display">\[
V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s).
\]</span></p>
<p>Then we’ll use this to show <span class="math inline">\(V^{\pi^{t+1}}(s) \ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)\)</span>. Note that</p>
<p><span class="math display">\[
\begin{aligned}
(s) &amp;= \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^{\pi^{t}}(s') \right] \\
    &amp;= r(s, \pi^{t+1}(s)) + \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} V^{\pi^{t}}(s')
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\([\mathcal{J}^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s)\)</span>, we then have</p>
<p><span id="eq-pi-iter-proof"><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) \\
    &amp;= \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right].
\end{aligned}
\tag{1.8}\]</span></span></p>
<p>But note that the expression being averaged is the same as the expression on the l.h.s. with <span class="math inline">\(s\)</span> replaced by <span class="math inline">\(s'\)</span>. So we can apply the same inequality recursively to get</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge  \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge \gamma^2 \E_{\substack{s' \sim P(s, \pi^{t+1}(s)) \\ s'' \sim P(s', \pi^{t+1}(s'))}} \left[V^{\pi^{t+1}}(s'') -  V^{\pi^{t}}(s'') \right]\\
    &amp;\ge \cdots
\end{aligned}
\]</span></p>
<p>which implies that <span class="math inline">\(V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)\)</span> for all <span class="math inline">\(s\)</span> (since the r.h.s. converges to zero). We can then plug this back into <a href="#eq-pi-iter-proof" class="quarto-xref">Equation&nbsp;<span>1.8</span></a> to get the desired result:</p>
<p><span class="math display">\[
\begin{aligned}
    V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) &amp;= \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge 0 \\
    V^{\pi^{t+1}}(s) &amp;\ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)
\end{aligned}
\]</span></p>
<p>This means we can now apply the Bellman convergence result <a href="#eq-bellman-convergence" class="quarto-xref">Equation&nbsp;<span>1.3</span></a> to get</p>
<p><span class="math display">\[
\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \|\mathcal{J}^{\star} (V^{\pi^{t}}) - V^{\star}\|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]</span></p>
</div>
</section>
</section>
</section>
<section id="summary" class="level2" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">1.6</span> Summary</h2>
<ul>
<li><p>Markov decision processes (MDPs) are a framework for sequential decision making under uncertainty. They consist of a state space <span class="math inline">\(\mathcal{S}\)</span>, an action space <span class="math inline">\(\mathcal{A}\)</span>, an initial state distribution <span class="math inline">\(\mu \in \Delta(\mathcal{S})\)</span>, a transition function <span class="math inline">\(P(s' \mid s, a)\)</span>, and a reward function <span class="math inline">\(r(s, a)\)</span>. They can be finite-horizon (ends after <span class="math inline">\(H\)</span> timesteps) or infinite-horizon (where rewards scale by <span class="math inline">\(\gamma \in (0, 1)\)</span> at each timestep).</p></li>
<li><p>Our goal is to find a policy <span class="math inline">\(\pi\)</span> that maximizes expected total reward. Policies can be <strong>deterministic</strong> or <strong>stochastic</strong>, <strong>state-dependent</strong> or <strong>history-dependent</strong>, <strong>stationary</strong> or <strong>time-dependent</strong>.</p></li>
<li><p>A policy induces a distribution over <strong>trajectories</strong>.</p></li>
<li><p>We can evaluate a policy by computing its <strong>value function</strong> <span class="math inline">\(V^\pi(s)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span> and following policy <span class="math inline">\(\pi\)</span>. We can also compute the <strong>state-action value function</strong> <span class="math inline">\(Q^\pi(s, a)\)</span>, which is the expected total reward starting from state <span class="math inline">\(s\)</span>, taking action <span class="math inline">\(a\)</span>, and then following policy <span class="math inline">\(\pi\)</span>. In the finite-horizon setting, these also depend on the timestep <span class="math inline">\(\hi\)</span>.</p></li>
<li><p>The <strong>Bellman consistency equation</strong> is an equation that the value function must satisfy. It can be used to solve for the value functions exactly. Thinking of the r.h.s. of this equation as an operator on value functions gives the <strong>Bellman operator</strong>.</p></li>
<li><p>In the finite-horizon setting, we can compute the optimal policy using <strong>dynamic programming</strong>.</p></li>
<li><p>In the infinite-horizon setting, we can compute the optimal policy using <strong>value iteration</strong> or <strong>policy iteration</strong>.</p></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./index.html" class="pagination-link" aria-label="Introduction">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Introduction</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./control.html" class="pagination-link" aria-label="Linear Quadratic Regulators">
        <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>