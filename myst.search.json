{"version":"1","records":[{"hierarchy":{"lvl1":"Appendix: Background"},"type":"lvl1","url":"/background","position":0},{"hierarchy":{"lvl1":"Appendix: Background"},"content":"","type":"content","url":"/background","position":1},{"hierarchy":{"lvl1":"Appendix: Background","lvl2":"O notation"},"type":"lvl2","url":"/background#o-notation","position":2},{"hierarchy":{"lvl1":"Appendix: Background","lvl2":"O notation"},"content":"Throughout this chapter and the rest of the book, we will describe the\nasymptotic behavior of a function using O notation.\n\nFor two functions f(t) and g(t), we say that f(t) \\le O(g(t)) if\nf is asymptotically upper bounded by g. Formally, this means that\nthere exists some constant C > 0 such that f(t) \\le C \\cdot g(t) for\nall t past some point t_0.\n\nWe say f(t) < o(g(t)) if asymptotically f grows strictly slower than\ng. Formally, this means that for any scalar C > 0, there exists\nsome t_0 such that f(t) \\le C \\cdot g(t) for all t > t_0.\nEquivalently, we say f(t) < o(g(t)) if\n\\lim_{t \\to \\infty} f(t)/g(t) = 0.\n\nf(t) = \\Theta(g(t)) means that f and g grow at the same rate\nasymptotically. That is, f(t) \\le O(g(t)) and g(t) \\le O(f(t)).\n\nFinally, we use f(t) \\ge \\Omega(g(t)) to mean that g(t) \\le O(f(t)),\nand f(t) > \\omega(g(t)) to mean that g(t) < o(f(t)).\n\nWe also use the notation \\tilde O(g(t)) to hide logarithmic factors.\nThat is, f(t) = \\tilde O(g(t)) if there exists some constant C such\nthat f(t) \\le C \\cdot g(t) \\cdot \\log^k(t) for some k and all t.\n\nOccasionally, we will also use O(f(t)) (or one of the other symbols)\nas shorthand to manipulate function classes. For example, we might write\nO(f(t)) + O(g(t)) = O(f(t) + g(t)) to mean that the sum of two\nfunctions in O(f(t)) and O(g(t)) is in O(f(t) + g(t)).","type":"content","url":"/background#o-notation","position":3},{"hierarchy":{"lvl1":"Appendix: Background","lvl2":"Python"},"type":"lvl2","url":"/background#python","position":4},{"hierarchy":{"lvl1":"Appendix: Background","lvl2":"Python"},"content":"","type":"content","url":"/background#python","position":5},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits"},"type":"lvl1","url":"/bandits","position":0},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits"},"content":"","type":"content","url":"/bandits","position":1},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Introduction"},"type":"lvl2","url":"/bandits#introduction","position":2},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Introduction"},"content":"The multi-armed bandits (MAB) setting is a simple setting for studying the basic challenges of sequential decision-making.\nIn this setting, an agent repeatedly chooses from a fixed set of actions, called arms, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period. \n| States | Actions | Rewards                             |\n| :----: | :-----: | :---------------------------------: |\n| None   | Finite  | $\\mathcal{A} \\to \\triangle([0, 1])$ |\n\n\nIn particular, we’ll spend a lot of time discussing the Exploration-Exploitation Tradeoff: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?\n\nOnline advertising\n\nLet’s suppose you, the agent, are an advertising company. You have K different ads that you can show to users; For concreteness, let’s suppose there’s just a single user. You receive 1 reward if the user clicks the ad, and 0 otherwise. Thus, the unknown reward distribution associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.\n\nClinical trials\n\nSuppose you’re a pharmaceutical company, and you’re testing a new drug. You have K different dosages of the drug that you can administer to patients. You receive 1 reward if the patient recovers, and 0 otherwise. Thus, the unknown reward distribution associated to each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.\n\nIn this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.\n\nfrom jaxtyping import Float, Array\nimport numpy as np\nimport latexify\nfrom typing import Callable, Union\nimport matplotlib.pyplot as plt\n\nimport solutions.bandits as solutions\n\nnp.random.seed(184)\n\ndef random_argmax(ary: Array) -> int:\n    \"\"\"Take an argmax and randomize between ties.\"\"\"\n    max_idx = np.flatnonzero(ary == ary.max())\n    return np.random.choice(max_idx).item()\n\n\n# used as decorator\nlatex = latexify.algorithmic(\n    prefixes={\"mab\"},\n    identifiers={\"arm\": \"a_t\", \"reward\": \"r\", \"means\": \"mu\"},\n    use_math_symbols=True,\n    escape_underscores=False,\n)\n\nNamesake\n\nThe name “multi-armed bandits” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and take money from the player.\n\nLet K denote the number of arms. We’ll label them 0, \\dots, K-1 and use superscripts to indicate the arm index; since we seldom need to raise a number to a power, this won’t cause much confusion. In this chapter, we’ll consider the Bernoulli bandit setting from the examples above, where arm k either returns reward 1 with probability \\mu^k or 0 otherwise. The agent gets to pull an arm T times in total. We can formalize the Bernoulli bandit in the following Python code:\n\nclass MAB:\n    \"\"\"\n    The Bernoulli multi-armed bandit environment.\n\n    :param means: the means (success probabilities) of the reward distributions for each arm\n    :param T: the time horizon\n    \"\"\"\n\n    def __init__(self, means: Float[Array, \" K\"], T: int):\n        assert all(0 <= p <= 1 for p in means)\n        self.means = means\n        self.T = T\n        self.K = self.means.size\n        self.best_arm = random_argmax(self.means)\n\n    def pull(self, k: int) -> int:\n        \"\"\"Pull the `k`-th arm and sample from its (Bernoulli) reward distribution.\"\"\"\n        reward = np.random.rand() < self.means[k].item()\n        return +reward\n\n\n\nmab = MAB(means=np.array([0.1, 0.8, 0.4]), T=100)\n\nIn pseudocode, the agent’s interaction with the MAB environment can be\ndescribed by the following process:\n\n@latex\ndef mab_loop(mab: MAB, agent: \"Agent\") -> int:\n    for t in range(mab.T):\n        arm = agent.choose_arm()  # in 0, ..., K-1\n        reward = mab.pull(arm)\n        agent.update_history(arm, reward)\n\n\nmab_loop\n\nThe Agent class stores the pull history and uses it to decide which arm to pull next. Since we are working with Bernoulli bandits, we can summarize the pull history concisely in a \\mathbb{N}^{K \\times 2} array.\n\nclass Agent:\n    def __init__(self, K: int, T: int):\n        \"\"\"The MAB agent that decides how to choose an arm given the past history.\"\"\"\n        self.K = K\n        self.T = T\n        self.rewards = []  # for plotting\n        self.choices = []\n        self.history = np.zeros((K, 2), dtype=int)\n\n    def choose_arm(self) -> int:\n        \"\"\"Choose an arm of the MAB. Algorithm-specific.\"\"\"\n        ...\n\n    def count(self) -> int:\n        \"\"\"The number of pulls made. Also the current step index.\"\"\"\n        return len(self.rewards)\n\n    def update_history(self, arm: int, reward: int):\n        self.rewards.append(reward)\n        self.choices.append(arm)\n        self.history[arm, reward] += 1\n\nWhat’s the optimal strategy for the agent, i.e. the one that achieves\nthe highest expected reward? Convince yourself that the agent should try\nto always pull the arm with the highest expected reward:\\mu^\\star := \\max_{k \\in [K]} \\mu^k.\n\nThe goal, then, can be rephrased as to minimize the regret, defined\nbelow:\n\nRegret\n\nThe agent’s regret after T timesteps is defined as\\text{Regret}_T := \\sum_{t=0}^{T-1} \\mu^\\star - \\mu^{a_t}.\n\ndef regret_per_step(mab: MAB, agent: Agent):\n    \"\"\"Get the difference from the average reward of the optimal arm. The sum of these is the regret.\"\"\"\n    return [mab.means[mab.best_arm] - mab.means[arm] for arm in agent.choices]\n\nNote that this depends on the true means of the pulled arms, not the actual\nobserved rewards.\nWe typically think of this as a random variable where\nthe randomness comes from the agent’s strategy (i.e. the sequence of\nactions a_0, \\dots, a_{T-1}).\n\nThroughout the chapter, we will try to upper bound the regret of various\nalgorithms in two different senses:\n\nUpper bound the expected regret, i.e. show\n\\E[\\text{Regret}_T] \\le M_T.\n\nFind a high-probability upper bound on the regret, i.e. show\n\\pr(\\text{Regret}_T \\le M_{T, \\delta}) \\ge 1-\\delta.\n\nNote that these two different approaches say very different things about the regret. The first approach says that the average regret is at most M_T. However, the agent might still achieve higher regret on many runs. The second approach says that, with high probability, the agent will achieve regret at most M_{T, \\delta}. However, it doesn’t say anything about the regret in the remaining δ fraction of runs, which might be arbitrarily high.\n\nWe’d like to achieve sublinear regret in expectation, i.e. \\E[\\text{Regret}_T] = o(T). That is, as we learn more about the environment, we’d like to be able to exploit that knowledge to take the optimal arm as often as possible.\n\nThe rest of the chapter comprises a series of increasingly sophisticated\nMAB algorithms.\n\ndef plot_strategy(mab: MAB, agent: Agent):\n    plt.figure(figsize=(10, 6))\n\n    # plot reward and cumulative regret\n    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label=\"reward\")\n    cum_regret = np.cumsum(regret_per_step(mab, agent))\n    plt.plot(np.arange(mab.T), cum_regret, label=\"cumulative regret\")\n\n    # draw colored circles for arm choices\n    colors = [\"red\", \"green\", \"blue\"]\n    color_array = [colors[k] for k in agent.choices]\n    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c=color_array, label=\"arm\")\n\n    # labels and title\n    plt.xlabel(\"timestep\")\n    plt.legend()\n    plt.title(f\"{agent.__class__.__name__} reward and regret\")\n    plt.show()\n\n","type":"content","url":"/bandits#introduction","position":3},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Pure exploration (random guessing)"},"type":"lvl2","url":"/bandits#pure-exploration-random-guessing","position":4},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Pure exploration (random guessing)"},"content":"A trivial strategy is to always choose arms at random (i.e. “pure\nexploration”).\n\nclass PureExploration(Agent):\n    def choose_arm(self):\n        \"\"\"Choose an arm uniformly at random.\"\"\"\n        return solutions.pure_exploration_choose_arm(self)\n\nNote that\\E_{a_t \\sim \\text{Unif}([K])}[\\mu^{a_t}] = \\bar \\mu = \\frac{1}{K} \\sum_{k=1}^K \\mu^k\n\nso the expected regret is simply\\begin{aligned}\n    \\E[\\text{Regret}_T] &= \\sum_{t=0}^{T-1} \\E[\\mu^\\star - \\mu^{a_t}] \\\\\n    &= T (\\mu^\\star - \\bar \\mu) > 0.\n\\end{aligned}\n\nThis scales as \\Theta(T), i.e. linear in the number of timesteps T. There’s no learning here: the agent doesn’t use any information about the environment to improve its strategy. You can see that the distribution over its arm choices always appears “(uniformly) random”.\n\nagent = PureExploration(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n","type":"content","url":"/bandits#pure-exploration-random-guessing","position":5},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Pure greedy"},"type":"lvl2","url":"/bandits#pure-greedy","position":6},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Pure greedy"},"content":"How might we improve on pure exploration? Instead, we could try each arm\nonce, and then commit to the one with the highest observed reward. We’ll\ncall this the pure greedy strategy.\n\nclass PureGreedy(Agent):\n    def choose_arm(self):\n        \"\"\"Choose the arm with the highest observed reward on its first pull.\"\"\"\n        return solutions.pure_greedy_choose_arm(self)\n\nNote we’ve used superscripts r^k during the exploration phase to\nindicate that we observe exactly one reward for each arm. Then we use\nsubscripts r_t during the exploitation phase to indicate that we\nobserve a sequence of rewards from the chosen greedy arm \\hat k.\n\nHow does the expected regret of this strategy compare to that of pure\nexploration? We’ll do a more general analysis in the following section.\nNow, for intuition, suppose there’s just K=2 arms, with Bernoulli\nreward distributions with means \\mu^0 > \\mu^1.\n\nLet’s let r^0 be the random reward from the first arm and r^1 be the\nrandom reward from the second. If r^0 > r^1, then we achieve zero\nregret. Otherwise, we achieve regret T(\\mu^0 - \\mu^1). Thus, the\nexpected regret is simply:\\begin{aligned}\n    \\E[\\text{Regret}_T] &= \\pr(r^0 < r^1) \\cdot T(\\mu^0 - \\mu^1) + c \\\\\n    &= (1 - \\mu^0) \\mu^1 \\cdot T(\\mu^0 - \\mu^1) + c\n\\end{aligned}\n\nWhich is still \\Theta(T), the same as pure exploration!\n\nagent = PureGreedy(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\nThe cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, if the greedy algorithm happens to get lucky on the first set of pulls, it may act entirely optimally for that episode! But its average regret is what measures its effectiveness.\n\n","type":"content","url":"/bandits#pure-greedy","position":7},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Explore-then-commit"},"type":"lvl2","url":"/bandits#etc","position":8},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Explore-then-commit"},"content":"We can improve the pure greedy algorithm as follows: let’s reduce the variance of the reward estimates by pulling each arm N_{\\text{explore}}> 1 times before committing. This is called the explore-then-commit strategy. Note that the “pure greedy” strategy above is just the special case where\nN_{\\text{explore}}= 1.\n\nclass ExploreThenCommit(Agent):\n    def __init__(self, K: int, T: int, N_explore: int):\n        super().__init__(K, T)\n        self.N_explore = N_explore\n\n    def choose_arm(self):\n        return solutions.etc_choose_arm(self)\n\n\n\nagent = ExploreThenCommit(mab.K, mab.T, mab.T // 15)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\nNotice that now, the graphs are much more consistent, and the algorithm finds the true optimal arm and sticks with it much more frequently. We would expect ETC to then have a better (i.e. lower) average regret. Can we prove this?\n\n","type":"content","url":"/bandits#etc","position":9},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"type":"lvl3","url":"/bandits#etc-regret-analysis","position":10},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"content":"Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up\ninto the exploration and exploitation phases.","type":"content","url":"/bandits#etc-regret-analysis","position":11},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl4":"Exploration phase.","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"type":"lvl4","url":"/bandits#exploration-phase","position":12},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl4":"Exploration phase.","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"content":"This phase takes N_{\\text{explore}}K timesteps. Since at each step we\nincur at most 1 regret, the total regret is at most\nN_{\\text{explore}}K.","type":"content","url":"/bandits#exploration-phase","position":13},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl4":"Exploitation phase.","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"type":"lvl4","url":"/bandits#exploitation-phase","position":14},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl4":"Exploitation phase.","lvl3":"ETC regret analysis","lvl2":"Explore-then-commit"},"content":"This will take a bit more effort. We’ll prove that for any total time T, we can choose N_{\\text{explore}} such that with arbitrarily high probability, the regret is sublinear.\n\nLet \\hat k denote the arm chosen after the exploration phase. We know the regret from the\nexploitation phase isT_{\\text{exploit}} (\\mu^\\star - \\mu^{\\hat k}) \\qquad \\text{where} \\qquad T_{\\text{exploit}} := T - N_{\\text{explore}}K.\n\nSo we’d like to bound \\mu^\\star - \\mu^{\\hat k} = o(1) (as a function\nof T) in order to achieve sublinear regret. How can we do this?\n\nLet’s define \\Delta^k = \\hat \\mu^k - \\mu^k to denote how far the mean\nestimate for arm k is from the true mean. How can we bound this\nquantity? We’ll use the following useful inequality for i.i.d. bounded\nrandom variables:\n\nHoeffding’s inequality\n\nLet X_0, \\dots, X_{n-1} be i.i.d. random variables with\nX_i \\in [0, 1] almost surely for each i \\in [n]. Then for any\n\\delta > 0,\\pr\\left( \\left| \\frac{1}{n} \\sum_{i=1}^n (X_i - \\E[X_i]) \\right| > \\sqrt{\\frac{\\ln(2/\\delta)}{2n}} \\right) \\le \\delta.\n\nThe proof of this inequality is beyond the scope of this book. See \n\nVershynin (2018) Chapter 2.2.\n\nWe can apply this directly to the rewards for a given arm k, since the rewards from that arm are i.i.d.:\\pr\\left(|\\Delta^k | > \\sqrt{\\frac{\\ln(2/\\delta)}{2N_{\\text{explore}}}} \\right) \\le \\delta.\n\nBut note that we can’t apply this to arm \\hat k directly since\n\\hat k is itself a random variable. Instead, we need to “uniform-ize”\nthis bound across all the arms, i.e. bound the error across all the\narms simultaneously, so that the resulting bound will apply no matter\nwhat \\hat k “crystallizes” to.\n\nThe union bound provides a simple way to do this:\n\nUnion bound\n\nConsider a set of events A_0, \\dots, A_{n-1}. Then\\pr(\\exists i \\in [n]. A_i) \\le \\sum_{i=0}^{n-1} \\pr(A_i).\n\nIn\nparticular, if \\pr(A_i) \\ge 1 - \\delta for each i \\in [n], we have\\pr(\\forall i \\in [n]. A_i) \\ge 1 - n \\delta.\n\nExercise: Prove the second statement above.\n\nApplying the union bound across the arms for the l.h.s. event of \n\n(3.8), we have\\begin{aligned}\n    \\pr\\left( \\forall k \\in [K], |\\Delta^k | \\le \\sqrt{\\frac{\\ln(2/\\delta)}{2N_{\\text{explore}}}} \\right) &\\ge 1-K\\delta\n\\end{aligned}\n\nThen to apply this bound to \\hat k in particular, we\ncan apply the useful trick of “adding zero”:\\begin{aligned}\n    \\mu^{k^\\star} - \\mu^{\\hat k} &= \\mu^{k^\\star} - \\mu^{\\hat k} + (\\hat \\mu^{k^\\star} - \\hat \\mu^{k^\\star}) + (\\hat \\mu^{\\hat k} - \\hat \\mu^{\\hat k}) \\\\\n    &= \\Delta^{\\hat k} - \\Delta^{k^*} + \\underbrace{(\\hat \\mu^{k^\\star} - \\hat \\mu^{\\hat k})}_{\\le 0 \\text{ by definition of } \\hat k} \\\\\n    &\\le 2 \\sqrt{\\frac{\\ln(2K/\\delta')}{2N_{\\text{explore}}}} \\text{ with probability at least } 1-\\delta'\n\\end{aligned}\n\nwhere we’ve set \\delta' = K\\delta. Putting this all\ntogether, we’ve shown that, with probability 1 - \\delta',\\text{Regret}_T \\le N_{\\text{explore}}K + T_{\\text{exploit}} \\cdot \\sqrt{\\frac{2\\ln(2K/\\delta')}{N_{\\text{explore}}}}.\n\nNote that it suffices for N_{\\text{explore}} to be on the order of\n\\sqrt{T} to achieve sublinear regret. In particular, we can find the\noptimal N_{\\text{explore}} by setting the derivative of the r.h.s. to\nzero:\\begin{aligned}\n    0 &= K - T_{\\text{exploit}} \\cdot \\frac{1}{2} \\sqrt{\\frac{2\\ln(2K/\\delta')}{N_{\\text{explore}}^3}} \\\\\n    N_{\\text{explore}}&= \\left( T_{\\text{exploit}} \\cdot \\frac{\\sqrt{\\ln(2K/\\delta')/2}}{K} \\right)^{2/3}\n\\end{aligned}\n\nPlugging this into the expression for the regret, we\nhave (still with probability 1-\\delta')\\begin{aligned}\n    \\text{Regret}_T &\\le 3 T^{2/3} \\sqrt[3]{K \\ln(2K/\\delta') / 2} \\\\\n    &= \\tilde{O}(T^{2/3} K^{1/3}).\n\\end{aligned}\n\nThe ETC algorithm is rather “abrupt” in that it switches from\nexploration to exploitation after a fixed number of timesteps. In\npractice, it’s often better to use a more gradual transition, which\nbrings us to the epsilon-greedy algorithm.\n\n","type":"content","url":"/bandits#exploitation-phase","position":15},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Epsilon-greedy"},"type":"lvl2","url":"/bandits#epsilon-greedy","position":16},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Epsilon-greedy"},"content":"Instead of doing all of the exploration and then all of the exploitation\nseparately – which additionally requires knowing the time horizon\nbeforehand – we can instead interleave exploration and exploitation by,\nat each timestep, choosing a random action with some probability. We\ncall this the epsilon-greedy algorithm.\n\nclass EpsilonGreedy(Agent):\n    def __init__(\n        self,\n        K: int,\n        T: int,\n        ε_array: Float[Array, \" T\"],\n    ):\n        super().__init__(K, T)\n        self.ε_array = ε_array\n\n    def choose_arm(self):\n        return solutions.epsilon_greedy_choose_arm(self)\n\n\n\nagent = EpsilonGreedy(mab.K, mab.T, np.full(mab.T, 0.1))\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\nNote that we let ε vary over time. In particular, we might want to gradually decrease ε as we learn more about the reward distributions and no longer need to spend time exploring.\n\nAttention\n\nWhat is the expected regret of the algorithm if we set ε to be a constant?\n\nIt turns out that setting \\epsilon_t = \\sqrt[3]{K \\ln(t)/t} also achieves a regret of \\tilde O(t^{2/3} K^{1/3}) (ignoring the logarithmic factors). (We will not prove this here.) TODO ADD PROOF CITATION\n\nIn ETC, we had to set N_{\\text{explore}} based on the total number of timesteps T. But the epsilon-greedy algorithm actually handles the exploration automatically: the regret rate holds for any t, and doesn’t depend on the final horizon T.\n\nBut the way these algorithms explore is rather naive: we’ve been exploring uniformly across all the arms. But what if we could be smarter about it, and explore more for arms that we’re less certain about?\n\n","type":"content","url":"/bandits#epsilon-greedy","position":17},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Upper Confidence Bound (UCB)"},"type":"lvl2","url":"/bandits#ucb","position":18},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Upper Confidence Bound (UCB)"},"content":"To quantify how certain we are about the mean of each arm, we’ll\ncompute confidence intervals for our estimators, and then choose the\narm with the highest upper confidence bound. This operates on the\nprinciple of the benefit of the doubt (i.e. optimism in the face of\nuncertainty): we’ll choose the arm that we’re most optimistic about.\n\nIn particular, for each arm k at time t, we’d like to compute some\nupper confidence bound M^k_t such that \\hat \\mu^k_t \\le M^k_t with\nhigh probability, and then choose a_t := \\arg \\max_{k \\in [K]} M^k_t.\nBut how should we compute M^k_t?\n\nIn \n\nSection 3.4.1, we were able to compute this bound\nusing Hoeffding’s inequality, which assumes that the number of samples\nis fixed. This was the case in ETC (where we pull each arm\nN_{\\text{explore}} times), but in UCB, the number of times we pull\neach arm depends on the agent’s actions, which in turn depend on the\nrandom rewards and are therefore stochastic. So we can’t use\nHoeffding’s inequality directly.\n\nInstead, we’ll apply the same trick we used in the ETC analysis: we’ll\nuse the union bound to compute a looser bound that holds\nuniformly across all timesteps and arms. Let’s introduce some notation\nto discuss this.\n\nLet N^k_t denote the (random) number of times arm k has been pulled\nwithin the first t timesteps, and \\hat \\mu^k_t denote the sample\naverage of those pulls. That is,\\begin{aligned}\n    N^k_t &:= \\sum_{\\tau=0}^{t-1} \\mathbf{1} \\{ a_\\tau = k \\} \\\\\n    \\hat \\mu^k_t &:= \\frac{1}{N^k_t} \\sum_{\\tau=0}^{t-1} \\mathbf{1} \\{ a_\\tau = k \\} r_\\tau.\n\\end{aligned}\n\nTo achieve the “fixed sample size” assumption, we’ll\nneed to shift our index from time to number of samples from each\narm. In particular, we’ll define \\tilde r^k_n to be the nth sample\nfrom arm k, and \\tilde \\mu^k_n to be the sample average of the first\nn samples from arm k. Then, for a fixed n, this satisfies the\n“fixed sample size” assumption, and we can apply Hoeffding’s inequality\nto get a bound on \\tilde \\mu^k_n.\n\nSo how can we extend our bound on \\tilde\\mu^k_n to \\hat \\mu^k_t?\nWell, we know N^k_t \\le t (where equality would be the case if and\nonly if we had pulled arm k every time). So we can apply the same\ntrick as last time, where we uniform-ize across all possible values of\nN^k_t:\\begin{aligned}\n    \\pr\\left( \\forall n \\le t, |\\tilde \\mu^k_n - \\mu^k | \\le \\sqrt{\\frac{\\ln(2/\\delta)}{2n}} \\right) &\\ge 1-t\\delta.\n\\end{aligned}\n\nIn particular, since N^k_t \\le t, and \\tilde \\mu^k_{N^k_t} = \\hat \\mu^k_t by definition, we have\\begin{aligned}\n    \\pr\\left( |\\hat \\mu^k_t - \\mu^k | \\le \\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}} \\right) &\\ge 1-\\delta' \\text{ where } \\delta' := t \\delta.\n\\end{aligned}\n\nThis bound would then suffice for applying the UCB algorithm! That is, the upper confidence bound for arm k would beM^k_t := \\hat \\mu^k_t + \\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}},\n\nwhere we can choose \\delta' depending on how tight we want the interval to be.\n\nA smaller \\delta' would give us a larger and higher-confidence interval, emphasizing the exploration term.\n\nA larger \\delta' would give a tighter and lower-confidence interval, prioritizing the current sample averages.\n\nWe can now use this to define the UCB algorithm.\n\nclass UCB(Agent):\n    def __init__(self, K: int, T: int, delta: float):\n        super().__init__(K, T)\n        self.delta = delta\n\n    def choose_arm(self):\n        return solutions.ucb_choose_arm(self)\n\nIntuitively, UCB prioritizes arms where:\n\n\\hat \\mu^k_t is large, i.e. the arm has a high sample average, and\nwe’d choose it for exploitation, and\n\n\\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}} is large, i.e. we’re still\nuncertain about the arm, and we’d choose it for exploration.\n\nAs desired, this explores in a smarter, adaptive way compared to the\nprevious algorithms. Does it achieve lower regret?\n\nagent = UCB(mab.K, mab.T, 0.9)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n","type":"content","url":"/bandits#ucb","position":19},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"UCB regret analysis","lvl2":"Upper Confidence Bound (UCB)"},"type":"lvl3","url":"/bandits#ucb-regret-analysis","position":20},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"UCB regret analysis","lvl2":"Upper Confidence Bound (UCB)"},"content":"First we’ll bound the regret incurred at each timestep. Then we’ll bound\nthe total regret across timesteps.\n\nFor the sake of analysis, we’ll use a slightly looser bound that applies\nacross the whole time horizon and across all arms. We’ll omit the\nderivation since it’s very similar to the above (walk through it\nyourself for practice).\\begin{aligned}\n    \\pr\\left(\\forall k \\le K, t < T. |\\hat \\mu^k_t - \\mu^k | \\le B^k_t \\right) &\\ge 1-\\delta'' \\\\\n    \\text{where} \\quad B^k_t &:= \\sqrt{\\frac{\\ln(2TK/\\delta'')}{2N^k_t}}.\n\\end{aligned}\n\nIntuitively, B^k_t denotes the width of the CI for arm k at time\nt. Then, assuming the above uniform bound holds (which occurs with\nprobability 1-\\delta''), we can bound the regret at each timestep as\nfollows:\\begin{aligned}\n    \\mu^\\star - \\mu^{a_t} &\\le \\hat \\mu^{k^*}_t + B_t^{k^*} - \\mu^{a_t} && \\text{applying UCB to arm } k^\\star \\\\\n    &\\le \\hat \\mu^{a_t}_t + B^{a_t}_t - \\mu^{a_t} && \\text{since UCB chooses } a_t = \\arg \\max_{k \\in [K]} \\hat \\mu^k_t + B_t^{k} \\\\\n    &\\le 2 B^{a_t}_t && \\text{since } \\hat \\mu^{a_t}_t - \\mu^{a_t} \\le B^{a_t}_t \\text{ by definition of } B^{a_t}_t \\\\\n\\end{aligned}\n\nSumming this across timesteps gives\\begin{aligned}\n    \\text{Regret}_T &\\le \\sum_{t=0}^{T-1} 2 B^{a_t}_t \\\\\n    &= \\sqrt{2\\ln(2TK/\\delta'')} \\sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} \\\\\n    \\sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} &= \\sum_{t=0}^{T-1} \\sum_{k=1}^K \\mathbf{1}\\{ a_t = k \\} (N^k_t)^{-1/2} \\\\\n    &= \\sum_{k=1}^K \\sum_{n=1}^{N_T^k} n^{-1/2} \\\\\n    &\\le K \\sum_{n=1}^T n^{-1/2} \\\\\n    \\sum_{n=1}^T n^{-1/2} &\\le 1 + \\int_1^T x^{-1/2} \\ \\mathrm{d}x \\\\\n    &= 1 + (2 \\sqrt{x})_1^T \\\\\n    &= 2 \\sqrt{T} - 1 \\\\\n    &\\le 2 \\sqrt{T} \\\\\n\\end{aligned}\n\nPutting everything together gives\\begin{aligned}\n    \\text{Regret}_T &\\le 2 K \\sqrt{2T \\ln(2TK/\\delta'')} && \\text{with probability } 1-\\delta'' \\\\\n    &= \\tilde O(K\\sqrt{T})\n\\end{aligned}\n\nIn fact, we can do a more sophisticated analysis to trim off a factor of \\sqrt{K}\nand show \\text{Regret}_T = \\tilde O(\\sqrt{TK}).\n\n","type":"content","url":"/bandits#ucb-regret-analysis","position":21},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"Lower bound on regret (intuition)","lvl2":"Upper Confidence Bound (UCB)"},"type":"lvl3","url":"/bandits#lower-bound-on-regret-intuition","position":22},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"Lower bound on regret (intuition)","lvl2":"Upper Confidence Bound (UCB)"},"content":"Is it possible to do better than \\Omega(\\sqrt{T}) in general? In fact,\nno! We can show that any algorithm must incur \\Omega(\\sqrt{T}) regret\nin the worst case. We won’t rigorously prove this here, but the\nintuition is as follows.\n\nThe Central Limit Theorem tells us that with T i.i.d. samples from\nsome distribution, we can only learn the mean of the distribution to\nwithin \\Omega(1/\\sqrt{T}) (the standard deviation). Then, since we get\nT samples spread out across the arms, we can only learn each arm’s\nmean to an even looser degree.\n\nThat is, if two arms have means that are within about 1/\\sqrt{T}, we\nwon’t be able to confidently tell them apart, and will sample them about\nequally. But then we’ll incur regret\\Omega((T/2) \\cdot (1/\\sqrt{T})) = \\Omega(\\sqrt{T}).\n\n","type":"content","url":"/bandits#lower-bound-on-regret-intuition","position":23},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Thompson sampling and Bayesian bandits"},"type":"lvl2","url":"/bandits#thompson-sampling","position":24},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Thompson sampling and Bayesian bandits"},"content":"So far, we’ve treated the parameters \\mu^0, \\dots, \\mu^{K-1} of the\nreward distributions as fixed. Instead, we can take a Bayesian\napproach where we treat them as random variables from some prior\ndistribution. Then, upon pulling an arm and observing a reward, we can\nsimply condition on this observation to exactly describe the\nposterior distribution over the parameters. This fully describes the\ninformation we gain about the parameters from observing the reward.\n\nFrom this Bayesian perspective, the Thompson sampling algorithm\nfollows naturally: just sample from the distribution of the optimal arm,\ngiven the observations!\n\nclass Distribution:\n    def sample(self) -> Float[Array, \" K\"]:\n        \"\"\"Sample a vector of means for the K arms.\"\"\"\n        ...\n\n    def update(self, arm: int, reward: float):\n        \"\"\"Condition on obtaining `reward` from the given arm.\"\"\"\n        ...\n\n\n\nclass ThompsonSampling(Agent):\n    def __init__(self, K: int, T: int, prior: Distribution):\n        super().__init__(K, T)\n        self.distribution = prior\n\n    def choose_arm(self):\n        means = self.distribution.sample()\n        return random_argmax(means)\n\n    def update_history(self, arm: int, reward: int):\n        super().update_history(arm, reward)\n        self.distribution.update(arm, reward)\n\nIn other words, we sample each arm proportionally to how likely we think\nit is to be optimal, given the observations so far. This strikes a good\nexploration-exploitation tradeoff: we explore more for arms that we’re\nless certain about, and exploit more for arms that we’re more certain\nabout. Thompson sampling is a simple yet powerful algorithm that\nachieves state-of-the-art performance in many settings.\n\nBayesian Bernoulli bandit\n\nWe’ve been working in the Bernoulli bandit setting, where arm k yields a reward of 1 with probability \\mu^k and no reward otherwise. The vector of success probabilities \\boldsymbol{\\mu} = (\\mu^1, \\dots, \\mu^K) thus describes the entire MAB.\n\nUnder the Bayesian perspective, we think of \\boldsymbol{\\mu} as a random vector drawn from some prior distribution \\pi(\\boldsymbol{\\mu}). For example, we might have π be the Uniform distribution over the unit hypercube [0, 1]^K, that is,\\pi(\\boldsymbol{\\mu}) = \\begin{cases}\n    1 & \\text{if } \\boldsymbol{\\mu}\\in [0, 1]^K \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\nIn this case, upon viewing some reward, we can exactly calculate the posterior distribution of \\boldsymbol{\\mu} using Bayes’s rule (i.e. the definition of conditional probability):\\begin{aligned}\n    \\pr(\\boldsymbol{\\mu} \\mid a_0, r_0) &\\propto \\pr(r_0 \\mid a_0, \\boldsymbol{\\mu}) \\pr(a_0 \\mid \\boldsymbol{\\mu}) \\pr(\\boldsymbol{\\mu}) \\\\\n    &\\propto (\\mu^{a_0})^{r_0} (1 - \\mu^{a_0})^{1-r_0}.\n\\end{aligned}\n\nThis is the PDF of the\n\\text{Beta}(1 + r_0, 1 + (1 - r_0)) distribution, which is a conjugate\nprior for the Bernoulli distribution. That is, if we start with a Beta\nprior on \\mu^k (note that \\text{Unif}([0, 1]) = \\text{Beta}(1, 1)),\nthen the posterior, after conditioning on samples from\n\\text{Bern}(\\mu^k), will also be Beta. This is a very convenient\nproperty, since it means we can simply update the parameters of the Beta\ndistribution upon observing a reward, rather than having to recompute\nthe entire posterior distribution from scratch.\n\nclass Beta(Distribution):\n    def __init__(self, K: int, alpha: int = 1, beta: int = 1):\n        self.alphas = np.full(K, alpha)\n        self.betas = np.full(K, beta)\n\n    def sample(self):\n        return np.random.beta(self.alphas, self.betas)\n\n    def update(self, arm: int, reward: int):\n        self.alphas[arm] += reward\n        self.betas[arm] += 1 - reward\n\n\n\nbeta_distribution = Beta(mab.K)\nagent = ThompsonSampling(mab.K, mab.T, beta_distribution)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\nIt turns out that asymptotically, Thompson sampling is optimal in the\nfollowing sense. \n\nLai & Robbins (1985) prove an\ninstance-dependent lower bound that says for any bandit algorithm,\\liminf_{T \\to \\infty} \\frac{\\E[N_T^k]}{\\ln(T)} \\ge \\frac{1}{\\text{KL}(\\mu^k \\parallel \\mu^\\star)}\n\nwhere\\text{KL}(\\mu^k \\parallel \\mu^\\star) = \\mu^k \\ln \\frac{\\mu^k}{\\mu^\\star} + (1 - \\mu^k) \\ln \\frac{1 - \\mu^k}{1 - \\mu^\\star}\n\nmeasures the Kullback-Leibler divergence from the Bernoulli\ndistribution with mean \\mu^k to the Bernoulli distribution with mean\n\\mu^\\star. It turns out that Thompson sampling achieves this lower\nbound with equality! That is, not only is the error rate optimal, but\nthe constant factor is optimal as well.\n\n","type":"content","url":"/bandits#thompson-sampling","position":25},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Contextual bandits"},"type":"lvl2","url":"/bandits#contextual-bandits","position":26},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Contextual bandits"},"content":"Note\n\nThis content is advanced material taught at the end of the course.\n\nIn the above MAB environment, the reward distributions of the arms\nremain constant. However, in many real-world settings, we might receive\nadditional information that affects these distributions. For example, in\nthe online advertising case where each arm corresponds to an ad we could\nshow the user, we might receive information about the user’s preferences\nthat changes how likely they are to click on a given ad. We can model\nsuch environments using contextual bandits.\n\nContextual bandit\n\nAt each timestep t, a new context\nx_t is drawn from some distribution \\nu_{\\text{x}}. The learner gets\nto observe the context, and choose an action a_t according to some\ncontext-dependent policy \\pi_t(x_t). Then, the learner observes the\nreward from the chosen arm r_t \\sim \\nu^{a_t}(x_t). The reward\ndistribution also depends on the context.\n\nAssuming our context is discrete, we can just perform the same\nalgorithms, treating each context-arm pair as its own arm. This gives us\nan enlarged MAB of K |\\mathcal{X}| arms.\n\nAttention\n\nWrite down the UCB algorithm for this enlarged MAB. That is, write an\nexpression for \\pi_t(x_t) = \\arg\\max_a \\dots.\n\nRecall that running UCB for T timesteps on an MAB with K arms\nachieves a regret bound of \\tilde{O}(\\sqrt{TK}). So in this problem,\nwe would achieve regret \\tilde{O}(\\sqrt{TK|\\mathcal{X}|}) in the\ncontextual MAB, which has a polynomial dependence on |\\mathcal{X}|.\nBut in a situation where we have large, or even infinitely many\ncontexts, e.g. in the case where our context is a continuous value, this\nbecomes intractable.\n\nNote that this “enlarged MAB” treats the different contexts as entirely\nunrelated to each other, while in practice, often contexts are related\nto each other in some way: for example, we might want to advertise\nsimilar products to users with similar preferences. How can we\nincorporate this structure into our solution?\n\n","type":"content","url":"/bandits#contextual-bandits","position":27},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"Linear contextual bandits","lvl2":"Contextual bandits"},"type":"lvl3","url":"/bandits#lin-ucb","position":28},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl3":"Linear contextual bandits","lvl2":"Contextual bandits"},"content":"We want to model the mean reward of arm k as a function of the\ncontext, i.e. \\mu^k(x). One simple model is the linear one:\n\\mu^k(x) = x^\\top \\theta^k, where x \\in \\mathcal{X} = \\mathbb{R}^d and\n\\theta^k \\in \\mathbb{R}^d describes a feature direction for arm k. Recall\nthat supervised learning gives us a way to estimate a conditional\nexpectation from samples: We learn a least squares estimator from the\ntimesteps where arm k was selected:\\hat \\theta_t^k = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\sum_{\\{ i \\in [t] : a_i = k \\}} (r_i - x_i^\\top \\theta)^2.\n\nThis has the closed-form solution known as the ordinary least squares\n(OLS) estimator:\\begin{aligned}\n    \\hat \\theta_t^k          & = (A_t^k)^{-1} \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i r_i \\\\\n    \\text{where} \\quad A_t^k & = \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i x_i^\\top.\n\\end{aligned}\n\nWe can now apply the UCB algorithm in this environment in order to\nbalance exploration of new arms and exploitation of arms that we\nbelieve to have high reward. But how should we construct the upper\nconfidence bound? Previously, we treated the pulls of an arm as i.i.d.\nsamples and used Hoeffding’s inequality to bound the distance of the\nsample mean, our estimator, from the true mean. However, now our\nestimator is not a sample mean, but rather the OLS estimator above \n\n(3.30). Instead, we’ll use Chebyshev’s\ninequality to construct an upper confidence bound.\n\nChebyshev’s inequality\n\nFor a random variable Y such that\n\\E Y = 0 and \\E Y^2 = \\sigma^2,|Y| \\le \\beta \\sigma \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\nSince the OLS estimator is known to be unbiased (try proving this\nyourself), we can apply Chebyshev’s inequality to\nx_t^\\top (\\hat \\theta_t^k - \\theta^k):\\begin{aligned}\n    x_t^\\top \\theta^k \\le x_t^\\top \\hat \\theta_t^k + \\beta \\sqrt{x_t^\\top (A_t^k)^{-1} x_t} \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\\end{aligned}\n\nAttention\n\nWe haven’t explained why x_t^\\top (A_t^k)^{-1} x_t is the correct\nexpression for the variance of x_t^\\top \\hat \\theta_t^k. This result\nfollows from some algebra on the definition of the OLS estimator \n\n(3.30).\n\nThe first term is exactly our predicted reward \\hat \\mu^k_t(x_t). To\ninterpret the second term, note thatx_t^\\top (A_t^k)^{-1} x_t = \\frac{1}{N_t^k} x_t^\\top (\\Sigma_t^k)^{-1} x_t,\n\nwhere\\Sigma_t^k = \\frac{1}{N_t^k} \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i x_i^\\top\n\nis the empirical covariance matrix of the contexts (assuming that the\ncontext has mean zero). That is, the learner is encouraged to choose\narms when x_t is not aligned with the data seen so far, or if arm\nk has not been explored much and so N_t^k is small.\n\nWe can now substitute these quantities into UCB to get the LinUCB\nalgorithm:\n\nclass LinUCBPseudocode(Agent):\n    def __init__(\n        self, K: int, T: int, D: int, lam: float, get_c: Callable[[int], float]\n    ):\n        super().__init__(K, T)\n        self.lam = lam\n        self.get_c = get_c\n        self.contexts = [None for _ in range(K)]\n        self.A = np.repeat(lam * np.eye(D)[...], K)\n        self.targets = np.zeros(K, D)\n        self.w = np.zeros(K, D)\n\n    def choose_arm(self, context: Float[Array, \" D\"]):\n        c = self.get_c(self.count)\n        scores = self.w @ context + c * np.sqrt(\n            context.T @ np.linalg.solve(self.A, context)\n        )\n        return random_argmax(scores)\n\n    def update_history(self, context: Float[Array, \" D\"], arm: int, reward: int):\n        self.A[arm] += np.outer(context, context)\n        self.targets[arm] += context * reward\n        self.w[arm] = np.linalg.solve(self.A[arm], self.targets[arm])\n\nAttention\n\nNote that the matrix A_t^k above might not be invertible. When does this occur? One way to address this is to include a \\lambda I regularization term to ensure that A_t^k is invertible. This is equivalent to solving a ridge regression problem instead of the unregularized least squares problem. Implement this solution. TODO SOLUTION CURRENTLY SHOWN\n\nc_t is similar to the \\log (2t/\\delta') term of UCB: It controls the\nwidth of the confidence interval. Here, we treat it as a tunable\nparameter, though in a theoretical analysis, it would depend on A_t^k\nand the probability δ with which the bound holds.\n\nUsing similar tools for UCB, we can also prove an \\tilde{O}(\\sqrt{T})\nregret bound. The full details of the analysis can be found in Section 3 of \n\nAgarwal et al. (2022).","type":"content","url":"/bandits#lin-ucb","position":29},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Summary"},"type":"lvl2","url":"/bandits#summary","position":30},{"hierarchy":{"lvl1":"3 Multi-Armed Bandits","lvl2":"Summary"},"content":"In this chapter,\nwe explored the multi-armed bandit setting for analyzing sequential decision-making in an unknown environment.","type":"content","url":"/bandits#summary","position":31},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators"},"type":"lvl1","url":"/control","position":0},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators"},"content":"","type":"content","url":"/control","position":1},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Introduction"},"type":"lvl2","url":"/control#introduction","position":2},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Introduction"},"content":"Up to this point, we have considered decision problems with finitely\nmany states and actions. However, in many applications, states and\nactions may take on continuous values. For example, consider autonomous\ndriving, controlling a robot’s joints, and automated manufacturing. How\ncan we teach computers to solve these kinds of problems? This is the\ntask of continuous control.\n\n\n\nFigure 2.1:Solving a Rubik’s Cube with a robot hand.\n\n\n\nFigure 2.2:Boston Dynamics’s Spot robot.\n\nAside from the change in the state and action spaces, the general\nproblem setup remains the same: we seek to construct an optimal policy\nthat outputs actions to solve the desired task. We will see that many\nkey ideas and algorithms, in particular dynamic programming algorithms,\ncarry over to this new setting.\n\nThis chapter introduces a fundamental tool to solve a simple class of\ncontinuous control problems: the linear quadratic regulator. We will\nthen extend this basic method to more complex settings.\n\nCartPole\n\nTry to balance a pencil on its point on a flat surface. It’s much more\ndifficult than it may first seem: the position of the pencil varies\ncontinuously, and the state transitions governing the system, i.e. the\nlaws of physics, are highly complex. This task is equivalent to the\nclassic control problem known as CartPole:\n\nThe state \\st \\in \\mathbb{R}^4 can be described by:\n\nthe position of the cart;\n\nthe velocity of the cart;\n\nthe angle of the pole;\n\nthe angular velocity of the pole.\n\nWe can control the cart by applying a horizontal force \\act \\in \\mathbb{R}.\n\nGoal: Stabilize the cart around an ideal state and action\n(\\st^\\star, \\act^\\star).","type":"content","url":"/control#introduction","position":3},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Optimal control"},"type":"lvl2","url":"/control#optimal-control","position":4},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Optimal control"},"content":"Recall that an MDP is defined by its state space \\mathcal{S}, action space\n\\mathcal{A}, state transitions P, reward function r, and discount factor\nγ or time horizon \\hor. These have equivalents in the control\nsetting:\n\nThe state and action spaces are continuous rather than finite.\nThat is, \\mathcal{S} \\subseteq \\mathbb{R}^{n_\\st} and \\mathcal{A} \\subseteq \\mathbb{R}^{n_\\act},\nwhere n_\\st and n_\\act are the corresponding dimensions of these\nspaces, i.e. the number of coordinates to specify a single state or\naction respectively.\n\nWe call the state transitions the dynamics of the system. In the\nmost general case, these might change across timesteps and also\ninclude some stochastic noise w_\\hi at each timestep. We\ndenote these dynamics as the function f_\\hi such that\n\\st_{\\hi+1} = f_\\hi(\\st_\\hi, \\act_\\hi, w_\\hi). Of course, we can\nsimplify to cases where the dynamics are deterministic/noise-free\n(no w_\\hi term) and/or time-homogeneous (the same function f\nacross timesteps).\n\nInstead of maximizing the reward function, we seek to minimize the\ncost function c_\\hi: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}. Often, the cost\nfunction describes how far away we are from a target\nstate-action pair (\\st^\\star, \\act^\\star). An important special\ncase is when the cost is time-homogeneous; that is, it remains the\nsame function c at each timestep h.\n\nWe seek to minimize the undiscounted cost within a finite time\nhorizon \\hor. Note that we end an episode at the final state\n\\st_\\hor -- there is no \\act_\\hor, and so we denote the cost for\nthe final state as c_\\hor(\\st_\\hor).\n\nWith all of these components, we can now formulate the optimal control\nproblem: compute a policy to minimize the expected undiscounted cost\nover \\hor timesteps. In this chapter, we will only consider\ndeterministic, time-dependent policies\n\\pi = (\\pi_0, \\dots, \\pi_{H-1}) where \\pi_h : \\mathcal{S} \\to \\mathcal{A} for each\n\\hi \\in [\\hor].\n\nGeneral optimal control problem\\begin{split}\n    \\min_{\\pi_0, \\dots, \\pi_{\\hor-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\E \\left[\n        \\left( \\sum_{\\hi=0}^{\\hor-1} c_\\hi(\\st_\\hi, \\act_\\hi) \\right) + c_\\hor(\\st_\\hor)\n        \\right] \\\\\n    \\text{where} \\quad & \\st_{\\hi+1} = f_\\hi(\\st_\\hi, \\act_\\hi, w_\\hi), \\\\\n    & \\act_\\hi = \\pi_\\hi(\\st_\\hi) \\\\\n    & \\st_0 \\sim \\mu_0 \\\\\n    & w_\\hi \\sim \\text{noise}\n\\end{split}","type":"content","url":"/control#optimal-control","position":5},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"A first attempt: Discretization","lvl2":"Optimal control"},"type":"lvl3","url":"/control#a-first-attempt-discretization","position":6},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"A first attempt: Discretization","lvl2":"Optimal control"},"content":"Can we solve this problem using tools from the finite MDP setting? If\n\\mathcal{S} and \\mathcal{A} were finite, then we’d be able to work backwards using the DP algorithm for computing the optimal policy in an MDP (\n\nDefinition 1.11).\nThis inspires us to try discretizing the\nproblem.\n\nSuppose \\mathcal{S} and \\mathcal{A} are bounded, that is,\n\\max_{\\st \\in \\mathcal{S}} \\|\\st\\| \\le B_\\st and\n\\max_{\\act \\in \\mathcal{A}} \\|\\act\\| \\le B_\\act. To make \\mathcal{S} and \\mathcal{A} finite,\nlet’s choose some small positive ε, and simply round each\ncoordinate to the nearest multiple of ε. For example, if\n\\epsilon = 0.01, then we round each element of \\st and \\act to two\ndecimal spaces.\n\nHowever, the discretized \\widetilde{\\mathcal{S}} and \\widetilde{\\mathcal{A}} may be finite, but\nthey may be infeasibly large: we must divide each dimension into\nintervals of length \\varepsilon, resulting in\n|\\widetilde{\\mathcal{S}}| = (B_\\st/\\varepsilon)^{n_\\st} and\n|\\widetilde{\\mathcal{A}}| = (B_\\act/\\varepsilon)^{n_\\act}. To get a sense of how\nquickly this grows, consider \\varepsilon = 0.01, n_\\st = n_\\act = 10.\nThen the number of elements in the transition matrix would be\n|\\widetilde{\\mathcal{S}}|^2 |\\widetilde{\\mathcal{A}}| = (100^{10})^2 (100^{10}) = 10^{60}! (That’s\na trillion trillion trillion trillion trillion.)\n\nWhat properties of the problem could we instead make use of? Note that\nby discretizing the state and action spaces, we implicitly assumed that\nrounding each state or action vector by some tiny amount \\varepsilon\nwouldn’t change the behavior of the system by much; namely, that the\ncost and dynamics were relatively continuous. Can we use this\ncontinuous structure in other ways? This leads us to the linear\nquadratic regulator.","type":"content","url":"/control#a-first-attempt-discretization","position":7},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"The Linear Quadratic Regulator"},"type":"lvl2","url":"/control#lqr","position":8},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"The Linear Quadratic Regulator"},"content":"The optimal control problem \n\nDefinition 2.1 seems highly complex in general. Is there a relevant simplification that we can analyze?\nThe linear quadratic regulator (LQR) is a solvable case and a fundamental tool in control theory.\n\nThe linear quadratic regulator\n\nThe LQR problem is a special case of the \n\nGeneral optimal control problem with linear dynamics and an upward-curved quadratic cost function.\nSolving the LQR problem will additionally enable us to locally approximate more complex setups using Taylor approximations.\n\nLinear, time-homogeneous dynamics: for each timestep \\hi \\in [\\hor],\\begin{aligned}\n    \\st_{\\hi+1} &= f(\\st_\\hi, \\act_\\hi, w_\\hi) = A \\st_\\hi + B \\act_\\hi + w_\\hi \\\\\n    \\text{where } w_\\hi &\\sim \\mathcal{N}(0, \\sigma^2 I).\n\\end{aligned}\n\nHere, w_\\hi is a spherical Gaussian noise term that makes the dynamics random.\nSetting \\sigma = 0 gives us deterministic state transitions.\nWe will find that the optimal policy actually does not depend on the noise, although the optimal value function and Q-function do.\n\nUpward-curved quadratic, time-homogeneous cost function:c(\\st_\\hi, \\act_\\hi) = \\begin{cases}\n    \\st_\\hi^\\top Q \\st_\\hi + \\act_\\hi^\\top R \\act_\\hi & \\hi < \\hor \\\\\n    \\st_\\hi^\\top Q \\st_\\hi                            & \\hi = \\hor\n\\end{cases}.\n\nThis cost function attempts to stabilize the state and action about (s^\\star, a^\\star) = (0, 0).\nWe require Q \\in \\R^{n_\\st \\times n_\\st} and R \\in \\R^{n_\\act \\times n_\\act} to both be positive definite matrices so that c has a well-defined unique minimum.\nWe can furthermore assume without loss of generality that they are both symmetric (see exercise below).\n\nThis results in the LQR optimization problem:\\begin{aligned}\n        \\min_{\\pi_0, \\dots, \\pi_{\\hor-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\E \\left[ \\left( \\sum_{\\hi=0}^{\\hor-1} \\st_\\hi^\\top Q \\st_\\hi + \\act_\\hi^\\top R \\act_\\hi \\right) + \\st_\\hor^\\top Q \\st_\\hor \\right] \\\\\n        \\textrm{where} \\quad                                & \\st_{\\hi+1} = A \\st_\\hi + B \\act_\\hi + w_\\hi                                                                                        \\\\\n                                                            & \\act_\\hi = \\pi_\\hi (\\st_\\hi)                                                                                                        \\\\\n                                                            & w_\\hi \\sim \\mathcal{N}(0, \\sigma^2 I)                                                                                               \\\\\n                                                            & \\st_0 \\sim \\mu_0.\n\\end{aligned}\n\nExercise\n\nHere we’ll show that we don’t lose generality by assuming that Q and R are symmetric.\nShow that replacing Q and R with (Q + Q^\\top) / 2 and (R + R^\\top) / 2 (which are symmetric) yields the same cost function.\n\nWe will henceforth abbreviate “symmetric positive definite” as s.p.d.\nand “positive definite” as p.d.\n\nIt will be helpful to reintroduce the value function notation for a policy to denote the average cost it incurs.\nThese will be instrumental in constructing the optimal policy via dynamic programming,\nas we did in \n\nSection 1.3.2 for MDPs.\n\nValue functions for LQR\n\nGiven a policy \\mathbf{\\pi} = (\\pi_0, \\dots, \\pi_{\\hor-1}),\nwe can define its value function V^\\pi_\\hi : \\mathcal{S} \\to \\mathbb{R} at time \\hi \\in [\\hor] as the average cost-to-go incurred by that policy:\\begin{split}\n    V^\\pi_\\hi (\\st) &= \\E \\left[ \\left( \\sum_{i=\\hi}^{\\hor-1} c(\\st_i, \\act_i) \\right) + c(\\st_\\hor) \\mid \\st_\\hi = \\st,  \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi \\le i < H \\right] \\\\\n    &= \\E \\left[ \\left( \\sum_{i=\\hi}^{\\hor-1} \\st_i^\\top Q \\st_i + \\act_i^\\top R \\act_i \\right) + \\st_\\hor^\\top Q \\st_\\hor \\mid \\st_\\hi = \\st, \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi \\le i < H \\right] \\\\\n\\end{split}\n\nThe Q-function additionally conditions on the first action we take:\\begin{split}\n    Q^\\pi_\\hi (\\st, \\act) &= \\E \\bigg[ \\left( \\sum_{i=\\hi}^{\\hor-1} c(\\st_i, \\act_i) \\right) + c(\\st_\\hor) \\\\\n        &\\qquad\\qquad \\mid  (\\st_\\hi, \\act_\\hi) = (\\st, \\act), \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi \\le i < H \\bigg] \\\\\n    &= \\E \\bigg[ \\left( \\sum_{i=\\hi}^{\\hor-1} \\st_i^\\top Q \\st_i + \\act_i^\\top R \\act_i \\right) + \\st_\\hor^\\top Q \\st_\\hor \\\\\n        &\\qquad\\qquad \\mid (\\st_\\hi, \\act_\\hi) = (\\st, \\act), \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi \\le i < H \\bigg] \\\\\n\\end{split}\n\nNote that since we use cost instead of reward,\nthe best policies are the ones with smaller values of the value function.","type":"content","url":"/control#lqr","position":9},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Optimality and the Riccati Equation"},"type":"lvl2","url":"/control#optimal-lqr","position":10},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Optimality and the Riccati Equation"},"content":"In this section,\nwe’ll compute the optimal value function V^\\star_h,\nQ-function Q^\\star_h,\nand policy \\pi^\\star_h in \n\nthe linear quadratic regulator using dynamic programming\nin a very similar way to the DP algorithms \n\nin the MDP setting.\nRecall the definition of the optimal value function:\n\nOptimal value function in LQR\n\nThe optimal value function is the one that,\nat any time and in any state,\nachieves minimum cost across all policies:\\begin{split}\n    V^\\star_\\hi(\\st) &= \\min_{\\pi_\\hi, \\dots, \\pi_{\\hor-1}} V^\\pi_\\hi(\\st) \\\\\n    &= \\min_{\\pi_{\\hi}, \\dots, \\pi_{\\hor-1}} \\E \\bigg[ \\left( \\sum_{i=\\hi}^{\\hor-1} \\st_\\hi^\\top Q \\st_\\hi + \\act_\\hi^\\top R \\act_\\hi \\right) + \\st_\\hor^\\top Q \\st_\\hor \\\\\n        &\\hspace{8em} \\mid \\st_\\hi = \\st, \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi \\le i < H \\bigg] \\\\\n\\end{split}\n\nThe optimal Q-function is defined similarly,\nconditioned on the starting action as well:\\begin{split}\n    Q^\\star_\\hi(\\st, \\act) &= \\min_{\\pi_\\hi, \\dots, \\pi_{\\hor-1}} Q^\\pi_\\hi(\\st, \\act) \\\\\n    &= \\min_{\\pi_{\\hi}, \\dots, \\pi_{\\hor-1}} \\E \\bigg[ \\left( \\sum_{i=\\hi}^{\\hor-1} \\st_\\hi^\\top Q \\st_\\hi + \\act_\\hi^\\top R \\act_\\hi \\right) + \\st_\\hor^\\top Q \\st_\\hor \\\\\n        &\\hspace{8em} \\mid \\st_\\hi = \\st, \\act_\\hi = \\act, \\act_i = \\pi_i(\\st_i) \\quad \\forall \\hi < i < H \\bigg] \\\\\n\\end{split}\n\nBoth of the definitions above assume deterministic policies. Otherwise we would have to take an expectation over actions drawn from the policy, i.e. \\act_\\hi \\sim \\pi_\\hi (\\st_\\hi).\n\nWe will prove the striking fact that the solution has very simple structure:\nV_h^\\star and Q^\\star_h are upward-curved quadratics\nand \\pi_h^\\star is linear and furthermore does not depend on the noise!\n\nOptimal value function in LQR is an upward-curved quadratic\n\nAt each timestep \\hi \\in [\\hor],V^\\star_\\hi(\\st) = \\st^\\top P_\\hi \\st + p_\\hi\n\nfor some s.p.d. matrix P_\\hi \\in \\mathbb{R}^{n_\\st \\times n_\\st} and scalar\np_\\hi \\in \\mathbb{R}.\n\nOptimal policy in LQR is linear\n\nAt each timestep \\hi \\in [\\hor],\\pi^\\star_\\hi (\\st) = - K_\\hi \\st\n\nfor some K_\\hi \\in \\mathbb{R}^{n_\\act \\times n_\\st}.\n(The negative is due to convention.)\n\nThe construction (and inductive proof) proceeds similarly to the one \n\nin the MDP setting.\n\nWe’ll compute V_\\hor^\\star (at the end of the horizon) as our base case.\n\nThen we’ll work step-by-step backwards in time, using V_{\\hi+1}^\\star to compute Q_\\hi^\\star, \\pi_{\\hi}^\\star, and V_\\hi^\\star. TODO insert reference for proof by induction \n\nBase case:\nAt the final timestep,\nthere are no possible actions to take,\nand so V^\\star_\\hor(\\st) = c(\\st) = \\st^\\top Q \\st.\nThus V_\\hor^\\star(\\st) = \\st^\\top P_\\hor \\st + p_\\hor\nwhere P_\\hor = Q and p_\\hor = 0.\n\nInductive hypothesis:\nWe seek to show that the inductive step holds for both theorems:\nIf V^\\star_{\\hi+1}(\\st) is an upward-curved quadratic,\nthen V^\\star_\\hi(\\st) must also be an upward-curved quadratic,\nand \\pi^\\star_\\hi(\\st) must be linear.\nWe’ll break this down into the following steps:\n\nShow that Q^\\star_\\hi(\\st, \\act) is an upward-curved quadratic (in both\n\\st and \\act).\n\nDerive the optimal policy\n\\pi^\\star_\\hi(\\st) = \\arg \\min_\\act Q^\\star_\\hi(\\st, \\act) and show\nthat it’s linear.\n\nShow that V^\\star_\\hi(\\st) is an upward-curved quadratic.\n\nWe first assume the inductive hypothesis that our theorems are true at\ntime \\hi+1. That is,V^\\star_{\\hi+1}(\\st) = \\st^\\top P_{\\hi+1} \\st + p_{\\hi+1} \\quad \\forall \\st \\in \\mathcal{S}.\n\nQ^\\star_\\hi(\\st, \\act) is an upward-curved quadratic\n\nLet us decompose Q^\\star_\\hi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\ninto the immediate reward plus the expected cost-to-go:Q^\\star_\\hi(\\st, \\act) = c(\\st, \\act) + \\E_{\\st' \\sim f(\\st, \\act, w_{\\hi+1})} [V^\\star_{\\hi+1}(\\st')].\n\nRecall c(\\st, \\act) := \\st^\\top Q \\st + \\act^\\top R \\act.\nLet’s consider the expectation over the next timestep.\nThe only randomness in the dynamics comes from the noise\nw_{\\hi+1} \\sim \\mathcal{N}(0, \\sigma^2 I),\nso we can expand the expectation as:\\begin{aligned}\n            & \\E_{\\st'} [V^\\star_{\\hi+1}(\\st')]                                                                                                         \\\\\n    {} = {} & \\E_{w_{\\hi+1}} [V^\\star_{\\hi+1}(A \\st + B \\act + w_{\\hi+1})]                                             &  & \\text{definition of } f     \\\\\n    {} = {} & \\E_{w_{\\hi+1}} [ (A \\st + B \\act + w_{\\hi+1})^\\top P_{\\hi+1} (A \\st + B \\act + w_{\\hi+1}) + p_{\\hi+1} ]. &  & \\text{inductive hypothesis}\n\\end{aligned}\n\nSumming and combining like terms, we get\\begin{aligned}\n    Q^\\star_\\hi(\\st, \\act) & = \\st^\\top Q \\st + \\act^\\top R \\act + \\E_{w_{\\hi+1}} [(A \\st + B \\act + w_{\\hi+1})^\\top P_{\\hi+1} (A \\st + B \\act + w_{\\hi+1}) + p_{\\hi+1}] \\\\\n                           & = \\st^\\top (Q + A^\\top P_{\\hi+1} A)\\st + \\act^\\top (R + B^\\top P_{\\hi+1} B) \\act + 2 \\st^\\top A^\\top P_{\\hi+1} B \\act                       \\\\\n                           & \\qquad + \\E_{w_{\\hi+1}} [w_{\\hi+1}^\\top P_{\\hi+1} w_{\\hi+1}] + p_{\\hi+1}.\n\\end{aligned}\n\nNote that the terms that are linear in w_\\hi have mean\nzero and vanish. Now consider the remaining expectation over the noise.\nBy expanding out the product and using linearity of expectation, we can\nwrite this out as\\begin{aligned}\n    \\E_{w_{\\hi+1}} [w_{\\hi+1}^\\top P_{\\hi+1} w_{\\hi+1}] & = \\sum_{i=1}^d \\sum_{j=1}^d (P_{\\hi+1})_{ij} \\E_{w_{\\hi+1}} [(w_{\\hi+1})_i (w_{\\hi+1})_j] \\\\\n    & = \\sigma^2 \\mathrm{Tr}(P_{\\hi + 1})\n\\end{aligned}\n\nQuadratic forms\n\nWhen solving quadratic forms, i.e. expressions of the form x^\\top A x,\nit’s often helpful to consider the terms on the diagonal (i = j) separately from those off the diagonal.\n\nIn this case, the expectation of each diagonal term becomes(P_{\\hi+1})_{ii} \\E (w_{\\hi+1})_i^2 = \\sigma^2 (P_{\\hi+1})_{ii}.\n\nOff the diagonal, since the elements of w_{\\hi+1} are independent, the\nexpectation factors, and since each element has mean zero, the term\nvanishes:(P_{\\hi+1})_{ij} \\E [(w_{\\hi+1})_i] \\E [(w_{\\hi+1})_j] = 0.\n\nThus,\nthe only terms left are the ones on the diagonal,\nso the sum of these can be expressed as the trace of \\sigma^2 P_{\\hi+1}:\\E_{w_{\\hi+1}} [w_{\\hi+1}^\\top P_{\\hi+1} w_{\\hi+1}] = \\sigma^2 \\mathrm{Tr}(P_{\\hi+1}).\n\nSubstituting this back into the expression for Q^\\star_\\hi, we have:\\begin{aligned}\n    Q^\\star_\\hi(\\st, \\act) & = \\st^\\top (Q + A^\\top P_{\\hi+1} A) \\st + \\act^\\top (R + B^\\top P_{\\hi+1} B) \\act\n    + 2\\st^\\top A^\\top P_{\\hi+1} B \\act                                                                        \\\\\n                            & \\qquad + \\sigma^2 \\mathrm{Tr}(P_{\\hi+1}) + p_{\\hi+1}.\n\\end{aligned}\n\nAs we hoped, this expression is quadratic in \\st and \\act.\nFurthermore,\nwe’d like to show that it also curves upwards\nwith respect to \\act\nso that its minimum with respect to \\act is well-defined.\nWe can do this by noting that the Hessian matrix of second derivatives is positive definite:\\nabla_{\\act \\act} Q_\\hi^\\star(\\st, \\act) = R + B^\\top P_{\\hi+1} B\n\nSince R is s.p.d. (by \n\nthe LQR definition),\nand P_{\\hi+1} is s.p.d. (by the inductive hypothesis),\nthis sum must also be s.p.d.,\nand so Q^\\star_\\hi is indeed an upward-curved quadratic with respect to \\act.\n(If this isn’t clear, try proving it as an exercise.)\nThe proof of its upward curvature with respect to \\st is equivalent.\n\n\\pi^\\star_\\hi is linear\n\nSince Q^\\star_\\hi is an upward-curved quadratic,\nfinding its minimum over \\act is easy:\nwe simply set the gradient with respect to \\act equal to zero and solve for \\act.\nFirst, we calculate the gradient:\\begin{aligned}\n    \\nabla_\\act Q^\\star_\\hi(\\st, \\act) & = \\nabla_\\act [ \\act^\\top (R + B^\\top P_{\\hi+1} B) \\act + 2 \\st^\\top A^\\top P_{\\hi+1} B \\act ] \\\\\n                                       & = 2 (R + B^\\top P_{\\hi+1} B) \\act + 2 (\\st^\\top A^\\top P_{\\hi+1} B)^\\top\n\\end{aligned}\n\nSetting this to zero, we get\\begin{aligned}\n    0                  & = (R + B^\\top P_{\\hi+1} B) \\pi^\\star_\\hi(\\st) + B^\\top P_{\\hi+1} A \\st \\nonumber \\\\\n    \\pi^\\star_\\hi(\\st) & = (R + B^\\top P_{\\hi+1} B)^{-1} (-B^\\top P_{\\hi+1} A \\st) \\nonumber              \\\\\n                       & = - K_\\hi \\st,\n\\end{aligned}\n\nwhereK_\\hi = (R + B^\\top P_{\\hi+1} B)^{-1} B^\\top P_{\\hi+1} A.\n\nNote that this optimal policy doesn’t depend on the starting distribution \\mu_0.\nIt’s also fully deterministic and isn’t affected by the noise terms\nw_0, \\dots, w_{\\hor-1}.\n\nV^\\star_\\hi(\\st) is an upward-curved quadratic\n\nUsing the identity V^\\star_\\hi(\\st) = Q^\\star_\\hi(\\st, \\pi^\\star(\\st)), we have:\\begin{aligned}\n    V^\\star_\\hi(\\st) & = Q^\\star_\\hi(\\st, \\pi^\\star(\\st))                                                                \\\\\n                     & = \\st^\\top (Q + A^\\top P_{\\hi+1} A) \\st + (-K_\\hi \\st)^\\top (R + B^\\top P_{\\hi+1} B) (-K_\\hi \\st)\n    + 2\\st^\\top A^\\top P_{\\hi+1} B (-K_\\hi \\st)                                                                          \\\\\n                     & \\qquad + \\mathrm{Tr}(\\sigma^2 P_{\\hi+1}) + p_{\\hi+1}\n\\end{aligned}\n\nNote that with respect to \\st,\nthis is the sum of a quadratic term and a constant,\nwhich is exactly what we were aiming for!\nThe scalar term is clearlyp_\\hi = \\mathrm{Tr}(\\sigma^2 P_{\\hi+1}) + p_{\\hi+1}.\n\nWe can simplify the quadratic term by substituting in K_\\hi from \n\n(2.23).\nNotice that when we do this,\nthe (R+B^\\top P_{\\hi+1} B) term in the expression is cancelled out by its inverse,\nand the remaining terms combine to give the Riccati equation:\n\nRiccati equationP_\\hi = Q + A^\\top P_{\\hi+1} A - A^\\top P_{\\hi+1} B (R + B^\\top P_{\\hi+1} B)^{-1} B^\\top P_{\\hi+1} A.\n\nThere are several nice properties to note about the Riccati equation:\n\nIt’s defined recursively.\nGiven the dynamics defined by A and B, and the state cost matrix Q,\nwe can recursively calculate P_\\hi across all timesteps starting from P_\\hor = Q.\n\nP_\\hi often appears in calculations surrounding optimality,\nsuch as V^\\star_\\hi, Q^\\star_\\hi, and \\pi^\\star_\\hi.\n\nTogether with the dynamics given by A and B,\nand the action coefficients R in the lost function,\nit fully defines the optimal policy \n\nLemma 2.2.\n\nIt remains to prove that V^\\star_\\hi curves upwards, that is, that P_\\hi is s.p.d. We will use the following fact about Schur complements:\n\nPositive definiteness of Schur complements\n\nLetD = \\begin{pmatrix}\nA & B \\\\\nB^\\top & C\n\\end{pmatrix}\n\nbe a symmetric (m+n) \\times (m+n) block matrix,\nwhere A \\in \\R^{m \\times m}, B \\in \\R^{m \\times n}, C \\in \\R^{n \\times n}.\nThe Schur complement of A is denotedD/A = C - B^\\top A^{-1} B.\n\nSchur complements have various uses in linear algebra and numerical computation.\n\nA useful fact for us is that\nif A is positive definite,\nthen D is positive semidefinite\nif and only if D/A is positive semidefinite.\n\nLet P denote P_{\\hi + 1} for brevity.\nWe already know Q is p.d.,\nso it suffices to show thatS = P - P B (R + B^\\top P B)^{-1} B^\\top P\n\nis p.s.d. (positive semidefinite),\nsince left- and right- multiplying by A^\\top and A respectively\npreserves p.s.d.\nWe note that S is the Schur complement D/(R + B^\\top P B), whereD = \\begin{pmatrix}\nR + B^\\top P B & B^\\top P \\\\\nP B & P\n\\end{pmatrix}.\n\nThus we must show that D is p.s.d..\nThis can be seen by computing\\begin{aligned}\n\\begin{pmatrix}\ny^\\top & z^\\top\n\\end{pmatrix}\nD\n\\begin{pmatrix}\ny \\\\ z\n\\end{pmatrix}\n&= y^\\top R y + y^\\top B^\\top P B y + 2 y^\\top B^\\top P z + z^\\top P z \\\\\n&= y^\\top R y + (By + z)^\\top P (By + z) \\\\\n&> 0.\n\\end{aligned}\n\nSince R + B^\\top P B is p.d. and D is p.s.d.,\nthen S = D / (R + B^\\top P B) must be p.s.d.,\nand P_\\hi = Q + A S A^\\top must be p.d.\n\nNow we’ve shown that V^\\star_\\hi(\\st) = \\st^\\top P_\\hi \\st + p_\\hi,\nwhere P_\\hi is s.p.d.,\nproving the inductive hypothesis and completing the proof of \n\nTheorem 2.2 and \n\nTheorem 2.1.\n\nIn summary, we just demonstrated that at each timestep \\hi \\in [\\hor],\nthe optimal value function V^\\star_\\hi\nand optimal Q-function Q^\\star_\\hi are both upward-curved quadratics\nand the optimal policy \\pi^\\star_\\hi is linear.\nWe also showed that all of these quantities can be calculated\nusing a sequence of s.p.d. matrices P_0, \\dots, P_H\nthat can be defined recursively using the Riccati equation \n\nDefinition 2.5.\n\nBefore we move on to some extensions of LQR, let’s consider how the\nstate at time \\hi behaves when we act according to this optimal\npolicy.","type":"content","url":"/control#optimal-lqr","position":11},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Expected state at time \\hi","lvl2":"Optimality and the Riccati Equation"},"type":"lvl3","url":"/control#expected-state-at-time-hi","position":12},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Expected state at time \\hi","lvl2":"Optimality and the Riccati Equation"},"content":"How can we compute the expected state at time \\hi when acting\naccording to the optimal policy? Let’s first express \\st_\\hi in a\ncleaner way in terms of the history. Note that having linear dynamics\nmakes it easy to expand terms backwards in time:\\begin{aligned}\n    \\st_\\hi & = A \\st_{\\hi-1} + B \\act_{\\hi-1} + w_{\\hi-1}                                 \\\\\n            & = A (A\\st_{\\hi-2} + B \\act_{\\hi-2} + w_{\\hi-2}) + B \\act_{\\hi-1} + w_{\\hi-1} \\\\\n            & = \\cdots                                                                     \\\\\n            & = A^\\hi \\st_0 + \\sum_{i=0}^{\\hi-1} A^i (B \\act_{\\hi-i-1} + w_{\\hi-i-1}).\n\\end{aligned}\n\nLet’s consider the average state at this time, given all the past\nstates and actions. Since we assume that \\E [w_\\hi] = 0 (this is the\nzero vector in d dimensions), when we take an expectation, the w_\\hi\nterm vanishes due to linearity, and so we’re left with\\E [\\st_\\hi \\mid \\st_{0:(\\hi-1)}, \\act_{0:(\\hi-1)}] = A^\\hi \\st_0 + \\sum_{i=0}^{\\hi-1} A^i B \\act_{\\hi-i-1}.\n\nExercise\n\nShow that if we choose actions according to the optimal policy \n\nLemma 2.2, \n\n(2.33) becomes\\E [\\st_\\hi \\mid \\st_0, \\act_i = \\pi^\\star_i(\\st_i)\\quad \\forall i \\le \\hi] = \\left( \\prod_{i=0}^{\\hi-1} (A - B K_i) \\right) \\st_0.\n\nThis introdces the quantity A - B K_i, which shows up frequently in\ncontrol theory. For example, one important question is: will \\st_\\hi\nremain bounded, or will it go to infinity as time goes on? To answer\nthis, let’s imagine for simplicity that these K_is are equal (call\nthis matrix K). Then the expression above becomes (A-BK)^\\hi \\st_0.\nNow consider the maximum eigenvalue \\lambda_{\\max} of A - BK. If\n|\\lambda_{\\max}| > 1, then there’s some nonzero initial state\n\\bar \\st_0, the corresponding eigenvector, for which\\lim_{\\hi \\to \\infty} (A - BK)^\\hi \\bar \\st_0\n    = \\lim_{\\hi \\to \\infty} \\lambda_{\\max}^\\hi \\bar \\st_0\n    = \\infty.\n\nOtherwise, if |\\lambda_{\\max}| < 1, then it’s impossible for your original state to explode as dramatically.","type":"content","url":"/control#expected-state-at-time-hi","position":13},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Extensions"},"type":"lvl2","url":"/control#extensions","position":14},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Extensions"},"content":"We’ve now formulated an optimal solution for the time-homogeneous LQR\nand computed the expected state under the optimal policy. However, real\nworld tasks rarely have such simple dynamics, and we may wish to design\nmore complex cost functions. In this section, we’ll consider more\ngeneral extensions of LQR where some of the assumptions we made above\nare relaxed. Specifically, we’ll consider:\n\nTime-dependency, where the dynamics and cost function might\nchange depending on the timestep.\n\nGeneral quadratic cost, where we allow for linear terms and a\nconstant term.\n\nTracking a goal trajectory rather than aiming for a single goal\nstate-action pair.\n\nCombining these will allow us to use the LQR solution to solve more\ncomplex setups by taking Taylor approximations of the dynamics and\ncost functions.","type":"content","url":"/control#extensions","position":15},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Time-dependent dynamics and cost function","lvl2":"Extensions"},"type":"lvl3","url":"/control#time-dep-lqr","position":16},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Time-dependent dynamics and cost function","lvl2":"Extensions"},"content":"So far, we’ve considered the time-homogeneous case, where the dynamics\nand cost function stay the same at every timestep. However, this might\nnot always be the case. As an example, in many sports, the rules and\nscoring system might change during an overtime period. To address these\nsorts of problems, we can loosen the time-homogeneous restriction, and\nconsider the case where the dynamics and cost function are\ntime-dependent. Our analysis remains almost identical; in fact, we can\nsimply add a time index to the matrices A and B that determine the\ndynamics and the matrices Q and R that determine the cost.\n\nThe modified problem is now defined as follows:\n\nTime-dependent LQR\\begin{aligned}\n        \\min_{\\pi_{0}, \\dots, \\pi_{\\hor-1}} \\quad & \\E \\left[ \\left( \\sum_{\\hi=0}^{\\hor-1} (\\st_\\hi^\\top Q_\\hi \\st_\\hi) + \\act_\\hi^\\top R_\\hi \\act_\\hi \\right) + \\st_\\hor^\\top Q_\\hor \\st_\\hor \\right] \\\\\n        \\textrm{where} \\quad                      & \\st_{\\hi+1} = f_\\hi(\\st_\\hi, \\act_\\hi, w_\\hi) = A_\\hi \\st_\\hi + B_\\hi \\act_\\hi + w_\\hi                                                             \\\\\n                                                  & \\st_0 \\sim \\mu_0                                                                                                                                   \\\\\n                                                  & \\act_\\hi = \\pi_\\hi (\\st_\\hi)                                                                                                                       \\\\\n                                                  & w_\\hi \\sim \\mathcal{N}(0, \\sigma^2 I).\n\\end{aligned}\n\nThe derivation of the optimal value functions and the optimal policy\nremains almost exactly the same, and we can modify the Riccati equation\naccordingly:\n\nTime-dependent Riccati EquationP_\\hi = Q_\\hi + A_\\hi^\\top P_{\\hi+1} A_\\hi - A_\\hi^\\top P_{\\hi+1} B_\\hi (R_\\hi + B_\\hi^\\top P_{\\hi+1} B_\\hi)^{-1} B_\\hi^\\top P_{\\hi+1} A_\\hi.\n\nNote that this is just the time-homogeneous Riccati equation\n(\n\nDefinition 2.5), but with the time index added to each of the\nrelevant matrices.\n\nExercise\n\nWalk through the proof in \n\nSection 2.4 to verify that we can simply add \\hi for the time-dependent case.\n\nAdditionally, by allowing the dynamics to vary across time, we gain the\nability to locally approximate nonlinear dynamics at each timestep.\nWe’ll discuss this later in the chapter.","type":"content","url":"/control#time-dep-lqr","position":17},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"More general quadratic cost functions","lvl2":"Extensions"},"type":"lvl3","url":"/control#more-general-quadratic-cost-functions","position":18},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"More general quadratic cost functions","lvl2":"Extensions"},"content":"Our original cost function had only second-order terms with respect to\nthe state and action, incentivizing staying as close as possible to\n(\\st^\\star, \\act^\\star) = (0, 0). We can also consider more general\nquadratic cost functions that also have first-order terms and a constant\nterm. Combining this with time-dependent dynamics results in the\nfollowing expression, where we introduce a new matrix M_\\hi for the\ncross term, linear coefficients q_\\hi and r_\\hi for the state and\naction respectively, and a constant term c_\\hi:c_\\hi(\\st_\\hi, \\act_\\hi) = ( \\st_\\hi^\\top Q_\\hi \\st_\\hi + \\st_\\hi^\\top M_\\hi \\act_\\hi + \\act_\\hi^\\top R_\\hi \\act_\\hi ) + (\\st_\\hi^\\top q_\\hi + \\act_\\hi^\\top r_\\hi) + c_\\hi.\n\nSimilarly, we can also include a\nconstant term v_\\hi \\in \\mathbb{R}^{n_\\st} in the dynamics (note that this is\ndeterministic at each timestep, unlike the stochastic noise w_\\hi):\\st_{\\hi+1} = f_\\hi(\\st_\\hi, \\act_\\hi, w_\\hi) = A_\\hi \\st_\\hi + B_\\hi \\act_\\hi + v_\\hi + w_\\hi.\n\nexercise\n\nDerive the optimal solution. You will need to slightly modify the\nproof in \n\nSection 2.4.","type":"content","url":"/control#more-general-quadratic-cost-functions","position":19},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Tracking a predefined trajectory","lvl2":"Extensions"},"type":"lvl3","url":"/control#tracking-a-predefined-trajectory","position":20},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Tracking a predefined trajectory","lvl2":"Extensions"},"content":"Consider applying LQR to a task like autonomous driving, where the\ntarget state-action pair changes over time. We might want the vehicle to\nfollow a predefined trajectory of states and actions\n(\\st_\\hi^\\star, \\act_\\hi^\\star)_{\\hi=0}^{\\hor-1}. To express this as a\ncontrol problem, we’ll need a corresponding time-dependent cost\nfunction:c_\\hi(\\st_\\hi, \\act_\\hi) = (\\st_\\hi - \\st^\\star_\\hi)^\\top Q (\\st_\\hi - \\st^\\star_\\hi) + (\\act_\\hi - \\act^\\star_\\hi)^\\top R (\\act_\\hi - \\act^\\star_\\hi).\n\nNote that this punishes states and actions that are far from the\nintended trajectory. By expanding out these multiplications, we can see\nthat this is actually a special case of the more general quadratic cost\nfunction above \n\n(2.38):M_\\hi = 0, \\qquad q_\\hi = -2Q \\st^\\star_\\hi, \\qquad r_\\hi = -2R \\act^\\star_\\hi, \\qquad c_\\hi = (\\st^\\star_\\hi)^\\top Q (\\st^\\star_\\hi) + (\\act^\\star_\\hi)^\\top R (\\act^\\star_\\hi).","type":"content","url":"/control#tracking-a-predefined-trajectory","position":21},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Approximating nonlinear dynamics"},"type":"lvl2","url":"/control#approx-nonlinear","position":22},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Approximating nonlinear dynamics"},"content":"The LQR algorithm solves for the optimal policy when the dynamics are\nlinear and the cost function is an upward-curved quadratic. However,\nreal settings are rarely this simple! Let’s return to the CartPole\nexample from the start of the chapter\n(\n\nExample 2.1). The dynamics (physics) aren’t linear. How\ncan we approximate this by an LQR problem?\n\nConcretely, let’s consider a noise-free problem since, as we saw, the\nnoise doesn’t factor into the optimal policy. Let’s assume the dynamics\nand cost function are stationary, and ignore the terminal state for\nsimplicity:\n\nNonlinear control problem\\begin{aligned}\n        \\min_{\\pi_0, \\dots, \\pi_{\\hor-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\E_{\\st_0} \\left[ \\sum_{\\hi=0}^{\\hor-1} c(\\st_\\hi, \\act_\\hi) \\right] \\\\\n        \\text{where} \\quad                                  & \\st_{\\hi+1} = f(\\st_\\hi, \\act_\\hi)                                   \\\\\n                                                            & \\act_\\hi = \\pi_\\hi(\\st_\\hi)                                          \\\\\n                                                            & \\st_0 \\sim \\mu_0                                                     \\\\\n                                                            & c(\\st, \\act) = d(\\st, \\st^\\star) + d(\\act, \\act^\\star).\n\\end{aligned}\n\nHere, d denotes a function that measures the\n“distance” between its two arguments.\n\nThis is now only slightly simplified from the general optimal control\nproblem (see\n\n\nDefinition 2.1). Here, we don’t know an analytical form\nfor the dynamics f or the cost function c, but we assume that we’re\nable to query/sample/simulate them to get their values at a given\nstate and action. To clarify, consider the case where the dynamics are\ngiven by real world physics. We can’t (yet) write down an expression for\nthe dynamics that we can differentiate or integrate analytically.\nHowever, we can still simulate the dynamics and cost function by\nrunning a real-world experiment and measuring the resulting states and\ncosts. How can we adapt LQR to this more general nonlinear case?","type":"content","url":"/control#approx-nonlinear","position":23},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Local linearization","lvl2":"Approximating nonlinear dynamics"},"type":"lvl3","url":"/control#local-linearization","position":24},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Local linearization","lvl2":"Approximating nonlinear dynamics"},"content":"How can we apply LQR when the dynamics are nonlinear or the cost\nfunction is more complex? We’ll exploit the useful fact that we can take\na function that’s locally continuous around (s^\\star, a^\\star) and\napproximate it nearby with low-order polynomials (i.e. its Taylor\napproximation). In particular, as long as the dynamics f are\ndifferentiable around (\\st^\\star, \\act^\\star) and the cost function\nc is twice differentiable at (\\st^\\star, \\act^\\star), we can take a\nlinear approximation of f and a quadratic approximation of c to\nbring us back to the regime of LQR.\n\nLinearizing the dynamics around (\\st^\\star, \\act^\\star) gives:\\begin{gathered}\n    f(\\st, \\act) \\approx f(\\st^\\star, \\act^\\star) + \\nabla_\\st f(\\st^\\star, \\act^\\star) (\\st - \\st^\\star) + \\nabla_\\act f(\\st^\\star, \\act^\\star) (\\act - \\act^\\star) \\\\\n    (\\nabla_\\st f(\\st, \\act))_{ij} = \\frac{d f_i(\\st, \\act)}{d \\st_j}, \\quad i, j \\le n_\\st \\qquad (\\nabla_\\act f(\\st, \\act))_{ij} = \\frac{d f_i(\\st, \\act)}{d \\act_j}, \\quad i \\le n_\\st, j \\le n_\\act\n\\end{gathered}\n\nand quadratizing the cost function around\n(\\st^\\star, \\act^\\star) gives:\\begin{aligned}\n    c(\\st, \\act) & \\approx c(\\st^\\star, \\act^\\star) \\quad \\text{constant term}                                                                                      \\\\\n                 & \\qquad + \\nabla_\\st c(\\st^\\star, \\act^\\star) (\\st - \\st^\\star) + \\nabla_\\act c(\\st^\\star, \\act^\\star) (a - \\act^\\star) \\quad \\text{linear terms} \\\\\n                 & \\left. \\begin{aligned}\n                               & \\qquad + \\frac{1}{2} (\\st - \\st^\\star)^\\top \\nabla_{\\st \\st} c(\\st^\\star, \\act^\\star) (\\st - \\st^\\star)       \\\\\n                               & \\qquad + \\frac{1}{2} (\\act - \\act^\\star)^\\top \\nabla_{\\act \\act} c(\\st^\\star, \\act^\\star) (\\act - \\act^\\star) \\\\\n                               & \\qquad + (\\st - \\st^\\star)^\\top \\nabla_{\\st \\act} c(\\st^\\star, \\act^\\star) (\\act - \\act^\\star)\n                          \\end{aligned} \\right\\} \\text{quadratic terms}\n\\end{aligned}\n\nwhere the gradients and Hessians are defined as\\begin{aligned}\n    (\\nabla_\\st c(\\st, \\act))_{i}         & = \\frac{d c(\\st, \\act)}{d \\st_i}, \\quad i \\le n_\\st\n                                          & (\\nabla_\\act c(\\st, \\act))_{i}                                               & = \\frac{d c(\\st, \\act)}{d \\act_i}, \\quad i \\le n_\\act               \\\\\n    (\\nabla_{\\st \\st} c(\\st, \\act))_{ij}  & = \\frac{d^2 c(\\st, \\act)}{d \\st_i d \\st_j}, \\quad i, j \\le n_\\st\n                                          & (\\nabla_{\\act \\act} c(\\st, \\act))_{ij}                                       & = \\frac{d^2 c(\\st, \\act)}{d \\act_i d \\act_j}, \\quad i, j \\le n_\\act \\\\\n    (\\nabla_{\\st \\act} c(\\st, \\act))_{ij} & = \\frac{d^2 c(\\st, \\act)}{d \\st_i d \\act_j}. \\quad i \\le n_\\st, j \\le n_\\act\n\\end{aligned}\n\nExercise: Note that this cost can be expressed in the general\nquadratic form seen in\n\n\n(2.38). Derive the corresponding\nquantities Q, R, M, q, r, c.","type":"content","url":"/control#local-linearization","position":25},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Finite differencing","lvl2":"Approximating nonlinear dynamics"},"type":"lvl3","url":"/control#finite-differencing","position":26},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Finite differencing","lvl2":"Approximating nonlinear dynamics"},"content":"To calculate these gradients and Hessians in practice,\nwe use a method known as finite differencing for numerically computing derivatives.\nNamely, we can simply use the limit definition of the derivative, and\nsee how the function changes as we add or subtract a tiny δ to\nthe input.\\frac{d}{dx} f(x) = \\lim_{\\delta \\to 0} \\frac{f(x + \\delta) - f(x)}{\\delta}\n\nNote that this only requires us to be able to query the function, not\nto have an analytical expression for it, which is why it’s so useful in\npractice.","type":"content","url":"/control#finite-differencing","position":27},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Local convexification","lvl2":"Approximating nonlinear dynamics"},"type":"lvl3","url":"/control#local-convexification","position":28},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Local convexification","lvl2":"Approximating nonlinear dynamics"},"content":"However, simply taking the second-order approximation of the cost\nfunction is insufficient, since for the LQR setup we required that the\nQ and R matrices were positive definite, i.e. that all of their\neigenvalues were positive.\n\nOne way to naively force some symmetric matrix D to be positive definite\nis to set any non-positive eigenvalues to some small positive value \\varepsilon > 0.\nRecall that any real symmetric matrix D \\in \\mathbb{R}^{n \\times n} has an basis of eigenvectors u_1, \\dots, u_n\nwith corresponding eigenvalues \\lambda_1, \\dots, \\lambda_n\nsuch that D u_i = \\lambda_i u_i.\nThen we can construct the positive definite approximation by\\widetilde{D} = \\left( \\sum_{i=1, \\dots, n \\mid \\lambda_i > 0} \\lambda_i u_i u_i^\\top \\right) + \\varepsilon I.\n\nExercise: Convince yourself that \\widetilde{D} is indeed positive\ndefinite.\n\nNote that Hessian matrices are generally symmetric, so we can apply this\nprocess to Q and R to obtain the positive definite approximations\n\\widetilde{Q} and \\widetilde{R}.\nNow that we have an upward-curved\nquadratic approximation to the cost function, and a linear approximation\nto the state transitions, we can simply apply the time-homogenous LQR\nmethods from \n\nSection 2.4.\n\nBut what happens when we enter states far away from \\st^\\star or want\nto use actions far from \\act^\\star? A Taylor approximation is only\naccurate in a local region around the point of linearization, so the\nperformance of our LQR controller will degrade as we move further away.\nWe’ll see how to address this in the next section using the iterative LQR algorithm.\n\n\n\nFigure 2.3:Local linearization might only be accurate in a small region around the\npoint of linearization.","type":"content","url":"/control#local-convexification","position":29},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Iterative LQR","lvl2":"Approximating nonlinear dynamics"},"type":"lvl3","url":"/control#iterative-lqr","position":30},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl3":"Iterative LQR","lvl2":"Approximating nonlinear dynamics"},"content":"To address these issues with local linearization, we’ll use an iterative\napproach, where we repeatedly linearize around different points to\ncreate a time-dependent approximation of the dynamics, and then solve\nthe resulting time-dependent LQR problem to obtain a better policy. This\nis known as iterative LQR or iLQR:\n\nIterative LQR\n\nFor each iteration of the algorithm:\n\nForm a time-dependent LQR problem around the current candidate\ntrajectory using local linearization.\n\nCompute the optimal policy using \n\nSection 2.5.1.\n\nGenerate a new series of actions using this policy.\n\nCompute a better candidate trajectory by interpolating between the\ncurrent and proposed actions.\n\nNow let’s go through the details of each step. We’ll use superscripts to\ndenote the iteration of the algorithm. We’ll also denote\n\\bar \\st_0 = \\E_{\\st_0 \\sim \\mu_0} [\\st_0] as the expected initial\nstate.\n\nAt iteration i of the algorithm, we begin with a candidate\ntrajectory\n\\bar \\tau^i = (\\bar \\st^i_0, \\bar \\act^i_0, \\dots, \\bar \\st^i_{\\hor-1}, \\bar \\act^i_{\\hor-1}).\n\nStep 1: Form a time-dependent LQR problem. At each timestep\n\\hi \\in [\\hor], we use the techniques from\n\n\nSection 2.6 to linearize the dynamics and\nquadratize the cost function around (\\bar \\st^i_\\hi, \\bar \\act^i_\\hi):\\begin{aligned}\n    f_\\hi(\\st, \\act) & \\approx f(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi) + \\nabla_{\\st } f(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)(\\st - \\bar {\\st}^i_\\hi) + \\nabla_{\\act } f(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)(\\act - \\bar {\\act}^i_\\hi)                         \\\\\n    c_\\hi(\\st, \\act) & \\approx c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi) + \\begin{bmatrix}\n                                                              \\st - \\bar {\\st }^i_\\hi& \\act - \\bar {\\act}^i_\\hi\n                                                          \\end{bmatrix} \\begin{bmatrix}\n                                                                            \\nabla_{\\st } c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)\\\\\n                                                                            \\nabla_{\\act} c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)\n                                                                        \\end{bmatrix}                                                      \\\\\n                     & \\qquad + \\frac{1}{2} \\begin{bmatrix}\n                                                \\st - \\bar {\\st }^i_\\hi& \\act - \\bar {\\act}^i_\\hi\n                                            \\end{bmatrix} \\begin{bmatrix}\n                                                              \\nabla_{\\st \\st} c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)  & \\nabla_{\\st \\act} c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)  \\\\\n                                                              \\nabla_{\\act \\st} c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi) & \\nabla_{\\act \\act} c(\\bar {\\st}^i_\\hi, \\bar {\\act}^i_\\hi)\n                                                          \\end{bmatrix}\n    \\begin{bmatrix}\n        \\st - \\bar {\\st }^i_\\hi\\\\\n        \\act - \\bar {\\act}^i_\\hi\n    \\end{bmatrix}.\n\\end{aligned}\n\nStep 2: Compute the optimal policy. We can now solve the\ntime-dependent LQR problem using the Riccati equation from\n\n\nSection 2.5.1 to compute the optimal policy\n\\pi^i_0, \\dots, \\pi^i_{\\hor-1}.\n\nStep 3: Generate a new series of actions. We can then generate a new\nsample trajectory by taking actions according to this optimal policy:\\bar \\st^{i+1}_0 = \\bar \\st_0, \\qquad \\widetilde \\act_\\hi = \\pi^i_\\hi(\\bar \\st^{i+1}_\\hi), \\qquad \\bar \\st^{i+1}_{\\hi+1} = f(\\bar \\st^{i+1}_\\hi, \\widetilde \\act_\\hi).\n\nNote that the states are sampled according to the true dynamics, which\nwe assume we have query access to.\n\nStep 4: Compute a better candidate trajectory., Note that we’ve\ndenoted these actions as \\widetilde \\act_\\hi and aren’t directly using\nthem for the next iteration \\bar \\act^{i+1}_\\hi. Rather, we want to\ninterpolate between them and the actions from the previous iteration\n\\bar \\act^i_0, \\dots, \\bar \\act^i_{\\hor-1}. This is so that the cost\nwill increase monotonically, since if the new policy turns out to\nactually be worse, we can stay closer to the previous trajectory. (Can\nyou think of an intuitive example where this might happen?)\n\nFormally, we want to find \\alpha \\in [0, 1] to generate the next\niteration of actions\n\\bar \\act^{i+1}_0, \\dots, \\bar \\act^{i+1}_{\\hor-1} such that the cost\nis minimized:\\begin{aligned}\n    \\min_{\\alpha \\in [0, 1]} \\quad & \\sum_{\\hi=0}^{\\hor-1} c(\\st_\\hi, \\bar \\act^{i+1}_\\hi)                     \\\\\n    \\text{where} \\quad             & \\st_{\\hi+1} = f(\\st_\\hi, \\bar \\act^{i+1}_\\hi)                             \\\\\n                                   & \\bar \\act^{i+1}_\\hi = \\alpha \\bar \\act^i_\\hi + (1-\\alpha) \\widetilde \\act_\\hi \\\\\n                                   & \\st_0 = \\bar \\st_0.\n\\end{aligned}\n\nNote that this optimizes over the closed interval\n[0, 1], so by the Extreme Value Theorem, it’s guaranteed to have a\nglobal maximum.\n\nThe final output of this algorithm is a policy \\pi^{n_\\text{steps}}\nderived after n_\\text{steps} of the algorithm. Though the proof is\nsomewhat complex, one can show that for many nonlinear control problems,\nthis solution converges to a locally optimal solution (in the policy\nspace).","type":"content","url":"/control#iterative-lqr","position":31},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Summary"},"type":"lvl2","url":"/control#summary","position":32},{"hierarchy":{"lvl1":"2 Linear Quadratic Regulators","lvl2":"Summary"},"content":"This chapter introduced some approaches to solving different variants of\nthe optimal control problem\n\n\nDefinition 2.1. We began with the simple case of linear\ndynamics and an upward-curved quadratic cost. This model is called the\nLQR and we solved for the optimal policy using dynamic programming. We\nthen extended these results to the more general nonlinear case via local\nlinearization. We finally saw the iterative LQR algorithm for solving\nnonlinear control problems.","type":"content","url":"/control#summary","position":33},{"hierarchy":{"lvl1":"9 Exploration in MDPs"},"type":"lvl1","url":"/exploration","position":0},{"hierarchy":{"lvl1":"9 Exploration in MDPs"},"content":"","type":"content","url":"/exploration","position":1},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Introduction"},"type":"lvl2","url":"/exploration#introduction","position":2},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Introduction"},"content":"One of the key challenges of reinforcement learning is the exploration-exploitation tradeoff. Should we exploit actions we know will give high reward, or should we explore different actions to discover potentially better strategies? An algorithm that doesn’t explore effectively might easily overfit to certain areas of the state space, and fail to generalize once they enter a region they haven’t yet seen. The algorithms we saw in the chapter on fitted DP \n\n5 Fitted Dynamic Programming Algorithms suffer from this issue.\n\nIn \n\n3 Multi-Armed Bandits, where the state never changes so all we care about are the actions, we saw algorithms like \n\nSection 3.6 and \n\nThompson sampling that incentivize the learner to explore arms that it is uncertain about. In this chapter, we will see how to generalize these ideas to the MDP setting.\n\nPer-episode regret\n\nTo quantify the performance of a learning algorithm, we will consider its per-episode regret over T timesteps/episodes:\\text{Regret}_T = \\E\\left[ \\sum_{t=0}^{T-1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right]\n\nwhere \\pi^t is the policy generated by the algorithm at the tth iteration.","type":"content","url":"/exploration#introduction","position":3},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Sparse reward","lvl2":"Introduction"},"type":"lvl3","url":"/exploration#sparse-reward","position":4},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Sparse reward","lvl2":"Introduction"},"content":"Exploration is especially crucial in sparse reward problems where reward doesn’t come until after many steps, and algorithms which do not systematically explore new states may fail to learn anything meaningful (within a reasonable amount of time).\n\nFor example, policy gradient algorithms require the gradient to be nonzero in order to learn. If we never observe any reward, the gradient will always be zero, and the policy will never change or improve.\n\nSparse Reward MDP\n\nHere’s a simple example of an MDP with sparse reward:\n\nThere are |\\mathcal{S}| states. The agent starts in the leftmost state. In every state, there are three possible actions, two of which move the agent left and one which moves the agent right. The reward function assigns r=1 to the rightmost cell.","type":"content","url":"/exploration#sparse-reward","position":5},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Exploration in deterministic MDPs","lvl2":"Introduction"},"type":"lvl3","url":"/exploration#exploration-in-deterministic-mdps","position":6},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Exploration in deterministic MDPs","lvl2":"Introduction"},"content":"Let us address the exploration problem in a deterministic MDP where taking action a in state s always leads to the state P(s, a) \\in \\mathcal{S}. In this simple setting, there will be no “automatic” exploration due to randomness, so our strategy must actively explore new states. One simple strategy is to visit every possible state-action pair to learn the entire MDP. Then, once the MDP is known, we can use DP to solve for the optimal policy. (This should remind you of the \n\nSection 3.4 algorithm.)\n\nExplore-then-exploit (for deterministic MDPs)\n\nWe’ll keep a set K of all the (s, a, r, s') pairs we’ve observed. Each episode, we’ll choose an unseen state-action pair for which the reward and the next state are unknown, and take the shortest path there. We assume that every state can be reached from the initial state within a single episode. :::{algorithmic}\n$K \\gets \\emptyset$ Using our known transitions $K$, compute the shortest path $\\tilde \\pi$ to $(s, a)$ Execute $\\tilde \\pi$ to visit $(s, a)$ and observe $r = r(s, a), s' = P(s, a)$ $K \\gets K \\cup \\{ (s, a, r, s') \\}$ Compute the optimal policy $\\pi^\\star$ in the MDP $K$ (e.g. using policy iteration). $\\pi^\\star$.\n::: \n\nThe shortest path computation can be implemented using DP. We leave this as an exercise.\n\nPerformance of explore-then-exploit\n\nAs long as every state can be reached from s_0 within a single episode, i.e. |\\mathcal{S}| \\le \\hor, this will eventually be able to explore all |\\mathcal{S}| |\\mathcal{A}| state-action pairs, adding one new transition per episode. We know it will take at most |\\mathcal{S}| |\\mathcal{A}| iterations to explore the entire MDP, after which \\pi^t = \\pi^\\star, incurring no additional regret.\nFor each \\pi^t up until then, corresponding to the shortest-path policies \\tilde \\pi, the value of policy \\pi^t will differ from that of \\pi^\\star by at most \\hor, since the policies will differ by at most 1 reward at each timestep. So,\\sum_{t=0}^{T-1} V^\\star_0 - V_0^{\\pi^t} \\le |\\mathcal{S}||\\mathcal{A}| \\hor.\n\n(Note that this MDP and algorithm are deterministic, so the regret is not random.)","type":"content","url":"/exploration#exploration-in-deterministic-mdps","position":7},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Treating an unknown MDP as a MAB"},"type":"lvl2","url":"/exploration#mdp-mab","position":8},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Treating an unknown MDP as a MAB"},"content":"We also explored the exploration-exploitation tradeoff in \n\n3 Multi-Armed Bandits. Recall tthat in the MAB setting, we have K arms, each of which has an unknown reward distribution, and we want to learn which of the arms is optimal, i.e. has the highest mean reward.\n\nOne algorithm that struck a good balance between exploration and exploitation was the upper confidence bound algorithm \n\nSection 3.6: For each arm, we construct a confidence interval for its true mean award, and then choose the arm with the highest upper confidence bound. In summary,k_{t+1} \\gets \\arg\\max_{k \\in [K]} \\frac{R^{k}_t}{N^{k}_t} + \\sqrt{\\frac{\\ln(2t/\\delta)}{2 N^{k}_t}}\n\nwhere N_t^k indicates the number of times arm k has been pulled up until time t, R_t^k indicates the total reward obtained by pulling arm k up until time t, and \\delta > 0 controls the width of the confidence interval. How might we extend UCB to the MDP case?\n\nLet us formally describe an unknown MDP as an MAB problem. In an unknown MDP, we want to learn which policy is optimal. So if we want to apply MAB techniques to solving an MDP, it makes sense to think of arms as policies. There are K = (|\\mathcal{A}|^{|\\mathcal{S}|})^\\hor deterministic policies in a finite MDP. Then, “pulling” arm π corresponds to using π to act through a trajectory in the MDP, and observing the total reward.\n\nAttention\n\nWhich quantity that we have seen so far equals the mean reward from arm π?\n\nRecall that UCB incurs regret \\tilde{O}(\\sqrt{TK}), where T is the number of pulls and K is the number of arms. So in the MDP-as-MAB problem, using UCB for T episodes would achieve regret\\tilde{O}(\\sqrt{|\\mathcal{A}|^{|\\mathcal{S}|\\hor} T})\n\nThis scales exponentially in |\\mathcal{S}| and \\hor, which quickly becomes intractable. Notably, this method doesn’t consider the information that we gain across different policies. We can illustrate this with the following example:\n\nTreating an MDP as a MAB\n\nConsider a “coin MDP” with two states “heads” and “tails”, two actions “Y” and “N”, and a time horizon of \\hor=2. The state transition flips the coin, and doesn’t depend on the action. The reward only depends on the action: Taking action Y gives reward 1, and taking action N gives reward 0.\n\nSuppose we collect data from the two constant policies \\pi_{\\text{Y}}(s) = \\text{Y} and \\pi_{\\text{N}}(s) = \\text{N}. Now we want to learn about the policy \\tilde{\\pi} that takes action Y and then N. Do we need to collect data from \\tilde{\\pi} to evaluate it? No: Since the reward only depends on the action, we can infer its value from our data on the policies \\pi_{\\text{Y}} and \\pi_{\\text{N}}. However, if we treat the MDP as a bandit in which \\tilde{\\pi} is a new, unknown arm, we ignore the known correlation between the action and the reward.","type":"content","url":"/exploration#mdp-mab","position":9},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"UCB-VI"},"type":"lvl2","url":"/exploration#ucb-vi","position":10},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"UCB-VI"},"content":"The approach above is inefficient: We shouldn’t need to consider all |\\mathcal{A}|^{|\\mathcal{S}| H} deterministic policies to achieve low regret. Rather, all we need to describe the optimal policy is Q^\\star, which has H |\\mathcal{S}||\\mathcal{A}| entries to be learned. Can we borrow ideas from UCB to reduce the regret to this order (i.e. polynomial in |\\mathcal{S}|, |\\mathcal{A}|, and H)?\n\nOne way to frame the UCB algorithm is that, when choosing arms, we optimize over a proxy reward that is the sum of the estimated mean reward and an exploration term. In the UCB-VI algorithm, we will extend this idea to the case of an unknown MDP \\mathcal{M}^{?} by modelling a proxy MDP \\tilde{\\mathcal{M}} with a reward function that encourages exploration. Then, we will use DP to solve for the optimal policy in \\tilde{\\mathcal{M}}.\n\nAssumptions: For simplicity, here we assume the reward function of \\mathcal{M}^{?} is known, so we only need to model the state transitions, though the rewards can be modelled similarly. We will also consider the more general case of a time-varying MDP, where the transition and reward functions can change over time. We take the convention that P_\\hi is the distribution of s_{h+1} \\mid s_{h}, a_{h} and r_\\hi is applied to s_\\hi, a_\\hi.\n\nAt a high level, the UCB-VI algorithm can be described as follows:\n\nModelling: Use previous data to model the transitions \\hat{P}_0, \\dots, \\hat{P}_{H-1}.\n\nReward bonus: Design a reward bonus b_\\hi(s, a) \\in \\mathbb{R} to encourage exploration, analogous to the UCB term.\n\nOptimistic planning: Use DP to compute the optimal policy \\hat \\pi_\\hi(s) in the modelled MDP\\tilde{\\mathcal{M}} = (\\mathcal{S}, \\mathcal{A}, \\{ \\hat{P}_\\hi \\}_{h \\in [H]}, \\{ r_\\hi + b_\\hi \\}_{h \\in [H]}, H).\n\nExecution: Use \\hat \\pi_\\hi(s) to collect a new trajectory, and repeat.\n\nWe detail each of these steps below. The full definition follows in \n\n(9.16).","type":"content","url":"/exploration#ucb-vi","position":11},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Modelling the transitions","lvl2":"UCB-VI"},"type":"lvl3","url":"/exploration#modelling-the-transitions","position":12},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Modelling the transitions","lvl2":"UCB-VI"},"content":"We seek to approximate P_\\hi(s_{h+1} \\mid s_\\hi, a_\\hi) = \\frac{\\pr(s_\\hi, a_\\hi, s_{h+1})}{\\pr(s_\\hi, a_\\hi)}. We can estimate these using their sample probabilities from the dataset. That is, define\\begin{aligned}\n    N_\\hi^t(s, a, s') & := \\sum_{i=0}^{t-1} \\ind{ (s_\\hi^i, a_\\hi^i, s_{h+1}^i) = (s, a, s') } \\\\\n    N_\\hi^t(s, a)     & := \\sum_{i=0}^{t-1} \\ind{ (s_\\hi^i, a_\\hi^i) = (s, a) }                \\\\\n\\end{aligned}\n\nThen we can model\\hat{P}_\\hi^t(s' \\mid s, a) = \\frac{N_\\hi^t(s, a, s')}{N_\\hi^t(s, a)}.\n\nNote that this is also a fairly naive, nonparametric estimator that doesn’t assume any underlying structure of the MDP. We’ll see how to incorporate assumptions about the MDP in the following section.","type":"content","url":"/exploration#modelling-the-transitions","position":13},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Reward bonus","lvl2":"UCB-VI"},"type":"lvl3","url":"/exploration#reward-bonus","position":14},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Reward bonus","lvl2":"UCB-VI"},"content":"To motivate the reward bonus term b_\\hi^t(s, a), recall how we designed the reward bonus term for UCB:\n\nWe used Hoeffding’s inequality to bound, with high probability, how far the sample mean \\hat \\mu_t^k deviated from the true mean \\mu^k.\n\nBy inverting this inequality, we obtained a (1-\\delta)-confidence interval for the true mean, centered at our estimate.\n\nTo make this bound uniform across all timesteps t \\in [T], we applied the union bound and multiplied δ by a factor of T.\n\nWe’d like to do the same for UCB-VI, and construct the bonus term such that V^\\star_\\hi(s) \\le \\hat{V}_\\hi^t(s) with high probability. However, our construction will be more complex than the MAB case, since \\hat{V}_\\hi^t(s) depends on the bonus b_\\hi^t(s, a) implicitly via DP. We claim that the bonus term that gives the proper bound isb_\\hi^t(s, a) = 2 H \\sqrt{\\frac{\\log( |\\mathcal{S}||\\mathcal{A}|H T/\\delta )}{N_\\hi^t(s, a)}}.\n\nWe will only provide a heuristic sketch of the proof; see \n\nAgarwal et al. (2022) (Section 7.3) for a full proof.\n\nUCB-VI reward bonus construction\n\nWe aim to show that, with high probability,V_\\hi^\\star(s) \\le \\hat{V}_\\hi^t(s) \\quad \\forall t \\in [T], h \\in [H], s \\in \\mathcal{S}.\n\nWe’ll do this by bounding the error incurred at each step of DP. Recall that DP solves for \\hat{V}_\\hi^t(s) recursively as follows:\\hat{V}_\\hi^t(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\tilde r^t_\\hi(s, a) + \\E_{s' \\sim \\hat{P}_\\hi^t(\\cdot \\mid s, a)} \\left[ \\hat{V}_{h+1}^t(s') \\right] \\right]\n\nwhere \\tilde r^t_\\hi(s, a) = r_\\hi(s, a) + b_\\hi^t(s, a) is the reward function of our modelled MDP \\tilde{\\mathcal{M}}^t. On the other hand, we know that V^\\star must satisfyV^\\star_\\hi(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\tilde r^t_\\hi(s, a) + \\E_{s' \\sim P^?_\\hi(\\cdot \\mid s, a)} [V^\\star_{\\hi+1}(s')] \\right]\n\nso it suffices to bound the difference between the two inner expectations. There are two sources of error:\n\nThe value functions \\hat{V}^t_{h+1} v.s. V^\\star_{h+1}\n\nThe transition probabilities \\hat{P}_\\hi^t v.s. P^?_\\hi.\n\nWe can bound these individually, and then combine them by the triangle inequality. For the former, we can simply bound the difference by H, assuming that the rewards are within [0, 1]. Now, all that is left is to bound the error from the transition probabilities:\\text{error} = \\left| \\E_{s' \\sim \\hat{P}_\\hi^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] - \\E_{s' \\sim P^?_\\hi(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]. \\right|\n\nLet us bound this term for a fixed s, a, h, t. (Later we can make this uniform across s, a, h, t using the union bound.) Note that expanding out the definition of \\hat{P}_\\hi^t gives\\begin{aligned}\n        \\E_{s' \\sim \\hat{P}_\\hi^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] & = \\sum_{s' \\in \\mathcal{S}} \\frac{N^t_\\hi(s, a, s')}{N^t_\\hi(s, a)} V^\\star_{h+1}(s')                                                     \\\\\n                                                                                   & = \\frac{1}{N^t_\\hi(s, a)} \\sum_{i=0}^{t-1} \\sum_{s' \\in \\mathcal{S}} \\ind{ (s_\\hi^i, a_\\hi^i, s_{h+1}^i) = (s, a, s') } V^\\star_{h+1}(s') \\\\\n                                                                                   & = \\frac{1}{N^t_\\hi(s, a)} \\sum_{i=0}^{t-1} \\underbrace{\\ind{ (s_\\hi^i, a_\\hi^i) = (s, a) } V^\\star_{h+1}(s_{h+1}^i)}_{X^i}\n\\end{aligned}\n\nsince the terms where s' \\neq s_{h+1}^i vanish.\n\nNow, in order to apply Hoeffding’s inequality, we would like to express the second term in \n\n(9.12) as a sum over t random variables as well. We will do this by redundantly averaging over all desired trajectories (i.e. where we visit state s and action a at time h):\\begin{aligned}\n        \\E_{s' \\sim P^?_\\hi(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]\n         & = \\sum_{s' \\in \\mathcal{S}} P^?_\\hi(s' \\mid s, a) V^\\star_{h+1}(s')                                                                              \\\\\n         & = \\sum_{s' \\in \\mathcal{S}} \\frac{1}{N^t_\\hi(s, a)} \\sum_{i=0}^{t-1} \\ind{ (s_\\hi^i, a_\\hi^i) = (s, a) } P^?_\\hi(s' \\mid s, a) V^\\star_{h+1}(s') \\\\\n         & = \\frac{1}{N^t_\\hi(s, a)} \\sum_{i=0}^{t-1} \\E_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_\\hi^i, a_\\hi^i)} X^i.\n\\end{aligned}\n\nNow we can apply Hoeffding’s inequality to X^i - \\E_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_\\hi^i, a_\\hi^i)} X^i, which is bounded by \\hor, to obtain that, with probability at least 1-\\delta,\\text{error} = \\left| \\frac{1}{N^t_\\hi(s, a)} \\sum_{i=0}^{t-1} \\left(X^i - \\E_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_\\hi^i, a_\\hi^i)} X^i \\right) \\right| \\le 2 H \\sqrt{\\frac{\\ln(1/\\delta)}{N_\\hi^t(s, a)}}.\n\nApplying a union bound over all s \\in \\mathcal{S}, a \\in \\mathcal{A}, t \\in [T], h \\in [H] gives the b_\\hi^t(s, a) term above.","type":"content","url":"/exploration#reward-bonus","position":15},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Definition","lvl2":"UCB-VI"},"type":"lvl3","url":"/exploration#definition","position":16},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Definition","lvl2":"UCB-VI"},"content":"Putting these parts together, we can define the algorithm as follows:3 + 1 = 4 TODO :::{algorithmic}\n$N_\\hi(s, a, s') \\gets \\sum_{i=0}^{t-1} \\ind{ (s_\\hi^i, a_\\hi^i, s_{h+1}^i) = (s, a, s') }$ $N_\\hi(s, a) \\gets \\sum_{i=0}^{t-1} \\ind{ (s_\\hi^i, a_\\hi^i) = (s, a) }$ $\\hat P_\\hi \\gets \\frac{N_\\hi(s, a, s')}{N_\\hi(s, a)}$ $b_\\hi(s, a) \\gets 2 H \\sqrt{\\frac{\\log( |\\mathcal{S}||\\mathcal{A}|H T/\\delta )}{N_\\hi(s, a)}}$ $\\tilde{\\mathcal{M}} \\gets (\\mathcal{S}, \\mathcal{A}, \\{ \\hat{P}_\\hi \\}_{h \\in [H-1]}, \\{ r_\\hi + b_\\hi \\}_{h \\in [H-1]}, H)$ $\\hat \\pi \\gets \\text{VI}(\\tilde{\\mathcal{M}})$ Use $\\hat \\pi_h(s)$ to collect a new trajectory $(s^t_\\hi, a^t_\\hi, s^t_{\\hi+1})_{\\hi \\in [\\hor]}$\n::: ","type":"content","url":"/exploration#definition","position":17},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Performance of UCB-VI","lvl2":"UCB-VI"},"type":"lvl3","url":"/exploration#performance-of-ucb-vi","position":18},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Performance of UCB-VI","lvl2":"UCB-VI"},"content":"How exactly does UCB-VI strike a good balance between exploration and exploitation? In UCB for MABs, the bonus exploration term is simple to interpret: It encourages the learner to take actions with a high exploration term. Here, the policy depends on the bonus term indirectly: The policy is obtained by planning in an MDP where the bonus term is added to the reward function. Note that the bonuses propagate backwards in DP, effectively enabling the learner to plan to explore unknown states. This effect takes some further interpretation.\n\nRecall we constructed b^t_\\hi so that, with high probability, V^\\star_\\hi(s) \\le \\hat{V}_\\hi^t(s) and soV^\\star_\\hi(s) - V^{\\pi^t}_\\hi(s) \\le \\hat{V}_\\hi^t(s) - V^{\\pi^t}_\\hi(s).\n\nThat is, the l.h.s. measures how suboptimal policy \\pi^t is in the true environment, while the r.h.s. is the difference in the policy’s value when acting in the modelled MDP \\tilde{\\mathcal{M}}^t instead of the true one \\mathcal{M}^{?}.\n\nIf the r.h.s. is small, this implies that the l.h.s. difference is also small, i.e. that \\pi^t is exploiting actions that are giving high reward.\n\nIf the r.h.s. is large, then we have overestimated the value: \\pi^t, the optimal policy of \\tilde{\\mathcal{M}}^t, does not perform well in the true environment \\mathcal{M}^{?}. This indicates that one of the b_h^t(s, a) terms must be large, or some \\hat P^t_\\hi(\\cdot \\mid s, a) must be inaccurate, indicating a state-action pair with a low visit count N^t_\\hi(s, a) that the learner was encouraged to explore.\n\nIt turns out that UCB-VI achieves a per-episode regret of\n\nUCB-VI regret\\E \\left[ \\sum_{t=0}^{T-1} \\left(V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right) \\right] = \\tilde{O}(H^2 \\sqrt{|\\mathcal{S}| |\\mathcal{A}| T})\n\nComparing this to the UCB regret bound \\tilde{O}(\\sqrt{T K}), where K is the number of arms of the MAB, we see that we’ve reduced the number of effective arms from |\\mathcal{A}|^{|\\mathcal{S}|\\hor} (in \n\n(9.4)) to H^4 |\\mathcal{S}||\\mathcal{A}|, which is indeed polynomial in |\\mathcal{S}|, |\\mathcal{A}|, and H, as desired. This is also roughly the number of episodes it takes to achieve constant-order average regret:\\frac{1}{T} \\E[\\text{Regret}_T] = \\tilde{O}\\left(\\sqrt{\\frac{H^4 |\\mathcal{S}||\\mathcal{A}|}{T}}\\right)\n\nNote that the time-dependent transition matrix has H |\\mathcal{S}|^2 |\\mathcal{A}| entries. Assuming H \\ll |\\mathcal{S}|, this shows that it’s possible to achieve low regret, and achieve a near-optimal policy, while only understanding a 1/|\\mathcal{S}| fraction of the world’s dynamics.","type":"content","url":"/exploration#performance-of-ucb-vi","position":19},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Linear MDPs"},"type":"lvl2","url":"/exploration#linear-mdps","position":20},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Linear MDPs"},"content":"A polynomial dependency on |\\mathcal{S}| and |\\mathcal{A}| is manageable when the state and action spaces are small. But for large or continuous state and action spaces, even this polynomial factor will become intractable. Can we find algorithms that don’t depend on |\\mathcal{S}| or |\\mathcal{A}| at all, effectively reducing the dimensionality of the MDP? In this section, we’ll explore linear MDPs: an example of a parameterized MDP where the rewards and state transitions depend only on some parameter space of dimension d that is independent from |\\mathcal{S}| or |\\mathcal{A}|.\n\nLinear MDP\n\nWe assume that the transition probabilities and rewards are linear in some feature vector\n\n\\phi(s, a) \\in \\mathbb{R}^d:\\begin{aligned}\n        P_\\hi(s' \\mid s, a) & = \\phi(s, a)^\\top \\mu^\\star_\\hi(s') \\\\\n        r_\\hi(s, a)         & = \\phi(s, a)^\\top \\theta_\\hi^\\star\n\\end{aligned}\n\nNote that we can also think of P_\\hi(\\cdot \\mid s, a) = \\mu_\\hi^\\star as an |\\mathcal{S}| \\times d matrix, and think of \\mu^\\star_\\hi(s') as indexing into the s'-th row of this matrix (treating it as a column vector). Thinking of V^\\star_{\\hi+1} as an |\\mathcal{S}|-dimensional vector, this allows us to write\\E_{s' \\sim P_\\hi(\\cdot \\mid s, a)}[V^\\star_{\\hi+1}(s)] = (\\mu^\\star_\\hi \\phi(s, a))^\\top V^\\star_{\\hi+1}.\n\nThe ϕ feature mapping can be designed to capture interactions between the state s and action a. In this book, we’ll assume that the feature map \\phi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}^d and the reward function (described by \\theta_\\hi^\\star) are known to the learner.","type":"content","url":"/exploration#linear-mdps","position":21},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Planning in a linear MDP","lvl2":"Linear MDPs"},"type":"lvl3","url":"/exploration#planning-in-a-linear-mdp","position":22},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"Planning in a linear MDP","lvl2":"Linear MDPs"},"content":"It turns out that Q^\\star_\\hi is also linear with respect to this feature mapping. We can prove this by simply computing it using DP. We initialize V_{H}^\\star(s) = 0 \\forall s. Then we iterate:\\begin{aligned}\n    Q^\\star_\\hi(s, a)  & = r_\\hi(s, a) + \\E_{s' \\sim P_\\hi(\\cdot \\mid s, a)} [V^\\star_{h+1}(s')]                          \\\\\n                     & = \\phi(s, a)^\\top \\theta_\\hi^\\star + (\\mu_\\hi^\\star \\phi(s, a))^\\top V^\\star_{h+1}               \\\\\n                     & = \\phi(s, a)^\\top \\underbrace{( \\theta_\\hi^\\star + (\\mu_\\hi^\\star)^\\top  V^\\star_{h+1})}_{w_\\hi} \\\\\n    V^\\star_\\hi(s)     & = \\max_a Q^\\star_\\hi(s, a)                                                                       \\\\\n    \\pi^\\star_\\hi(s) & = \\arg\\max_a Q^\\star_\\hi(s, a)\n\\end{aligned}\n\nAttention\n\nShow that Q^\\pi_\\hi is also linear with respect to \\phi(s, a) for any policy π.","type":"content","url":"/exploration#planning-in-a-linear-mdp","position":23},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"type":"lvl3","url":"/exploration#lin-ucb-vi","position":24},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"content":"","type":"content","url":"/exploration#lin-ucb-vi","position":25},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Modelling the transitions","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"type":"lvl4","url":"/exploration#modelling-the-transitions-1","position":26},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Modelling the transitions","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"content":"This linear assumption on the MDP will also allow us to model the unknown dynamics P^?_\\hi(s' \\mid s, a) with techniques from supervised learning (SL). Recall that SL is useful for estimating conditional expectations by minimizing mean squared error. We can rephrase the estimation of P^?_\\hi(s' \\mid s, a) as a least-squares problem as follows: Write \\delta_s to denote a one-hot vector in \\mathbb{R}^{|\\mathcal{S}|}, with a 1 in the s-th entry and 0 everywhere else. Note that\\E_{s' \\sim P_h(\\cdot \\mid s, a)} [\\delta_{s'}] = P_h(\\cdot \\mid s, a) = \\mu_h^\\star \\phi(s, a).\n\nFurthermore, since the expectation here is linear with respect to \\phi(s, a), we can directly apply least-squares multi-target linear regression to construct the estimate\\hat \\mu = \\arg\\min_{\\mu \\in \\mathbb{R}^{|\\mathcal{S}| \\times d}} \\sum_{t=0}^{T-1} \\|\\mu \\phi(s_h^i, a_h^i) - \\delta_{s_{h+1}^i} \\|_2^2.\n\nThis has a well-known closed-form solution:\\begin{aligned}\n    \\hat \\mu^\\top            & = (A_h^t)^{-1} \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\delta_{s_{h+1}^i}^\\top \\\\\n    \\text{where} \\quad A_h^t & = \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\phi(s_h^i, a_h^i)^\\top + \\lambda I\n\\end{aligned}\n\nwhere we include a \\lambda I term to ensure that the matrix A^t_h is invertible. (This can also be derived by adding a \\lambda \\|\\mu\\|_{\\text{F}}^2 regularization term to the objective.) We can directly plug in this estimate into \\hat{P}^t_h(\\cdot \\mid s, a) = \\hat \\mu^t_h \\phi(s, a).","type":"content","url":"/exploration#modelling-the-transitions-1","position":27},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Reward bonus","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"type":"lvl4","url":"/exploration#reward-bonus-1","position":28},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Reward bonus","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"content":"Now, to design the reward bonus, we can’t apply Hoeffding anymore, since the terms no longer involve sample means of bounded random variables; Instead, we’re incorporating information across different states and actions. Rather, we can construct an upper bound using Chebyshev’s inequality in the same way we did for the LinUCB algorithm in the MAB setting \n\nSection 3.8.1:b^t_\\hi(s, a) = \\beta \\sqrt{\\phi(s, a)^\\top (A^t_h)^{-1} \\phi(s, a)}, \\quad \\beta = \\tilde O(d \\hor).\n\nNote that this isn’t explicitly inversely proportional to N_h^t(s, a) as in the original UCB-VI bonus term \n\n(9.8). Rather, it is inversely proportional to the amount that the direction \\phi(s, a) has been explored in the history. That is, if A_h^t has a large component in the direction \\phi(s, a), implying that this direction is well explored, then the bonus term will be small, and vice versa.\n\nWe can now plug in these transition estimates and reward bonuses into the UCB-VI algorithm \n\n(9.16).","type":"content","url":"/exploration#reward-bonus-1","position":29},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Performance","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"type":"lvl4","url":"/exploration#performance","position":30},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl4":"Performance","lvl3":"UCB-VI in a linear MDP","lvl2":"Linear MDPs"},"content":"LinUCB-VI regret\n\nThe LinUCB-VI algorithm achieves expected regret\\E[\\text{Regret}_T] = \\E\\left[\\sum_{t=0}^{T-1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right] \\le \\tilde O(H^2 d^{1.5} \\sqrt{T})\n\nComparing this to our bound for UCB-VI in an environment without this linear assumption, we see that we go from a sample complexity of \\tilde \\Omega(H^4 |\\mathcal{S}||\\mathcal{A}|) to \\tilde \\Omega(H^4 d^{3}). This new sample complexity only depends on the feature dimension and not on the state or action space of the MDP!","type":"content","url":"/exploration#performance","position":31},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Summary"},"type":"lvl2","url":"/exploration#summary","position":32},{"hierarchy":{"lvl1":"9 Exploration in MDPs","lvl2":"Summary"},"content":"In this chapter, we’ve explored how to explore in an unknown MDP.\n\nWe first discussed the explore-then-exploit algorithm \n\nDefinition 9.2, a simple way to explore a deterministic MDP by visiting all state-action pairs.\n\nWe then discussed how to treat an unknown MDP as a MAB \n\nSection 9.2, and how this approach is inefficient since it doesn’t make use of relationships between policies.\n\nWe then introduced the UCB-VI algorithm \n\n(9.16), which models the unknown MDP by a proxy MDP with a reward bonus term that encourages exploration.\n\nFinally, assuming that the transitions and rewards are linear with respect to a feature transformation of the state and action, we introduced the LinUCB-VI algorithm \n\nSection 9.4.2, which has a sample complexity independent of the size of the state and action spaces.","type":"content","url":"/exploration#summary","position":33},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms"},"type":"lvl1","url":"/fitted-dp","position":0},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms"},"content":"","type":"content","url":"/fitted-dp","position":1},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Introduction"},"type":"lvl2","url":"/fitted-dp#introduction","position":2},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Introduction"},"content":"We borrow these definitions from the \n\n1 Markov Decision Processes chapter:\n\nfrom typing import NamedTuple, Callable, Optional\nfrom jaxtyping import Float, Array\nimport jax.numpy as np\nfrom jax import grad, vmap\nimport jax.random as rand\nfrom tqdm import tqdm\nimport gymnasium as gym\n\nkey = rand.PRNGKey(184)\n\n\nclass Transition(NamedTuple):\n    s: int\n    a: int\n    r: float\n\n\nTrajectory = list[Transition]\n\n\ndef get_num_actions(trajectories: list[Trajectory]) -> int:\n    \"\"\"Get the number of actions in the dataset. Assumes actions range from 0 to A-1.\"\"\"\n    return max(max(t.a for t in τ) for τ in trajectories) + 1\n\n\nState = Float[Array, \"...\"]  # arbitrary shape\n\n# assume finite `A` actions and f outputs an array of Q-values\n# i.e. Q(s, a, h) is implemented as f(s, h)[a]\nQFunction = Callable[[State, int], Float[Array, \" A\"]]\n\n\ndef Q_zero(A: int) -> QFunction:\n    \"\"\"A Q-function that always returns zero.\"\"\"\n    return lambda s, a: np.zeros(A)\n\n\n# a deterministic time-dependent policy\nPolicy = Callable[[State, int], int]\n\n\ndef q_to_greedy(Q: QFunction) -> Policy:\n    \"\"\"Get the greedy policy for the given state-action value function.\"\"\"\n    return lambda s, h: np.argmax(Q(s, h))\n\nThe \n\n1 Markov Decision Processes chapter discussed the case of finite MDPs, where the state and action spaces \\mathcal{S} and \\mathcal{A} were finite.\nThis gave us a closed-form expression for computing the r.h.s. of \n\nthe Bellman one-step consistency equation.\nIn this chapter, we consider the case of large or continuous state spaces, where the state space is too large to be enumerated.\nIn this case, we need to approximate the value function and Q-function using methods from supervised learning.\n\nWe will first take a quick detour to introduce the empirical risk minimization framework for function approximation.\nWe will then see its application to fitted RL algorithms,\nwhich attempt to learn the optimal value function (and the optimal policy) from a dataset of trajectories.","type":"content","url":"/fitted-dp#introduction","position":3},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Empirical risk minimization"},"type":"lvl2","url":"/fitted-dp#erm","position":4},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Empirical risk minimization"},"content":"The supervised learning task is as follows:\nWe seek to learn the relationship between some input variables x and some output variable y\n(drawn from their joint distribution).\nPrecisely, we want to find a function \\hat f : x \\mapsto y that minimizes the\nsquared error of the prediction:\\hat f = \\arg\\min_{f} \\E[(y - f(x))^2]\n\nAn equivalent framing is that we seek to approximate the conditional expectation of y given x:\n\nConditional expectation minimizes mean squared error\\arg\\min_{f} \\E[(y - f(x))^2] = (x \\mapsto \\E[y \\mid x])\n\nWe can decompose the mean squared error as\\begin{aligned}\n\\E[(y - f(x))^2] &= \\E[ (y - \\E[y \\mid x] + \\E[y \\mid x] - f(x))^2 ] \\\\\n&= \\E[ (y - \\E[y \\mid x])^2 ] + \\E[ (\\E[y \\mid x] - f(x))^2 ] + 2 \\E[ (y - \\E[y \\mid x])(\\E[y \\mid x] - f(x)) ] \\\\\n\\end{aligned}\n\nAttention\n\nUse the law of iterated expectations to show that the last term is zero.\n\nThe first term is the irreducible error, and the second term is the error due to the approximation,\nwhich is minimized at 0 when f(x) = \\E[y \\mid x].\n\nIn most applications, the joint distribution of x, y is unknown or extremely complex, and so we can’t\nanalytically evaluate \\E [y \\mid x].\nInstead, our strategy is to draw N samples (x_i, y_i) from the joint distribution of x and y,\nand then use the sample average \\sum_{i=1}^N (y_i - f(x_i))^2 / N to approximate the mean squared error.\nThen we use a fitting method to find a function \\hat f that minimizes this objective\nand thus approximates the conditional expectation.\nThis approach is called empirical risk minimization.\n\nEmpirical risk minimization\n\nGiven a dataset of samples (x_1, y_1), \\dots, (x_N, y_N), empirical risk minimization seeks to find a function f (from some class of functions \\mathcal{F}) that minimizes the empirical risk:\\hat f = \\arg\\min_{f \\in \\mathcal{F}} \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2\n\nWe will cover the details of the minimization process in [](#the next section <supervised_learning>).\n\nAttention\n\nWhy is it important that we constrain our search to a class of functions \\mathcal{F}?\n\nHint: Consider the function f(x) = \\sum_{i=1}^N y_i \\mathbb{1}_{\\{ x = x_i \\}}. What is the empirical risk of this function? Would you consider it a good approximation of the conditional expectation?","type":"content","url":"/fitted-dp#erm","position":5},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted value iteration"},"type":"lvl2","url":"/fitted-dp#fitted-value-iteration","position":6},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted value iteration"},"content":"Let us apply ERM to the RL problem of computing the optimal policy / value function.\n\nHow did we compute the optimal value function in MDPs with finite state and action spaces?\n\nIn a [](#finite-horizon MDP <finite_horizon_mdps>), we can use \n\ndynamic programming, working backwards from the end of the time horizon, to compute the optimal value function exactly.\n\nIn an [](#infinite-horizon MDP <infinite_horizon_mdps>), we can use [](#value iteration <value_iteration>), which iterates the Bellman optimality operator \n\n(1.54) to approximately compute the optimal value function.\n\nOur existing approaches represent the value function, and the MDP itself,\nin matrix notation.\nBut what happens if the state space is extremely large, or even infinite (e.g. real-valued)?\nThen computing a weighted sum over all possible next states, which is required to compute the Bellman operator,\nbecomes intractable.\n\nInstead, we will need to use function approximation methods from supervised learning to solve for the value function in an alternative way.\n\nIn particular, suppose we have a dataset of N trajectories \\tau_1, \\dots, \\tau_N \\sim \\rho_{\\pi} from some policy π (called the data collection policy) acting in the MDP of interest.\nLet us indicate the trajectory index in the superscript, so that\\tau_i = \\{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \\dots, s_{\\hor-1}^i, a_{\\hor-1}^i, r_{\\hor-1}^i \\}.\n\ndef collect_data(\n    env: gym.Env, N: int, H: int, key: rand.PRNGKey, π: Optional[Policy] = None\n) -> list[Trajectory]:\n    \"\"\"Collect a dataset of trajectories from the given policy (or a random one).\"\"\"\n    trajectories = []\n    seeds = [rand.bits(k).item() for k in rand.split(key, N)]\n    for i in tqdm(range(N)):\n        τ = []\n        s, _ = env.reset(seed=seeds[i])\n        for h in range(H):\n            # sample from a random policy\n            a = π(s, h) if π else env.action_space.sample()\n            s_next, r, terminated, truncated, _ = env.step(a)\n            τ.append(Transition(s, a, r))\n            if terminated or truncated:\n                break\n            s = s_next\n        trajectories.append(τ)\n    return trajectories\n\nenv = gym.make(\"LunarLander-v2\")\ntrajectories = collect_data(env, 100, 300, key)\ntrajectories[0][:5]  # show first five transitions from first trajectory\n\nCan we view the dataset of trajectories as a “labelled dataset” in order to apply supervised learning to approximate the optimal Q-function? Yes!\nRecall that we can characterize the optimal Q-function using the \n\nBellman optimality equations,\nwhich don’t depend on an actual policy:Q_\\hi^\\star(s, a) = r(s, a) + \\E_{s' \\sim P(s, a)} [\\max_{a'} Q_{\\hi+1}^\\star(s', a')]\n\nWe can think of the arguments to the Q-function -- i.e. the current state, action, and timestep \\hi --\nas the inputs x, and the r.h.s. of the above equation as the label f(x). Note that the r.h.s. can also be expressed as a conditional expectation:f(x) = \\E [y \\mid x] \\quad \\text{where} \\quad y = r(s_\\hi, a_\\hi) + \\max_{a'} Q^\\star_{\\hi + 1}(s', a').\n\nApproximating the conditional expectation is precisely the task that \n\nSection 5.2 is suited for!\n\nOur above dataset would give us N \\cdot \\hor samples in the dataset:x_{i \\hi} = (s_\\hi^i, a_\\hi^i, \\hi) \\qquad y_{i \\hi} = r(s_\\hi^i, a_\\hi^i) + \\max_{a'} Q^\\star_{\\hi + 1}(s_{\\hi + 1}^i, a')\n\ndef get_X(trajectories: list[Trajectory]):\n    \"\"\"\n    We pass the state and timestep as input to the Q-function\n    and return an array of Q-values.\n    \"\"\"\n    rows = [(τ[h].s, τ[h].a, h) for τ in trajectories for h in range(len(τ))]\n    return [np.stack(ary) for ary in zip(*rows)]\n\n\ndef get_y(\n    trajectories: list[Trajectory],\n    f: Optional[QFunction] = None,\n    π: Optional[Policy] = None,\n):\n    \"\"\"\n    Transform the dataset of trajectories into a dataset for supervised learning.\n    If `π` is None, instead estimates the optimal Q function.\n    Otherwise, estimates the Q function of π.\n    \"\"\"\n    f = f or Q_zero(get_num_actions(trajectories))\n    y = []\n    for τ in trajectories:\n        for h in range(len(τ) - 1):\n            s, a, r = τ[h]\n            Q_values = f(s, h + 1)\n            y.append(r + (Q_values[π(s, h + 1)] if π else Q_values.max()))\n        y.append(τ[-1].r)\n    return np.array(y)\n\ns, a, h = get_X(trajectories[:1])\nprint(\"states:\", s[:5])\nprint(\"actions:\", a[:5])\nprint(\"timesteps:\", h[:5])\n\nget_y(trajectories[:1])[:5]\n\nThen we can use empirical risk minimization to find a function \\hat f that approximates the optimal Q-function.\n\n# We will see some examples of fitting methods in the next section\nFittingMethod = Callable[[Float[Array, \"N D\"], Float[Array, \" N\"]], QFunction]\n\nBut notice that the definition of y_{i \\hi} depends on the Q-function itself!\nHow can we resolve this circular dependency?\nRecall that we faced the same issue \n\nwhen evaluating a policy in an infinite-horizon MDP. There, we iterated the \n\nDefinition 1.8 since we knew that the policy’s value function was a fixed point of the policy’s Bellman operator.\nWe can apply the same strategy here, using the \\hat f from the previous iteration to compute the labels y_{i \\hi},\nand then using this new dataset to fit the next iterate.\n\nFitted Q-function iteration\n\nInitialize some function \\hat f(s, a, h) \\in \\mathbb{R}.\n\nIterate the following:\n\nGenerate a supervised learning dataset X, y from the trajectories and the current estimate f, where the labels come from the r.h.s. of the Bellman optimality operator \n\n(1.54)\n\nSet \\hat f to the function that minimizes the empirical risk:\\hat f \\gets \\arg\\min_f \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2.\n\ndef fitted_q_iteration(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    epochs: int,\n    Q_init: Optional[QFunction] = None,\n) -> QFunction:\n    \"\"\"\n    Run fitted Q-function iteration using the given dataset.\n    Returns an estimate of the optimal Q-function.\n    \"\"\"\n    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))\n    X = get_X(trajectories)\n    for _ in range(epochs):\n        y = get_y(trajectories, Q_hat)\n        Q_hat = fit(X, y)\n    return Q_hat\n\n","type":"content","url":"/fitted-dp#fitted-value-iteration","position":7},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted policy evaluation"},"type":"lvl2","url":"/fitted-dp#fitted-pi-eval","position":8},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted policy evaluation"},"content":"We can also use this fixed-point interation to evaluate a policy using the dataset (not necessarily the one used to generate the trajectories):\n\nFitted policy evaluation\n\nInput: Policy \\pi : \\mathcal{S} \\times [H] \\to \\Delta(\\mathcal{A}) to be evaluated.\n\nOutput: An approximation of the value function Q^\\pi of the policy.\n\nInitialize some function \\hat f(s, a, h) \\in \\mathbb{R}.\n\nIterate the following:\n\nGenerate a supervised learning dataset X, y from the trajectories and the current estimate f, where the labels come from the r.h.s. of the \n\nBellman consistency equation for the given policy.\n\nSet \\hat f to the function that minimizes the empirical risk:\\hat f \\gets \\arg\\min_f \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2.\n\ndef fitted_evaluation(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    π: Policy,\n    epochs: int,\n    Q_init: Optional[QFunction] = None,\n) -> QFunction:\n    \"\"\"\n    Run fitted policy evaluation using the given dataset.\n    Returns an estimate of the Q-function of the given policy.\n    \"\"\"\n    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))\n    X = get_X(trajectories)\n    for _ in tqdm(range(epochs)):\n        y = get_y(trajectories, Q_hat, π)\n        Q_hat = fit(X, y)\n    return Q_hat\n\nAttention\n\nSpot the difference between fitted_evaluation and fitted_q_iteration. (See the definition of get_y.)\nHow would you modify this algorithm to evaluate the data collection policy?","type":"content","url":"/fitted-dp#fitted-pi-eval","position":9},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted policy iteration"},"type":"lvl2","url":"/fitted-dp#fitted-policy-iteration","position":10},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Fitted policy iteration"},"content":"We can use this policy evaluation algorithm to adapt the [](#policy iteration algorithm <policy_iteration>) to this new setting. The algorithm remains exactly the same -- repeatedly make the policy greedy w.r.t. its own value function -- except now we must evaluate the policy (i.e. compute its value function) using the iterative fitted_evaluation algorithm.\n\ndef fitted_policy_iteration(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    epochs: int,\n    evaluation_epochs: int,\n    π_init: Optional[Policy] = lambda s, h: 0,  # constant zero policy\n):\n    \"\"\"Run fitted policy iteration using the given dataset.\"\"\"\n    π = π_init\n    for _ in range(epochs):\n        Q_hat = fitted_evaluation(trajectories, fit, π, evaluation_epochs)\n        π = q_to_greedy(Q_hat)\n    return π\n\n","type":"content","url":"/fitted-dp#fitted-policy-iteration","position":11},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Summary"},"type":"lvl2","url":"/fitted-dp#summary","position":12},{"hierarchy":{"lvl1":"5 Fitted Dynamic Programming Algorithms","lvl2":"Summary"},"content":"","type":"content","url":"/fitted-dp#summary","position":13},{"hierarchy":{"lvl1":"7 Imitation Learning"},"type":"lvl1","url":"/imitation-learning","position":0},{"hierarchy":{"lvl1":"7 Imitation Learning"},"content":"","type":"content","url":"/imitation-learning","position":1},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Introduction"},"type":"lvl2","url":"/imitation-learning#introduction","position":2},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Introduction"},"content":"Imagine you are tasked with learning how to drive. How do, or did, you go about it?\nAt first, this task might seem insurmountable: there are a vast array of controls, and the cost of making a single mistake could be extremely high, making it hard to explore by trial and error.\nLuckily, there are already people in the world who know how to drive who can get you started.\nIn almost every challenge we face,\nwe “stand on the shoulders of giants” and learn skills from experts who have already mastered them.\n\nNow in machine learning,\nwe are often trying to teach machines to accomplish tasks that humans are already proficient at.\nIn such cases, the machine learning algorithm is the one learning the new skill, and humans are the “experts” that can demonstrate how to perform the task.\nImitation learning is a strategy for getting the learner to perform at least as well as the expert.\nWe’ll see that the most naive form of imitation learning, called behavioral cloning, is really an application of supervised learning to interactive tasks.\nWe’ll then explore dataset aggregation (DAgger) as a way to query an expert and learn even more effectively.","type":"content","url":"/imitation-learning#introduction","position":3},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Behavioral cloning"},"type":"lvl2","url":"/imitation-learning#behavioral-cloning","position":4},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Behavioral cloning"},"content":"This notion of “learning from human-provided data” may remind you of the basic premise of \n\n4 Supervised learning.\nIn supervised learning,\nthere is some mapping from inputs to outputs,\nsuch as the task of assigning the correct label to an image,\nthat humans can implicitly compute.\nTo teach a machine to calculate this mapping,\nwe first collect a large training dataset by getting people to label a lot of inputs,\nand then use some optimization algorithm to produce a predictor that maps from the inputs to the outputs as closely as possible.\n\nHow does this relate to interactive tasks?\nHere, the input is the observation seen by the agent and the output is the action it selects,\nso the mapping is the agent’s policy.\nWhat’s stopping us from applying supervised learning techniques to mimic the expert’s policy?\nIn principle, nothing!\nThis is called behavioral cloning.\n\nBehavioral cloning\n\nCollect a training dataset of trajectories \\mathcal{D} = (s^n, a^n)_{n=1}^{N} generated by an expert policy \\pi_\\text{expert}. (For example, if the dataset contains M trajectories, each with a finite horizon H, then N = M \\times H.)\n\nUse a SL algorithm \\texttt{fit} : \\mathcal{D} \\mapsto \\widetilde{\\pi} to extract a policy \\widetilde{\\pi} that approximates the expert policy.\n\nTypically, this second task can be framed as empirical loss minimization:\\widetilde{\\pi} = \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=0}^{N-1} \\text{loss}(\\pi(s^n), a^n)\n\nwhere Π is some class of possible policies, \\text{loss} is the loss function to measure how different the policy’s prediction is from the true observed action,\nand the SL algorithm itself, also known as the fitting method, tells us how to compute this \\arg\\min.\n\nHow should we choose the loss function?\nIn supervised learning, we saw that the mean squared error is a good choice for continuous outputs.\nHowever, how should we measure the difference between two actions in a discrete action space?\nIn this setting, the policy acts more like a classifier that picks the best action in a given state.\nRather than considering a deterministic policy that just outputs a single action,\nwe’ll consider a stochastic policy π that outputs a distribution over actions.\nThis allows us to assign a likelihood to observing the entire dataset \\mathcal{D} under the policy π,\nassuming the state-action pairs are independent:\\pr_\\pi (\\mathcal{D}) = \\prod_{n=1}^{N} \\pi(a_n \\mid s_n)\n\nNote that the states and actions are not, however, actually independent! A key property of interactive tasks is that the agent’s output -- the action that it takes -- may influence its next observation.\nWe want to find a policy under which the training dataset \\mathcal{D} is the most likely.\nThis is called the maximum likelihood estimate of the policy that generated the dataset:\\widetilde{\\pi} = \\arg\\max_{\\pi \\in \\Pi} \\pr_{\\pi}(\\mathcal{D})\n\nThis is also equivalent to picking the negative log likelihood as the loss function:\\begin{align*}\n\\widetilde{\\pi} &= \\arg\\min_{\\pi \\in \\Pi} - \\log \\pr_\\pi(\\mathcal{D}) \\\\\n&= \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=1}^N - \\log \\pi(a_n \\mid s_n)\n\\end{align*}","type":"content","url":"/imitation-learning#behavioral-cloning","position":5},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl3":"Performance of behavioral cloning","lvl2":"Behavioral cloning"},"type":"lvl3","url":"/imitation-learning#performance-of-behavioral-cloning","position":6},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl3":"Performance of behavioral cloning","lvl2":"Behavioral cloning"},"content":"Can we quantify how well this algorithm works?\nFor simplicity, let’s consider the case where the action space is finite and both the expert policy and learned policy are deterministic.\nSuppose the learned policy obtains \\varepsilon classification error.\nThat is, for trajectories drawn from the expert policy,\nthe learned policy chooses a different action at most \\varepsilon of the time:\\mathbb{E}_{\\tau \\sim \\rho_{\\pi_{\\text{expert}}}} \\left[ \\frac 1 \\hor \\sum_{\\hi=0}^{\\hor-1} \\ind{ \\widetilde{\\pi}(s_\\hi) \\ne \\pi_{\\text{expert}} (s_\\hi) } \\right] \\le \\varepsilon\n\nThen, their value functions differ by| V^{\\pi_{\\text{expert}}} - V^{\\widetilde{\\pi}} | \\le H^2 \\varepsilon\n\nwhere H is the horizon.\n\nPerformance of behavioral cloning\n\nRecall the \n\nTheorem 1 allows us to express the difference between \\pi_{\\text{expert}} and \\widetilde{\\pi} asV_0^{\\pi_{\\text{expert}}}(s) - V_0^{\\widetilde{\\pi}} (s) = \\E_{\\tau \\sim \\rho^{\\pi_{\\text{expert}}} \\mid s_0 = s} \\left[ \\sum_{\\hi=0}^{\\hor-1} A_\\hi^{\\widetilde{\\pi}} (s_\\hi, a_\\hi) \\right].\n\nNow since the expert policy is deterministic, we can substitute a_\\hi = \\pi_{\\text{expert}}(s_\\hi).\nThis allows us to make a further simplification:\nsince \\pi_{\\text{expert}} is deterministic,\nthe advantage of the chosen action is exactly zero:A^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) = Q^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) - V^{\\pi_{\\text{expert}}}(s) = 0.\n\nBut the right-hand-side of \n\n(7.7) uses A^{\\widetilde{\\pi}}, not A^{\\pi_{\\text{expert}}}.\nTo bridge this gap,\nwe now use the assumption that \\widetilde{\\pi} obtains \\varepsilon classification error.\nNote that A_\\hi^{\\widetilde{\\pi}}(s_\\hi, \\pi_{\\text{expert}}(s_\\hi)) = 0 when \\pi_{\\text{expert}}(s_\\hi) = \\widetilde{\\pi}(s_\\hi).\nIn the case where the two policies differ on s_\\hi, which occurs with probability \\varepsilon, the advantage is naively upper bounded by H (assuming rewards are bounded between 0 and 1).\nTaking the final sum gives the desired bound. TODO ADD DISTRIBUTION SHIFT EXAMPLE FROM SLIDES ","type":"content","url":"/imitation-learning#performance-of-behavioral-cloning","position":7},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Distribution shift"},"type":"lvl2","url":"/imitation-learning#distribution-shift","position":8},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Distribution shift"},"content":"Let us return to the driving analogy. Suppose you have taken some driving lessons and now feel comfortable in your neighbourhood. But today you have to travel to an area you haven’t visited before, such as a highway, where it would be dangerous to try and apply the techniques you’ve already learned.\nThis is the issue of distribution shift: a policy learned under a certain distribution of states may not perform well if this distribution changes.\n\nThis is already a common issue in supervised learning, where the training dataset for a model might not resemble the environment where it gets deployed.\nIn interactive environments, this issue is further exacerbated by the dependency between the observations and the agent’s behavior; if you take a wrong turn early on, it may be difficult or impossible to recover in that trajectory.\n\nHow could you learn a strategy for these new settings?\nIn the driving example, you might decide to install a dashcam to record the car’s surroundings. That way, once you make it back to safety, you can show the recording to an expert, who can provide feedback at each step of the way.\nThen the next time you go for a drive, you can remember the expert’s advice, and take a safer route.\nYou could then repeat this training as many times as desired, thereby collecting the expert’s feedback over a diverse range of locations.\nThis is the key idea behind dataset aggregation.","type":"content","url":"/imitation-learning#distribution-shift","position":9},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Dataset aggregation (DAgger)"},"type":"lvl2","url":"/imitation-learning#dataset-aggregation-dagger","position":10},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Dataset aggregation (DAgger)"},"content":"The DAgger algorithm is due to \n\nRoss et al. (2010).\nIt assumes that we have query access to the expert policy.\nThat is, for a given state s,\nwe can ask for the expert’s action \\pi_{\\text{expert}}(s) in that state.\nWe also need access to the environment for rolling out policies.\nThis makes DAgger an online algorithm,\nas opposed to pure behavioral cloning,\nwhich is offline since we don’t need to act in the environment at all.\n\nYou can think of DAgger as a specific way of collecting the dataset \\mathcal{D}.\n\nDAgger\n\nInputs: \\pi_{\\text{expert}}, an initial policy \\pi_{\\text{init}}, the number of iterations T, and the number of trajectories N to collect per iteration.\n\nInitialize \\mathcal{D} = \\{\\} (the empty set) and \\pi = \\pi_{\\text{init}}.\n\nFor t = 1, \\dots, T:\n\nCollect N trajectories \\tau_1, \\dots, \\tau_N using the current policy π.\n\nFor each trajectory \\tau_n:\n\nReplace each action a_h in \\tau_n with the expert action \\pi_{\\text{expert}}(s_h).\n\nCall the resulting trajectory \\tau^{\\text{expert}}_n.\n\n\\mathcal{D} \\gets \\mathcal{D} \\cup \\{ \\tau^{\\text{expert}}_1, \\dots, \\tau^{\\text{expert}}_n \\}.\n\nLet \\pi \\gets \\texttt{fit}(\\mathcal{D}), where \\texttt{fit} is a behavioral cloning algorithm.\n\nReturn π.\n\nHow well does DAgger perform?\nWe omit a proof here, but under certain assumptions,\nthe DAgger algorithm can better approximate the expert policy:|V^{\\pi_{\\text{expert}}} - V^{\\pi_{\\text{DAgger}}}| \\le H \\varepsilon\n\nwhere \\varepsilon is the “classification error” guaranteed by the supervised learning algorithm. TODO ","type":"content","url":"/imitation-learning#dataset-aggregation-dagger","position":11},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Summary"},"type":"lvl2","url":"/imitation-learning#summary","position":12},{"hierarchy":{"lvl1":"7 Imitation Learning","lvl2":"Summary"},"content":"For tasks where it is too difficult or expensive to learn from scratch,\nwe can instead start off with a collection of expert demonstrations.\nThen we can use supervised learning techniques to find a policy that imitates the expert demonstrations.\n\nThe simplest way to do this is to apply a supervised learning algorithm to an already-collected dataset of expert state-action pairs.\nThis is called behavioral cloning.\nHowever, given query access to the expert policy,\nwe can do better by integrating its feedback in an online loop.\nThe DAgger algorithm is one way of doing this,\nwhere we use the expert policy to augment trajectories and then learn from this augmented dataset using behavioral cloning.","type":"content","url":"/imitation-learning#summary","position":13},{"hierarchy":{"lvl1":"Introduction"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Introduction"},"content":"Welcome to the study of reinforcement learning!\nThis textbook accompanies the undergraduate course \n\nCS 1840/STAT 184 taught at Harvard.\nIt is intended to be a friendly yet rigorous introduction to this active subfield of machine learning.\n\n","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prerequisites"},"type":"lvl2","url":"/#prerequisites","position":2},{"hierarchy":{"lvl1":"Introduction","lvl2":"Prerequisites"},"content":"This book assumes the same prerequisites as the course: You should be familiar with multivariable calculus, linear algebra, and probability.\nFor Harvard undergraduates, this is fulfilled by Math 21a, Math 21b, and Stat 110, or their equivalents.\nStat 111 is strongly recommended but not required.\nSpecifically, we will assume that you know the following topics. The italicized terms have brief re-introductions in the text or in the \n\nAppendix: Background:\n\nLinear Algebra: Vectors and matrices, matrix multiplication, matrix\ninversion, eigenvalues and eigenvectors.\n\nMultivariable Calculus: Partial derivatives, the chain rule, Taylor series, gradients, directional derivatives, Lagrange multipliers.\n\nProbability: Random variables, probability distributions,\nexpectation and variance, the law of iterated expectations (Adam’s rule), covariance, conditional probability, Bayes’s rule, and the law of total probability.\n\nYou should also be comfortable with programming in Python.\nSee \n\nSection 6 for more about this textbook’s philosophy regarding programming.\n\n","type":"content","url":"/#prerequisites","position":3},{"hierarchy":{"lvl1":"Introduction","lvl2":"Reinforcement learning in a nutshell"},"type":"lvl2","url":"/#reinforcement-learning-in-a-nutshell","position":4},{"hierarchy":{"lvl1":"Introduction","lvl2":"Reinforcement learning in a nutshell"},"content":"Broadly speaking,\nRL studies sequential decision-making in dynamic environments.\nAn RL algorithm finds a strategy, called a policy, that maximizes the reward it obtains from the environment.\n\nRL provides a powerful framework for attacking a wide variety of problems,\nincluding robotic control, video games and board games, resource management, language modelling, and more.\nIt also provides an interdisciplinary paradigm for studying animal and human behavior.\nMany of the most stunning results in machine learning, ranging from AlphaGo to ChatGPT, are built using RL algorithms.\n\nHow does RL compare to the other two core machine learning paradigms,\nsupervised learning and unsupervised learning?\n\nSupervised learning (SL) concerns itself with learning a mapping from inputs to outputs.\nTypically the data takes the form of statistically independent input-output pairs.\nIn RL, however, the data is generated by the agent interacting with the environment,\nmeaning the sequential observations of the state are not independent from each other.\n\nConversely, SL is a well-studied field that provides many useful tools for RL.\n\nUnsupervised learning concerns itself with learning the structure of data without the use of outside feedback or labels.\nIn RL, though, the agent receives a reward signal from the environment,\nwhich can be thought of as a sort of feedback.\n\nUnsupervised learning is crucial in many real-world applications of RL for dimensionality reduction and other purposes.\n\n","type":"content","url":"/#reinforcement-learning-in-a-nutshell","position":5},{"hierarchy":{"lvl1":"Introduction","lvl2":"Core tasks of reinforcement learning"},"type":"lvl2","url":"/#core-tasks-of-reinforcement-learning","position":6},{"hierarchy":{"lvl1":"Introduction","lvl2":"Core tasks of reinforcement learning"},"content":"What tasks, exactly, does RL comprise?\nAn RL algorithm must typically solve two main subtasks:\n\nPolicy evaluation (prediction):\nHow ‘good’ is a specific state, or state-action pair (under a given policy)?\nThat is, how much reward does it lead to in the long run?\n\nPolicy optimization (control):\nSuppose we fully understand how the environment behaves.\nWhat is the best action to take in every scenario? **Recursion (bootstrapping):** How can we \"reuse\" our current predictions to generate new information?  **Exploration-exploitation tradeoff:** Should we try new actions, or capitalize on actions that we currently believe to be good? \n\n","type":"content","url":"/#core-tasks-of-reinforcement-learning","position":7},{"hierarchy":{"lvl1":"Introduction","lvl2":"Course overview"},"type":"lvl2","url":"/#course-overview","position":8},{"hierarchy":{"lvl1":"Introduction","lvl2":"Course overview"},"content":"The course will progress through the following units:\n\n1 Markov Decision Processes introduces Markov Decision Processes,\nthe core mathematical framework for describing a large class of interactive environments.\n\n2 Linear Quadratic Regulators is a standalone chapter on the linear quadratic regulator (LQR),\nan important tool for continuous control,\nin which the state and action spaces are no longer finite but rather continuous.\nThis has widespread applications in robotics.\n\n3 Multi-Armed Bandits introduces the multi-armed bandit (MAB) model for stateless sequential decision-making tasks.\nIn exploring a number of algorithms,\nwe will see how each of them strikes a different balance between exploring new options and exploiting known options.\nThis exploration-exploitation tradeoff is a core consideration in RL algorithm design.\n\n4 Supervised learning is a standalone crash course on some tools from supervised learning that we will use in later chapters.\n\n5 Fitted Dynamic Programming Algorithms introduces fitted dynamic programming (fitted DP) algorithms for solving MDPs.\nThese algorithms use supervised learning to approximately evaluate policies when they cannot be evaluated exactly.\n\n6  Policy Gradient Methods explores an important class of algorithms based on iteratively improving a policy.\nWe will also encounter the use of deep neural networks to express more complicated policies and approximate complicated functions.\n\n7 Imitation Learning attempts to learn a good policy from expert demonstrations.\nAt its most basic, this is an application of supervised learning to RL tasks.\n\n8 Tree Search Methods looks at ways to explicitly plan ahead when the environment’s dynamics are known.\nWe will study the Monte Carlo Tree Search heuristic,\nwhich has been used to great success in the famous AlphaGo algorithm and its successors.\n\n9 Exploration in MDPs continues to investigate the exploration-exploitation tradeoff.\nWe will extend ideas from multi-armed bandits to the MDP setting.\n\nAppendix: Background contains an overview of selected background mathematical content and programming content. \n| Chapter | States | Actions | Rewards (or costs) |\n|:-------:|:------:|:-------:|:-------:|\n| [](#bandits) | N/A | Finite | Stochastic |\n| [](#mdps) | Finite | Finite | Deterministic |\n| [](#fitted_dp) | Large or continuous | Finite | Deterministic |\n| [](#lqr) | Continuous | Continuous | Deterministic |\n\n\n","type":"content","url":"/#course-overview","position":9},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"type":"lvl2","url":"/#notation","position":10},{"hierarchy":{"lvl1":"Introduction","lvl2":"Notation"},"content":"We will use the following notation throughout the book.\nThis notation is inspired by \n\nSutton & Barto (2018) and \n\nAgarwal et al. (2022).\nWe use [N] as shorthand for the set \\{ 0, 1, \\dots, N-1 \\}.\n\nElement\n\nSpace\n\nDefinition (of element)\n\ns\n\n\\mathcal{S}\n\nA state.\n\na\n\n\\mathcal{A}\n\nAn action.\n\nr\n\n\n\nA reward.\n\nγ\n\n\n\nA discount factor.\n\nτ\n\n\\mathcal{T}\n\nA trajectory.\n\nπ\n\nΠ\n\nA policy.\n\nV^\\pi\n\n\\mathcal{S} \\to \\mathbb{R}\n\nThe value function of policy π.\n\nQ^\\pi\n\n\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\n\nThe action-value function (a.k.a. Q-function) of policy π.\n\nA^\\pi\n\n\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\n\nThe advantage function of policy π.\n\n\n\n\\triangle(\\mathcal{X})\n\nA distribution supported on \\mathcal{X}.\n\n\\hi\n\n[\\hor]\n\nTime horizon index of an MDP (subscript).\n\nk\n\n[K]\n\nArm index of a multi-armed bandit (superscript).\n\nt\n\n[T]\n\nIteration index of an algorithm (subscript).\n\nθ\n\nΘ\n\nA set of parameters.\n\nNote that throughout the text, certain symbols will stand for either random variables or fixed values.\nWe aim to clarify in ambiguous settings.\nBe warned that\n\n","type":"content","url":"/#notation","position":11},{"hierarchy":{"lvl1":"Introduction","lvl2":"Programming"},"type":"lvl2","url":"/#programming","position":12},{"hierarchy":{"lvl1":"Introduction","lvl2":"Programming"},"content":"Why include code in a textbook?\nWe believe that implementing an algorithm is a strong test of your understanding of it;\nmathematical notation can often abstract away details,\nwhile a computer must be given every single instruction.\nWe have sought to write readable Python code that is self-contained within each file.\nThis approach is inspired by \n\nSussman et al. (2013).\nThere are some ways in which the code style differs from typical software projects:\n\nWe keep use of language features to a minimum,\neven if it leads to code that could otherwise be more concisely or idiomatically expressed.\n\nThe variable names used in the code match those used in the main text.\nFor example, the variable s will be used instead of the more explicit state.\n\nWe also make extensive use of Python type annotations to explicitly specify variable types, including shapes of vectors and matrices using the \n\njaxtyping library.\n\nThis is an interactive book built with \n\nJupyter Book.\nIt uses \n\nPython 3.11.\nIt uses the \n\nJAX library for numerical computing.\nJAX was chosen for the clarity of its functional style and due to its mature RL ecosystem,\nsustained in large part by the Google DeepMind research group and a large body of open-source contributors.\nWe use the standard \n\nGymnasium library for interfacing with RL environments.\n\nThe following names are exported from the utils module:import matplotlib.pyplot as plt\n\n# convenient class builder\nfrom typing import NamedTuple\n\n# function typings\nfrom collections.abc import Callable\n\n# array typings\nfrom jaxtyping import Float, Array\n\n# convenient function composition\nfrom functools import partial\n\n# numerical computing and linear algebra\nimport jax\nimport jax.numpy as jnp\n\n# print functions as latex\nimport latexify\n\nplt.style.use(\"fivethirtyeight\")","type":"content","url":"/#programming","position":13},{"hierarchy":{"lvl1":"1 Markov Decision Processes"},"type":"lvl1","url":"/mdps","position":0},{"hierarchy":{"lvl1":"1 Markov Decision Processes"},"content":"","type":"content","url":"/mdps","position":1},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Introduction"},"type":"lvl2","url":"/mdps#introduction","position":2},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Introduction"},"content":"The field of RL studies how an agent can learn to make sequential decisions in an interactive environment.\nThis is a very general problem!\nHow can we formalize this task in a way that is both sufficiently general yet also tractable enough for fruitful analysis?\n\nLet’s consider some examples of sequential decision problems to identify the key common properties we’d like to capture:\n\nBoard games and video games, where a player takes actions in a virtual environment.\n\nInventory management, where a company must efficiently move resources from producers to consumers.\n\nRobotic control, where a robot can move and interact with the real world to complete some task.\n\nIn these environments and many others, the state transitions,\nthe “rules” of the environment,\nonly depend on the most recent state and action (generally speaking).\nFor example, if you want to take a break while playing a game of chess,\nyou could take a picture of the board,\nand later on reset the board to that state and continue playing;\nthe past history of moves doesn’t matter (generally speaking).\nThis is called the Markov property.\n\nMarkov property\n\nAn interactive environment satisfies the Markov property if the\nprobability of transitioning to a new state only depends on the current\nstate and action:\\pr(s_{\\hi+1} \\mid s_0, a_0, \\dots, s_\\hi, a_\\hi) = P(s_{\\hi+1} \\mid s_\\hi, a_\\hi)\n\nwhere P : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S}) describes the state transitions.\n(We’ll elaborate on this notation later in the chapter.)\n\nEnvironments that satisfy the Markov property are called Markov decision processes (MDPs).\nThis chapter will focus on introducing core vocabulary for MDPs that will be useful throughout the book.\n\nAttention\n\nWhat information might be encoded in the state for each of the above examples?\nWhat might the valid set of actions be?\nDescribe the state transitions heuristically and verify that they satisfy the Markov property.\n\nMDPs are usually classified as finite-horizon, where the interactions end after some finite number of time steps,\nor infinite-horizon, where the interactions can continue indefinitely.\nWe’ll begin with the finite-horizon case and discuss the infinite-horizon case in the second half of the chapter.\n\nWe’ll describe how to evaluate different strategies, called policies, and how to compute (or approximate)\nthe optimal policy for a given MDP.\nWe’ll introduce the Bellman consistency condition, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.\n\nfrom utils import NamedTuple, Float, Array, partial, jax, jnp, latexify\n\n","type":"content","url":"/mdps#introduction","position":3},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Finite-horizon MDPs"},"type":"lvl2","url":"/mdps#finite-horizon-mdps","position":4},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Finite-horizon MDPs"},"content":"","type":"content","url":"/mdps#finite-horizon-mdps","position":5},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Definition","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#definition","position":6},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Definition","lvl2":"Finite-horizon MDPs"},"content":"Finite-horizon Markov decision process\n\nThe components of a finite-horizon Markov decision process are:\n\nThe state that the agent interacts with. We use \\mathcal{S} to denote\nthe set of possible states, called the state space.\n\nThe actions that the agent can take. We use \\mathcal{A} to denote the\nset of possible actions, called the action space.\n\nSome initial state distribution \\mu \\in \\triangle(\\mathcal{S}).\n\nThe state transitions (a.k.a. dynamics)\nP : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S}) that describe what state the agent\ntransitions to after taking an action.\n\nThe reward signal. In this course we’ll take it to be a\ndeterministic function on state-action pairs,\nr : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}, but in general many results will\nextend to a stochastic reward signal.\n\nA time horizon \\hor \\in \\mathbb{N} that specifies the number of\ninteractions in an episode.\n\nCombined together, these objects specify a finite-horizon Markov\ndecision process:M = (\\mathcal{S}, \\mathcal{A}, \\mu, P, r, \\hor).\n\nWhen there are finitely many states and actions, i.e.\n|\\mathcal{S}|, |\\mathcal{A}| < \\infty, we can express\nthe relevant quantities as vectors and matrices (i.e. tables of\nvalues):\\begin{aligned}\n    \\mu &\\in [0, 1]^{|\\mathcal{S}|} &\n    P &\\in [0, 1]^{(|\\mathcal{S} \\times \\mathcal{A}|) \\times |\\mathcal{S}|} &\n    r &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}\n\\end{aligned}\n\nAttention\n\nVerify that the types and shapes provided above make sense!\n\nclass MDP(NamedTuple):\n    \"\"\"A description of a Markov decision process with finitely many states and actions.\"\"\"\n    S: int  # number of states\n    A: int  # number of actions\n    μ: Float[Array, \" S\"]\n    P: Float[Array, \"S A S\"]  # \"current\" state, \"current\" action, \"next\" state\n    r: Float[Array, \"S A\"]\n    H: int\n    γ: float = 1.0  # discount factor (used later)\n\nTidying MDP\n\nLet’s consider a simple decision problem throughout this chapter:\nthe task of keeping your room tidy!\n\nYour room has the possible states\n\\mathcal{S} = \\{ \\text{orderly}, \\text{messy} \\}.\nYou can take either of the actions \\mathcal{A} = \\{ \\text{ignore}, \\text{tidy} \\}.\nThe room starts off orderly.\n\nThe state transitions are as follows:\nif you tidy the room, it becomes (or remains) orderly;\nif you ignore the room, it might become messy (see table below).\n\nThe rewards are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room,\nbut you get rewarded for ignoring an orderly room (since you can enjoy your additional time).\nTidying a messy room is a chore that gives no reward.\n\nThese are summarized in the following table:\\begin{array}{ccccc}\n    s & a & P(\\text{orderly} \\mid s, a) & P(\\text{messy} \\mid s, a) & r(s, a) \\\\\n    \\text{orderly} & \\text{ignore} & 0.7 & 0.3 & 1 \\\\\n    \\text{orderly} & \\text{tidy} & 1 & 0 & -1 \\\\\n    \\text{messy} & \\text{ignore} & 0 & 1 & -1 \\\\\n    \\text{messy} & \\text{tidy} & 1 & 0 & 0 \\\\\n\\end{array}\n\nConsider a time horizon of \\hor = 7 days (one interaction per day). Let\nt = 0 correspond to Monday and t = 6 correspond to Sunday.\n\ntidy_mdp = MDP(\n    S=2,  # 0 = orderly, 1 = messy\n    A=2,  # 0 = ignore, 1 = tidy\n    μ=jnp.array([1.0, 0.0]),  # start in orderly state\n    P=jnp.array([\n        [\n            [0.7, 0.3],  # orderly, ignore\n            [1.0, 0.0],  # orderly, tidy\n        ],\n        [\n            [0.0, 1.0],  # messy, ignore\n            [1.0, 0.0],  # messy, tidy\n        ],\n    ]),\n    r=jnp.array([\n        [\n            1.0,   # orderly, ignore\n            -1.0,  # orderly, tidy\n        ],\n        [\n            -1.0,  # messy, ignore\n            0.0,   # messy, tidy\n        ]\n    ]),\n    H=7,\n)\n\n","type":"content","url":"/mdps#definition","position":7},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policies","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#policies","position":8},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policies","lvl2":"Finite-horizon MDPs"},"content":"Policies\n\nA policy π describes the agent’s strategy:\nwhich actions it takes in a given situation.\nA key goal of RL is to find the optimal policy that maximizes the total reward on average.\n\nThere are three axes along which policies can vary: their outputs,\ninputs, and time-dependence.\n\nDeterministic or stochastic. A deterministic policy outputs\nactions while a stochastic policy outputs distributions over\nactions.\n\n\n\nA deterministic policy.\n\n\n\nA stochastic policy.\n\nState-dependent or history-dependent. A state-dependent (a.k.a.\n“Markovian”) policy only depends on the current state, while a\nhistory-dependent policy depends on the sequence of past states,\nactions, and rewards. We’ll only consider state-dependent policies\nin this course.\n\nStationary or time-dependent. A stationary (a.k.a. time-homogeneous) policy\nremains the same function at all time steps, while a time-dependent policy can depend on the current timestep.\nFor consistency with states and actions, we will denote the timestep as a subscript,\ni.e. \\pi = \\{ \\pi_0, \\dots, \\pi_{\\hor-1} \\}.\n\nNote that for finite state and action spaces,\nwe can represent a randomized mapping \\mathcal{S} \\to \\Delta(\\mathcal{A})\nas a matrix \\pi \\in [0, 1]^{\\mathcal{S} \\times \\mathcal{A}} where each row describes\nthe policy’s distribution over actions for the corresponding state.\n\nA fascinating result is that every finite-horizon MDP has an optimal deterministic time-dependent policy!\nIntuitively, the Markov property implies that the current state contains all the information we need to make the optimal decision.\nWe’ll prove this result constructively later in the chapter.\n\nPolicies for the tidying MDP\n\nHere are some possible policies for the tidying MDP \n\nExample 1.1:\n\nAlways tidy: \\pi(s) = \\text{tidy}.\n\nOnly tidy on weekends: \\pi_\\hi(s) = \\text{tidy} if\n\\hi \\in \\{ 5, 6 \\} and \\pi_\\hi(s) = \\text{ignore} otherwise.\n\nOnly tidy if the room is messy: \\pi_\\hi(\\text{messy}) = \\text{tidy}\nand \\pi_\\hi(\\text{orderly}) = \\text{ignore} for all \\hi.\n\n# arrays of shape (H, S, A) represent time-dependent policies\ntidy_policy_always_tidy = (\n    jnp.zeros((7, 2, 2))\n    .at[:, :, 1].set(1.0)\n)\ntidy_policy_weekends = (\n    jnp.zeros((7, 2, 2))\n    .at[5:7, :, 1].set(1.0)\n    .at[0:5, :, 0].set(1.0)\n)\ntidy_policy_messy_only = (\n    jnp.zeros((7, 2, 2))\n    .at[:, 1, 1].set(1.0)\n    .at[:, 0, 0].set(1.0)\n)\n\nNote\n\nArray objects in Jax are immutable, that is, they cannot be changed.\nThis might seem inconvenient, but in larger projects,\nimmutability makes code much easier to reason about.\n\n","type":"content","url":"/mdps#policies","position":9},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Trajectories","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#trajectories","position":10},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Trajectories","lvl2":"Finite-horizon MDPs"},"content":"Trajectories\n\nA sequence of states, actions, and rewards is called a trajectory:\\tau = (s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1})\n\nwhere r_\\hi = r(s_\\hi, a_\\hi).\n(Note that some sources omit the reward at the final time step. This is a minor detail.)\n\nclass Transition(NamedTuple):\n    \"\"\"A single state-action-reward interaction with the environment.\n\n    A trajectory comprises a sequence of transitions.\n    \"\"\"\n    s: int\n    a: int\n    r: float\n\nOnce we’ve chosen a policy,\nwe can sample trajectories by repeatedly choosing actions according to the policy,\ntransitioning according to the state transitions, and observing the rewards.\n\nThat is, a policy induces a distribution \\rho^{\\pi} over trajectories.\n(We assume that μ and P are clear from context.)\n\nTrajectories in the tidying environment\n\nHere is a possible trajectory for the tidying example:\n\n\\hi\n\n0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\ns\n\norderly\n\norderly\n\norderly\n\nmessy\n\nmessy\n\norderly\n\norderly\n\na\n\ntidy\n\nignore\n\nignore\n\nignore\n\ntidy\n\nignore\n\nignore\n\nr\n\n-1\n\n1\n\n1\n\n-1\n\n0\n\n1\n\n1\n\nCould any of the policies in \n\nExample 1.2 have generated this trajectory?\n\nNote that for a state-dependent policy, using the Markov property \n\nDefinition 1.1,\nwe can write down the likelihood function of this probability distribution in an autoregressive way (i.e. one timestep at a time):\n\nAutoregressive trajectory distribution\\rho^{\\pi}(\\tau) := \\mu(s_0) \\pi_0(a_0 \\mid s_0) P(s_1 \\mid s_0, a_0) \\cdots P(s_{\\hor-1} \\mid s_{\\hor-2}, a_{\\hor-2}) \\pi_{\\hor-1}(a_{\\hor-1} \\mid s_{\\hor-1})\n\ndef trajectory_log_likelihood(\n    mdp: MDP,\n    τ: list[Transition],\n    π: Float[Array, \"S A\"],\n) -> float:\n    \"\"\"Compute the log-likelihood of a trajectory under a given MDP and policy.\"\"\"\n\n    # initial distribution and action\n    total = jnp.log(mdp.μ[τ[0].s])\n    total += jnp.log(π[τ[0].s, τ[0].a])\n\n    # remaining state transitions and actions\n    for i in range(1, mdp.H):\n        total += jnp.log(mdp.P[τ[i - 1].s, τ[i - 1].a, τ[i].s])\n        total += jnp.log(π[τ[i].s, τ[i].a])\n\n    return total\n\nAttention\n\nHow would you modify this to include stochastic rewards?\n\nFor a deterministic policy π, we have that \\pi_\\hi(a \\mid s) = \\mathbb{I}[a = \\pi_\\hi(s)];\nthat is, the probability of taking an action is 1 if it’s the unique action prescribed by the policy for that state and 0 otherwise.\nIn this case, the only randomness in sampling trajectories comes from the initial state distribution μ and the state transitions P.\n\n","type":"content","url":"/mdps#trajectories","position":11},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#value-functions","position":12},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"content":"The main goal of RL is to find a policy that maximizes the expected total\nreward \\E [r_0 + \\cdots + r_{\\hor-1}].\n\nAttention\n\nNote that r_0 + \\cdots + r_{\\hor-1} is a random variable.\nWhat sources of randomness does it depend on?\nDescribe the generating process.\n\nLet’s introduce some notation for analyzing this quantity.\n\nA policy’s value function at time \\hi is its expected remaining reward from a given state:\n\nValue functionV_\\hi^\\pi(s) := \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s]\n\nSimilarly, we can define the action-value function (aka the\nQ-function) at time h as the expected remaining reward from a given state and taking a given action:\n\nAction-value functionQ_\\hi^\\pi(s, a) := \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s, a_\\hi = a]\n\n","type":"content","url":"/mdps#value-functions","position":13},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Relating the value function and action-value function","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"type":"lvl4","url":"/mdps#relating-the-value-function-and-action-value-function","position":14},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Relating the value function and action-value function","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"content":"Note that the value function is just the expected action-value over\nactions drawn from the policy:V_\\hi^\\pi(s) = \\E_{a \\sim \\pi_\\hi(s)} [Q_\\hi^\\pi(s, a)]\n\ndef q_to_v(\n    policy: Float[Array, \"S A\"],\n    q: Float[Array, \"S A\"],\n) -> Float[Array, \" S\"]:\n    \"\"\"\n    Compute the value function for a given policy in a known finite MDP\n    at a single timestep from its action-value function.\n    \"\"\"\n    return jnp.average(q, weights=policy, axis=1)\n\nand the action-value is the sum of the immediate reward and the expected value of the following\nstate:Q_\\hi^\\pi(s, a) = r(s, a) + \\E_{s' \\sim P(s, a)} [V_{\\hi+1}^\\pi(s')]\n\ndef v_to_q(\n    mdp: MDP,\n    v_next: Float[Array, \" S\"],\n) -> Float[Array, \"S A\"]:\n    \"\"\"\n    Compute the action-value function in a known finite MDP\n    at a single timestep from the corresponding value function.\n    \"\"\"\n    # the discount factor is relevant later\n    return mdp.r + mdp.γ * mdp.P @ v_next\n\n\n# convert a list of v functions to a list of q functions\nv_ary_to_q_ary = jax.vmap(v_to_q, in_axes=(None, 0))\n\n","type":"content","url":"/mdps#relating-the-value-function-and-action-value-function","position":15},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Greedy policies","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"type":"lvl4","url":"/mdps#greedy-policies","position":16},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Greedy policies","lvl3":"Value functions","lvl2":"Finite-horizon MDPs"},"content":"For any given Q \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}, we can define the greedy policy \\hat \\pi_Q as the deterministic policy that selects the action with the highest Q-value at each state:\\hat \\pi_Q(s) = \\arg\\max_{a} Q_{sa}\n\ndef q_to_greedy(q: Float[Array, \"S A\"]) -> Float[Array, \"S A\"]:\n    \"\"\"\n    Get the (deterministic) greedy policy with respect to an action-value function.\n    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.\n    \"\"\"\n    A = q.shape[1]\n    a_ary = jnp.argmax(q, axis=1)\n    return jnp.eye(A)[a_ary]\n\n\ndef v_to_greedy(mdp: MDP, v: Float[Array, \" S\"]) -> Float[Array, \"S A\"]:\n    \"\"\"Get the (deterministic) greedy policy with respect to a value function.\"\"\"\n    return q_to_greedy(v_to_q(mdp, v))\n\n","type":"content","url":"/mdps#greedy-policies","position":17},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The one-step (Bellman) consistency equation","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#the-one-step-bellman-consistency-equation","position":18},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The one-step (Bellman) consistency equation","lvl2":"Finite-horizon MDPs"},"content":"Note that by simply considering the cumulative reward as the sum of the\ncurrent reward and the future cumulative reward, we can describe the\nvalue function recursively (in terms of itself). This is named the\nBellman consistency equation after Richard Bellman (1920--1984),\nwho is credited with introducing dynamic programming in 1953.\n\nBellman consistency equation for the value functionV_\\hi^\\pi(s) = \\E_{\\substack{a \\sim \\pi_\\hi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + V_{\\hi+1}^\\pi(s')]\n\ndef check_bellman_consistency_v(\n    mdp: MDP,\n    policy: Float[Array, \"H S A\"],\n    v_ary: Float[Array, \"H S\"],\n) -> bool:\n    \"\"\"\n    Check that the given (time-dependent) \"value function\"\n    satisfies the Bellman consistency equation.\n    \"\"\"\n    return all(\n        jnp.allclose(\n            # lhs\n            v_ary[h],\n            # rhs\n            jnp.sum(policy[h] * (mdp.r + mdp.γ * mdp.P @ v_ary[h + 1]), axis=1),\n        )\n        for h in range(mdp.H - 1)\n    )\n\nAttention\n\nVerify that this equation holds by expanding V_\\hi^\\pi(s)\nand V_{\\hi+1}^\\pi(s').\n\nOne can analogously derive the Bellman consistency equation for the\naction-value function:\n\nBellman consistency equation for action-valuesQ_\\hi^\\pi(s, a) = r(s, a) + \\E_{\\substack{s' \\sim P(s, a) \\\\ a' \\sim \\pi_{\\hi+1}(s')}} [Q_{\\hi+1}^\\pi(s', a')]\n\nAttention\n\nWrite a check_bellman_consistency_q function for the action-value function.\n\nThe Bellman consistency equation for deterministic policies\n\nNote that for deterministic policies, the Bellman consistency equation\nsimplifies to\\begin{aligned}\n    V_\\hi^\\pi(s) &= r(s, \\pi_\\hi(s)) + \\E_{s' \\sim P(s, \\pi_\\hi(s))} [V_{\\hi+1}^\\pi(s')] \\\\\n    Q_\\hi^\\pi(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [Q_{\\hi+1}^\\pi(s', \\pi_{\\hi+1}(s'))]\n\\end{aligned}\n\n","type":"content","url":"/mdps#the-one-step-bellman-consistency-equation","position":19},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The one-step Bellman operator","lvl2":"Finite-horizon MDPs"},"type":"lvl3","url":"/mdps#the-one-step-bellman-operator","position":20},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The one-step Bellman operator","lvl2":"Finite-horizon MDPs"},"content":"Fix a policy π. Consider the higher-order operator that takes in a\n“value function” v : \\mathcal{S} \\to \\mathbb{R} and returns the r.h.s. of the Bellman\nequation for that “value function”:\n\nBellman operator[\\mathcal{J}^{\\pi}(v)](s) := \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + v(s')].\n\nThis is a crucial tool for reasoning about MDPs.\nIntuitively, it answers the following question:\nif we evaluate the next state using v,\nhow good is the current state, according to the given policy?\n\ndef bellman_operator_looping(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -> Float[Array, \" S\"]:\n    \"\"\"\n    Looping definition of the Bellman operator.\n    Concise version is below\n    \"\"\"\n    v_new = jnp.zeros(mdp.S)\n    for s in range(mdp.S):\n        for a in range(mdp.A):\n            for s_next in range(mdp.S):\n                v_new[s] += (\n                    policy[s, a]\n                    * mdp.P[s, a, s_next]\n                    * (mdp.r[s, a] + mdp.γ * v[s_next])\n                )\n    return v_new\n\nNote that we can concisely implement this using the q_to_v and v_to_q utilities from above:\n\ndef bellman_operator(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -> Float[Array, \" S\"]:\n    \"\"\"For a known finite MDP, the Bellman operator can be exactly evaluated.\"\"\"\n    return q_to_v(policy, v_to_q(mdp, v))  # equivalent\n    return jnp.sum(policy * (mdp.r + mdp.γ * mdp.P @ v), axis=1)\n\nWe’ll call \\mathcal{J}^\\pi : \\mathbb{R}^\\mathcal{S} \\to \\mathbb{R}^\\mathcal{S} the Bellman\noperator of π.\nNote that it’s defined on any “value function” mapping states to real numbers;\nv doesn’t have to be a well-defined value function for some policy (hence the lowercase notation).\nThe Bellman operator also gives us a concise way to express \n\nTheorem 1.1 for the value function:V_\\hi^\\pi = \\mathcal{J}^{\\pi}(V_{\\hi+1}^\\pi)\n\nIntuitively, the output of the Bellman operator, a new “value function”,\nevaluates states as follows: from a given state, take one action\naccording to π, observe the reward, and then evaluate the next state\nusing the input “value function”.\n\nWhen we discuss infinite-horizon MDPs, the Bellman operator will turn\nout to be more than just a notational convenience: We’ll use it to\nconstruct algorithms for computing the optimal policy.","type":"content","url":"/mdps#the-one-step-bellman-operator","position":21},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Solving finite-horizon MDPs"},"type":"lvl2","url":"/mdps#finite-horizon-mdps-1","position":22},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Solving finite-horizon MDPs"},"content":"","type":"content","url":"/mdps#finite-horizon-mdps-1","position":23},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policy evaluation in finite-horizon MDPs","lvl2":"Solving finite-horizon MDPs"},"type":"lvl3","url":"/mdps#eval-dp","position":24},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policy evaluation in finite-horizon MDPs","lvl2":"Solving finite-horizon MDPs"},"content":"How can we actually compute the value function of a given policy? This\nis the task of policy evaluation.\n\nDP algorithm to evaluate a policy in a finite-horizon MDP\n\nThe Bellman consistency equation\n\n\nTheorem 1.1\ngives us a convenient algorithm for\nevaluating stationary policies: it expresses the value function at\ntimestep \\hi as a function of the value function at timestep \\hi+1. This\nmeans we can start at the end of the time horizon, where the value is\nknown, and work backwards in time, using the Bellman consistency\nequation to compute the value function at each time step.\n\ndef dp_eval_finite(mdp: MDP, policy: Float[Array, \"S A\"]) -> Float[Array, \"H S\"]:\n    \"\"\"Evaluate a policy using dynamic programming.\"\"\"\n    V_ary = [None] * mdp.H + [jnp.zeros(mdp.S)]  # initialize to 0 at end of time horizon\n    for h in range(mdp.H - 1, -1, -1):\n        V_ary[h] = bellman_operator(mdp, policy[h], V_ary[h + 1])\n    return jnp.stack(V_ary[:-1])\n\nThis runs in time O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|) by counting the\nloops.\n\nAttention\n\nDo you see where we compute Q^\\pi_\\hi along the way? Make\nthis step explicit.\n\nTidying policy evaluation\n\nLet’s evaluate the policy from\n\n\nExample 1.2 in the tidying MDP\nthat tidies if and only if the room is\nmessy. We’ll use the Bellman consistency equation to compute the value\nfunction at each time step.\\begin{aligned}\nV_{H-1}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) \\\\\n&= 1 \\\\\nV_{H-1}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) \\\\\n&= 0 \\\\\nV_{H-2}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\E_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-1}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1 + 0.3 \\cdot 0 \\\\\n&= 1.7 \\\\\nV_{H-2}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\E_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-1}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 \\\\\nV_{H-3}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\E_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-2}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1.7 + 0.3 \\cdot 1 \\\\\n&= 2.49 \\\\\nV_{H-3}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\E_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-2}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1.7\n\\end{aligned}\n\netc. You may wish to repeat this computation for the\nother policies to get a better sense of this algorithm.\n\nV_messy = dp_eval_finite(tidy_mdp, tidy_policy_messy_only)\nV_messy\n\n","type":"content","url":"/mdps#eval-dp","position":25},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Optimal policies in finite-horizon MDPs","lvl2":"Solving finite-horizon MDPs"},"type":"lvl3","url":"/mdps#opt-dynamic-programming","position":26},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Optimal policies in finite-horizon MDPs","lvl2":"Solving finite-horizon MDPs"},"content":"We’ve just seen how to evaluate a given policy. But how can we find\nthe optimal policy for a given environment?\n\nOptimal policies\n\nWe call a policy optimal, and denote it by \\pi^\\star, if it does at\nleast as well as any other policy π (including stochastic and\nhistory-dependent ones) in all situations:\\begin{aligned}\n    V_\\hi^{\\pi^\\star}(s) &= \\E_{\\tau \\sim \\rho^{\\pi^{\\star}}}[r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s] \\\\\n    &\\ge \\E_{\\tau \\sim \\rho^{\\pi}}[r_\\hi + \\cdots + r_{H-1} \\mid \\tau_\\hi] \\quad \\forall \\pi, \\tau_\\hi, \\hi \\in [H]\n\\end{aligned}\n\nwhere we condition on the\ntrajectory up to time \\hi, denoted\n\\tau_\\hi = (s_0, a_0, r_0, \\dots, s_\\hi), where s_\\hi = s.\n\nConvince yourself that all optimal policies must have the same value\nfunction. We call this the optimal value function and denote it by\nV_\\hi^\\star(s). The same goes for the action-value function\nQ_\\hi^\\star(s, a).\n\nIt is a stunning fact that every finite-horizon MDP has an optimal\npolicy that is time-dependent and deterministic. In particular, we can\nconstruct such a policy by acting greedily with respect to the optimal\naction-value function:\n\nIt is optimal to be greedy with respect to the optimal value function\\pi_\\hi^\\star(s) = \\arg\\max_a Q_\\hi^\\star(s, a).\n\nProof\n\nLet V^{\\star} and Q^{\\star} denote the optimal value and\naction-value functions. Consider the greedy policy\\hat \\pi_\\hi(s) := \\arg\\max_a Q_\\hi^{\\star}(s, a).\n\nWe aim to show that\n\\hat \\pi is optimal; that is, V^{\\hat \\pi} = V^{\\star}.\n\nFix an arbitrary state s \\in \\mathcal{S} and time \\hi \\in [H].\n\nFirstly, by the definition of V^{\\star}, we already know\nV_\\hi^{\\star}(s) \\ge V_\\hi^{\\hat \\pi}(s). So for equality to hold we just\nneed to show that V_\\hi^{\\star}(s) \\le V_\\hi^{\\hat \\pi}(s). We’ll first\nshow that the Bellman operator \\mathcal{J}^{\\hat \\pi} never decreases\nV_\\hi^{\\star}. Then we’ll apply this result recursively to show that\nV^{\\star} = V^{\\hat \\pi}.\n\nThe Bellman operator never decreases the optimal value function\n\n\\mathcal{J}^{\\hat \\pi} never decreases V_\\hi^{\\star}\n(elementwise):[\\mathcal{J}^{\\hat \\pi} (V_{\\hi+1}^{\\star})](s) \\ge V_\\hi^{\\star}(s).\n\nProof:\\begin{aligned}\n    V_\\hi^{\\star}(s) &= \\max_{\\pi \\in \\Pi} V_\\hi^{\\pi}(s) \\\\\n    &= \\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{a \\sim \\pi(\\dots)}\\left[r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^\\pi(s') \\right] && \\text{Bellman consistency} \\\\\n    &\\le \\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{a \\sim \\pi(\\dots)}\\left[r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^{\\star}(s') \\right] && \\text{definition of } V^\\star \\\\\n    &= \\max_{a} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^{\\star}(s') \\right] && \\text{only depends on } \\pi \\text{ via } a \\\\\n    &= [\\mathcal{J}^{\\hat \\pi}(V_{\\hi+1}^{\\star})](s).    \n\\end{aligned}\n\nNote that the chosen action a \\sim \\pi(\\dots) above\nmight depend on the past history; this isn’t shown in the notation and\ndoesn’t affect our result (make sure you see why).\n\nWe can now apply this result recursively to getV^{\\star}_t(s) \\le V^{\\hat \\pi}_t(s)\n\nas follows. (Note that even\nthough \\hat \\pi is deterministic, we’ll use the a \\sim \\hat \\pi(s)\nnotation to make it explicit that we’re sampling a trajectory from it.)\\begin{aligned}\n    V_{t}^{\\star}(s) &\\le [\\mathcal{J}^{\\hat \\pi}(V_{\\hi+1}^{\\star})](s) \\\\\n    &= \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} \\left[ {\\color{blue} V_{\\hi+1}^{\\star}(s')} \\right] \\right] && \\text{definition of } \\mathcal{J}^{\\hat \\pi} \\\\\n    &\\le \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} \\left[ {\\color{blue}[ \\mathcal{J}^{\\hat \\pi} (V_{t+2}^{\\star})] (s')} \\right] \\right] && \\text{above lemma} \\\\\n    &= \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}{\\color{blue} \\left[ \\mathop{\\mathbb{E}}_{a' \\sim \\hat \\pi}  r(s', a') + \\mathop{\\mathbb{E}}_{s''} V_{t+2}^{\\star}(s'') \\right]} \\right] && \\text{definition of } \\mathcal{J}^{\\hat \\pi} \\\\\n    &\\le \\cdots && \\text{apply at all timesteps} \\\\\n    &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\hat \\pi}} [G_{t} \\mid s_\\hi = s] && \\text{rewrite expectation} \\\\\n    &= V_{t}^{\\hat \\pi}(s) && \\text{definition}\n\\end{aligned}\n\nAnd so we have V^{\\star} = V^{\\hat \\pi}, making \\hat \\pi optimal.\n\nNote that this also gives simplified forms of the \n\nBellman consistency equations for the optimal policy:\n\nBellman consistency equations for the optimal policy\\begin{aligned}\n    V_\\hi^\\star(s) &= \\max_a Q_\\hi^\\star(s, a) \\\\\n    Q_\\hi^\\star(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [V_{\\hi+1}^\\star(s')]\n\\end{aligned}\n\nNow that we’ve shown this particular greedy policy is optimal, all we\nneed to do is compute the optimal value function and optimal policy. We\ncan do this by working backwards in time using dynamic programming\n(DP).\n\nDP algorithm to compute an optimal policy in a finite-horizon MDP\n\nBase case. At the end of the episode (time step H-1), we can’t\ntake any more actions, so the Q-function is simply the reward that\nwe obtain:Q^\\star_{H-1}(s, a) = r(s, a)\n\nso the best thing to do\nis just act greedily and get as much reward as we can!\\pi^\\star_{H-1}(s) = \\arg\\max_a Q^\\star_{H-1}(s, a)\n\nThen\nV^\\star_{H-1}(s), the optimal value of state s at the end of the\ntrajectory, is simply whatever action gives the most reward.V^\\star_{H-1} = \\max_a Q^\\star_{H-1}(s, a)\n\nRecursion. Then, we can work backwards in time, starting from the\nend, using our consistency equations! i.e. for each\nt = H-2, \\dots, 0, we set\\begin{aligned}\n    Q^\\star_{t}(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [V^\\star_{\\hi+1}(s')] \\\\\n    \\pi^\\star_{t}(s) &= \\arg\\max_a Q^\\star_{t}(s, a) \\\\\n    V^\\star_{t}(s) &= \\max_a Q^\\star_{t}(s, a)\n\\end{aligned}\n\ndef find_optimal_policy(mdp: MDP):\n    Q = [None] * mdp.H\n    pi = [None] * mdp.H\n    V = [None] * mdp.H + [jnp.zeros(mdp.S)]  # initialize to 0 at end of time horizon\n\n    for h in range(mdp.H - 1, -1, -1):\n        Q[h] = mdp.r + mdp.P @ V[h + 1]\n        pi[h] = jnp.eye(mdp.S)[jnp.argmax(Q[h], axis=1)]  # one-hot\n        V[h] = jnp.max(Q[h], axis=1)\n\n    Q = jnp.stack(Q)\n    pi = jnp.stack(pi)\n    V = jnp.stack(V[:-1])\n\n    return pi, V, Q\n\nAt each of the H timesteps, we must compute Q^{\\star} for each of\nthe |\\mathcal{S}| |\\mathcal{A}| state-action pairs. Each computation takes |\\mathcal{S}|\noperations to evaluate the average value over s'. This gives a total\ncomputation time of O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|).\n\nNote that this algorithm is identical to the policy evaluation algorithm\n\n\ndp_eval_finite, but instead of averaging over the\nactions chosen by a policy, we instead simply take a maximum over the\naction-values. We’ll see this relationship between policy evaluation\nand optimal policy computation show up again in the infinite-horizon\nsetting.\n\nπ_opt, V_opt, Q_opt = find_optimal_policy(tidy_mdp)\nassert jnp.allclose(π_opt, tidy_policy_messy_only)\nassert jnp.allclose(V_opt, V_messy)\nassert jnp.allclose(Q_opt[:-1], v_ary_to_q_ary(tidy_mdp, V_messy)[1:])\n\"Assertions passed (the 'tidy when messy' policy is optimal)\"\n\n","type":"content","url":"/mdps#opt-dynamic-programming","position":27},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Infinite-horizon MDPs"},"type":"lvl2","url":"/mdps#infinite-horizon-mdps","position":28},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Infinite-horizon MDPs"},"content":"What happens if a trajectory is allowed to continue forever (i.e.\nH = \\infty)? This is the setting of infinite horizon MDPs.\n\nIn this chapter, we’ll describe the necessary adjustments from the\nfinite-horizon case to make the problem tractable. We’ll show that the\n\n\nBellman operator in the discounted reward setting is a\ncontraction mapping for any policy.\nWe’ll discuss how to evaluate\npolicies (i.e. compute their corresponding value functions). Finally,\nwe’ll present and analyze two iterative algorithms, based on the Bellman\noperator, for computing the optimal policy: value iteration and\npolicy iteration.","type":"content","url":"/mdps#infinite-horizon-mdps","position":29},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Discounted rewards","lvl2":"Infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#discounted-rewards","position":30},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Discounted rewards","lvl2":"Infinite-horizon MDPs"},"content":"First of all, note that maximizing the cumulative reward\nr_\\hi + r_{\\hi+1} + r_{\\hi+2} + \\cdots is no longer a good idea since it\nmight blow up to infinity. Instead of a time horizon H, we now need a\ndiscount factor \\gamma \\in [0, 1) such that rewards become less\nvaluable the further into the future they are:r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k r_{\\hi+k}.\n\nWe can think of γ as measuring how much we care about the future:\nif it’s close to 0, we only care about the near-term rewards; it’s\nclose to 1, we put more weight into future rewards.\n\nYou can also analyze γ as the probability of continuing the\ntrajectory at each time step. (This is equivalent to H being\ndistributed by a First Success distribution with success probability\nγ.) This accords with the above interpretation: if γ is\nclose to 0, the trajectory will likely be very short, while if\nγ is close to 1, the trajectory will likely continue for a long\ntime.\n\nAttention\n\nAssuming that r_\\hi \\in [0, 1] for all \\hi \\in \\mathbb{N},\nwhat is the maximum discounted cumulative reward? You may find it\nuseful to review geometric series.\n\nThe other components of the MDP remain the same:M = (\\mathcal{S}, \\mathcal{A}, \\mu, P, r, \\gamma).\n\nCode-wise, we can reuse the MDP class from before \n\nDefinition 1.2 and set mdp.H = float('inf').\n\ntidy_mdp_inf = tidy_mdp._replace(H=float(\"inf\"), γ=0.95)\n\n","type":"content","url":"/mdps#discounted-rewards","position":31},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Stationary policies","lvl2":"Infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#stationary-policies","position":32},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Stationary policies","lvl2":"Infinite-horizon MDPs"},"content":"The time-dependent policies from the finite-horizon case become\ndifficult to handle in the infinite-horizon case. In particular, many of\nthe DP approaches we saw required us to start at the end of the\ntrajectory, which is no longer possible. We’ll shift to stationary\npolicies \\pi : \\mathcal{S} \\to \\mathcal{A} (deterministic) or \\Delta(\\mathcal{A}) (stochastic).\n\nAttention\n\nWhich of the policies in \n\nExample 1.2 are stationary?","type":"content","url":"/mdps#stationary-policies","position":33},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Value functions and Bellman consistency","lvl2":"Infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#value-functions-and-bellman-consistency","position":34},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Value functions and Bellman consistency","lvl2":"Infinite-horizon MDPs"},"content":"We also consider stationary value functions V^\\pi : \\mathcal{S} \\to \\mathbb{R} and\nQ^\\pi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}. We need to insert a factor of γ\ninto the Bellman consistency equation \n\nTheorem 1.1 to account for the discounting:\\begin{aligned}\n    V^\\pi(s) &= \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} \\cdots \\mid s_\\hi = s] && \\text{for any } \\hi \\in \\mathbb{N} \\\\\n    &= \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma V^\\pi(s')]\\\\\n    Q^\\pi(s, a) &= \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots \\mid s_\\hi = s, a_\\hi = a] && \\text{for any } \\hi \\in \\mathbb{N} \\\\\n    &= r(s, a) + \\gamma \\E_{\\substack{s' \\sim P(s, a) \\\\ a' \\sim \\pi(s')}} [Q^\\pi(s', a')]\n\\end{aligned}\n\nAttention\n\nHeuristically speaking, why does it no longer matter which\ntime step we condition on when defining the value function?","type":"content","url":"/mdps#value-functions-and-bellman-consistency","position":35},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl2","url":"/mdps#solving-infinite-horizon-mdps","position":36},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Solving infinite-horizon MDPs"},"content":"","type":"content","url":"/mdps#solving-infinite-horizon-mdps","position":37},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The Bellman operator is a contraction mapping","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#the-bellman-operator-is-a-contraction-mapping","position":38},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"The Bellman operator is a contraction mapping","lvl2":"Solving infinite-horizon MDPs"},"content":"Recall from \n\nDefinition 1.8 that the Bellman operator \\mathcal{J}^{\\pi}\nfor a policy π takes in a “value function” v : \\mathcal{S} \\to \\mathbb{R} and\nreturns the r.h.s. of the Bellman equation for that “value function”. In\nthe infinite-horizon setting, this is[\\mathcal{J}^{\\pi}(v)](s) := \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma v(s')].\n\nThe crucial property of the Bellman operator is that it is a\ncontraction mapping for any policy. Intuitively, if we start with\ntwo “value functions” v, u : \\mathcal{S} \\to \\mathbb{R}, if we repeatedly apply the\nBellman operator to each of them, they will get closer and closer\ntogether at an exponential rate.\n\nContraction mapping\n\nLet X be some space with a norm \\|\\cdot\\|. We call an operator\nf: X \\to X a contraction mapping if for any x, y \\in X,\\|f(x) - f(y)\\| \\le \\gamma \\|x - y\\|\n\nfor some fixed \\gamma \\in (0, 1).\nIntuitively, this means that if two points are δ far apart,\nafter applying the mapping,\n\nAttention\n\nShow that for a contraction mapping f with coefficient\nγ, for all t \\in \\mathbb{N},\\|f^{(t)}(x) - f^{(t)}(y)\\| \\le \\gamma^t \\|x - y\\|,\n\ni.e. that any\ntwo points will be pushed closer by at least a factor of γ at\neach iteration.\n\nIt is a powerful fact (known as the Banach fixed-point theorem) that\nevery contraction mapping has a unique fixed point x^\\star such\nthat f(x^\\star) = x^\\star. This means that if we repeatedly apply f\nto any starting point, we will eventually converge to x^\\star:\\|f^{(t)}(x) - x^\\star\\| \\le \\gamma^t \\|x - x^\\star\\|.\n\nLet’s return to the RL setting and apply this result to the Bellman\noperator. How can we measure the distance between two “value functions”\nv, u : \\mathcal{S} \\to \\mathbb{R}? We’ll take the supremum norm as our distance\nmetric:\\| v - u \\|_{\\infty} := \\sup_{s \\in \\mathcal{S}} |v(s) - u(s)|,\n\ni.e.\nwe compare the “value functions” on the state that causes the biggest\ngap between them. Then \n\n(1.36) implies that if we repeatedly\napply \\mathcal{J}^\\pi to any starting “value function”, we will eventually\nconverge to V^\\pi:\\|(\\mathcal{J}^\\pi)^{(t)}(v) - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v - V^\\pi\\|_{\\infty}.\n\nWe’ll use this useful fact to prove the convergence of several\nalgorithms later on.\n\nThe Bellman operator is a contraction mapping\\|\\mathcal{J}^{\\pi} (v) - \\mathcal{J}^{\\pi} (u) \\|_{\\infty} \\le \\gamma \\|v - u \\|_{\\infty}.\n\nProof of \n\nTheorem 1.4\n\nFor all states s \\in \\mathcal{S},\\begin{aligned}\n|[\\mathcal{J}^{\\pi} (v)](s) - [\\mathcal{J}^{\\pi} (u)](s)|&= \\Big| \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v(s') \\right] \\\\\n&\\qquad - \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} u(s') \\right] \\Big| \\\\\n&= \\gamma \\left|\\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [v(s') - u(s')] \\right| \\\\\n&\\le \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}|v(s') - u(s')| \\qquad \\text{(Jensen's inequality)} \\\\\n&\\le \\gamma \\max_{s'} |v(s') - u(s')| \\\\\n&= \\gamma \\|v - u \\|_{\\infty}.\n\\end{aligned}","type":"content","url":"/mdps#the-bellman-operator-is-a-contraction-mapping","position":39},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#policy-evaluation-in-infinite-horizon-mdps","position":40},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"The backwards DP technique we used in \n\nthe finite-horizon case no\nlonger works since there is no “final timestep” to start from. We’ll\nneed another approach to policy evaluation.\n\nThe Bellman consistency conditions yield a system of equations we can\nsolve to evaluate a deterministic policy exactly. For a faster approximate solution,\nwe can iterate the policy’s Bellman operator, since we know that it has\na unique fixed point at the true value function.","type":"content","url":"/mdps#policy-evaluation-in-infinite-horizon-mdps","position":41},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Matrix inversion for deterministic policies","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl4","url":"/mdps#matrix-inversion-for-deterministic-policies","position":42},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Matrix inversion for deterministic policies","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"Note that when the policy π is deterministic, the actions can be\ndetermined from the states, and so we can chop off the action dimension\nfor the rewards and state transitions:\\begin{aligned}\n    r^{\\pi} &\\in \\mathbb{R}^{|\\mathcal{S}|} & P^{\\pi} &\\in [0, 1]^{|\\mathcal{S}| \\times |\\mathcal{S}|} & \\mu &\\in [0, 1]^{|\\mathcal{S}|} \\\\\n    \\pi &\\in \\mathcal{A}^{|\\mathcal{S}|} & V^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}|} & Q^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}.\n\\end{aligned}\n\nFor P^\\pi, we’ll treat the rows as the states and the\ncolumns as the next states. Then P^\\pi_{s, s'} is the probability of\ntransitioning from state s to state s' under policy π.\n\nTidying MDP\n\nThe tabular MDP from before has |\\mathcal{S}| = 2 and |\\mathcal{A}| = 2. Let’s write\ndown the quantities for the policy π that tidies if and only if the\nroom is messy:r^{\\pi} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad\n        P^{\\pi} = \\begin{bmatrix} 0.7 & 0.3 \\\\ 1 & 0 \\end{bmatrix}, \\quad\n        \\mu = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n\nWe’ll see how to\nevaluate this policy in the next section.\n\nThe Bellman consistency equation for a deterministic policy can be\nwritten in tabular notation asV^\\pi = r^\\pi + \\gamma P^\\pi V^\\pi.\n\n(Unfortunately, this notation doesn’t simplify the expression for\nQ^\\pi.) This system of equations can be solved with a matrix\ninversion:V^\\pi = (I - \\gamma P^\\pi)^{-1} r^\\pi.\n\nAttention\n\nNote we’ve assumed that I - \\gamma P^\\pi is invertible. Can you see\nwhy this is the case?\n\n(Recall that a linear operator, i.e. a square matrix, is invertible if\nand only if its null space is trivial; that is, it doesn’t map any\nnonzero vector to zero. In this case, we can see that I - \\gamma P^\\pi\nis invertible because it maps any nonzero vector to a vector with at\nleast one nonzero element.)\n\ndef eval_deterministic_infinite(\n    mdp: MDP, policy: Float[Array, \"S A\"]\n) -> Float[Array, \" S\"]:\n    pi = jnp.argmax(policy, axis=1)  # un-one-hot\n    P_π = mdp.P[jnp.arange(mdp.S), pi]\n    r_π = mdp.r[jnp.arange(mdp.S), pi]\n    return jnp.linalg.solve(jnp.eye(mdp.S) - mdp.γ * P_π, r_π)\n\nTidying policy evaluation\n\nLet’s use the same policy π that tidies if and only if the room is\nmessy. Setting \\gamma = 0.95, we must invertI - \\gamma P^{\\pi} = \\begin{bmatrix} 1 - 0.95 \\times 0.7 & - 0.95 \\times 0.3 \\\\ - 0.95 \\times 1 & 1 - 0.95 \\times 0 \\end{bmatrix} = \\begin{bmatrix} 0.335 & -0.285 \\\\ -0.95 & 1 \\end{bmatrix}.\n\nThe inverse to two decimal points is(I - \\gamma P^{\\pi})^{-1} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix}.\n\nThus the value function isV^{\\pi} = (I - \\gamma P^{\\pi})^{-1} r^{\\pi} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 15.56 \\\\ 14.79 \\end{bmatrix}.\n\nLet’s sanity-check this result. Since rewards are at most 1, the\nmaximum cumulative return of a trajectory is at most\n1/(1-\\gamma) = 20. We see that the value function is indeed slightly\nlower than this.\n\neval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[0])\n\n","type":"content","url":"/mdps#matrix-inversion-for-deterministic-policies","position":43},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Iterative policy evaluation","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl4","url":"/mdps#iterative-pe","position":44},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Iterative policy evaluation","lvl3":"Policy evaluation in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"The matrix inversion above takes roughly O(|\\mathcal{S}|^3) time.\nIt also only works for deterministic policies.\nCan we trade off the requirement of finding the exact value function for a faster\napproximate algorithm that will also extend to stochastic policies?\n\nLet’s use the Bellman operator to define an iterative algorithm for\ncomputing the value function. We’ll start with an initial guess\nv^{(0)} with elements in [0, 1/(1-\\gamma)] and then iterate the\nBellman operator:v^{(t+1)} = \\mathcal{J}^{\\pi}(v^{(t)}),\n\ni.e. v^{(t)} = (\\mathcal{J}^{\\pi})^{(t)} (v^{(0)}). Note that each iteration\ntakes O(|\\mathcal{S}|^2) time for the matrix-vector multiplication.\n\ndef supremum_norm(v):\n    return jnp.max(jnp.abs(v))  # same as jnp.linalg.norm(v, jnp.inf)\n\n\ndef loop_until_convergence(op, v, ε=1e-6):\n    \"\"\"Repeatedly apply op to v until convergence (in supremum norm).\"\"\"\n    while True:\n        v_new = op(v)\n        if supremum_norm(v_new - v) < ε:\n            return v_new\n        v = v_new\n\n\ndef iterative_evaluation(mdp: MDP, pi: Float[Array, \"S A\"], ε=1e-6) -> Float[Array, \" S\"]:\n    op = partial(bellman_operator, mdp, pi)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\nThen, as we showed in \n\n(1.38), by the Banach fixed-point theorem:\\|v^{(t)} - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v^{(0)} - V^\\pi\\|_{\\infty}.\n\niterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[0])\n\nConvergence of iterative policy evaluation\n\nHow many iterations do we need for an ε-accurate estimate? We\ncan work backwards to solve for t:\\begin{aligned}\n    \\gamma^t \\|v^{(0)} - V^\\pi\\|_{\\infty} &\\le \\epsilon \\\\\n    t &\\ge \\frac{\\log (\\epsilon / \\|v^{(0)} - V^\\pi\\|_{\\infty})}{\\log \\gamma} \\\\\n    &= \\frac{\\log (\\|v^{(0)} - V^\\pi\\|_{\\infty} / \\epsilon)}{\\log (1 / \\gamma)},\n\\end{aligned}\n\nand so the number of iterations required for an\nε-accurate estimate isT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\nNote that we’ve applied the inequalities\n\\|v^{(0)} - V^\\pi\\|_{\\infty} \\le 1/(1-\\gamma) and\n\\log (1/x) \\ge 1-x.","type":"content","url":"/mdps#iterative-pe","position":45},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl3","url":"/mdps#optimal-policies-in-infinite-horizon-mdps","position":46},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"Now let’s move on to solving for an optimal policy in the\ninfinite-horizon case. As in \n\nthe finite-horizon case, an optimal policy \\pi^\\star\nis one that does at least as well as any other policy in all situations.\nThat is, for all policies π, states s \\in \\mathcal{S}, times\n\\hi \\in \\mathbb{N}, and initial trajectories\n\\tau_\\hi = (s_0, a_0, r_0, \\dots, s_\\hi) where s_\\hi = s,\\begin{aligned}\n    V^{\\pi^\\star}(s) &= \\E_{\\tau \\sim \\rho^{\\pi^{\\star}}}[r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2}  + \\cdots \\mid s_\\hi = s] \\\\\n    &\\ge \\E_{\\tau \\sim \\rho^{\\pi}}[r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots \\mid \\tau_\\hi]\n\\end{aligned}\n\nOnce again, all optimal policies share the same optimal value function V^\\star, and the greedy policy with respect to this value function\nis optimal.\n\nAttention\n\nVerify this by modifying the proof \n\nTheorem 1.3 from the finite-horizon case.\n\nSo how can we compute such an optimal policy? We can’t use the backwards\nDP approach from the finite-horizon case \n\nDefinition 1.11 since there’s no “final timestep” to start\nfrom. Instead, we’ll exploit the fact that the Bellman consistency\nequation \n\n(1.32) for the optimal value\nfunction doesn’t depend on any policy:V^\\star(s) = \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} V^\\star(s'). \\right]\n\nAttention\n\nVerify this by substituting the greedy policy into the\nBellman consistency equation.\n\nAs before, thinking of the r.h.s. of \n\n(1.53) as an operator on value functions\ngives the Bellman optimality operator[\\mathcal{J}^{\\star}(v)](s) = \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} v(s') \\right]\n\ndef bellman_optimality_operator(mdp: MDP, v: Float[Array, \" S\"]) -> Float[Array, \" S\"]:\n    return jnp.max(mdp.r + mdp.γ * mdp.P @ v, axis=1)\n\n\ndef check_optimal(v: Float[Array, \" S\"], mdp: MDP):\n    return jnp.allclose(v, bellman_optimality_operator(v, mdp))\n\n","type":"content","url":"/mdps#optimal-policies-in-infinite-horizon-mdps","position":47},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Value iteration","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl4","url":"/mdps#value-iteration","position":48},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Value iteration","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"Since the optimal policy is still a policy, our result that the Bellman\noperator is a contracting map still holds, and so we can repeatedly\napply this operator to converge to the optimal value function! This\nalgorithm is known as value iteration.\n\ndef value_iteration(mdp: MDP, ε: float = 1e-6) -> Float[Array, \" S\"]:\n    \"\"\"Iterate the Bellman optimality operator until convergence.\"\"\"\n    op = partial(bellman_optimality_operator, mdp)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\n\n\nvalue_iteration(tidy_mdp_inf)\n\nNote that the runtime analysis for an ε-optimal value function\nis exactly the same as \n\niterative policy evaluation! This is because value iteration is simply\nthe special case of applying iterative policy evaluation to the\noptimal value function.\n\nAs the final step of the algorithm, to return an actual policy\n\\hat \\pi, we can simply act greedily with respect to the final iteration\nv^{(T)} of our above algorithm:\\hat \\pi(s) = \\arg\\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} v^{(T)}(s') \\right].\n\nWe must be careful, though: the value function of this greedy policy,\nV^{\\hat \\pi}, is not the same as v^{(T)}, which need not even be a\nwell-defined value function for some policy!\n\nThe bound on the policy’s quality is actually quite loose: if\n\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\epsilon, then the greedy policy\n\\hat \\pi satisfies\n\\|V^{\\hat \\pi} - V^\\star\\|_{\\infty} \\le \\frac{2\\gamma}{1-\\gamma} \\epsilon,\nwhich might potentially be very large.\n\nGreedy policy value worsening\\|V^{\\hat \\pi} - V^\\star \\|_{\\infty} \\le \\frac{2 \\gamma}{1-\\gamma} \\|v - V^\\star\\|_{\\infty}\n\nwhere \\hat \\pi(s) = \\arg\\max_a q(s, a) is the greedy policy with respect toq(s, a) = r(s, a) + \\E_{s' \\sim P(s, a)} v(s').\n\nProof\n\nWe first have\\begin{aligned}\n        V^{\\star}(s) - V^{\\hat \\pi}(s) &= Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\\\\\n        &= [Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s))] + [Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))].\n\\end{aligned}\n\nLet’s bound these two quantities separately.\n\nFor the first quantity, note that by the definition of \\hat \\pi, we haveq(s, \\hat \\pi(s)) \\ge q(s,\\pi^\\star(s)).\n\nLet’s add q(s, \\hat \\pi(s)) - q(s,\\pi^\\star(s)) \\ge 0 to the first term to get\\begin{aligned}\n        Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s)) &\\le [Q^{\\star}(s,\\pi^\\star(s))- q(s,\\pi^\\star(s))] + [q(s, \\hat \\pi(s)) - Q^{\\star}(s, \\hat \\pi(s))] \\\\\n        &= \\gamma \\E_{s' \\sim P(s, \\pi^{\\star}(s))} [ V^{\\star}(s') - v(s') ] + \\gamma \\E_{s' \\sim P(s, \\hat \\pi(s))} [ v(s') - V^{\\star}(s') ] \\\\\n        &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty}.\n\\end{aligned}\n\nThe second quantity is bounded by\\begin{aligned}\n        Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\n        &=\n        \\gamma \\E_{s'\\sim P(s, \\hat \\pi(s))}\\left[ V^\\star(s') - V^{\\hat \\pi}(s') \\right] \\\\\n        & \\leq \n        \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty\n\\end{aligned}\n\nand thus\\begin{aligned}\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty} + \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty \\\\\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le \\frac{2 \\gamma \\|v - V^{\\star}\\|_{\\infty}}{1-\\gamma}.\n\\end{aligned}\n\nSo in order to compensate and achieve \\|V^{\\hat \\pi} - V^{\\star}\\| \\le \\epsilon, we must have\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\frac{1-\\gamma}{2 \\gamma} \\epsilon.\n\nThis means, using \n\nRemark 1.2, we need to run value iteration forT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{\\gamma}{\\epsilon (1-\\gamma)^2}\\right) \\right)\n\niterations to achieve an ε-accurate estimate of the optimal value function.","type":"content","url":"/mdps#value-iteration","position":49},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Policy iteration","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"type":"lvl4","url":"/mdps#policy-iteration","position":50},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl4":"Policy iteration","lvl3":"Optimal policies in infinite-horizon MDPs","lvl2":"Solving infinite-horizon MDPs"},"content":"Can we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function together? This is the idea behind policy iteration. In each step, we simply set the policy to act greedily with respect to its own value function.\n\ndef policy_iteration(mdp: MDP, ε=1e-6) -> Float[Array, \"S A\"]:\n    \"\"\"Iteratively improve the policy and value function.\"\"\"\n    def op(pi):\n        return v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))\n    π_init = jnp.ones((mdp.S, mdp.A)) / mdp.A  # uniform random policy\n    return loop_until_convergence(op, π_init, ε)\n\n\n\npolicy_iteration(tidy_mdp_inf)\n\nAlthough PI appears more complex than VI, we’ll use the same contraction property \n\nTheorem 1.4 to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an ε-optimal value function \n\nRemark 1.2, although in practice, PI often converges much faster.\n\nPolicy Iteration runtime and convergence\n\nWe aim to show that the number of iterations required for an\nε-accurate estimate of the optimal value function isT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\nThis bound follows from the contraction property \n\n(1.38):\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.\n\nWe’ll prove that the iterates of PI respect the contraction property by\nshowing that the policies improve monotonically:V^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s).\n\nThen we’ll use this to show\nV^{\\pi^{t+1}}(s) \\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s). Note that\\begin{aligned}\n(s) &= \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} V^{\\pi^{t}}(s') \\right] \\\\\n    &= r(s, \\pi^{t+1}(s)) + \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} V^{\\pi^{t}}(s')\n\\end{aligned}\n\nSince\n[\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s) \\ge V^{\\pi^{t}}(s), we then have\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) \\\\\n    &= \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right].\n\\end{aligned}\n\nBut note that the\nexpression being averaged is the same as the expression on the l.h.s.\nwith s replaced by s'. So we can apply the same inequality\nrecursively to get\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge  \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge \\gamma^2 \\E_{\\substack{s' \\sim P(s, \\pi^{t+1}(s)) \\\\ s'' \\sim P(s', \\pi^{t+1}(s'))}} \\left[V^{\\pi^{t+1}}(s'') -  V^{\\pi^{t}}(s'') \\right]\\\\\n    &\\ge \\cdots\n\\end{aligned}\n\nwhich implies that V^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s)\nfor all s (since the r.h.s. converges to zero). We can then plug this\nback into\n\n\n(1.69)\nto get the desired result:\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) &= \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge 0 \\\\\n    V^{\\pi^{t+1}}(s) &\\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s)\n\\end{aligned}\n\nThis means we can now apply the Bellman convergence result \n\n(1.38) to get\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\|\\mathcal{J}^{\\star} (V^{\\pi^{t}}) - V^{\\star}\\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.","type":"content","url":"/mdps#policy-iteration","position":51},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Summary"},"type":"lvl2","url":"/mdps#summary","position":52},{"hierarchy":{"lvl1":"1 Markov Decision Processes","lvl2":"Summary"},"content":"Markov decision processes (MDPs) are a framework for sequential\ndecision making under uncertainty. They consist of a state space\n\\mathcal{S}, an action space \\mathcal{A}, an initial state distribution\n\\mu \\in \\Delta(\\mathcal{S}), a transition function P(s' \\mid s, a), and a\nreward function r(s, a). They can be finite-horizon (ends after\nH timesteps) or infinite-horizon (where rewards scale by\n\\gamma \\in (0, 1) at each timestep).\n\nOur goal is to find a policy π that maximizes expected total\nreward. Policies can be deterministic or stochastic,\nstate-dependent or history-dependent, stationary or\ntime-dependent.\n\nA policy induces a distribution over trajectories.\n\nWe can evaluate a policy by computing its value function\nV^\\pi(s), which is the expected total reward starting from state\ns and following policy π. We can also compute the\nstate-action value function Q^\\pi(s, a), which is the expected\ntotal reward starting from state s, taking action a, and then\nfollowing policy π. In the finite-horizon setting, these also\ndepend on the timestep \\hi.\n\nThe Bellman consistency equation is an equation that the value\nfunction must satisfy. It can be used to solve for the value\nfunctions exactly. Thinking of the r.h.s. of this equation as an\noperator on value functions gives the Bellman operator.\n\nIn the finite-horizon setting, we can compute the optimal policy\nusing dynamic programming.\n\nIn the infinite-horizon setting, we can compute the optimal policy\nusing value iteration or policy iteration.","type":"content","url":"/mdps#summary","position":53},{"hierarchy":{"lvl1":"6  Policy Gradient Methods"},"type":"lvl1","url":"/pg","position":0},{"hierarchy":{"lvl1":"6  Policy Gradient Methods"},"content":"","type":"content","url":"/pg","position":1},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Introduction"},"type":"lvl2","url":"/pg#introduction","position":2},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Introduction"},"content":"The core task of RL is finding the optimal policy in a given environment.\nThis is essentially an optimization problem:\nout of some space of policies,\nwe want to find the one that achieves the maximum total reward (in expectation).\n\nIt’s typically intractable to compute the optimal policy exactly in some finite number of steps.\nInstead, policy optimization algorithms start from some randomly initialized policy,\nand then improve it step by step.\nWe’ve already seen some examples of these,\nnamely \n\nSection 1.5.3.2 for finite MDPs and \n\nSection 2.6.4 in continuous control.\n\nIn particular, we often use policies that can be described by some finite set of parameters.\nWe will see some examples in \n\nSection 3.1.\nFor such parameterized policies,\nwe can approximate the policy gradient:\nthe gradient of the expected total reward with respect to the parameters.\nThis tells us the direction the parameters should be updated to achieve a higher expected total reward.\nPolicy gradient methods are responsible for groundbreaking applications including AlphaGo, OpenAI Five, and large language models,\nmany of which use policies parameterized as deep neural networks.\n\nWe begin the chapter with a short review of gradient ascent,\na general optimization method.\n\nWe’ll then see how to estimate the policy gradient,\nenabling us to apply (stochastic) gradient ascent in the RL setting.\n\nThen we’ll explore some proximal optimization techniques that ensure the steps taken are “not too large”.\nThis is helpful to stabilize training and widely used in practice.\n\nfrom utils import plt, Array, Callable, jax, jnp, latexify\n\n","type":"content","url":"/pg#introduction","position":3},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Gradient Ascent"},"type":"lvl2","url":"/pg#gradient-ascent","position":4},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Gradient Ascent"},"content":"Note\n\nYou may have previously heard of gradient descent for minimizing functions.\nOptimization problems are usually posed as minimization problems by convention.\nHowever, in RL, we usually talk about maximizing the expected total reward,\nand so we perform gradient ascent instead.\n\nGradient ascent is a general optimization algorithm for any differentiable function.\nA suitable analogy for this algorithm is hiking up a mountain,\nwhere you keep taking steps in the steepest direction upwards.\nHere, your vertical position y is the function being optimized,\nand your horizontal position (x, z) is the input to the function.\nThe slope of the mountain at your current position is given by the gradient,\nwritten \\nabla y(x, z) \\in \\mathbb{R}^2.\n\ndef f(x, y):\n    \"\"\"Himmelblau's function\"\"\"\n    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n# Create a grid of points\nx = jnp.linspace(-5, 5, 400)\ny = jnp.linspace(-5, 5, 400)\nX, Y = jnp.meshgrid(x, y)\nZ = f(X, Y)\n\n# Create the plot\nfig, ax = plt.subplots(figsize=(6, 6))\n\n# Plot the function using imshow\nimg = ax.imshow(Z, extent=[-5, 5, -5, 5], origin='lower')\n\n# Add color bar\nfig.colorbar(img, ax=ax)\n\n# Gradient computation using JAX\ntx, ty = 1.0, 1.0\ngx, gy = jax.grad(f, argnums=(0, 1))(tx, ty)\n\n# Scatter point\nax.scatter(tx, ty, color='red', s=100)\n\n# Add arrow representing the gradient\nax.arrow(tx, ty, gx * 0.01, gy * 0.01, head_width=0.3, head_length=0.3, fc='blue', ec='blue')\n\n# Add plot title\nax.set_title(\"Gradient ascent example\")\n\nplt.show()\n\nFor differentiable functions, this can be thought of as the vector of partial derivatives,\\nabla y(x, z) = \\begin{pmatrix}\n\\frac{\\partial y}{\\partial x} \\\\\n\\frac{\\partial y}{\\partial z}\n\\end{pmatrix}.\n\nTo calculate the slope (aka “directional derivative”) of the mountain in a given direction (\\Delta x, \\Delta z),\nyou take the dot product of the difference vector with the gradient.\nThis means that the direction with the highest slope is exactly the gradient itself,\nso we can describe the gradient ascent algorithm as follows:\n\nGradient ascent\\begin{pmatrix}\nx^{k+1} \\\\ z^{k+1}\n\\end{pmatrix}\n= \n\\begin{pmatrix}\nx^{k} \\\\ z^{k}\n\\end{pmatrix}\n+\n\\eta \\nabla y(x^{k}, z^{k})\n\nwhere k denotes the iteration of the algorithm and \\eta > 0 is a “step size” hyperparameter that controls the size of the steps we take.\n(Note that we could also vary the step size across iterations, that is, \\eta^0, \\dots, \\eta^K.)\n\nThe case of a two-dimensional input is easy to visualize.\nBut this idea can be straightforwardly extended to higher-dimensional inputs.\n\nFrom now on, we’ll use J to denote the function we’re trying to maximize,\nand θ to denote the parameters being optimized over. (In the above example, \\theta = \\begin{pmatrix} x & z \\end{pmatrix}^\\top).\n\nNotice that our parameters will stop changing once \\nabla J(\\theta) = 0.\nOnce we reach this stationary point, our current parameters are ‘locally optimal’ in some sense;\nit’s impossible to increase the function by moving in any direction.\nIf J is convex, then the only point where this happens is at the global optimum.\nOtherwise, if J is nonconvex, the best we can hope for is a local optimum.\n\nNote\n\nHow does a computer compute the gradient of a function?\n\nOne way is symbolic differentiation,\nwhich is similar to the way you might compute it by hand:\nthe computer applies a list of rules to transform the symbols involved.\nPython’s sympy package supports symbolic differentiation.\nHowever, functions implemented in code may not always have a straightforward symbolic representation.\n\nAnother way is numerical differentiation,\nwhich is based on the limit definition of a (directional) derivative:\\nabla_{\\boldsymbol{u}} J(\\boldsymbol{x}) = \\lim_{\\varepsilon \\to 0}\n\\frac{J(\\boldsymbol{x} + \\varepsilon \\boldsymbol{u}) - J(\\boldsymbol{x})}{\\varepsilon}\n\nThen, we can substitute a small value of \\varepsilon on the r.h.s. to approximate the directional derivative.\nHow small, though? If we need an accurate estimate,\nwe may need such a small value of \\varepsilon that typical computers will run into rounding errors.\nAlso, to compute the full gradient,\nwe would need to compute the r.h.s. once for each input dimension.\nThis is an issue if computing J is expensive.\n\nAutomatic differentiation achieves the best of both worlds.\nLike symbolic differentiation,\nwe manually implement the derivative rules for a few basic operations.\nHowever, instead of executing these on the symbols,\nwe execute them on the values when the function gets called,\nlike in numerical differentiation.\nThis allows us to differentiate through programming constructs such as branches or loops,\nand doesn’t involve any arbitrarily small values.\n\n\nBaydin et al. (2018) provides an accessible survey of automatic differentiation.\n\n","type":"content","url":"/pg#gradient-ascent","position":5},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Stochastic gradient ascent","lvl2":"Gradient Ascent"},"type":"lvl3","url":"/pg#stochastic-gradient-ascent","position":6},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Stochastic gradient ascent","lvl2":"Gradient Ascent"},"content":"In real applications,\ncomputing the gradient of the target function is not so simple.\nAs an example from supervised learning, J(\\theta) might be the sum of squared prediction errors across an entire training dataset.\nHowever, if our dataset is very large, it might not fit into our computer’s memory!\nIn these cases, we often compute some estimate of the gradient at each step, \\tilde \\nabla J(\\theta), and walk in that direction instead.\nThis is called stochastic gradient ascent.\nIn the SL example above, we might randomly choose a minibatch of samples and use them to estimate the true prediction error. (This approach is known as minibatch SGD.)\n\ndef sgd(\n    theta_init: Array,\n    estimate_gradient: Callable[[Array], Array],\n    η: float,\n    n_steps: int,\n):\n    \"\"\"Perform `n_steps` steps of SGD.\n\n    `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.\n    \"\"\"\n    θ = theta_init\n    for step in range(n_steps):\n        θ += η * estimate_gradient(θ)\n    return θ\n\nWhat makes one gradient estimator better than another?\nIdeally, we want this estimator to be unbiased; that is, on average, it matches a single true gradient step:\\E [\\tilde \\nabla J(\\theta)] = \\nabla J(\\theta).\n\nWe also want the variance of the estimator to be low so that its performance doesn’t change drastically at each step.\n\nWe can actually show that, for many “nice” functions, in a finite number of steps, SGD will find a θ that is “close” to a stationary point.\nIn another perspective, for such functions, the local “landscape” of J around θ becomes flatter and flatter the longer we run SGD.\n\nSGD convergence\n\nMore formally, suppose we run SGD for K steps, using an unbiased gradient estimator.\nLet the step size \\eta^k scale as O(1/\\sqrt{k}).\nThen if J is bounded and β-smooth (see below),\nand the norm of the gradient estimator has a bounded second moment \\sigma^2,\\|\\nabla J(\\theta^K)\\|^2 \\le O \\left( M \\beta \\sigma^2 / K\\right).\n\nWe call a function β-smooth if its gradient is Lipschitz continuous with constant β:\\|\\nabla J(\\theta) - \\nabla J(\\theta')\\| \\le \\beta \\|\\theta - \\theta'\\|.\n\nWe’ll now see a concrete application of gradient ascent in the context of policy optimization.\n\n","type":"content","url":"/pg#stochastic-gradient-ascent","position":7},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Policy (stochastic) gradient ascent"},"type":"lvl2","url":"/pg#policy-stochastic-gradient-ascent","position":8},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Policy (stochastic) gradient ascent"},"content":"Remember that in RL, the primary goal is to find the optimal policy that achieves the maximimum total reward, which we can express using the value function we defined in \n\nDefinition 1.6:\\begin{aligned}\n    J(\\pi) := \\E_{s_0 \\sim \\mu_0} V^{\\pi} (s_0) = & \\E_{\\tau \\sim \\rho^\\pi} \\sum_{\\hi=0}^{\\hor-1} r(s_\\hi, a_\\hi)\n\\end{aligned}\n\nwhere \\rho^\\pi is the distribution over trajectories induced by π (see \n\nDefinition 1.5).\n\n(Note that we’ll continue to work in the undiscounted, finite-horizon case. Analogous results hold for the discounted, infinite-horizon setup.)\n\nAs shown by the notation, this is exactly the function J that we want to maximize using gradient ascent.\nWhat variables are we optimizing over in this problem?\nWell, the objective function J is a function of the policy π,\nbut in general, π is a function,\nand optimizing over the entire space of arbitrary input-output mappings would be intractable.\nInstead, we need to describe π in terms of some finite set of parameters θ.\n\n","type":"content","url":"/pg#policy-stochastic-gradient-ascent","position":9},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Example policy parameterizations","lvl2":"Policy (stochastic) gradient ascent"},"type":"lvl3","url":"/pg#parameterizations","position":10},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Example policy parameterizations","lvl2":"Policy (stochastic) gradient ascent"},"content":"What are some ways we could parameterize our policy?\n\nTabular representation\n\nIf both the state and action spaces are finite, perhaps we could simply learn a preference value \\theta_{s,a} for each state-action pair.\nThen to turn this into a valid distribution, we perform a softmax operation:\nwe exponentiate each of them,\nand then normalize to form a valid distribution:\\pi^\\text{softmax}_\\theta(a | s) = \\frac{\\exp(\\theta_{s,a})}{\\sum_{s,a'} \\exp (\\theta_{s,a'})}.\n\nHowever, this doesn’t make use of any structure in the states or actions,\nso while this is flexible, it is also prone to overfitting.\n\nLinear in features\n\nAnother approach is to map each state-action pair into some feature space \\phi(s, a) \\in \\mathbb{R}^p. Then, to map a feature vector to a probability, we take a linear combination of the features and take a softmax:\\pi^\\text{linear in features}_{\\theta}(a|s) = \\frac{\\exp(\\theta^\\top \\phi(s, a))}{\\sum_{a'} \\exp(\\theta^\\top \\phi(s, a'))}.\n\nAnother interpretation is that θ represents the feature vector of the “desired” state-action pair, as state-action pairs whose features align closely with θ are given higher probability.\n\nNeural policies\n\nMore generally, we could map states and actions to unnormalized scores via some parameterized function f_\\theta : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}, such as a neural network, and choose actions according to a softmax: \\pi^\\text{general}_\\theta(a|s) = \\frac{\\exp(f_{\\theta}(s,a))}{\\sum_{a'} \\exp(f_{\\theta}(s,a'))}.\n\nDiagonal Gaussian policies for continuous action spaces\n\nConsider a continuous n-dimensional action space \\mathcal{A} = \\mathbb{R}^n. Then for a stochastic policy, we could use a function to predict the mean action and then add some random noise about it. For example, we could use a neural network to predict the mean action \\mu_\\theta(s) and then add some noise \\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I) to it:\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma^2 I). **Exercise:** Can you extend the \"linear in features\" policy to continuous action spaces in a similar way? \n\nNow that we have seen some examples of parameterized policies,\nwe will write the total reward in terms of the parameters,\noverloading notation and letting \\rho_\\theta := \\rho^{\\pi_\\theta}:J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} R(\\tau)\n\nwhere R(\\tau) = \\sum_{\\hi=0}^{\\hor-1} r(s_\\hi, a_\\hi) denotes the total reward in the trajectory.\n\nNow how do we maximize this function (the expected total reward) over the parameters?\nOne simple idea would be to directly apply gradient ascent:\\theta^{k+1} = \\theta^k + \\eta \\nabla J(\\theta^k).\n\nIn order to apply this technique, we need to be able to evaluate the gradient \\nabla J(\\theta).\nBut J(\\theta) is very difficult, or even intractable, to compute exactly, since it involves taking an expectation over all possible trajectories \\tau.\nCan we rewrite it in a form that’s more convenient to implement?\n\n","type":"content","url":"/pg#parameterizations","position":11},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Importance Sampling","lvl2":"Policy (stochastic) gradient ascent"},"type":"lvl3","url":"/pg#importance-sampling","position":12},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl3":"Importance Sampling","lvl2":"Policy (stochastic) gradient ascent"},"content":"There is a general trick called importance sampling for evaluating difficult expectations.\nSuppose we want to estimate \\E_{x \\sim p}[f(x)] where p is hard or expensive to sample from,\nbut easy to evaluate the likelihood p(x) of.\nSuppose that we can easily sample from a different distribution q.\nSince an expectation is just a weighted average, we can sample x from q, compute f(x), and then reweight the results:\nif x is very likely under p but unlikely under q,\nwe should boost its weighting,\nand if it is common under q but uncommon under p,\nwe should lower its weighting.\nThe reweighting factor is exactly the likelihood ratio between the target distribution p and the sampling distribution q:\\E_{x \\sim p}[f(x)] = \\sum_{x \\in \\mathcal{X}} f(x) p(x) = \\sum_{x \\in \\mathcal{X}} f(x) \\frac{p(x)}{q(x)} q(x) = \\E_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} f(x) \\right].\n\nDoesn’t this seem too good to be true? If there were no drawbacks, we could use this to estimate any expectation of any function on any arbitrary distribution! The drawback is that the variance may be very large due to the likelihood ratio term.\nIf there are values of x that are very rare in the sampling distribution q,\nbut common under p,\nthen the likelihood ratio p(x)/q(x) will cause the variance to blow up.","type":"content","url":"/pg#importance-sampling","position":13},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"The REINFORCE policy gradient"},"type":"lvl2","url":"/pg#the-reinforce-policy-gradient","position":14},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"The REINFORCE policy gradient"},"content":"Returning to RL, suppose there is some trajectory distribution \\rho(\\tau) that is easy to sample from, such as a database of existing trajectories.\nWe can then rewrite \\nabla J(\\theta), a.k.a. the policy gradient, as follows.\nAll gradients are being taken with respect to θ.\\begin{aligned}\n    \\nabla J(\\theta) & = \\nabla \\E_{\\tau \\sim \\rho_\\theta} [ R(\\tau) ]                                                                                         \\\\\n                     & = \\nabla \\E_{\\tau \\sim \\rho} \\left[ \\frac{\\rho_\\theta(\\tau)}{\\rho(\\tau)} R(\\tau) \\right] &  & \\text{likelihood ratio trick}             \\\\\n                     & = \\E_{\\tau \\sim \\rho} \\left[ \\frac{\\nabla \\rho_\\theta(\\tau)}{\\rho(\\tau)} R(\\tau) \\right] &  & \\text{switching gradient and expectation}\n\\end{aligned}\n\nNote that for \\rho = \\rho_\\theta, the inside term becomes\\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} [ \\nabla \\log \\rho_\\theta(\\tau) \\cdot R(\\tau)].\n\n(The order of operations is \\nabla (\\log \\rho_\\theta)(\\tau).)\n\nRecall that when the state transitions are Markov (i.e. s_{t} only depends on s_{t-1}, a_{t-1}) and the policy is time-homogeneous (i.e. a_\\hi \\sim \\pi_\\theta (s_\\hi)), we can write out the likelihood of a trajectory under the policy \\pi_\\theta autoregressively, as in \n\nDefinition 1.5. Taking the log of the trajectory likelihood turns it into a sum of terms:\\log \\rho_\\theta(\\tau) = \\log \\mu(s_0) + \\sum_{\\hi=0}^{\\hor-1} \\log \\pi_\\theta(a_\\hi \\mid s_\\hi) + \\log P(s_{\\hi+1} \\mid s_\\hi, a_\\hi)\n\nWhen we take the gradient with respect to the parameters θ,\nonly the \\pi_\\theta(a_\\hi | s_\\hi) terms depend on θ.\nThis gives the following expression for the policy gradient, known as the “REINFORCE” policy gradient \n\nWilliams (1992):\\begin{aligned}\n    \\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_\\hi | s_\\hi) R(\\tau) \\right]\n\\end{aligned}\n\nThis expression allows us to estimate the gradient by sampling a few sample trajectories from \\pi_\\theta,\ncalculating the likelihoods of the chosen actions,\nand substituting these into the expression inside the brackets of \n\n(18).\nThen we can update the parameters θ in this direction to perform stochastic gradient ascent.\n\nThe rest of this chapter investigates ways to reduce the variance of this estimator by subtracting off certain correlated quantities.\n\nNote\n\nHere is an alternative, intuitive presentation of \n\n(18).\n\nIntuitively speaking,\nwe want to update the policy parameters to maximize the probability of taking optimal actions.\nThat is, suppose we are in state s, and a^\\star is an optimal action to take.\nThen we want to solve \\theta = \\arg\\max_{\\theta'} \\pi_{\\theta'}(a^\\star \\mid s),\nwhich would lead to the gradient ascent expression\\theta \\gets \\theta + \\nabla \\pi_{\\theta}(a^\\star \\mid s).\n\nHowever, we don’t know the optimal action a^\\star in practice.\nSo instead, we must try many actions,\nand increase the probability of the “good” ones\nand decrease the probability of the “bad” ones.\nSuppose A(s, a) is a measure of how good action a is in state s.\nThen we could write\\theta \\gets \\theta + \\sum_a \\pi_{\\theta}(a \\mid s) A(s, a) \\nabla \\pi_{\\theta}(a \\mid s).\n\nBut this has an issue: the size of each step doesn’t just depend on how good it is,\nbut also how often the policy takes it already.\nThis could lead to a positive feedback loop where likely actions become more and more likely,\nwithout respect to the quality of the action.\nSo we divide by the likelihood to cancel out this factor:\\theta \\gets \\theta + \\sum_a \\pi_{\\theta}(a \\mid s) A(s, a) \\frac{\\nabla \\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta}(a \\mid s)}.\n\nBut once we simplify, and sum across timesteps, this becomes almost exactly the gradient written above!\\theta \\gets \\theta + \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)} [\\sum_{\\hi=0}^{\\hor-1} A(s_\\hi, a_\\hi) \\nabla \\log \\pi_{\\theta}(a_\\hi \\mid s_\\hi) ].\n\nWe will see later on what A concretely corresponds to.def estimate_gradient_reinforce_pseudocode(env, π, θ):\n    τ = sample_trajectory(env, π(θ))\n    gradient_hat = 0\n    for s, a, r in τ:\n        def policy_log_likelihood(θ):\n            return log(π(θ)(s, a))\n        gradient_hat += jax.grad(policy_log_likelihood)(θ) * τ.total_reward\n    return gradient_hat\n\nFor some intuition into how this method works, recall that we update our parameters according to\\begin{aligned}\n    \\theta_{t+1} &= \\theta_t + \\eta \\nabla J(\\theta_t) \\\\\n    &= \\theta_t + \\eta \\E_{\\tau \\sim \\rho_{\\theta_t}} [\\nabla \\log \\rho_{\\theta_t}(\\tau) \\cdot R(\\tau)].\n\\end{aligned}\n\nConsider the “good” trajectories where R(\\tau) is large. Then θ gets updated so that these trajectories become more likely. To see why, recall that \\rho_{\\theta}(\\tau) is the likelihood of the trajectory τ under the policy \\pi_\\theta, so the gradient points in the direction that makes τ more likely.\n\n","type":"content","url":"/pg#the-reinforce-policy-gradient","position":15},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Baselines and advantages"},"type":"lvl2","url":"/pg#baselines-and-advantages","position":16},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Baselines and advantages"},"content":"A central idea from supervised learning is the bias-variance decomposition,\nwhich shows that the mean squared error of an estimator is the sum of its squared bias and its variance.\nThe REINFORCE gradient estimator \n\n(18) is already unbiased, meaning that its expectation over trajectories is the true policy gradient.\nCan we find ways to reduce its variance as well?\n\nAs a first step,\nconsider that the action taken at step t does not affect the reward from previous timesteps, since they’re already in the past.\nYou can also show rigorously that this is the case,\nand that we only need to consider the present and future rewards to calculate the policy gradient:\\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_\\hi | s_\\hi) \\sum_{\\hi' = \\hi}^{\\hor-1} r(s_{\\hi'}, a_{\\hi'}) \\right]\n\nFurthermore, by a conditioning argument, we can replace the inner sum over remaining rewards with the policy’s Q-function,\nevaluated at the current state:\\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_\\hi | s_\\hi) Q^{\\pi_\\theta}(s_{\\hi}, a_{\\hi}) \\right]\n\nExercise: Prove that this is equivalent to the previous definitions. What modification to the expression must be made for the discounted, infinite-horizon setting?\n\nWe can further reduce variance by subtracting a baseline function b_\\hi : \\mathcal{S} \\to \\mathbb{R} at each timestep \\hi.\nThis modifies the policy gradient as follows:\\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} \\left[\n    \\sum_{\\hi=0}^{H-1} \\nabla \\log \\pi_\\theta (a_\\hi | s_\\hi) \\left(\n    Q^{\\pi_\\theta}(s_\\hi, a_\\hi)\n    - b_\\hi(s_\\hi)\n    \\right)\n    \\right].\n\n(Again, you should try to prove that this equality still holds.)\nFor example, we might want b_\\hi to estimate the average reward-to-go at a given timestep:b_\\hi^\\theta = \\E_{\\tau \\sim \\rho_\\theta} R_\\hi(\\tau).\n\nAs a better baseline, we could instead choose the value function.\nNote that the random variable Q^\\pi_\\hi(s, a) - V^\\pi_\\hi(s),\nwhere the randomness is taken over the actions, is centered around zero.\n(Recall V^\\pi_\\hi(s) = \\E_{a \\sim \\pi} Q^\\pi_\\hi(s, a).)\nThis quantity matches the intuition given in \n\nNote 1:\nit is positive for actions that are better than average (in state s),\nand negative for actions that are worse than average.\nIn fact, this quantity has a particular name: the advantage function.\n\nAdvantage functionA^\\pi_\\hi(s) = Q^\\pi_\\hi(s, a) - V^\\pi_\\hi(s)\n\nThis measures how much better this action does than the average for that policy.\n(Note that for an optimal policy \\pi^\\star, the advantage of a given state-action pair is always zero or negative.)\n\nWe can now express the policy gradient as follows. Note that the advantage function effectively replaces the Q-function from \n\n(25):\\nabla J(\\theta) = \\E_{\\tau \\sim \\rho_\\theta} \\left[\n        \\sum_{\\hi=0}^{\\hor-1} \\nabla \\log \\pi_\\theta(a_\\hi | s_\\hi) A^{\\pi_\\theta}_\\hi (s_\\hi, a_\\hi)\n\\right].\n\nNote that to avoid correlations between the gradient estimator and the value estimator (i.e. baseline), we must estimate them with independently sampled trajectories: TODO could use more explanation _why_ we want to avoid correlations  Policy gradient with a learned baseline \n\ndef pg_with_learned_baseline_pseudocode(env, π, η, θ_init, K, N):\n    θ = θ_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, π(θ), N)\n        V_hat = fit(trajectories)  # estimates the value function of π(θ)\n        τ = sample_trajectories(env, π(θ), 1)\n        g = jnp.zeros_like(θ)  # gradient estimator\n\n        for h, (s, a) in enumerate(τ):\n            def log_likelihood(θ_):\n                return jnp.log(π(θ_)(s, a))\n            g = g + jax.grad(log_likelihood)(θ) * (return_to_go(τ, h) - V_hat(s))\n        \n        θ = θ + η * g\n    return θ\n\nNote that you could also generalize this by allowing the learning rate η to vary across steps,\nor take multiple trajectories τ and compute the sample average of the gradient estimates.\n\nThe baseline estimation step fit can be done using any appropriate supervised learning algorithm.\nNote that the gradient estimator will be unbiased regardless of the baseline.\n\n","type":"content","url":"/pg#baselines-and-advantages","position":17},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Comparing policy gradient algorithms to policy iteration"},"type":"lvl2","url":"/pg#comparing-policy-gradient-algorithms-to-policy-iteration","position":18},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Comparing policy gradient algorithms to policy iteration"},"content":" TODO maybe restructure this part \n\nWhat advantages does the policy gradient algorithm have over the policy iteration algorithms covered in \n\nSection 1.5.3.2?\n\nPolicy iteration recap\n\nRecall that policy iteration is an algorithm for MDPs with unknown state transitions where we alternate between these two steps:\n\nEstimating the Q-function (or advantage function) of the current policy;\n\nUpdating the policy to be greedy with respect to this approximate Q-function (or advantage function).\n\nTo analyze the difference between them, we’ll make use of the performance difference lemma, which provides an expression for comparing the difference between two value functions.\n\nPerformance difference lemma\n\nSuppose Alice is playing a game (an MDP).\nBob is spectating, and can evaluate how good an action is compared to his own strategy.\n(That is, Bob can compute his advantage function A_\\hi^{\\text{Bob}}(s_\\hi, a_\\hi)).\nThe performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:V_0^{\\text{Alice}}(s) - V_0^{\\text{Bob}}(s) = \\E_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\sum_{h=0}^{H-1} A_\\hi^{\\text{Bob}} (s_\\hi, a_\\hi) \\right]\n\nwhere \\rho_{\\text{Alice}, s} denotes the distribution over trajectories starting in state s when Alice is playing.\n\nTo see why, consider a specific step \\hi in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average.\nBut this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!\n\nFormally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that\\begin{aligned}\nA^\\pi_\\hi(s_\\hi, a_\\hi) &= Q^\\pi_\\hi(s_\\hi, a_\\hi) - V^\\pi_\\hi(s_\\hi) \\\\\n&= r_\\hi(s_\\hi, a_\\hi) + \\E_{s_{\\hi+1} \\sim P(s_\\hi, a_\\hi)} [V^\\pi_{\\hi+1}(s_{\\hi+1})] - V^\\pi_\\hi(s_\\hi)\n\\end{aligned}\n\nso expanding out the r.h.s. expression of \n\n(30) and grouping terms together gives\\begin{aligned}\n\\E_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\sum_{\\hi=0}^{\\hor-1} A_\\hi^{\\text{Bob}} (s_\\hi, a_\\hi) \\right] &= \\E_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\left( \\sum_{\\hi=0}^{\\hor-1} r_\\hi(s_\\hi, a_\\hi) \\right) + \\left( V^{\\text{Bob}}_1(s_1) + \\cdots + V^{\\text{Bob}}_\\hor(s_\\hor) \\right) - \\left( V^{\\text{Bob}_0}(s_0) + \\cdots + V^{\\text{Bob}}_{\\hor-1}(s_{\\hor-1}) \\right) \\right] \\\\\n&= V^{\\text{Alice}}_0(s) - V^{\\text{Bob}}_0(s)\n\\end{aligned}\n\nas desired. (Note that the “inner” expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid.)\n\nThe PDL gives insight into why fitted approaches such as PI don’t work as well in the “full” RL setting.\nTo see why, let’s consider a single iteration of policy iteration, where policy π gets updated to \\tilde \\pi. We’ll assume these policies are deterministic.\nSuppose the new policy \\tilde \\pi chooses some action with a negative advantage with respect to π.\nThat is, when acting according to π, taking the action from \\tilde \\pi would perform worse than expected.\nDefine \\Delta_\\infty to be the most negative advantage, that is, \\Delta_\\infty = \\min_{s \\in \\mathcal{S}} A^{\\pi}_\\hi(s, \\tilde \\pi(s)).\nPlugging this into the \n\nTheorem 1 gives\\begin{aligned}\nV_0^{\\tilde \\pi}(s) - V_0^{\\pi}(s) &= \\E_{\\tau \\sim \\rho_{\\tilde \\pi, s}} \\left[\n\\sum_{\\hi=0}^{\\hor-1} A_\\hi^{\\pi}(s_\\hi, a_\\hi)\n\\right] \\\\\n&\\ge H \\Delta_\\infty \\\\\nV_0^{\\tilde \\pi}(s) &\\ge V_0^{\\pi}(s) - H|\\Delta_\\infty|.\n\\end{aligned}\n\nThat is, for some state s, the lower bound on the performance of \\tilde \\pi is lower than the performance of π.\nThis doesn’t state that \\tilde \\pi will necessarily perform worse than π,\nonly suggests that it might be possible.\nIf these worst case states do exist, though,\nPI does not avoid situations where the new policy often visits them;\nIt does not enforce that the trajectory distributions \\rho_\\pi and \\rho_{\\tilde \\pi} be close to each other.\nIn other words, the “training distribution” that our prediction rule is fitted on, \\rho_\\pi, may differ significantly from the “evaluation distribution” \\rho_{\\tilde \\pi}. \nThis is an instance of *distributional shift*.\nTo begin, let's ask, where *do* fitted approaches work well?\nThey are commonly seen in SL,\nwhere a prediction rule is fit using some labelled training set,\nand then assessed on a test set from the same distribution.\nBut policy iteration isn't performed in the same scenario:\nthere is now _distributional shift_ between the different iterations of the policy. \n\nOn the other hand, policy gradient methods do, albeit implicitly,\nencourage \\rho_\\pi and \\rho_{\\tilde \\pi} to be similar.\nSuppose that the mapping from policy parameters to trajectory distributions is relatively smooth.\nThen, by adjusting the parameters only a small distance,\nthe new policy will also have a similar trajectory distribution.\nBut this is not very rigorous, and in practice the parameter-to-distribution mapping may not be so smooth.\nCan we constrain the distance between the resulting distributions more explicitly?\n\nThis brings us to the next three methods:\n\ntrust region policy optimization (TRPO), which explicitly constrains the difference between the distributions before and after each step;\n\nthe natural policy gradient (NPG), a first-order approximation of TRPO;\n\nproximal policy optimization (PPO), a “soft relaxation” of TRPO.\n\n","type":"content","url":"/pg#comparing-policy-gradient-algorithms-to-policy-iteration","position":19},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Trust region policy optimization"},"type":"lvl2","url":"/pg#trust-region-policy-optimization","position":20},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Trust region policy optimization"},"content":"We saw above that policy gradient methods are effective because they implicitly constrain how much the policy changes at each iteration.\nCan we design an algorithm that explicitly constrains the “step size”?\nThat is, we want to improve the policy as much as possible,\nmeasured in terms of the r.h.s. of the \n\nTheorem 1,\nwhile ensuring that its trajectory distribution does not change too much:\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{\\theta^{\\text{opt}}} \\E_{s_0, \\dots, s_{H-1} \\sim \\pi^{k}} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\E_{a_\\hi \\sim \\pi^{\\theta^\\text{opt}}(s_\\hi)} A^{\\pi^{k}}(s_\\hi, a_\\hi) \\right] \\\\\n& \\text{where } \\text{distance}(\\rho_{\\theta^{\\text{opt}}}, \\rho_{\\theta^k}) < \\delta\n\\end{aligned}\n\nNote that we have made a small change to the r.h.s. expression:\nwe use the states sampled from the old policy, and only use the actions from the new policy.\nIt would be computationally infeasible to sample entire trajectories from \\pi_\\theta as we are optimizing over θ.\nOn the other hand, if \\pi_\\theta returns a vector representing a probability distribution over actions,\nthen evaluating the expected advantage with respect to this distribution only requires taking a dot product.\nThis approximation also matches the r.h.s. of the PDL to first order in θ.\n(We will elaborate more on this later.)\n\nHow do we describe the distance between \\rho_{\\theta^{\\text{opt}}} and \\rho_{\\theta^k}?\nWe’ll use the Kullback-Leibler divergence (KLD):\n\nKullback-Leibler divergence\n\nFor two PDFs p, q,\\kl{p}{q} := \\E_{x \\sim p} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n\nThis can be interpreted in many different ways, many stemming from information theory.\nOne such interpretation is that \\kl{p}{q} describes my average “surprise” if I think data is being generated by q but it’s actually generated by p.\n(The surprise of an event with probability p is - \\log_2 p.)\nNote that \\kl{p}{q} = 0 if and only if p = q. Also note that it is generally not symmetric.\n\nBoth the objective function and the KLD constraint involve a weighted average over the space of all trajectories.\nThis is intractable in general, so we need to estimate the expectation.\nAs before, we can do this by taking an empirical average over samples from the trajectory distribution.\nThis gives us the following pseudocode:\n\nTrust region policy optimization (exact)def trpo_pseudocode(env, δ, θ_init, M):\n    θ = θ_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, π(θ), M)\n        A_hat = fit(trajectories)\n        \n        def approximate_gain(θ_):\n            total_advantage = 0\n            for τ in trajectories:\n                for s, _a, _r in τ:\n                    for a in env.action_space:\n                        total_advantage += π(θ)(s, a) * A_hat(s, a)\n            return total_advantage\n        \n        def constraint(θ_):\n            kl_div = 0\n            for τ in trajectories:\n                for s, a, _r in τ:\n                    kl_div += jnp.log(π(θ)(s, a)) - jnp.log(π(θ_)(s, a))\n            return kl_div <= δ\n        \n        θ = optimize(approximate_gain, constraint)\n\n    return θ\nApplying importance sampling allows us to estimate the TRPO objective as follows:\n\n::::{prf:definition} Trust region policy optimization (implementation)\n:label: trpo_implement\n\n:::{prf:definitionic} TODO\nInitialize $\\theta^0$\n\nSample $N$ trajectories from $\\rho^k$ to learn a value estimator $\\tilde b_\\hi(s) \\approx V^{\\pi^k}_\\hi(s)$\n\nSample $M$ trajectories $\\tau_0, \\dots, \\tau_{M-1} \\sim \\rho^k$\n\n$$\\begin{gathered}\n            \\theta^{k+1} \\gets \\arg\\max_{\\theta} \\frac{1}{M} \\sum_{m=0}^{M-1} \\sum_{h=0}^{H-1} \\frac{\\pi_\\theta(a_\\hi \\mid s_\\hi)}{\\pi^k(a_\\hi \\mid s_\\hi)} [ R_\\hi(\\tau_m) - \\tilde b_\\hi(s_\\hi) ] \\\\\n            \\text{where } \\sum_{m=0}^{M-1} \\sum_{h=0}^{H-1} \\log \\frac{\\pi_k(a_\\hi^m \\mid s_\\hi^m)}{\\pi_\\theta(a_\\hi^m \\mid s_\\hi^m)} \\le \\delta\n        \n\\end{gathered}$$\n:::\n:::: \n\nThe above isn’t entirely complete:\nwe still need to solve the actual optimization problem at each step.\nUnless we know additional properties of the problem,\nthis might be an intractable optimization.\nDo we need to solve it exactly, though?\nInstead, if we assume that both the objective function and the constraint are somewhat smooth in terms of the policy parameters,\nwe can use their Taylor expansions to give us a simpler optimization problem with a closed-form solution.\nThis brings us to the natural policy gradient algorithm.\n\n","type":"content","url":"/pg#trust-region-policy-optimization","position":21},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Natural policy gradient"},"type":"lvl2","url":"/pg#natural-policy-gradient","position":22},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Natural policy gradient"},"content":"We take a linear (first-order) approximation to the objective function and a quadratic (second-order) approximation to the KL divergence constraint about the current estimate \\theta^k.\nThis results in the optimization problem\\begin{gathered}\n    \\max_\\theta \\nabla_\\theta J(\\pi_{\\theta^k})^\\top (\\theta - \\theta^k) \\\\\n    \\text{where } \\frac{1}{2} (\\theta - \\theta^k)^\\top F_{\\theta^k} (\\theta - \\theta^k) \\le \\delta\n\\end{gathered}\n\nwhere F_{\\theta^k} is the Fisher information matrix defined below.\n\nFisher information matrix\n\nLet p_\\theta denote a parameterized distribution.\nIts Fisher information matrix F_\\theta can be defined equivalently as:\\begin{aligned}\n        F_{\\theta} & = \\E_{x \\sim p_\\theta} \\left[ (\\nabla_\\theta \\log p_\\theta(x)) (\\nabla_\\theta \\log p_\\theta(x))^\\top \\right] & \\text{covariance matrix of the Fisher score}          \\\\\n                   & = \\E_{x \\sim p_{\\theta}} [- \\nabla_\\theta^2 \\log p_\\theta(x)]                                                & \\text{average Hessian of the negative log-likelihood}\n\\end{aligned}\n\nRecall that the Hessian of a function describes its curvature:\nfor a vector \\delta \\in \\Theta,\nthe quantity \\delta^\\top F_\\theta \\delta describes how rapidly the negative log-likelihood changes if we move by δ.\nThe Fisher information matrix is precisely the Hessian of the KL divergence (with respect to either one of the parameters).\n\nIn particular, when p_\\theta = \\rho_{\\theta} denotes a trajectory distribution, we can further simplify the expression:F_{\\theta} = \\E_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{h=0}^{H-1} (\\nabla \\log \\pi_\\theta (a_\\hi \\mid s_\\hi)) (\\nabla \\log \\pi_\\theta(a_\\hi \\mid s_\\hi))^\\top \\right]\n\nNote that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.\n\nThis is a convex optimization problem with a closed-form solution.\nTo see why, it helps to visualize the case where θ is two-dimensional:\nthe constraint describes the inside of an ellipse,\nand the objective function is linear,\nso we can find the extreme point on the boundary of the ellipse.\nWe recommend \n\nBoyd & Vandenberghe (2004) for a comprehensive treatment of convex optimization.\n\nMore generally, for a higher-dimensional θ,\nwe can compute the global optima by setting the gradient of the Lagrangian to zero:\\begin{aligned}\n    \\mathcal{L}(\\theta, \\alpha)                     & = \\nabla J(\\pi_{\\theta^k})^\\top (\\theta - \\theta^k) - \\alpha \\left[ \\frac{1}{2} (\\theta - \\theta^k)^\\top F_{\\theta^k} (\\theta - \\theta^k) - \\delta \\right] \\\\\n    \\nabla \\mathcal{L}(\\theta^{k+1}, \\alpha) & := 0                                                                                                                                                             \\\\\n    \\implies \\nabla J(\\pi_{\\theta^k})        & = \\alpha F_{\\theta^k} (\\theta^{k+1} - \\theta^k)                                                                                                                   \\\\\n    \\theta^{k+1}                           & = \\theta^k + \\eta F_{\\theta^k}^{-1} \\nabla J(\\pi_{\\theta^k})                                                                                             \\\\\n    \\text{where } \\eta                     & = \\sqrt{\\frac{2 \\delta}{\\nabla J(\\pi_{\\theta^k})^\\top F_{\\theta^k}^{-1} \\nabla J(\\pi_{\\theta^k})}}\n\\end{aligned}\n\nThis gives us the closed-form update.\nNow the only challenge is to estimate the Fisher information matrix,\nsince, as with the KL divergence constraint, it is an expectation over trajectories, and computing it exactly is therefore typically intractable.\n\nNatural policy gradient\n\nHow many trajectory samples do we need to accurately estimate the Fisher information matrix?\nAs a rule of thumb, the sample complexity should scale with the dimension of the parameter space.\nThis makes this approach intractable in the deep learning setting where we might have a very large number of parameters.\n\nAs you can see, the NPG is the “basic” policy gradient algorithm we saw above,\nbut with the gradient transformed by the inverse Fisher information matrix.\nThis matrix can be understood as accounting for the geometry of the parameter space.\nThe typical gradient descent algorithm implicitly measures distances between parameters using the typical Euclidean distance.\nHere, where the parameters map to a distribution, using the natural gradient update is equivalent to optimizing over distribution space rather than parameter space,\nwhere distance between distributions is measured by the \n\nDefinition 3.\n\nNatural gradient on a simple problem\n\nLet’s step away from RL and consider the following optimization problem over Bernoulli distributions \\pi \\in \\Delta(\\{ 0, 1 \\}):\\begin{aligned}\n        J(\\pi) & = 100 \\cdot \\pi(1) + 1 \\cdot \\pi(0)\n\\end{aligned}\n\nWe can think of the space of such distributions as the line between (0, 1) to (1, 0) on the Cartesian plane:\n\nClearly the optimal distribution is the constant one \\pi(1) = 1. Suppose we optimize over the parameterized family \\pi_\\theta(1) = \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}.\nThen our optimization algorithm should set θ to be unboundedly large.\nThen the “vanilla” gradient is\\nabla_\\theta J(\\pi_\\theta) = \\frac{99 \\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\nNote that as \\theta \\to \\infty that the increments get closer and closer to 0;\nthe rate of increase becomes exponentially slow.\n\nHowever, if we compute the Fisher information “matrix” (which is just a scalar in this case), we can account for the geometry induced by the parameterization.\\begin{aligned}\n        F_\\theta & = \\E_{x \\sim \\pi_\\theta} [ (\\nabla_\\theta \\log \\pi_\\theta(x))^2 ] \\\\\n                 & = \\frac{\\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\\end{aligned}\n\nThis gives the natural gradient update\\begin{aligned}\n        \\theta^{k+1} & = \\theta^k + \\eta F_{\\theta^k}^{-1} \\nabla_ \\theta J(\\theta^k) \\\\\n                     & = \\theta^k + 99 \\eta\n\\end{aligned}\n\nwhich increases at a constant rate, i.e. improves the objective more quickly than “vanilla” gradient ascent.\n\nThough the NPG now gives a closed-form optimization step,\nit requires computing the inverse Fisher information matrix,\nwhich typically scales as O((\\dim \\Theta)^3).\nThis can be expensive if the parameter space is large.\nCan we find an algorithm that works in linear time with respect to the dimension of the parameter space?\n\n","type":"content","url":"/pg#natural-policy-gradient","position":23},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Proximal policy optimization"},"type":"lvl2","url":"/pg#proximal-policy-optimization","position":24},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Proximal policy optimization"},"content":"We can relax the TRPO optimization problem in a different way:\nRather than imposing a hard constraint on the KL distance,\nwe can instead impose a soft constraint by incorporating it into the objective and penalizing parameter values that drastically change the trajectory distribution.\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{\\theta} \\E_{s_0, \\dots, s_{H-1} \\sim \\rho_{\\pi^{k}}} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\E_{a_\\hi \\sim \\pi_{\\theta}(s_\\hi)} A^{\\pi^{k}}(s_\\hi, a_\\hi) \\right] - \\lambda \\kl{\\rho_{\\theta}}{\\rho_{\\theta^k}}\n\\end{aligned}\n\nHere λ is a regularization hyperparameter that controls the tradeoff between the two terms.\nThis is the objective of the proximal policy optimization algorithm \n\nSchulman et al. (2017).\n\nLike the original TRPO algorithm \n\nDefinition 4,\nPPO is not gradient-based; rather, at each step, we try to maximize local advantage relative to the current policy.\n\nHow do we solve this optimization?\nLet us begin by simplifying the \\kl{\\rho_{\\pi^k}}{\\rho_{\\pi_{\\theta}}} term. Expanding gives\\begin{aligned}\n    \\kl{\\rho_{\\pi^k}}{\\rho_{\\pi_{\\theta}}} & = \\E_{\\tau \\sim \\rho_{\\pi^k}} \\left[\\log \\frac{\\rho_{\\pi^k}(\\tau)}{\\rho_{\\pi_{\\theta}}(\\tau)}\\right]                                                       \\\\\n                                           & = \\E_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{\\pi^k(a_\\hi \\mid s_\\hi)}{\\pi_{\\theta}(a_\\hi \\mid s_\\hi)}\\right] & \\text{state transitions cancel} \\\\\n                                           & = \\E_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_\\hi \\mid s_\\hi)}\\right] + c\n\\end{aligned}\n\nwhere c is some constant with respect to θ, and can be ignored.\nThis gives the objective\\ell^k(\\theta)\n=\n\\E_{s_0, \\dots, s_{H-1} \\sim \\rho_{\\pi^{k}}} \\left[ \\sum_{\\hi=0}^{\\hor-1} \\E_{a_\\hi \\sim \\pi_{\\theta}(s_\\hi)} A^{\\pi^{k}}(s_\\hi, a_\\hi) \\right] - \\lambda \\E_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_\\hi \\mid s_\\hi)}\\right]\n\nOnce again, this takes an expectation over trajectories.\nBut here we cannot directly sample trajectories from \\pi^k,\nsince in the first term, the actions actually come from \\pi_\\theta.\nTo make this term line up with the other expectation,\nwe would need the actions to also come from \\pi^k.\n\nThis should sound familiar:\nwe want to estimate an expectation over one distribution by sampling from another.\nWe can once again use \n\nSection 3.2 to rewrite the inner expectation:\\E_{a_\\hi \\sim \\pi_{\\theta}(s_\\hi)} A^{\\pi^{k}}(s_\\hi, a_\\hi)\n=\n\\E_{a_\\hi \\sim \\pi^k(s_\\hi)} \\frac{\\pi_\\theta(a_\\hi \\mid s_\\hi)}{\\pi^k(a_\\hi \\mid s_\\hi)} A^{\\pi^{k}}(s_\\hi, a_\\hi)\n\nNow we can combine the expectations together to get the objective\\ell^k(\\theta) = \\E_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\left( \\frac{\\pi_\\theta(a_\\hi \\mid s_\\hi)}{\\pi^k(a_\\hi \\mid s_\\hi)} A^{\\pi^k}(s_\\hi, a_\\hi) - \\lambda \\log \\frac{1}{\\pi_\\theta(a_\\hi \\mid s_\\hi)} \\right) \\right]\n\nNow we can estimate this function by a sample average over trajectories from \\pi^k.\nRemember that to complete a single iteration of PPO,\nwe execute\\theta^{k+1} \\gets \\arg\\max_{\\theta} \\ell^k(\\theta).\n\nIf \\ell^k is differentiable, we can optimize it by gradient ascent, completing a single iteration of PPO.def ppo_pseudocode(\n    env,\n    π: Callable[[Params], Callable[[State, Action], Float]],\n    λ: float,\n    θ_init: Params,\n    n_iters: int,\n    n_fit_trajectories: int,\n    n_sample_trajectories: int,\n):\n    θ = θ_init\n    for k in range(n_iters):\n        fit_trajectories = sample_trajectories(env, π(θ), n_fit_trajectories)\n        A_hat = fit(fit_trajectories)\n\n        sample_trajectories = sample_trajectories(env, π(θ), n_sample_trajectories)\n        \n        def objective(θ_opt):\n            total_objective = 0\n            for τ in sample_trajectories:\n                for s, a, _r in τ:\n                    total_objective += π(θ_opt)(s, a) / π(θ)(s, a) * A_hat(s, a) + λ * jnp.log(π(θ_opt)(s, a))\n            return total_objective / n_sample_trajectories\n        \n        θ = optimize(objective, θ)\n\n    return θ","type":"content","url":"/pg#proximal-policy-optimization","position":25},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Summary"},"type":"lvl2","url":"/pg#summary","position":26},{"hierarchy":{"lvl1":"6  Policy Gradient Methods","lvl2":"Summary"},"content":"Policy gradient methods are a powerful family of algorithms that directly optimize the expected total reward by iteratively updating the policy parameters.\nPrecisely,\nwe estimate the gradient of the expected total reward (with respect to the parameters),\nand update the parameters in that direction.\nBut estimating the gradient is a tricky task!\nWe saw many ways to reduce the variance of the gradient estimator,\nculminating in the advantage-based expression \n\n(29).\n\nBut updating the parameters doesn’t entirely solve the problem:\nSometimes, a small step in the parameters might lead to a big step in the policy.\nTo avoid changing the policy too much at each step,\nwe must account for the curvature in the parameter space.\nWe first did this explicitly with \n\nDefinition 4,\nand then saw ways to relax the constraint in \n\nDefinition 6 and \n\nSection 9.\n\nThese are still popular methods to this day,\nespecially because they efficiently integrate with deep neural networks for representing complex functions.","type":"content","url":"/pg#summary","position":27},{"hierarchy":{"lvl1":"8 Tree Search Methods"},"type":"lvl1","url":"/planning","position":0},{"hierarchy":{"lvl1":"8 Tree Search Methods"},"content":"","type":"content","url":"/planning","position":1},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Introduction"},"type":"lvl2","url":"/planning#introduction","position":2},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Introduction"},"content":"Have you ever lost a strategy game against a skilled opponent?\nIt probably seemed like they were ahead of you at every turn.\nThey might have been planning ahead and anticipating your actions,\nthen planning around them in order to win.\nIf this opponent was a computer,\nthey might have been using one of the strategies that we are about to explore.","type":"content","url":"/planning#introduction","position":3},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Deterministic, zero sum, fully observable two-player games"},"type":"lvl2","url":"/planning#deterministic-zero-sum-fully-observable-two-player-games","position":4},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Deterministic, zero sum, fully observable two-player games"},"content":"In this chapter, we will focus on games that are:\n\ndeterministic,\n\nzero sum (one player wins and the other loses),\n\nfully observable, that is, the state of the game is perfectly known by both players,\n\nfor two players that alternate turns,\n\nWe can represent such a game as a complete game tree.\nEach possible state is a node in the tree,\nand since we only consider deterministic games,\nwe can represent actions as edges leading from the current state to the next.\nEach path through the tree, from root to leaf, represents a single game.\n\n\n\nThe first two layers of the complete game tree of tic-tac-toe.\nFrom Wikimedia.\n\nIf you could store the complete game tree on a computer,\nyou would be able to win every potentially winnable game\nby searching all paths from your current state and taking a winning move.\nWe will see an explicit algorithm for this in \n\nthe next section.\nHowever, as games become more complex,\nit becomes computationally impossible to search every possible path.\n\nFor instance,\na chess player has roughly 30 actions to choose from at each turn,\nand each game takes roughly 40 moves per player,\nso trying to solve chess exactly using minimax\nwould take somewhere on the order of 30^{80} \\approx 10^{118} operations.\nThat’s 10 billion billion billion billion billion billion billion billion billion billion billion billion billion operations.\nAs of the time of writing,\nthe fastest processor can achieve almost 10 GHz (10 billion operations per second),\nso to fully solve chess using minimax is many, many orders of magnitude out of reach.\n\nIt is thus intractable, in any realistic setting, to solve the complete game tree exactly.\nLuckily, only a small fraction of those games ever occur in reality;\nLater in this chapter,\nwe will explore ways to prune away parts of the tree that we know we can safely ignore.\nWe can also approximate the value of a state without fully evaluating it.\nUsing these approximations, we can no longer guarantee winning the game,\nbut we can come up with strategies that will do well against most opponents.","type":"content","url":"/planning#deterministic-zero-sum-fully-observable-two-player-games","position":5},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Notation","lvl2":"Deterministic, zero sum, fully observable two-player games"},"type":"lvl3","url":"/planning#notation","position":6},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Notation","lvl2":"Deterministic, zero sum, fully observable two-player games"},"content":"Let us now describe these games formally.\nWe’ll call the first player Max and the second player Min.\nMax seeks to maximize the final game score,\nwhile Min seeks to minimize the final game score.\n\nWe’ll use \\mathcal{S} to denote the set of all possible game states.\n\nThe game begins in some initial state s_0 \\in \\mathcal{S}.\n\nMax moves on even turn numbers h = 2n,\nand Min moves on odd turn numbers h = 2n+1,\nwhere n is a natural number.\n\nThe space of possible actions, \\mathcal{A}_h(s),\ndepends on the state itself, as well as whose turn it is.\n(For example, in tic-tac-toe, Max can only play Xs while Min can only play Os.)\n\nThe game ends after H total moves (which might be even or odd). We call the final state a terminal state.\n\nP denotes the state transitions, that is,\nP(s, a) denotes the resulting state when taking action a \\in \\mathcal{A}(s) in state s. We’ll assume that this function is time-homogeneous (a.k.a. stationary) and doesn’t change across timesteps.\n\nr(s) denotes the game score of the terminal state s.\nNote that this is some positive or negative value seen by both players:\nA positive value indicates Max winning, a negative value indicates Min winning, and a value of 0 indicates a tie.\n\nWe also call the sequence of states and actions a trajectory.\n\nAttention\n\nAbove, we suppose that the game ends after H total moves.\nBut most real games have a variable length.\nHow would you describe this?\n\nTic-tac-toe\n\nLet us frame tic-tac-toe in this setting.\n\nEach of the 9 squares is either empty, marked X, or marked O.\nSo there are |\\mathcal{S}| = 3^9 potential states.\nNot all of these may be reachable!\n\nThe initial state s_0 is the empty board.\n\nThe set of possible actions for Max in state s, \\mathcal{A}_{2n}(s), is the set of tuples (\\text{``X''}, i) where i refers to an empty square in s.\nSimilarly, \\mathcal{A}_{2n+1}(s) is the set of tuples (\\text{``O''}, i) where i refers to an empty square in s.\n\nWe can take H = 9 as the longest possible game length.\n\nP(s, a) for a nonterminal state s is simply the board with the symbol and square specified by a marked into s. Otherwise, if s is a terminal state, i.e. it already has three symbols in a row, the state no longer changes.\n\nr(s) at a terminal state is +1 if there are three Xs in a row, -1 if there are three Os in a row, and 0 otherwise.\n\nOur notation may remind you of \n\nMarkov decision processes.\nGiven that these games also involve a sequence of states and actions,\ncan we formulate them as finite-horizon MDPs?\nThe two settings are not exactly analogous,\nsince in MDPs we only consider a single policy,\nwhile these games involve two distinct players with opposite objectives.\nSince we want to analyze the behavior of both players at the same time,\ndescribing such a game as an MDP is more trouble than it’s worth.","type":"content","url":"/planning#notation","position":7},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Min-max search *"},"type":"lvl2","url":"/planning#min-max-search","position":8},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Min-max search *"},"content":"Important\n\nThe course (Fall 2024) does not cover min-max search.\nThis content is here to provide background on optimally solving these deterministic, zero-sum, two-player games.\n\nIn the introduction,\nwe claimed that we could win any potentially winnable game by looking ahead and predicting the opponent’s actions.\nThis would mean that each nonterminal state already has some predetermined game score,\nthat is, in each state,\nit is already “obvious” which player is going to win.\n\nLet V_\\hi^\\star(s) denote the game score under optimal play from both players starting in state s at time \\hi.\n\nMin-max search algorithmV_\\hi^{\\star}(s) = \\begin{cases}\nr(s) & \\hi = \\hor \\\\\n\\max_{a \\in \\mathcal{A}_\\hi(s)} V_{\\hi+1}^{\\star}(P(s, a)) & \\hi \\text{ is even and } \\hi < H \\\\\n\\min_{a \\in \\mathcal{A}_\\hi(s)} V_{\\hi+1}^{\\star}(P(s, a)) & \\hi \\text{ is odd and } \\hi < H \\\\\n\\end{cases}\n\nWe can compute this by starting at the terminal states,\nwhen the game’s outcome is known,\nand working backwards,\nassuming that Max chooses the action that leads to the highest score\nand Min chooses the action that leads to the lowest score.\n\nThis translates directly into a recursive depth-first search algorithm for searching the complete game tree.def minimax_search(s, player) -> Tuple[\"Action\", \"Value\"]:\n    \"\"\"Return the value of the state (for Max) and the best action for Max to take.\"\"\"\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), min)\n            if v > v_max:\n                a_max, v_max = a, v\n        return a_max, v_max\n    else:\n        a_min, v_min = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), max)\n            if v < v_min:\n                a_min, v_min = a, v\n        return a_min, v_min\n\nMin-max search for a simple game\n\nConsider a simple game with just two steps: Max chooses one of three possible actions (A, B, C),\nand then Min chooses one of three possible actions (D, E, F).\nThe combination leads to a certain integer outcome,\nshown in the table below:\n\n\n\nD\n\nE\n\nF\n\nA\n\n4\n\n-2\n\n5\n\nB\n\n-3\n\n3\n\n1\n\nC\n\n0\n\n3\n\n-1\n\nWe can visualize this as the following complete game tree,\nwhere each box contains the value V_\\hi^\\star(s) of that node.\nThe min-max values of the terminal states are already known:\n\nWe begin min-max search at the root,\nexploring each of Max’s actions.\nSuppose Max chooses action A.\nThen Min will choose action E to minimize the game score,\nmaking the value of this game node \\min(4, -2, 5) = -2.\n\nSimilarly, if Max chooses action B,\nthen Min will choose action D,\nand if Max chooses action C,\nthen Min will choose action F.\nWe can fill in the values of these nodes accordingly:\n\nThus, Max’s best move is to take action C,\nresulting in a game score of \\max(-2, -3, -1) = -1.","type":"content","url":"/planning#min-max-search","position":9},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Complexity of min-max search","lvl2":"Min-max search *"},"type":"lvl3","url":"/planning#complexity-of-min-max-search","position":10},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Complexity of min-max search","lvl2":"Min-max search *"},"content":"At each of the \\hor timesteps,\nthis algorithm iterates through the entire action space at that state,\nand therefore has a time complexity of \\hor^{n_A}\n(where n_A is the largest number of actions possibly available at once).\nThis makes the min-max algorithm impractical for even moderately sized games.\n\nBut do we need to compute the exact value of every possible state?\nInstead, is there some way we could “ignore” certain actions and their subtrees\nif we already know of better options?\nThe alpha-beta search makes use of this intuition.","type":"content","url":"/planning#complexity-of-min-max-search","position":11},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Alpha-beta search"},"type":"lvl2","url":"/planning#alpha-beta-search","position":12},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Alpha-beta search"},"content":"The intuition behind alpha-beta search is as follows:\nSuppose Max is in state s,\nand considering whether to take action a or a'.\nIf at any point they find out that action a' is definitely worse than (or equal to) action a,\nthey don’t need to evaluate action a' any further.\n\nConcretely, we run min-max search as above,\nexcept now we keep track of two additional parameters \\alpha(s) and \\beta(s) while evaluating each state:\n\nStarting in state s, Max can achieve a game score of at least \\alpha(s) assuming Min plays optimally. That is, V^\\star_\\hi(s) \\ge \\alpha(s) at all points.\n\nAnalogously, starting in state s, Min can ensure a game score of at most \\beta(s) assuming Max plays optimally. That is, V^\\star_\\hi(s) \\le \\beta(s) at all points.\n\nSuppose we are evaluating V^\\star_\\hi(s),\nwhere it is Max’s turn (\\hi is even).\nWe update \\alpha(s) to be the highest minimax value achievable from s so far.\nThat is, the value of s is at least \\alpha(s).\nSuppose Max chooses action a, which leads to state s', in which it is Min’s turn.\nIf any of Min’s actions in s' achieve a value V^\\star_{\\hi+1}(s') \\le \\alpha(s),\nwe know that Max would not choose action a,\nsince they know that it is worse than whichever action gave the value \\alpha(s).\nSimilarly, to evaluate a state on Min’s turn,\nwe update \\beta(s) to be the lowest value achievable from s so far.\nThat is, the value of s is at most \\beta(s).\nSuppose Min chooses action a,\nwhich leads to state s' for Max.\nIf Max has any actions that do better than \\beta(s),\nthey would take it,\nmaking action a a suboptimal choice for Min.\n\nAlpha-beta search for a simple game\n\nLet us use the same simple game from \n\nExample 8.2.\nWe list the values of \\alpha(s), \\beta(s) in each node throughout the algorithm.\nThese values are initialized to -\\infty, +\\infty respectively.\nWe shade any squares that have not been visited by the algorithm,\nand we assume that actions are evaluated from left to right.\n\nSuppose Max takes action A. Let s' be the resulting game state.\nThe values of \\alpha(s') and \\beta(s')\nare initialized at the same values as the root state,\nsince we want to prune a subtree if there exists a better action at any step higher in the tree.\n\nThen we iterate through Min’s possible actions,\nupdating the value of \\beta(s') as we go.\n\n\n\n\nOnce the value of state s' is fully evaluated,\nwe know that Max can achieve a value of at least -2 starting from the root,\nand so we update \\alpha(s), where s is the root state:\n\nThen Max imagines taking action B. Again, let s' denote the resulting game state.\nWe initialize \\alpha(s') and \\beta(s') from the root:\n\nNow suppose Min takes action D, resulting in a value of -3.\nWe see that V^\\star_\\hi(s') = \\min(-3, x, y),\nwhere x and y are the values of the remaining two actions.\nBut since \\min(-3, x, y) \\le -3,\nwe know that the value of s' is at most -3.\nBut Max can achieve a better value of \\alpha(s') = -2 by taking action A,\nand so Max will never take action B,\nand we can prune the search here.\nWe will use dotted lines to indicate states that have been ruled out from the search:\n\nFinally, suppose Max takes action C.\nFor Min’s actions D and E,\nthere is still a chance that action C might outperform action A,\nso we continue expanding:\n\n\n\n\nFinally, we see that Min taking action F achieves the minimum value at this state.\nThis shows that optimal play is for Max to take action C,\nand Min to take action F.def alpha_beta_search(s, player, alpha, beta) -> Tuple[\"Action\", \"Value\"]:\n    \"\"\"Return the value of the state (for Max) and the best action for Max to take.\"\"\"\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), min, alpha, beta)\n            if v > v_max:\n                a_max, v_max = a, v\n                alpha = max(alpha, v)\n            if v_max >= beta:\n                # we know Min will not choose the action that leads to this state\n                return a_max, v_max\n        return a_max, v_max\n\n    else:\n        a_min, v_min = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), max)\n            if v < v_min:\n                a_min, v_min = a, v\n                beta = min(beta, v)\n            if v_min <= alpha:\n                # we know Max will not choose the action that leads to this state\n                return a_min, v_min\n        return a_min, v_min\n\nHow do we choose what order to explore the branches?\nAs you can tell, this significantly affects the efficiency of the pruning algorithm.\nIf Max explores the possible actions in order from worst to best,\nthey will not be able to prune any branches at all!\nAdditionally, to verify that an action is suboptimal,\nwe must run the search recursively from that action,\nwhich ultimately requires traversing the tree all the way to a leaf node.\nThe longer the game might possibly last,\nthe more computation we have to run.\n\nIn practice, we can often use background information about the game to develop a heuristic for evaluating possible actions.\nIf a technique is based on background information or intuition,\nespecially if it isn’t rigorously justified,\nwe call it a heuristic.\n\nCan we develop heuristic methods for tree exploration that works for all sorts of games? Here's where we can incorporate the _reinforcement learning_ ","type":"content","url":"/planning#alpha-beta-search","position":13},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Monte Carlo Tree Search"},"type":"lvl2","url":"/planning#monte-carlo-tree-search","position":14},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Monte Carlo Tree Search"},"content":"The task of evaluating actions in a complex environment might seem familiar.\nWe’ve encountered this problem before in both the \n\nmulti-armed bandits setting and the \n\nMarkov decision process setting.\nNow we’ll see how to combine concepts from these to form a more general and efficient tree search heuristic called Monte Carlo Tree Search (MCTS).\n\nWhen a problem is intractable to solve exactly,\nwe often turn to approximate algorithms that sacrifice some accuracy in exchange for computational efficiency.\nMCTS also improves on alpha-beta search in this sense.\nAs the name suggests,\nMCTS uses Monte Carlo simulation, that is, collecting random samples and computing the sample statistics,\nin order to approximate the value of each action.\n\nAs before, we imagine a complete game tree in which each path represents an entire game.\nThe goal of MCTS is to assign values to only the game states that are relevant to the current game;\nWe gradually expand the tree at each move.\nFor comparison, in alpha-beta search,\nthe entire tree only needs to be solved once,\nand from then on,\nchoosing an action is as simple as taking a maximum over the previously computed values.\n\nThe crux of MCTS is approximating the win probability of a state by a sample probability.\nIn practice, MCTS is used for games with binary outcomes where r(s) \\in \\{ +1, -1 \\},\nand so this is equivalent to approximating the final game score.\nTo approximate the win probability from state s,\nMCTS samples random games starting in s and computes the sample proportion of those that the player wins.\n\nNote that, for a given state s,\nchoosing the best action a can be framed as a \n\nmulti-armed bandits problem,\nwhere each action corresponds to an arm,\nand the reward distribution of arm k is the distribution of the game score over random games after choosing that arm.\nThe most commonly used bandit algorithm in practice for MCTS is the \n\nUpper Confidence Bound (UCB) algorithm.\n\nSummary of UCB\n\nLet us quickly review the UCB bandit algorithm.\nFor each arm k, we track the sample mean\\hat \\mu^k_t = \\frac{1}{N_t^k} \\sum_{\\tau=0}^{t-1} \\ind{a_\\tau = k} r_\\tau\n\nof all rewards from that arm up to time t.\nThen we construct a confidence intervalC_t^k = [\\hat \\mu^k_t - B_t^k, \\hat \\mu^k_t + B_t^k],\n\nwhere B_t^k = \\sqrt{\\frac{\\ln(2 t / \\delta)}{2 N_t^k}} is given by Hoeffding’s inequality,\nso that with probability δ (some fixed parameter we choose),\nthe true mean \\mu^k lies within C_t^k.\nNote that B_t^k scales like \\sqrt{1/N^k_t},\ni.e. the more we have visited that arm,\nthe more confident we get about it,\nand the narrower the confidence interval.\n\nTo select an arm, we pick the arm with the highest upper confidence bound.\n\nThis means that, for each edge (corresponding to a state-action pair (s, a)) in the game tree,\nwe keep track of the statistics required to compute its UCB:\n\nHow many times it has been “visited” (N_t^{s, a})\n\nHow many of those visits resulted in victory (\\sum_{\\tau=0}^{t-1} \\ind{(s_\\tau, a_\\tau) = (s, a)} r_\\tau).\nLet us call this latter value W^{s, a}_t (for number of “wins”).\n\nWhat does t refer to in the above expressions?\nRecall t refers to the number of time steps elapsed in the bandit environment.\nAs mentioned above,\neach state s corresponds to its own bandit environment,\nand so t refers to N^s, that is,\nhow many actions have been taken from state s.\nThis term, N^s, gets incremented as the algorithm runs;\nfor simplicity, we won’t introduce another index to track how it changes.\n\nMonte Carlo tree search algorithm\n\nInputs:\n\nT, the number of iterations per move\n\n\\pi_{\\text{rollout}}, the rollout policy for randomly sampling games\n\nc, a positive value that encourages exploration\n\nTo choose a single move starting at state s_{\\text{start}},\nMCTS first tries to estimate the UCB values for each of the possible actions \\mathcal{A}(s_\\text{start}),\nand then chooses the best one.\nTo estimate the UCB values,\nit repeats the following four steps T times:\n\nSelection: We start at s = s_{\\text{start}}. Let τ be an empty list that we will use to track states and actions.\n\nUntil s has at least one action that hasn’t been taken:\n\nChoose a \\gets \\argmax_k \\text{UCB}^{s, k}, where\n\\text{UCB}^{s, a} = \\frac{W^{s, a}}{N^s} + c \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\nAppend (s, a) to τ\n\nSet s \\gets P(s, a)\n\nExpansion: Let s_\\text{new} denote the final state in τ (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from s_\\text{new}. Call it a_{\\text{new}}. Add it to τ.\n\nSimulation: Simulate a complete game episode by starting with the action a_{\\text{new}}\nand then playing according to \\pi_\\text{rollout}.\nThis results in the outcome r \\in \\{ +1, -1 \\}.\n\nBackup: For each (s, a) \\in \\tau:\n\nSet N^{s, a} \\gets N^{s, a} + 1\n\nW^{s, a} \\gets W^{s, a} + r\n\nSet N^s \\gets N^s + 1\n\nAfter T repeats of the above,\nwe return the action with the highest UCB value \n\n(8.4).\nThen play continues.\n\nBetween turns, we can keep the subtree whose statistics we have visited so far.\nHowever, the rest of the tree for the actions we did not end up taking gets discarded.\n\nThe application which brought the MCTS algorithm to fame was DeepMind’s AlphaGo \n\nSilver et al. (2016).\nSince then, it has been used in numerous applications ranging from games to automated theorem proving.\n\nHow accurate is this Monte Carlo estimation?\nIt depends heavily on the rollout policy \\pi_\\text{rollout}.\nIf the distribution \\pi_\\text{rollout} induces over games is very different from the distribution seen during real gameplay,\nwe might end up with a poor value approximation.","type":"content","url":"/planning#monte-carlo-tree-search","position":15},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Incorporating value functions and policies","lvl2":"Monte Carlo Tree Search"},"type":"lvl3","url":"/planning#incorporating-value-functions-and-policies","position":16},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Incorporating value functions and policies","lvl2":"Monte Carlo Tree Search"},"content":"To remedy this,\nwe might make use of a value function v : \\mathcal{S} \\to \\mathbb{R} that more efficiently approximates the value of a state.\nThen, we can replace the simulation step of \n\nMCTS with evaluating r = v(s_\\text{next}), where s_\\text{next} = P(s_\\text{new}, a_\\text{new}).\n\nWe might also make use of a “guiding” policy \\pi_\\text{guide} : \\mathcal{S} \\to \\triangle(\\mathcal{A}) that provides “intuition” as to which actions are more valuable in a given state.\nWe can scale the exploration term of \n\n(8.4) according to the policy’s outputs.\n\nPutting these together,\nwe can describe an updated version of MCTS that makes use of these value functions and policy:\n\nMonte Carlo tree search with policy and value functions\n\nInputs:\n\nT, the number of iterations per move\n\nv, a value function that evaluates how good a state is\n\n\\pi_\\text{guide}, a guiding policy that encourages certain actions\n\nc, a positive value that encourages exploration\n\nTo select a move in state s_\\text{start}, we repeat the following four steps T times:\n\nSelection: We start at s = s_{\\text{start}}. Let τ be an empty list that we will use to track states and actions.\n\nUntil s has at least one action that hasn’t been taken:\n\nChoose a \\gets \\argmax_k \\text{UCB}^{s, k}, where\n\\text{UCB}^{s, a} = \\frac{W^{s, a}}{N^s} + c \\cdot \\pi_\\text{guide}(a \\mid s) \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\nAppend (s, a) to τ\n\nSet s \\gets P(s, a)\n\nExpansion: Let s_\\text{new} denote the final state in τ (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from s_\\text{new}. Call it a_{\\text{new}}. Add it to τ.\n\nSimulation: Let s_\\text{next} = P(s_\\text{new}, a_\\text{new}). Evaluate r = v(s_\\text{next}). This approximates the value of the game after taking the action a_\\text{new}.\n\nBackup: For each (s, a) \\in \\tau:\n\nN^{s, a} \\gets N^{s, a} + 1\n\nW^{s, a} \\gets W^{s, a} + r\n\nN^s \\gets N^s + 1\n\nWe finally return the action with the highest UCB value \n\n(8.5).\nThen play continues. As before, we can reuse the tree across timesteps.\n\nHow do we actually compute a useful \\pi_\\text{guide} and v?\nIf we have some existing dataset of trajectories,\nwe could use \n\nsupervised learning (that is, imitation learning)\nto generate a policy \\pi_\\text{guide} via behavioral cloning\nand learn v by regressing the game outcomes onto states.\nThen, plugging these into \n\nthe above algorithm\nresults in a stronger policy by using tree search to “think ahead”.\n\nBut we don’t have to stop at just one improvement step;\nwe could iterate this process via self-play.","type":"content","url":"/planning#incorporating-value-functions-and-policies","position":17},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Self-play","lvl2":"Monte Carlo Tree Search"},"type":"lvl3","url":"/planning#self-play","position":18},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl3":"Self-play","lvl2":"Monte Carlo Tree Search"},"content":"Recall the \n\npolicy iteration algorithm from the \n\nMDPs chapter.\nPolicy iteration alternates between policy evaluation (taking π and computing V^\\pi)\nand policy improvement (setting π to be greedy with respect to V^\\pi).\nAbove, we saw how MCTS can be thought of as a “policy improvement” operation:\nfor a given policy \\pi^0,\nwe can use it to guide MCTS,\nresulting in an algorithm that is itself a policy \\pi^0_\\text{MCTS} that maps from states to actions.\nNow, we can use \n\nbehavioral cloning\nto obtain a new policy \\pi^1 that imitates \\pi^0_\\text{MCTS}.\nWe can now use \\pi^1 to guide MCTS,\nand repeat.\n\nMCTS with self-play\n\nInput:\n\nA parameterized policy class \\pi_\\theta : \\mathcal{S} \\to \\triangle(\\mathcal{A})\n\nA parameterized value function class v_\\lambda : \\mathcal{S} \\to \\mathbb{R}\n\nA number of trajectories M to generate\n\nThe initial parameters \\theta^0, \\lambda^0\n\nFor t = 0, \\dots, T-1:\n\nPolicy improvement: Let \\pi^t_\\text{MCTS} denote the policy obtained by \n\nAlgorithm 8.2 with \\pi_{\\theta^t} and v_{\\lambda^t}. We use \\pi^t_\\text{MCTS} to play against itself M times. This generates M trajectories \\tau_0, \\dots, \\tau_{M-1}.\n\nPolicy evaluation: Use behavioral cloning to find a set of policy parameters \\theta^{t+1} that mimic the behavior of \\pi^t_\\text{MCTS} and a set of value function parameters \\lambda^{t+1} that approximate its value function. That is,\\begin{align*}\n  \\theta^{t+1} &\\gets \\argmin_\\theta \\sum_{m=0}^{M-1} \\sum_{\\hi=0}^{H-1} - \\log \\pi_\\theta(a^m_\\hi \\mid s^m_\\hi) \\\\\n  \\lambda^{t+1} &\\gets \\argmin_\\lambda \\sum_{m=0}^{M-1} \\sum_{\\hi=0}^{H-1} (v_\\lambda(s^m_\\hi) - R(\\tau_m))^2\n  \\end{align*}\n\nNote that in implementation,\nthe policy and value are typically both returned by a single deep neural network,\nthat is, with a single set of parameters,\nand the two loss functions are added together.\n\nThis algorithm was brought to fame by AlphaGo Zero \n\nSilver et al. (2017).","type":"content","url":"/planning#self-play","position":19},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Summary"},"type":"lvl2","url":"/planning#summary","position":20},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"Summary"},"content":"In this chapter,\nwe explored tree search-based algorithms for deterministic, zero sum, fully observable two-player games.\nWe began with \n\nmin-max search,\nan algorithm for exactly solving the game value of every possible state.\nHowever, this is impossible to execute in practice,\nand so we must resort to various ways to reduce the number of states and actions that we must explore.\n\n\nAlpha-beta search does this by pruning away states that we already know to be suboptimal,\nand \n\nMonte Carlo Tree Search approximates the value of states instead of evaluating them exactly.","type":"content","url":"/planning#summary","position":21},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"References"},"type":"lvl2","url":"/planning#references","position":22},{"hierarchy":{"lvl1":"8 Tree Search Methods","lvl2":"References"},"content":"Chapter 5 of \n\nRussell & Norvig (2021) provides an excellent overview of search methods in games.\nThe original AlphaGo paper \n\nSilver et al. (2016) was a groundbreaking application of these technologies.\n\n\nSilver et al. (2017) removed the imitation learning phase,\nlearning from scratch.\nAlphaZero \n\nSilver et al. (2018) then extended to other games beyond Go,\nnamely shogi and chess,\nalso learning from scratch.\nIn MuZero \n\nSchrittwieser et al. (2020),\nthis was further extended by learning a model of the game dynamics.","type":"content","url":"/planning#references","position":23},{"hierarchy":{"lvl1":"4 Supervised learning"},"type":"lvl1","url":"/supervised-learning","position":0},{"hierarchy":{"lvl1":"4 Supervised learning"},"content":"","type":"content","url":"/supervised-learning","position":1},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Introduction"},"type":"lvl2","url":"/supervised-learning#introduction","position":2},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Introduction"},"content":"This section will cover the details of implementing the fit function above:\nThat is, how to use a dataset of labelled samples (x_1, y_1), \\dots, (x_N, y_N) to find a function f that minimizes the empirical risk.\nThis requires two ingredients:\n\nA function class \\mathcal{F} to search over\n\nA fitting method for minimizing the empirical risk over this class\n\nThe two main function classes we will cover are linear models and neural networks.\nBoth of these function classes are parameterized by some parameters θ,\nand the fitting method will search over these parameters to minimize the empirical risk:\n\nParameterized empirical risk minimization\n\nGiven a dataset of samples (x_1, y_1), \\dots, (x_N, y_N) and a class of functions \\mathcal{F} parameterized by θ,\nwe to find a parameter (vector) \\hat \\theta that minimizes the empirical risk:\\hat \\theta = \\arg\\min_{\\theta} \\frac{1}{N} \\sum_{i=1}^N (y_i - f_\\theta(x_i))^2\n\nThe most common fitting method for parameterized models is gradient descent.\n\nGradient descent\n\nLetting L(\\theta) \\in \\mathbb{R} denote the empirical risk in terms of the parameters,\nthe gradient descent algorithm updates the parameters according to the rule\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(\\theta^t)\n\nwhere \\eta > 0 is the learning rate.\n\nfrom jaxtyping import Float, Array\nfrom collections.abc import Callable\n\nParams = Float[Array, \" D\"]\n\n\ndef gradient_descent(\n    loss: Callable[[Params], float],\n    θ_init: Params,\n    η: float,\n    epochs: int,\n):\n    \"\"\"\n    Run gradient descent to minimize the given loss function\n    (expressed in terms of the parameters).\n    \"\"\"\n    θ = θ_init\n    for _ in range(epochs):\n        θ = θ - η * grad(loss)(θ)\n    return θ\n\n","type":"content","url":"/supervised-learning#introduction","position":3},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Linear regression"},"type":"lvl2","url":"/supervised-learning#linear-regression","position":4},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Linear regression"},"content":"In linear regression, we assume that the function f is linear in the parameters:\\mathcal{F} = \\{ x \\mapsto \\theta^\\top x \\mid \\theta \\in \\mathbb{R}^D \\}\n\nThis function class is extremely simple and only contains linear functions.\nTo expand its expressivity, we can transform the input x using some feature function ϕ,\ni.e. \\widetilde x = \\phi(x), and then fit a linear model in the transformed space instead.\n\ndef fit_linear(X: Float[Array, \"N D\"], y: Float[Array, \" N\"], φ=lambda x: x):\n    \"\"\"Fit a linear model to the given dataset using ordinary least squares.\"\"\"\n    X = vmap(φ)(X)\n    θ = np.linalg.lstsq(X, y, rcond=None)[0]\n    return lambda x: np.dot(φ(x), θ)\n\n","type":"content","url":"/supervised-learning#linear-regression","position":5},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Neural networks"},"type":"lvl2","url":"/supervised-learning#neural-networks","position":6},{"hierarchy":{"lvl1":"4 Supervised learning","lvl2":"Neural networks"},"content":"In neural networks, we assume that the function f is a composition of linear functions (represented by matrices W_i) and non-linear activation functions (denoted by σ):\\mathcal{F} = \\{ x \\mapsto \\sigma(W_L \\sigma(W_{L-1} \\dots \\sigma(W_1 x + b_1) \\dots + b_{L-1}) + b_L) \\}\n\nwhere W_i \\in \\mathbb{R}^{D_{i+1} \\times D_i} and b_i \\in \\mathbb{R}^{D_{i+1}} are the parameters of the i-th layer, and σ is the activation function.\n\nThis function class is much more expressive and contains many more parameters.\nThis makes it more susceptible to overfitting on smaller datasets,\nbut also allows it to represent more complex functions.\nIn practice, however, neural networks exhibit interesting phenomena during training,\nand are often able to generalize well even with many parameters.\n\nAnother reason for their popularity is the efficient backpropagation algorithm for computing the gradient of the empirical risk with respect to the parameters.\nEssentially, the hierarchical structure of the neural network,\ni.e. computing the output of the network as a composition of functions,\nallows us to use the chain rule to compute the gradient of the output with respect to the parameters of each layer.\n\nNielsen (2015) provides a comprehensive introduction to neural networks and backpropagation.","type":"content","url":"/supervised-learning#neural-networks","position":7}]}