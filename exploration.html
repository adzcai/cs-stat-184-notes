<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>9&nbsp; Exploration in MDPs – CS 1840: Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./background.html" rel="next">
<link href="./planning.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./exploration.html"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CS 1840: Introduction to Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">9.1</span> Introduction</a>
  <ul class="collapse">
  <li><a href="#sparse-reward" id="toc-sparse-reward" class="nav-link" data-scroll-target="#sparse-reward"><span class="header-section-number">9.1.1</span> Sparse reward</a></li>
  <li><a href="#exploration-in-deterministic-mdps" id="toc-exploration-in-deterministic-mdps" class="nav-link" data-scroll-target="#exploration-in-deterministic-mdps"><span class="header-section-number">9.1.2</span> Exploration in deterministic MDPs</a></li>
  </ul></li>
  <li><a href="#explore-then-exploit-for-deterministic-mdps" id="toc-explore-then-exploit-for-deterministic-mdps" class="nav-link" data-scroll-target="#explore-then-exploit-for-deterministic-mdps"><span class="header-section-number">9.2</span> Explore-then-exploit (for deterministic MDPs)</a></li>
  <li><a href="#sec-mdp-mab" id="toc-sec-mdp-mab" class="nav-link" data-scroll-target="#sec-mdp-mab"><span class="header-section-number">9.3</span> Treating an unknown MDP as a MAB</a></li>
  <li><a href="#ucb-vi" id="toc-ucb-vi" class="nav-link" data-scroll-target="#ucb-vi"><span class="header-section-number">9.4</span> UCB-VI</a>
  <ul class="collapse">
  <li><a href="#modeling-the-transitions" id="toc-modeling-the-transitions" class="nav-link" data-scroll-target="#modeling-the-transitions"><span class="header-section-number">9.4.1</span> modeling the transitions</a></li>
  <li><a href="#reward-bonus" id="toc-reward-bonus" class="nav-link" data-scroll-target="#reward-bonus"><span class="header-section-number">9.4.2</span> Reward bonus</a></li>
  <li><a href="#performance-of-ucb-vi" id="toc-performance-of-ucb-vi" class="nav-link" data-scroll-target="#performance-of-ucb-vi"><span class="header-section-number">9.4.3</span> Performance of UCB-VI</a></li>
  </ul></li>
  <li><a href="#linear-mdps" id="toc-linear-mdps" class="nav-link" data-scroll-target="#linear-mdps"><span class="header-section-number">9.5</span> Linear MDPs</a>
  <ul class="collapse">
  <li><a href="#planning-in-a-linear-mdp" id="toc-planning-in-a-linear-mdp" class="nav-link" data-scroll-target="#planning-in-a-linear-mdp"><span class="header-section-number">9.5.1</span> Planning in a linear MDP</a></li>
  <li><a href="#sec-lin-ucb-vi" id="toc-sec-lin-ucb-vi" class="nav-link" data-scroll-target="#sec-lin-ucb-vi"><span class="header-section-number">9.5.2</span> UCB-VI in a linear MDP</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">9.6</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-exploration" class="quarto-section-identifier"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<hr>
<hr>
<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="9.1">
<h2 data-number="9.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">9.1</span> Introduction</h2>
<p>One of the key challenges of reinforcement learning is the <em>exploration-exploitation tradeoff</em>. Should we <em>exploit</em> actions we know will give high reward, or should we <em>explore</em> different actions to discover potentially better strategies? An algorithm that doesn’t explore effectively might easily <em>overfit</em> to certain areas of the state space, and fail to generalize once they enter a region they haven’t yet seen. The algorithms we saw in the chapter on fitted DP <a href="fitted_dp.html" class="quarto-xref"><span>Chapter 5</span></a> suffer from this issue.</p>
<p>In <a href="bandits.html" class="quarto-xref"><span>Chapter 3</span></a>, where the state never changes so all we care about are the actions, we saw algorithms like <a href="bandits.html#sec-ucb" class="quarto-xref"><span>Section 3.6</span></a> and <a href="bandits.html#sec-thompson-sampling" class="quarto-xref"><span>Section 3.7</span></a> that incentivize the learner to explore arms that it is uncertain about. In this chapter, we will see how to generalize these ideas to the MDP setting.</p>
<div id="def-per-episode-regret" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.1 (Per-episode regret)</strong></span> To quantify the performance of a learning algorithm, we will consider its per-episode regret over <span class="math inline">\(T\)</span> timesteps/episodes:</p>
<p><span class="math display">\[
\text{Regret}_T = \mathbb{E}\left[ \sum_{t=0}^{T-1} V^\star_0(s_0) - V^{\pi^t}_0(s_0) \right]
\]</span></p>
<p>where <span class="math inline">\(\pi^t\)</span> is the policy generated by the algorithm at the <span class="math inline">\(t\)</span>th iteration.</p>
</div>
<section id="sparse-reward" class="level3" data-number="9.1.1">
<h3 data-number="9.1.1" class="anchored" data-anchor-id="sparse-reward"><span class="header-section-number">9.1.1</span> Sparse reward</h3>
<p>Exploration is crucial in unknown <strong>sparse reward</strong> problems where reward doesn’t come until after many steps, and algorithms which do not <em>systematically</em> explore new states may fail to learn anything meaningful (within a reasonable amount of time). We can see this by considering the following simple MDP:</p>
<div id="exm-sparse-reward-mdp" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.1 (Sparse Reward MDP)</strong></span> Here’s a simple example of an MDP with sparse reward:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/sparse_reward_mdp.png" class="img-fluid figure-img"></p>
<figcaption>image</figcaption>
</figure>
</div>
<p>There are <span class="math inline">\(|\mathcal{S}|\)</span> states. The agent starts in the leftmost state. In every state, there are three possible actions, two of which move the agent left and one which moves the agent right. The reward function assigns <span class="math inline">\(r=1\)</span> to the rightmost cell.</p>
</div>
<p>How well would the algorithms we’ve covered so far do on this problem? For example, <a href="pg.html" class="quarto-xref"><span>Chapter 6</span></a> require the gradient to be nonzero in order to learn. If we never observe any reward, the gradient will always be zero, and the policy will never change or improve. <a href="fitted_dp.html" class="quarto-xref"><span>Chapter 5</span></a> will run into a similar issue: as we randomly interact with the environment, we will never observe any reward, and so the reward model simply gives zero for every state-action pair. In expectation, it would take a computationally infeasible number of rollouts to observe the reward by chance.</p>
<p>The rest of this chapter will consider ways to <em>explicitly</em> add exploration to these algorithms.</p>
</section>
<section id="exploration-in-deterministic-mdps" class="level3" data-number="9.1.2">
<h3 data-number="9.1.2" class="anchored" data-anchor-id="exploration-in-deterministic-mdps"><span class="header-section-number">9.1.2</span> Exploration in deterministic MDPs</h3>
<p>Let us address the exploration problem in a <em>deterministic</em> MDP, that is, where taking action <span class="math inline">\(a\)</span> in state <span class="math inline">\(s\)</span> always leads to the state <span class="math inline">\(P(s, a) \in \mathcal{S}\)</span>. How can we methodically visit every single state-action pair? In the bandit setting, it was trivial to visit every arm: just pull each arm once. How might I navigate to a particular state <span class="math inline">\(s\)</span> in the MDP setting? Doing so requires planning out a path from the original state. In fact, solving navigation itself is a complex RL problem!</p>
</section>
</section>
<section id="explore-then-exploit-for-deterministic-mdps" class="level2" data-number="9.2">
<h2 data-number="9.2" class="anchored" data-anchor-id="explore-then-exploit-for-deterministic-mdps"><span class="header-section-number">9.2</span> Explore-then-exploit (for deterministic MDPs)</h2>
<p>In this simple deterministic setting, the environment never randomly takes us to unseen states, so our strategy must actively explore new states.</p>
<div id="def-explore-then-exploit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.2</strong></span> We’ll keep a set <span class="math inline">\(\mathcal{D}\)</span> of all the <span class="math inline">\((s, a, r, s')\)</span> pairs we’ve observed. Each episode, we’ll use <span class="math inline">\(\mathcal{D}\)</span> to construct a fully known MDP, <span class="math inline">\(M_{\mathcal{D}}\)</span>, in which unseen state-action pairs are rewarded. We’ll then use the planning algorithms from <a href="mdps.html#sec-opt-dynamic-programming" class="quarto-xref"><span>Section 1.3.2</span></a> to reach the unknown states in <span class="math inline">\(M-{\mathcal{D}}\)</span>.</p>
<p>We assume that every state can be reached from the initial state within a single episode, and that the action space <span class="math inline">\(\mathcal{A}\)</span> is fixed.</p>
<ol type="1">
<li><span class="math inline">\(\mathcal{D} \gets \emptyset\)</span></li>
<li>For <span class="math inline">\(T = 0, 1, 2, \dots\)</span> (until the entire MDP has been explored):
<ol type="1">
<li>Construct <span class="math inline">\(M_{\mathcal{D}}\)</span> using <span class="math inline">\(\mathcal{D}\)</span>. That is, the state transitions are set to those observed in <span class="math inline">\(\mathcal{D}\)</span>, and the reward is set to <span class="math inline">\(0\)</span> for all state-action pairs in <span class="math inline">\(\mathcal{D}\)</span>, and <span class="math inline">\(1\)</span> otherwise.</li>
<li>Execute <a href="mdps.html#sec-opt-dynamic-programming" class="quarto-xref"><span>Section 1.3.2</span></a> on the known MDP <span class="math inline">\(M-{\mathcal{D}}\)</span> to compute policy <span class="math inline">\(\pi^\star_{\mathcal{D}}\)</span>.</li>
<li>Execute <span class="math inline">\(\pi^\star_{\mathcal{D}}\)</span>. This will visit some <span class="math inline">\((s, a)\)</span> not yet observed in <span class="math inline">\(\mathcal{D}\)</span>.</li>
<li><span class="math inline">\(\mathcal{D} \gets \mathcal{D} \cup \{ (s, a, r, s') \}\)</span>, where <span class="math inline">\(s' = P(s, a), r = r(s, a)\)</span> are the observed state transition and reward.</li>
</ol></li>
</ol>
</div>
<!-- Compute the optimal policy $\pi^\star$ in the MDP $K$ (e.g. using policy iteration). $\pi^\star$. -->
<div id="thm-explore-then-exploit-performance" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.1 (Performance of explore-then-exploit)</strong></span> As long as every state can be reached from <span class="math inline">\(s_0\)</span> within a single episode, i.e.&nbsp;<span class="math inline">\(|\mathcal{S}| \le H\)</span>, this will eventually be able to explore all <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> state-action pairs, adding one new transition per episode. We know it will take at most <span class="math inline">\(|\mathcal{S}| |\mathcal{A}|\)</span> iterations to explore the entire MDP, after which <span class="math inline">\(\pi^t = \pi^\star\)</span>, incurring no additional regret. For each <span class="math inline">\(\pi^t\)</span> up until then, corresponding to the shortest-path policies <span class="math inline">\(\tilde \pi\)</span>, the value of policy <span class="math inline">\(\pi^t\)</span> will differ from that of <span class="math inline">\(\pi^\star\)</span> by at most <span class="math inline">\(H\)</span>, since the policies will differ by at most <span class="math inline">\(1\)</span> reward at each timestep. So,</p>
<p><span class="math display">\[
\sum_{t=0}^{T-1} V^\star_0 - V_0^{\pi^t} \le |\mathcal{S}||\mathcal{A}| H.
\]</span></p>
<p>(Note that this MDP and algorithm are deterministic, so the regret is not random.)</p>
</div>
</section>
<section id="sec-mdp-mab" class="level2" data-number="9.3">
<h2 data-number="9.3" class="anchored" data-anchor-id="sec-mdp-mab"><span class="header-section-number">9.3</span> Treating an unknown MDP as a MAB</h2>
<p>We also explored the exploration-exploitation tradeoff in <a href="bandits.html" class="quarto-xref"><span>Chapter 3</span></a>. Recall tthat in the MAB setting, we have <span class="math inline">\(K\)</span> arms, each of which has an unknown reward distribution, and we want to learn which of the arms is <em>optimal</em>, i.e.&nbsp;has the highest mean reward.</p>
<p>One algorithm that struck a good balance between exploration and exploitation was the <strong>upper confidence bound</strong> algorithm <a href="bandits.html#sec-ucb" class="quarto-xref"><span>Section 3.6</span></a>: For each arm, we construct a <em>confidence interval</em> for its true mean award, and then choose the arm with the highest upper confidence bound. In summary,</p>
<p><span class="math display">\[
k_{t+1} \gets \arg\max_{k \in [K]} \frac{R^{k}_t}{N^{k}_t} + \sqrt{\frac{\ln(2t/\delta)}{2 N^{k}_t}}
\]</span></p>
<p>where <span class="math inline">\(N_t^k\)</span> indicates the number of times arm <span class="math inline">\(k\)</span> has been pulled up until time <span class="math inline">\(t\)</span>, <span class="math inline">\(R_t^k\)</span> indicates the total reward obtained by pulling arm <span class="math inline">\(k\)</span> up until time <span class="math inline">\(t\)</span>, and <span class="math inline">\(\delta &gt; 0\)</span> controls the width of the confidence interval. How might we extend UCB to the MDP case?</p>
<p>Let us formally describe an unknown MDP as an MAB problem. In an unknown MDP, we want to learn which <em>policy</em> is optimal. So if we want to apply MAB techniques to solving an MDP, it makes sense to think of <em>arms</em> as <em>policies</em>. There are <span class="math inline">\(K = (|\mathcal{A}|^{|\mathcal{S}|})^H\)</span> deterministic policies in a finite MDP. Then, “pulling” arm <span class="math inline">\(\pi\)</span> corresponds to using <span class="math inline">\(\pi\)</span> to act through a trajectory in the MDP, and observing the total reward.</p>
<div id="exr-mdp-mab" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 9.1 (Notation for expected reward)</strong></span> Which quantity that we have seen so far equals the expected reward from arm <span class="math inline">\(\pi\)</span>?</p>
</div>
<p>Recall that UCB incurs regret <span class="math inline">\(\tilde{O}(\sqrt{TK})\)</span>, where <span class="math inline">\(T\)</span> is the number of pulls and <span class="math inline">\(K\)</span> is the number of arms. So in the MDP-as-MAB problem, using UCB for <span class="math inline">\(T\)</span> episodes would achieve regret</p>
<p><span id="eq-mdp-as-mab"><span class="math display">\[
\tilde{O}(\sqrt{|\mathcal{A}|^{|\mathcal{S}|H} T})
\tag{9.1}\]</span></span></p>
<p>This scales <em>exponentially</em> in <span class="math inline">\(|\mathcal{S}|\)</span> and <span class="math inline">\(H\)</span>, which quickly becomes intractable. Notably, this method doesn’t consider the information that we gain across different policies. We can illustrate this with the following example:</p>
<div id="exm-ineffective-mdp" class="theorem example">
<p><span class="theorem-title"><strong>Example 9.2 (Treating an MDP as a MAB)</strong></span> Consider a “coin MDP” with two states “heads” and “tails”, two actions “Y” and “N”, and a time horizon of <span class="math inline">\(H=2\)</span>. The state transition flips the coin, and doesn’t depend on the action. The reward only depends on the action: Taking action Y gives reward <span class="math inline">\(1\)</span>, and taking action N gives reward <span class="math inline">\(0\)</span>.</p>
<p>Suppose we collect data from the two constant policies <span class="math inline">\(\pi_{\text{Y}}(s) = \text{Y}\)</span> and <span class="math inline">\(\pi_{\text{N}}(s) = \text{N}\)</span>. Now we want to learn about the policy <span class="math inline">\(\tilde{\pi}\)</span> that takes action Y and then N. Do we need to collect data from <span class="math inline">\(\tilde{\pi}\)</span> to evaluate it? No: Since the reward only depends on the action, we can infer its value from our data on the policies <span class="math inline">\(\pi_{\text{Y}}\)</span> and <span class="math inline">\(\pi_{\text{N}}\)</span>. However, if we treat the MDP as a bandit in which <span class="math inline">\(\tilde{\pi}\)</span> is a new, unknown arm, we ignore the known correlation between the action and the reward.</p>
</div>
</section>
<section id="ucb-vi" class="level2" data-number="9.4">
<h2 data-number="9.4" class="anchored" data-anchor-id="ucb-vi"><span class="header-section-number">9.4</span> UCB-VI</h2>
<p>The approach above is inefficient: We shouldn’t need to consider all <span class="math inline">\(|\mathcal{A}|^{|\mathcal{S}| H}\)</span> deterministic policies to achieve low regret. Rather, all we need to describe the optimal policy is <span class="math inline">\(Q^\star\)</span>, which has <span class="math inline">\(H |\mathcal{S}||\mathcal{A}|\)</span> entries to be learned. Can we borrow ideas from UCB to reduce the regret to a polynomial in <span class="math inline">\(|\mathcal{S}|\)</span>, <span class="math inline">\(|\mathcal{A}|\)</span>, and <span class="math inline">\(H\)</span>?</p>
<p>One way to frame the UCB algorithm is that, when choosing arms, we optimize over a <em>proxy reward</em> that is the sum of the estimated mean reward and an <strong>exploration term</strong>. In the <strong>UCB-VI</strong> algorithm, we will extend this idea to the case of an unknown MDP <span class="math inline">\(\mathcal{M}^{?}\)</span> by modeling a proxy MDP <span class="math inline">\(\widetilde{\mathcal{M}}\)</span> with a reward function that encourages exploration. Then, we will use DP to solve for the optimal policy in <span class="math inline">\(\widetilde{\mathcal{M}}\)</span>.</p>
<p><strong>Assumptions:</strong> For simplicity, here we assume the reward function of <span class="math inline">\(\mathcal{M}^{?}\)</span> is known, so we only need to model the state transitions, though the rewards can be modelled similarly. We will also consider the more general case of a <strong>time-varying</strong> MDP, where the transition and reward functions may change over time. We take the convention that <span class="math inline">\(P_h\)</span> is the distribution of <span class="math inline">\(s_{h+1} \mid s_{h}, a_{h}\)</span> and <span class="math inline">\(r_h\)</span> is applied to <span class="math inline">\(s_h, a_h\)</span>.</p>
<p>At a high level, the UCB-VI algorithm can be described as follows:</p>
<ol type="1">
<li><p><strong>modeling:</strong> Use previous data to model the transitions <span class="math inline">\(\widehat{P}_0, \dots, \widehat{P}_{H-1}\)</span>.</p></li>
<li><p><strong>Reward bonus:</strong> Design a reward bonus <span class="math inline">\(b_h(s, a) \in \mathbb{R}\)</span> to encourage exploration, analogous to the UCB term.</p></li>
<li><p><strong>Optimistic planning:</strong> Use DP to compute the optimal policy <span class="math inline">\(\widehat \pi_h(s)\)</span> in the modelled MDP</p></li>
</ol>
<p><span class="math display">\[
\tilde{\mathcal{M}} = (\mathcal{S}, \mathcal{A}, \{ \widehat{P}_h\}_{h \in [H]}, \{ r_h+ b_h\}_{h \in [H]}, H).
\]</span></p>
<ol start="4" type="1">
<li><strong>Execution:</strong> Use <span class="math inline">\(\widehat \pi_h(s)\)</span> to collect a new trajectory, and repeat.</li>
</ol>
<p>We detail each of these steps below. The full definition follows in <a href="#def-ucb-vi" class="quarto-xref">Definition&nbsp;<span>9.3</span></a>.</p>
<section id="modeling-the-transitions" class="level3" data-number="9.4.1">
<h3 data-number="9.4.1" class="anchored" data-anchor-id="modeling-the-transitions"><span class="header-section-number">9.4.1</span> modeling the transitions</h3>
<p>We seek to approximate <span class="math inline">\(P_h(s_{h+1} \mid s_h, a_h) = \frac{\mathbb{P}(s_h, a_h, s_{h+1})}{\mathbb{P}(s_h, a_h)}\)</span>. We can estimate these using their sample probabilities from the dataset. That is, define</p>
<p><span class="math display">\[
\begin{aligned}
    N_h^t(s, a, s') &amp; := \sum_{i=0}^{t-1} \mathbf{1}\left\{ (s_h^i, a_h^i, s_{h+1}^i) = (s, a, s') \right\} \\
    N_h^t(s, a)     &amp; := \sum_{i=0}^{t-1} \mathbf{1}\left\{ (s_h^i, a_h^i) = (s, a) \right\}                \\
\end{aligned}
\]</span></p>
<p>Then we can model</p>
<p><span class="math display">\[
\widehat{P}_h^t(s' \mid s, a) = \frac{N_h^t(s, a, s')}{N_h^t(s, a)}.
\]</span></p>
<div class="{prf:remark}">
<p>Note that this is also a fairly naive, nonparametric estimator that doesn’t assume any underlying structure of the MDP. We’ll see how to incorporate assumptions about the MDP in the following section.</p>
</div>
</section>
<section id="reward-bonus" class="level3" data-number="9.4.2">
<h3 data-number="9.4.2" class="anchored" data-anchor-id="reward-bonus"><span class="header-section-number">9.4.2</span> Reward bonus</h3>
<p>To motivate the reward bonus term <span class="math inline">\(b_h^t(s, a)\)</span>, recall how we designed the reward bonus term for UCB:</p>
<ol type="1">
<li><p>We used Hoeffding’s inequality to bound, with high probability, how far the sample mean <span class="math inline">\(\widehat \mu_t^k\)</span> deviated from the true mean <span class="math inline">\(\mu^k\)</span>.</p></li>
<li><p>By inverting this inequality, we obtained a <span class="math inline">\((1-\delta)\)</span>-confidence interval for the true mean, centered at our estimate.</p></li>
<li><p>To make this bound <em>uniform</em> across all timesteps <span class="math inline">\(t \in [T]\)</span>, we applied the union bound and multiplied <span class="math inline">\(\delta\)</span> by a factor of <span class="math inline">\(T\)</span>.</p></li>
</ol>
<p>We’d like to do the same for UCB-VI, and construct the bonus term such that <span class="math inline">\(V^\star_h(s) \le \widehat{V}_h^t(s)\)</span> with high probability. However, our construction will be more complex than the MAB case, since <span class="math inline">\(\widehat{V}_h^t(s)\)</span> depends on the bonus <span class="math inline">\(b_h^t(s, a)\)</span> implicitly via DP. We claim that the bonus term that gives the proper bound is</p>
<p><span id="eq-ucb-vi-bonus"><span class="math display">\[
b_h^t(s, a) = 2 H \sqrt{\frac{\log( |\mathcal{S}||\mathcal{A}|H T/\delta )}{N_h^t(s, a)}}.
\tag{9.2}\]</span></span></p>
<p>We will only provide a heuristic sketch of the proof; see <span class="citation" data-cites="agarwal_reinforcement_2022">(<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">Agarwal et al. 2022</a>)</span> (Section 7.3) for a full proof.</p>
<div id="rem-ucb-vi-bonus" class="proof remark">
<p><span class="proof-title"><em>Remark 9.1</em> (UCB-VI reward bonus construction). </span>We aim to show that, with high probability,</p>
<p><span class="math display">\[
V_h^\star(s) \le \widehat{V}_h^t(s) \quad \forall t \in [T], h \in [H], s \in \mathcal{S}.
\]</span></p>
<p>We’ll do this by bounding the error incurred at each step of DP. Recall that DP solves for <span class="math inline">\(\widehat{V}_h^t(s)\)</span> recursively as follows:</p>
<p><span class="math display">\[
\widehat{V}_h^t(s) = \max_{a \in \mathcal{A}} \left[ \tilde r^t_h(s, a) + \mathbb{E}_{s' \sim \widehat{P}_h^t(\cdot \mid s, a)} \left[ \widehat{V}_{h+1}^t(s') \right] \right]
\]</span></p>
<p>where <span class="math inline">\(\tilde r^t_h(s, a) = r_h(s, a) + b_h^t(s, a)\)</span> is the reward function of our modelled MDP <span class="math inline">\(\tilde{\mathcal{M}}^t\)</span>. On the other hand, we know that <span class="math inline">\(V^\star\)</span> must satisfy</p>
<p><span class="math display">\[
V^\star_h(s) = \max_{a \in \mathcal{A}} \left[ \tilde r^t_h(s, a) + \mathbb{E}_{s' \sim P^?_h(\cdot \mid s, a)} [V^\star_{h+1}(s')] \right]
\]</span></p>
<p>so it suffices to bound the difference between the two inner expectations. There are two sources of error:</p>
<ol type="1">
<li><p>The value functions <span class="math inline">\(\widehat{V}^t_{h+1}\)</span> v.s. <span class="math inline">\(V^\star_{h+1}\)</span></p></li>
<li><p>The transition probabilities <span class="math inline">\(\widehat{P}_h^t\)</span> v.s. <span class="math inline">\(P^?_h\)</span>.</p></li>
</ol>
<p>We can bound these individually, and then combine them by the triangle inequality. For the former, we can simply bound the difference by <span class="math inline">\(H\)</span>, assuming that the rewards are within <span class="math inline">\([0, 1]\)</span>. Now, all that is left is to bound the error from the transition probabilities:</p>
<p><span id="eq-err"><span class="math display">\[
\text{error} = \left| \mathbb{E}_{s' \sim \widehat{P}_h^t(\cdot \mid s, a)} \left[ V^\star_{h+1}(s') \right] - \mathbb{E}_{s' \sim P^?_h(\cdot \mid s, a)} \left[ V^\star_{h+1}(s') \right]. \right|
\tag{9.3}\]</span></span></p>
<p>Let us bound this term for a fixed <span class="math inline">\(s, a, h, t\)</span>. (Later we can make this uniform across <span class="math inline">\(s, a, h, t\)</span> using the union bound.) Note that expanding out the definition of <span class="math inline">\(\widehat{P}_h^t\)</span> gives</p>
<p><span class="math display">\[
\begin{aligned}
        \mathbb{E}_{s' \sim \widehat{P}_h^t(\cdot \mid s, a)} \left[ V^\star_{h+1}(s') \right] &amp; = \sum_{s' \in \mathcal{S}} \frac{N^t_h(s, a, s')}{N^t_h(s, a)} V^\star_{h+1}(s')                                                     \\
                                                                                   &amp; = \frac{1}{N^t_h(s, a)} \sum_{i=0}^{t-1} \sum_{s' \in \mathcal{S}} \mathbf{1}\left\{ (s_h^i, a_h^i, s_{h+1}^i) = (s, a, s') \right\} V^\star_{h+1}(s') \\
                                                                                   &amp; = \frac{1}{N^t_h(s, a)} \sum_{i=0}^{t-1} \underbrace{\mathbf{1}\left\{ (s_h^i, a_h^i) = (s, a) \right\} V^\star_{h+1}(s_{h+1}^i)}_{X^i}
\end{aligned}
\]</span></p>
<p>since the terms where <span class="math inline">\(s' \neq s_{h+1}^i\)</span> vanish.</p>
<p>Now, in order to apply Hoeffding’s inequality, we would like to express the second term in <a href="#eq-err" class="quarto-xref">Equation&nbsp;<span>9.3</span></a> as a sum over <span class="math inline">\(t\)</span> random variables as well. We will do this by redundantly averaging over all desired trajectories (i.e.&nbsp;where we visit state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span> at time <span class="math inline">\(h\)</span>):</p>
<p><span class="math display">\[
\begin{aligned}
        \mathbb{E}_{s' \sim P^?_h(\cdot \mid s, a)} \left[ V^\star_{h+1}(s') \right]
         &amp; = \sum_{s' \in \mathcal{S}} P^?_h(s' \mid s, a) V^\star_{h+1}(s')                                                                              \\
         &amp; = \sum_{s' \in \mathcal{S}} \frac{1}{N^t_h(s, a)} \sum_{i=0}^{t-1} \mathbf{1}\left\{ (s_h^i, a_h^i) = (s, a) \right\} P^?_h(s' \mid s, a) V^\star_{h+1}(s') \\
         &amp; = \frac{1}{N^t_h(s, a)} \sum_{i=0}^{t-1} \mathbb{E}_{s_{h+1}^i \sim P^?_{h}(\cdot \mid s_h^i, a_h^i)} X^i.
\end{aligned}
\]</span></p>
<p>Now we can apply Hoeffding’s inequality to <span class="math inline">\(X^i - \mathbb{E}_{s_{h+1}^i \sim P^?_{h}(\cdot \mid s_h^i, a_h^i)} X^i\)</span>, which is bounded by <span class="math inline">\(H\)</span>, to obtain that, with probability at least <span class="math inline">\(1-\delta\)</span>,</p>
<p><span class="math display">\[
\text{error} = \left| \frac{1}{N^t_h(s, a)} \sum_{i=0}^{t-1} \left(X^i - \mathbb{E}_{s_{h+1}^i \sim P^?_{h}(\cdot \mid s_h^i, a_h^i)} X^i \right) \right| \le 2 H \sqrt{\frac{\ln(1/\delta)}{N_h^t(s, a)}}.
\]</span></p>
<p>Applying a union bound over all <span class="math inline">\(s \in \mathcal{S}, a \in \mathcal{A}, t \in [T], h \in [H]\)</span> gives the <span class="math inline">\(b_h^t(s, a)\)</span> term above.</p>
</div>
<p>Putting these parts together, we can define the algorithm as follows:</p>
<div id="def-ucb-vi" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.3 (UCB-VI algorithm)</strong></span> TODO</p>
</div>
</section>
<section id="performance-of-ucb-vi" class="level3" data-number="9.4.3">
<h3 data-number="9.4.3" class="anchored" data-anchor-id="performance-of-ucb-vi"><span class="header-section-number">9.4.3</span> Performance of UCB-VI</h3>
<p>How exactly does UCB-VI strike a good balance between exploration and exploitation? In UCB for MABs, the bonus exploration term is simple to interpret: It encourages the learner to take actions with a high exploration term. Here, the policy depends on the bonus term indirectly: The policy is obtained by planning in an MDP where the bonus term is added to the reward function. Note that the bonuses <em>propagate backwards</em> in DP, effectively enabling the learner to <em>plan to explore</em> unknown states. This effect takes some further interpretation.</p>
<p>Recall we constructed <span class="math inline">\(b^t_h\)</span> so that, with high probability, <span class="math inline">\(V^\star_h(s) \le \widehat{V}_h^t(s)\)</span> and so</p>
<p><span class="math display">\[
V^\star_h(s) - V^{\pi^t}_h(s) \le \widehat{V}_h^t(s) - V^{\pi^t}_h(s).
\]</span></p>
<p>That is, the l.h.s. measures how suboptimal policy <span class="math inline">\(\pi^t\)</span> is in the true environment, while the r.h.s. is the difference in the policy’s value when acting in the modelled MDP <span class="math inline">\(\tilde{\mathcal{M}}^t\)</span> instead of the true one <span class="math inline">\(\mathcal{M}^{?}\)</span>.</p>
<p>If the r.h.s. is <em>small</em>, this implies that the l.h.s. difference is also small, i.e.&nbsp;that <span class="math inline">\(\pi^t\)</span> is <em>exploiting</em> actions that are giving high reward.</p>
<p>If the r.h.s. is <em>large</em>, then we have overestimated the value: <span class="math inline">\(\pi^t\)</span>, the optimal policy of <span class="math inline">\(\tilde{\mathcal{M}}^t\)</span>, does not perform well in the true environment <span class="math inline">\(\mathcal{M}^{?}\)</span>. This indicates that one of the <span class="math inline">\(b_h^t(s, a)\)</span> terms must be large, or some <span class="math inline">\(\widehat P^t_h(\cdot \mid s, a)\)</span> must be inaccurate, indicating a state-action pair with a low visit count <span class="math inline">\(N^t_h(s, a)\)</span> that the learner was encouraged to explore.</p>
<p>It turns out that UCB-VI achieves a regret of</p>
<div id="thm-ucb-vi-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.2 (UCB-VI regret)</strong></span> <span class="math display">\[
\mathbb{E}\left[ \sum_{t=0}^{T-1} \left(V^\star_0(s_0) - V^{\pi^t}_0(s_0) \right) \right] = \tilde{O}(H^2 \sqrt{|\mathcal{S}| |\mathcal{A}| T})
\]</span></p>
</div>
<p>Comparing this to the UCB regret bound <span class="math inline">\(\tilde{O}(\sqrt{T K})\)</span>, where <span class="math inline">\(K\)</span> is the number of arms of the MAB, we see that we’ve reduced the number of effective arms from <span class="math inline">\(|\mathcal{A}|^{|\mathcal{S}|H}\)</span> (in <a href="#eq-mdp-as-mab" class="quarto-xref">Equation&nbsp;<span>9.1</span></a>) to <span class="math inline">\(H^4 |\mathcal{S}||\mathcal{A}|\)</span>, which is indeed polynomial in <span class="math inline">\(|\mathcal{S}|\)</span>, <span class="math inline">\(|\mathcal{A}|\)</span>, and <span class="math inline">\(H\)</span>, as desired. This is also roughly the number of episodes it takes to achieve constant-order average regret:</p>
<p><span class="math display">\[
\frac{1}{T} \mathbb{E}[\text{Regret}_T] = \tilde{O}\left(\sqrt{\frac{H^4 |\mathcal{S}||\mathcal{A}|}{T}}\right)
\]</span></p>
<p>Note that the time-dependent transition matrix has <span class="math inline">\(H |\mathcal{S}|^2 |\mathcal{A}|\)</span> entries. Assuming <span class="math inline">\(H \ll |\mathcal{S}|\)</span>, this shows that it’s possible to achieve low regret, and achieve a near-optimal policy, while only understanding a <span class="math inline">\(1/|\mathcal{S}|\)</span> fraction of the world’s dynamics.</p>
</section>
</section>
<section id="linear-mdps" class="level2" data-number="9.5">
<h2 data-number="9.5" class="anchored" data-anchor-id="linear-mdps"><span class="header-section-number">9.5</span> Linear MDPs</h2>
<p>A polynomial dependency on <span class="math inline">\(|\mathcal{S}|\)</span> and <span class="math inline">\(|\mathcal{A}|\)</span> is manageable when the state and action spaces are small. But for large or continuous state and action spaces, even this polynomial factor will become intractable. Can we find algorithms that don’t depend on <span class="math inline">\(|\mathcal{S}|\)</span> or <span class="math inline">\(|\mathcal{A}|\)</span> at all, effectively reducing the dimensionality of the MDP? In this section, we’ll explore <strong>linear MDPs</strong>: an example of a <em>parameterized</em> MDP where the rewards and state transitions depend only on some parameter space of dimension <span class="math inline">\(d\)</span> that is independent from <span class="math inline">\(|\mathcal{S}|\)</span> or <span class="math inline">\(|\mathcal{A}|\)</span>.</p>
<div id="def-linear-mdp" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 9.4 (Linear MDP)</strong></span> We assume that the transition probabilities and rewards are <em>linear</em> in some feature vector</p>
<p><span class="math inline">\(\phi(s, a) \in \mathbb{R}^d\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
        P_h(s' \mid s, a) &amp; = \phi(s, a)^\top \mu^\star_h(s') \\
        r_h(s, a)         &amp; = \phi(s, a)^\top \theta_h^\star
\end{aligned}
\]</span></p>
<p>Note that we can also think of <span class="math inline">\(P_h(\cdot \mid s, a) = \mu_h^\star\)</span> as an <span class="math inline">\(|\mathcal{S}| \times d\)</span> matrix, and think of <span class="math inline">\(\mu^\star_h(s')\)</span> as indexing into the <span class="math inline">\(s'\)</span>-th row of this matrix (treating it as a column vector). Thinking of <span class="math inline">\(V^\star_{h+1}\)</span> as an <span class="math inline">\(|\mathcal{S}|\)</span>-dimensional vector, this allows us to write</p>
<p><span class="math display">\[
\mathbb{E}_{s' \sim P_h(\cdot \mid s, a)}[V^\star_{h+1}(s)] = (\mu^\star_h\phi(s, a))^\top V^\star_{h+1}.
\]</span></p>
<p>The <span class="math inline">\(\phi\)</span> feature mapping can be designed to capture interactions between the state <span class="math inline">\(s\)</span> and action <span class="math inline">\(a\)</span>. In this book, we’ll assume that the feature map <span class="math inline">\(\phi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}^d\)</span> and the reward function (described by <span class="math inline">\(\theta_h^\star\)</span>) are known to the learner.</p>
</div>
<section id="planning-in-a-linear-mdp" class="level3" data-number="9.5.1">
<h3 data-number="9.5.1" class="anchored" data-anchor-id="planning-in-a-linear-mdp"><span class="header-section-number">9.5.1</span> Planning in a linear MDP</h3>
<p>It turns out that <span class="math inline">\(Q^\star_h\)</span> is also linear with respect to this feature mapping. We can prove this by simply computing it using DP. We initialize the value function at the end of the time horizon by setting <span class="math inline">\(V_{H}^\star(s) = 0\)</span> for all states <span class="math inline">\(s\)</span>. Then we iterate:</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\star_h(s, a)  &amp; = r_h(s, a) + \mathbb{E}_{s' \sim P_h(\cdot \mid s, a)} [V^\star_{h+1}(s')]                          \\
                     &amp; = \phi(s, a)^\top \theta_h^\star + (\mu_h^\star \phi(s, a))^\top V^\star_{h+1}               \\
                     &amp; = \phi(s, a)^\top \underbrace{( \theta_h^\star + (\mu_h^\star)^\top  V^\star_{h+1})}_{w_h} \\
    V^\star_h(s)     &amp; = \max_a Q^\star_h(s, a)                                                                       \\
    \pi^\star_h(s) &amp; = \arg\max_a Q^\star_h(s, a)
\end{aligned}
\]</span></p>
<div class="{attention}">
<p>Show that <span class="math inline">\(Q^\pi_h\)</span> is also linear with respect to <span class="math inline">\(\phi(s, a)\)</span> for any policy <span class="math inline">\(\pi\)</span>.</p>
</div>
</section>
<section id="sec-lin-ucb-vi" class="level3" data-number="9.5.2">
<h3 data-number="9.5.2" class="anchored" data-anchor-id="sec-lin-ucb-vi"><span class="header-section-number">9.5.2</span> UCB-VI in a linear MDP</h3>
<section id="modeling-the-transitions-1" class="level4" data-number="9.5.2.1">
<h4 data-number="9.5.2.1" class="anchored" data-anchor-id="modeling-the-transitions-1"><span class="header-section-number">9.5.2.1</span> modeling the transitions</h4>
<p>This linear assumption on the MDP will also allow us to model the unknown dynamics <span class="math inline">\(P^?_h(s' \mid s, a)\)</span> with techniques from <strong>supervised learning</strong> (SL). Recall that SL is useful for estimating conditional expectations by minimizing mean squared error. We can rephrase the estimation of <span class="math inline">\(P^?_h(s' \mid s, a)\)</span> as a least-squares problem as follows: Write <span class="math inline">\(\delta_s\)</span> to denote a one-hot vector in <span class="math inline">\(\mathbb{R}^{|\mathcal{S}|}\)</span>, with a <span class="math inline">\(1\)</span> in the <span class="math inline">\(s\)</span>-th entry and <span class="math inline">\(0\)</span> everywhere else. Note that</p>
<p><span class="math display">\[
\mathbb{E}_{s' \sim P_h(\cdot \mid s, a)} [\delta_{s'}] = P_h(\cdot \mid s, a) = \mu_h^\star \phi(s, a).
\]</span></p>
<p>Furthermore, since the expectation here is linear with respect to <span class="math inline">\(\phi(s, a)\)</span>, we can directly apply least-squares multi-target linear regression to construct the estimate</p>
<p><span class="math display">\[
\widehat \mu = \arg\min_{\mu \in \mathbb{R}^{|\mathcal{S}| \times d}} \sum_{t=0}^{T-1} \|\mu \phi(s_h^i, a_h^i) - \delta_{s_{h+1}^i} \|_2^2.
\]</span></p>
<p>This has a well-known closed-form solution:</p>
<p><span class="math display">\[
\begin{aligned}
    \widehat \mu^\top            &amp; = (A_h^t)^{-1} \sum_{i=0}^{t-1} \phi(s_h^i, a_h^i) \delta_{s_{h+1}^i}^\top \\
    \text{where} \quad A_h^t &amp; = \sum_{i=0}^{t-1} \phi(s_h^i, a_h^i) \phi(s_h^i, a_h^i)^\top + \lambda I
\end{aligned}
\]</span></p>
<p>where we include a <span class="math inline">\(\lambda I\)</span> term to ensure that the matrix <span class="math inline">\(A^t_h\)</span> is invertible. (This can also be derived by adding a <span class="math inline">\(\lambda \|\mu\|_{\text{F}}^2\)</span> regularization term to the objective.) We can directly plug in this estimate into <span class="math inline">\(\widehat{P}^t_h(\cdot \mid s, a) = \widehat \mu^t_h \phi(s, a)\)</span>.</p>
</section>
<section id="reward-bonus-1" class="level4" data-number="9.5.2.2">
<h4 data-number="9.5.2.2" class="anchored" data-anchor-id="reward-bonus-1"><span class="header-section-number">9.5.2.2</span> Reward bonus</h4>
<p>Now, to design the reward bonus, we can’t apply Hoeffding’s inequality anymore, since the terms no longer involve sample means of bounded random variables; Instead, we’re incorporating information across different states and actions. Rather, we can construct an upper bound using <em>Chebyshev’s inequality</em> in the same way we did for the LinUCB algorithm in the MAB setting <a href="bandits.html#sec-lin-ucb" class="quarto-xref"><span>Section 3.8.1</span></a>:</p>
<p><span class="math display">\[
b^t_h(s, a) = \beta \sqrt{\phi(s, a)^\top (A^t_h)^{-1} \phi(s, a)}, \quad \beta = \tilde O(d H).
\]</span></p>
<p>Note that this isn’t explicitly inversely proportional to <span class="math inline">\(N_h^t(s, a)\)</span> as in the original UCB-VI bonus term <a href="#eq-ucb-vi-bonus" class="quarto-xref">Equation&nbsp;<span>9.2</span></a>. Rather, it is inversely proportional to the amount that the direction <span class="math inline">\(\phi(s, a)\)</span> has been explored in the history. That is, if <span class="math inline">\(A-h^t\)</span> has a large component in the direction <span class="math inline">\(\phi(s, a)\)</span>, implying that this direction is well explored, then the bonus term will be small, and vice versa.</p>
<p>We can now plug in these transition estimates and reward bonuses into the UCB-VI algorithm <a href="#def-ucb-vi" class="quarto-xref">Definition&nbsp;<span>9.3</span></a>.</p>
<div id="thm-lin-ucb-vi-regret" class="theorem">
<p><span class="theorem-title"><strong>Theorem 9.3 (LinUCB-VI regret)</strong></span> The LinUCB-VI algorithm achieves expected regret</p>
<p><span class="math display">\[
\mathbb{E}[\text{Regret}_T] = \mathbb{E}\left[\sum_{t=0}^{T-1} V^\star_0(s_0) - V^{\pi^t}_0(s_0) \right] \le \tilde O(H^2 d^{1.5} \sqrt{T})
\]</span></p>
</div>
<p>Comparing this to our bound for UCB-VI in an environment without this linear assumption, we see that we go from a sample complexity of <span class="math inline">\(\tilde \Omega(H^4 |\mathcal{S}||\mathcal{A}|)\)</span> to <span class="math inline">\(\tilde \Omega(H^4 d^{3})\)</span>. This new sample complexity only depends on the feature dimension and not on the state or action space of the MDP!</p>
</section>
</section>
</section>
<section id="summary" class="level2" data-number="9.6">
<h2 data-number="9.6" class="anchored" data-anchor-id="summary"><span class="header-section-number">9.6</span> Summary</h2>
<p>In this chapter, we’ve explored how to explore in an unknown MDP.</p>
<ul>
<li><p>We first discussed the explore-then-exploit algorithm <a href="#def-explore-then-exploit" class="quarto-xref">Definition&nbsp;<span>9.2</span></a>, a simple way to explore a deterministic MDP by visiting all state-action pairs.</p></li>
<li><p>We then discussed how to treat an unknown MDP as a MAB <a href="#sec-mdp-mab" class="quarto-xref"><span>Section 9.3</span></a>, and how this approach is inefficient since it doesn’t make use of relationships between policies.</p></li>
<li><p>We then introduced the UCB-VI algorithm <a href="#def-ucb-vi" class="quarto-xref">Definition&nbsp;<span>9.3</span></a>, which models the unknown MDP by a proxy MDP with a reward bonus term that encourages exploration.</p></li>
<li><p>Finally, assuming that the transitions and rewards are linear with respect to a feature transformation of the state and action, we introduced the LinUCB-VI algorithm <a href="#sec-lin-ucb-vi" class="quarto-xref"><span>Section 9.5.2</span></a>, which has a sample complexity independent of the size of the state and action spaces.</p></li>
</ul>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-agarwal_reinforcement_2022" class="csl-entry" role="listitem">
Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. <em>Reinforcement <span>Learning</span>: <span>Theory</span> and <span>Algorithms</span></em>. <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./planning.html" class="pagination-link" aria-label="Tree Search Methods">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./background.html" class="pagination-link" aria-label="Appendix: Background">
        <span class="nav-page-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>