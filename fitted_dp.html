
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4. Fitted Dynamic Programming Algorithms &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'fitted_dp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="5. Linear Quadratic Regulators" href="control.html" />
    <link rel="prev" title="3. Finite Markov Decision Processes" href="mdps.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">2. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="mdps.html">3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">5. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="pg.html">6. Policy Gradient Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">7. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="imitation_learning.html">8. Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">9. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/fitted_dp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fitted Dynamic Programming Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">4.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">4.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">4.4. Supervised learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">4.4.1. Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">4.4.2. Neural networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-correction-for-q-learning">4.5. Bias correction for Q-learning</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="fitted-dynamic-programming-algorithms">
<span id="fitted-dp"></span><h1><span class="section-number">4. </span>Fitted Dynamic Programming Algorithms<a class="headerlink" href="#fitted-dynamic-programming-algorithms" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id3">Introduction</a></p></li>
<li><p><a class="reference internal" href="#empirical-risk-minimization" id="id4">Empirical risk minimization</a></p></li>
<li><p><a class="reference internal" href="#fitted-value-iteration" id="id5">Fitted value iteration</a></p></li>
<li><p><a class="reference internal" href="#supervised-learning" id="id6">Supervised learning</a></p>
<ul>
<li><p><a class="reference internal" href="#linear-regression" id="id7">Linear regression</a></p></li>
<li><p><a class="reference internal" href="#neural-networks" id="id8">Neural networks</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#bias-correction-for-q-learning" id="id9">Bias correction for Q-learning</a></p></li>
</ul>
</nav>
<p>We borrow these definitions from the <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">rand</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">184</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Transition</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">a</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">r</span><span class="p">:</span> <span class="nb">float</span>


<span class="n">Trajectory</span> <span class="o">=</span> <span class="nb">list</span><span class="p">[</span><span class="n">Transition</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the number of actions in the dataset. Assumes actions range from 0 to A-1.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">a</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">τ</span><span class="p">)</span> <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>


<span class="n">State</span> <span class="o">=</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">]</span>  <span class="c1"># arbitrary shape</span>

<span class="c1"># assume finite `A` actions and f outputs an array of Q-values</span>
<span class="c1"># i.e. Q(s, a, h) is implemented as f(s, h)[a]</span>
<span class="n">QFunction</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; A&quot;</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">Q_zero</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Q-function that always returns zero.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>


<span class="c1"># a deterministic time-dependent policy</span>
<span class="n">Policy</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">q_to_greedy</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">QFunction</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the greedy policy for the given state-action value function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="introduction">
<h2><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">4.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>The <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter discussed the case of <strong>finite</strong> MDPs, where the state and action spaces <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> were finite.
This gave us a closed-form expression for computing the r.h.s. of <a class="reference internal" href="mdps.html#bellman_consistency">the Bellman one-step consistency equation</a>.
In this chapter, we consider the case of <strong>large</strong> or <strong>continuous</strong> state spaces, where the state space is too large to be enumerated.
In this case, we need to <em>approximate</em> the value function and Q-function using methods from <strong>supervised learning</strong>.</p>
<p>We will first take a quick detour to introduce the <em>empirical risk minimization</em> framework for function approximation.
We will then see its application to <em>fitted</em> RL algorithms,
which attempt to learn the optimal value function (and the optimal policy) from a dataset of trajectories.</p>
</section>
<section id="empirical-risk-minimization">
<span id="erm"></span><h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">4.2. </span>Empirical risk minimization</a><a class="headerlink" href="#empirical-risk-minimization" title="Link to this heading">#</a></h2>
<p>The <strong>supervised learning</strong> task is as follows:
We seek to learn the relationship between some input variables <span class="math notranslate nohighlight">\(x\)</span> and some output variable <span class="math notranslate nohighlight">\(y\)</span>
(drawn from their joint distribution).
Precisely, we want to find a function <span class="math notranslate nohighlight">\(\hat f : x \mapsto y\)</span> that minimizes the
<em>squared error</em> of the prediction:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f} \E[(y - f(x))^2]
\]</div>
<p>An equivalent framing is that we seek to approximate the <em>conditional expectation</em> of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="proof theorem admonition" id="conditional_expectation_minimizes_mse">
<p class="admonition-title"><span class="caption-number">Theorem 4.1 </span> (Conditional expectation minimizes mean squared error)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[
\arg\min_{f} \E[(y - f(x))^2] = (x \mapsto \E[y \mid x])
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We can decompose the mean squared error as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\E[(y - f(x))^2] &amp;= \E[ (y - \E[y \mid x] + \E[y \mid x] - f(x))^2 ] \\
&amp;= \E[ (y - \E[y \mid x])^2 ] + \E[ (\E[y \mid x] - f(x))^2 ] + 2 \E[ (y - \E[y \mid x])(\E[y \mid x] - f(x)) ] \\
\end{aligned}
\end{split}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Use the law of iterated expectations to show that the last term is zero.</p>
</div>
<p>The first term is the irreducible error, and the second term is the error due to the approximation,
which is minimized at <span class="math notranslate nohighlight">\(0\)</span> when <span class="math notranslate nohighlight">\(f(x) = \E[y \mid x]\)</span>.</p>
</div>
<p>In most applications, the joint distribution of <span class="math notranslate nohighlight">\(x, y\)</span> is unknown or extremely complex, and so we can’t
analytically evaluate <span class="math notranslate nohighlight">\(\E [y \mid x]\)</span>.
Instead, our strategy is to draw <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> from the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>,
and then use the <em>sample average</em> <span class="math notranslate nohighlight">\(\sum_{i=1}^N (y_i - f(x_i))^2 / N\)</span> to approximate the mean squared error.
Then we use a <em>fitting method</em> to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that minimizes this objective
and thus approximates the conditional expectation.
This approach is called <strong>empirical risk minimization</strong>.</p>
<div class="proof definition admonition" id="empirical_risk_minimization">
<p class="admonition-title"><span class="caption-number">Definition 4.1 </span> (Empirical risk minimization)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset of samples <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_N, y_N)\)</span>, empirical risk minimization seeks to find a function <span class="math notranslate nohighlight">\(f\)</span> (from some class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>) that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2
\]</div>
<p>We will cover the details of the minimization process in <a class="reference internal" href="#supervised-learning"><span class="std std-ref">the next section</span></a>.</p>
</section>
</div><div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Why is it important that we constrain our search to a class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>?</p>
<p>Hint: Consider the function <span class="math notranslate nohighlight">\(f(x) = \sum_{i=1}^N y_i \mathbb{1}_{\{ x = x_i \}}\)</span>. What is the empirical risk of this function? Would you consider it a good approximation of the conditional expectation?</p>
</div>
</section>
<section id="fitted-value-iteration">
<h2><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">4.3. </span>Fitted value iteration</a><a class="headerlink" href="#fitted-value-iteration" title="Link to this heading">#</a></h2>
<p>Let us apply ERM to the RL problem of computing the optimal policy / value function.</p>
<p>How did we compute the optimal value function in MDPs with <em>finite</em> state and action spaces?</p>
<ul class="simple">
<li><p>In a <a class="reference internal" href="mdps.html#id2"><span class="std std-ref">finite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#pi_star_dp">dynamic programming</a>, working backwards from the end of the time horizon, to compute the optimal value function exactly.</p></li>
<li><p>In an <a class="reference internal" href="mdps.html#infinite-horizon-mdps"><span class="std std-ref">infinite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#value-iteration"><span class="std std-ref">value iteration</span></a>, which iterates the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(3.7)</a> to approximately compute the optimal value function.</p></li>
</ul>
<p>Our existing approaches represent the value function, and the MDP itself,
in matrix notation.
But what happens if the state space is extremely large, or even infinite (e.g. real-valued)?
Then computing a weighted sum over all possible next states, which is required to compute the Bellman operator,
becomes intractable.</p>
<p>Instead, we will need to use <em>function approximation</em> methods from supervised learning to solve for the value function in an alternative way.</p>
<p>In particular, suppose we have a dataset of <span class="math notranslate nohighlight">\(N\)</span> trajectories <span class="math notranslate nohighlight">\(\tau_1, \dots, \tau_N \sim \rho_{\pi}\)</span> from some policy <span class="math notranslate nohighlight">\(\pi\)</span> (called the <strong>data collection policy</strong>) acting in the MDP of interest.
Let us indicate the trajectory index in the superscript, so that</p>
<div class="math notranslate nohighlight">
\[
\tau_i = \{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \dots, s_{\hor-1}^i, a_{\hor-1}^i, r_{\hor-1}^i \}.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">collect_data</span><span class="p">(</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span> <span class="n">π</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collect a dataset of trajectories from the given policy (or a random one).&quot;&quot;&quot;</span>
    <span class="n">trajectories</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="n">rand</span><span class="o">.</span><span class="n">bits</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">rand</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
        <span class="n">τ</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
            <span class="c1"># sample from a random policy</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="n">π</span> <span class="k">else</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">s_next</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">τ</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Transition</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">s_next</span>
        <span class="n">trajectories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trajectories</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">)</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">collect_data</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># show first five transitions from first trajectory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                    | 0/100 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 39%|████████████████▍                         | 39/100 [00:00&lt;00:00, 388.67it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 90%|█████████████████████████████████████▊    | 90/100 [00:00&lt;00:00, 458.33it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|█████████████████████████████████████████| 100/100 [00:00&lt;00:00, 453.82it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Transition(s=array([-0.00767412,  1.4020356 , -0.77731264, -0.3948966 ,  0.00889908,
         0.17607284,  0.        ,  0.        ], dtype=float32), a=1, r=-2.1191279854808145),
 Transition(s=array([-0.01542616,  1.3925703 , -0.7859692 , -0.4207744 ,  0.01954689,
         0.21297753,  0.        ,  0.        ], dtype=float32), a=2, r=0.39769165458876615),
 Transition(s=array([-0.02307358,  1.3831106 , -0.77605283, -0.42061192,  0.03073004,
         0.22368379,  0.        ,  0.        ], dtype=float32), a=3, r=-0.3487214817021129),
 Transition(s=array([-0.03063736,  1.3730472 , -0.7655495 , -0.4474775 ,  0.03980196,
         0.18145534,  0.        ,  0.        ], dtype=float32), a=1, r=-2.2423600035067452),
 Transition(s=array([-0.03827467,  1.3623825 , -0.77475727, -0.47430995,  0.05071578,
         0.21829662,  0.        ,  0.        ], dtype=float32), a=1, r=-2.436894065119587)]
</pre></div>
</div>
</div>
</div>
<p>Can we view the dataset of trajectories as a “labelled dataset” in order to apply supervised learning to approximate the optimal Q-function? Yes!
Recall that we can characterize the optimal Q-function using the <a class="reference internal" href="mdps.html#bellman_consistency_optimal">Bellman optimality equations</a>,
which don’t depend on an actual policy:</p>
<div class="math notranslate nohighlight">
\[
Q_\hi^\star(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [\max_{a'} Q_{\hi+1}^\star(s', a')]
\]</div>
<p>We can think of the arguments to the Q-function – i.e. the current state, action, and timestep <span class="math notranslate nohighlight">\(\hi\)</span> –
as the inputs <span class="math notranslate nohighlight">\(x\)</span>, and the r.h.s. of the above equation as the label <span class="math notranslate nohighlight">\(f(x)\)</span>. Note that the r.h.s. can also be expressed as a <strong>conditional expectation</strong>:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \E [y \mid x] \quad \text{where} \quad y = r(s_\hi, a_\hi) + \max_{a'} Q^\star_{\hi + 1}(s', a').
\]</div>
<p>Approximating the conditional expectation is precisely the task that <a class="reference internal" href="#erm"><span class="std std-ref">empirical risk minimization</span></a> is suited for!</p>
<p>Our above dataset would give us <span class="math notranslate nohighlight">\(N \cdot \hor\)</span> samples in the dataset:</p>
<div class="math notranslate nohighlight">
\[
x_{i \hi} = (s_\hi^i, a_\hi^i, \hi) \qquad y_{i \hi} = r(s_\hi^i, a_\hi^i) + \max_{a'} Q^\star_{\hi + 1}(s_{\hi + 1}^i, a')
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We pass the state and timestep as input to the Q-function</span>
<span class="sd">    and return an array of Q-values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[(</span><span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">τ</span><span class="p">))]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span> <span class="k">for</span> <span class="n">ary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rows</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">get_y</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">π</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the dataset of trajectories into a dataset for supervised learning.</span>
<span class="sd">    If `π` is None, instead estimates the optimal Q function.</span>
<span class="sd">    Otherwise, estimates the Q function of π.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
            <span class="n">Q_values</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="k">if</span> <span class="n">π</span> <span class="k">else</span> <span class="n">Q_values</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">τ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;states:&quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;actions:&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;timesteps:&quot;</span><span class="p">,</span> <span class="n">h</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>states: [[-0.00767412  1.4020356  -0.77731264 -0.3948966   0.00889908  0.17607284
   0.          0.        ]
 [-0.01542616  1.3925703  -0.7859692  -0.4207744   0.01954689  0.21297753
   0.          0.        ]
 [-0.02307358  1.3831106  -0.77605283 -0.42061192  0.03073004  0.22368379
   0.          0.        ]
 [-0.03063736  1.3730472  -0.7655495  -0.4474775   0.03980196  0.18145534
   0.          0.        ]
 [-0.03827467  1.3623825  -0.77475727 -0.47430995  0.05071578  0.21829662
   0.          0.        ]]
actions: [1 2 3 1 1]
timesteps: [0 1 2 3 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([-2.119128  ,  0.39769167, -0.34872147, -2.24236   , -2.4368942 ],      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Then we can use empirical risk minimization to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that approximates the optimal Q-function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will see some examples of fitting methods in the next section</span>
<span class="n">FittingMethod</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;N D&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; N&quot;</span><span class="p">]],</span> <span class="n">QFunction</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>But notice that the definition of <span class="math notranslate nohighlight">\(y_{i \hi}\)</span> depends on the Q-function itself!
How can we resolve this circular dependency?
Recall that we faced the same issue <a class="reference internal" href="mdps.html#iterative-pe"><span class="std std-ref">when evaluating a policy in an infinite-horizon MDP</span></a>. There, we iterated the <a class="reference internal" href="mdps.html#bellman_operator">Bellman operator</a> since we knew that the policy’s value function was a fixed point of the policy’s Bellman operator.
We can apply the same strategy here, using the <span class="math notranslate nohighlight">\(\hat f\)</span> from the previous iteration to compute the labels <span class="math notranslate nohighlight">\(y_{i \hi}\)</span>,
and then using this new dataset to fit the next iterate.</p>
<div class="proof algorithm admonition" id="fitted_q_iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 4.1 </span> (Fitted Q-function iteration)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(3.7)</a></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_q_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted Q-function iteration using the given dataset.</span>
<span class="sd">    Returns an estimate of the optimal Q-function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use this fixed-point interation to <em>evaluate</em> a policy using the dataset (not necessarily the one used to generate the trajectories):</p>
<div class="proof algorithm admonition" id="fitted_evaluation">
<p class="admonition-title"><span class="caption-number">Algorithm 4.2 </span> (Fitted policy evaluation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy <span class="math notranslate nohighlight">\(\pi : \mathcal{S} \times [H] \to \Delta(\mathcal{A})\)</span> to be evaluated.</p>
<p><strong>Output:</strong> An approximation of the value function <span class="math notranslate nohighlight">\(Q^\pi\)</span> of the policy.</p>
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the <a class="reference internal" href="mdps.html#bellman_consistency">Bellman consistency equation</a> for the given policy.</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_evaluation</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">π</span><span class="p">:</span> <span class="n">Policy</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted policy evaluation using the given dataset.</span>
<span class="sd">    Returns an estimate of the Q-function of the given policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">,</span> <span class="n">π</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Spot the difference between <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> and <code class="docutils literal notranslate"><span class="pre">fitted_q_iteration</span></code>. (See the definition of <code class="docutils literal notranslate"><span class="pre">get_y</span></code>.)
How would you modify this algorithm to evaluate the data collection policy?</p>
</div>
<p>We can use this policy evaluation algorithm to adapt the <a class="reference internal" href="mdps.html#policy-iteration"><span class="std std-ref">policy iteration algorithm</span></a> to this new setting. The algorithm remains exactly the same – repeatedly make the policy greedy w.r.t. its own value function – except now we must evaluate the policy (i.e. compute its value function) using the iterative <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_policy_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">evaluation_epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">π_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># constant zero policy</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run fitted policy iteration using the given dataset.&quot;&quot;&quot;</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">π_init</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fitted_evaluation</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">π</span><span class="p">,</span> <span class="n">evaluation_epochs</span><span class="p">)</span>
        <span class="n">π</span> <span class="o">=</span> <span class="n">q_to_greedy</span><span class="p">(</span><span class="n">Q_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">π</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="supervised-learning">
<span id="id1"></span><h2><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">4.4. </span>Supervised learning</a><a class="headerlink" href="#supervised-learning" title="Link to this heading">#</a></h2>
<p>This section will cover the details of implementing the <code class="docutils literal notranslate"><span class="pre">fit</span></code> function above:
That is, how to use a dataset of labelled samples <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_N, y_N)\)</span> to find a function <span class="math notranslate nohighlight">\(f\)</span> that minimizes the empirical risk.
This requires two ingredients:</p>
<ol class="arabic simple">
<li><p>A <strong>function class</strong> <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> to search over</p></li>
<li><p>A <strong>fitting method</strong> for minimizing the empirical risk over this class</p></li>
</ol>
<p>The two main function classes we will cover are <strong>linear models</strong> and <strong>neural networks</strong>.
Both of these function classes are <em>parameterized</em> by some parameters <span class="math notranslate nohighlight">\(\theta\)</span>,
and the fitting method will search over these parameters to minimize the empirical risk:</p>
<div class="proof definition admonition" id="parameterized_empirical_risk_minimization">
<p class="admonition-title"><span class="caption-number">Definition 4.2 </span> (Parameterized empirical risk minimization)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset of samples <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_N, y_N)\)</span> and a class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> parameterized by <span class="math notranslate nohighlight">\(\theta\)</span>,
we to find a parameter (vector) <span class="math notranslate nohighlight">\(\hat \theta\)</span> that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^N (y_i - f_\theta(x_i))^2
\]</div>
</section>
</div><p>The most common fitting method for parameterized models is <strong>gradient descent</strong>.</p>
<div class="proof algorithm admonition" id="algorithm-5">
<p class="admonition-title"><span class="caption-number">Algorithm 4.3 </span> (Gradient descent)</p>
<section class="algorithm-content" id="proof-content">
<p>Letting <span class="math notranslate nohighlight">\(L(\theta) \in \mathbb{R}\)</span> denote the empirical risk in terms of the parameters,
the gradient descent algorithm updates the parameters according to the rule</p>
<div class="math notranslate nohighlight">
\[
\theta^{t+1} = \theta^t - \eta \nabla_\theta L(\theta^t)
\]</div>
<p>where <span class="math notranslate nohighlight">\(\eta &gt; 0\)</span> is the <strong>learning rate</strong>.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Params</span> <span class="o">=</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; D&quot;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">gradient_descent</span><span class="p">(</span>
    <span class="n">loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Params</span><span class="p">],</span> <span class="nb">float</span><span class="p">],</span>
    <span class="n">θ_init</span><span class="p">:</span> <span class="n">Params</span><span class="p">,</span>
    <span class="n">η</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run gradient descent to minimize the given loss function</span>
<span class="sd">    (expressed in terms of the parameters).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">θ_init</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">θ</span> <span class="o">=</span> <span class="n">θ</span> <span class="o">-</span> <span class="n">η</span> <span class="o">*</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">θ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">θ</span>
</pre></div>
</div>
</div>
</div>
<section id="linear-regression">
<h3><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">4.4.1. </span>Linear regression</a><a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h3>
<p>In linear regression, we assume that the function <span class="math notranslate nohighlight">\(f\)</span> is linear in the parameters:</p>
<div class="math notranslate nohighlight">
\[
\mathcal{F} = \{ x \mapsto \theta^\top x \mid \theta \in \mathbb{R}^D \}
\]</div>
<p>This function class is extremely simple and only contains linear functions.
To expand its expressivity, we can <em>transform</em> the input <span class="math notranslate nohighlight">\(x\)</span> using some feature function <span class="math notranslate nohighlight">\(\phi\)</span>,
i.e. <span class="math notranslate nohighlight">\(\widetilde x = \phi(x)\)</span>, and then fit a linear model in the transformed space instead.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_linear</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;N D&quot;</span><span class="p">],</span> <span class="n">y</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; N&quot;</span><span class="p">],</span> <span class="n">φ</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Fit a linear model to the given dataset using ordinary least squares.&quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">φ</span><span class="p">)(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">lstsq</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">rcond</span><span class="o">=</span><span class="kc">None</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">φ</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">θ</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="neural-networks">
<h3><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">4.4.2. </span>Neural networks</a><a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>In neural networks, we assume that the function <span class="math notranslate nohighlight">\(f\)</span> is a composition of linear functions (represented by matrices <span class="math notranslate nohighlight">\(W_i\)</span>) and non-linear activation functions (denoted by <span class="math notranslate nohighlight">\(\sigma\)</span>):</p>
<div class="math notranslate nohighlight">
\[
\mathcal{F} = \{ x \mapsto \sigma(W_L \sigma(W_{L-1} \dots \sigma(W_1 x + b_1) \dots + b_{L-1}) + b_L) \}
\]</div>
<p>where <span class="math notranslate nohighlight">\(W_i \in \mathbb{R}^{D_{i+1} \times D_i}\)</span> and <span class="math notranslate nohighlight">\(b_i \in \mathbb{R}^{D_{i+1}}\)</span> are the parameters of the <span class="math notranslate nohighlight">\(i\)</span>-th layer, and <span class="math notranslate nohighlight">\(\sigma\)</span> is the activation function.</p>
<p>This function class is much more expressive and contains many more parameters.
This makes it more susceptible to overfitting on smaller datasets,
but also allows it to represent more complex functions.
In practice, however, neural networks exhibit interesting phenomena during training,
and are often able to generalize well even with many parameters.</p>
<p>Another reason for their popularity is the efficient <strong>backpropagation</strong> algorithm
for computing the gradient of the empirical risk with respect to the parameters.
Essentially, the hierarchical structure of the neural network, i.e. computing the
output of the network as a composition of functions, allows us to use the chain rule
to compute the gradient of the output with respect to the parameters of each layer.</p>
<p><span id="id2">[<a class="reference internal" href="bibliography.html#id15" title="Michael A. Nielsen. Neural Networks and Deep Learning. Determination Press, 2015.">Nielsen, 2015</a>]</span> provides a comprehensive introduction to neural networks and backpropagation.</p>
</section>
</section>
<section id="bias-correction-for-q-learning">
<h2><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">4.5. </span>Bias correction for Q-learning</a><a class="headerlink" href="#bias-correction-for-q-learning" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="mdps.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Finite Markov Decision Processes</p>
      </div>
    </a>
    <a class="right-next"
       href="control.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Linear Quadratic Regulators</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">4.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">4.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">4.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">4.4. Supervised learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">4.4.1. Linear regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">4.4.2. Neural networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-correction-for-q-learning">4.5. Bias correction for Q-learning</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>