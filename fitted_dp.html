
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5. Fitted Dynamic Programming Algorithms &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'fitted_dp';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="6. Policy Optimization" href="pg.html" />
    <link rel="prev" title="4. Supervised learning" href="supervised_learning.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mdps.html">1. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">2. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">3. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised_learning.html">4. Supervised learning</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="pg.html">6. Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="imitation_learning.html">7. Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="planning.html">8. Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">9. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="contextual_bandits.html">10. Contextual bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">11. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="background.html">12. Appendix: Background</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/fitted_dp.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Fitted Dynamic Programming Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">5.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">5.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">5.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="fitted-dynamic-programming-algorithms">
<span id="fitted-dp"></span><h1><span class="section-number">5. </span>Fitted Dynamic Programming Algorithms<a class="headerlink" href="#fitted-dynamic-programming-algorithms" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#introduction" id="id1">Introduction</a></p></li>
<li><p><a class="reference internal" href="#empirical-risk-minimization" id="id2">Empirical risk minimization</a></p></li>
<li><p><a class="reference internal" href="#fitted-value-iteration" id="id3">Fitted value iteration</a></p></li>
<li><p><a class="reference internal" href="#summary" id="id4">Summary</a></p></li>
</ul>
</nav>
<p>We borrow these definitions from the <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">rand</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">gymnasium</span> <span class="k">as</span> <span class="nn">gym</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">184</span><span class="p">)</span>


<span class="k">class</span> <span class="nc">Transition</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">a</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">r</span><span class="p">:</span> <span class="nb">float</span>


<span class="n">Trajectory</span> <span class="o">=</span> <span class="nb">list</span><span class="p">[</span><span class="n">Transition</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the number of actions in the dataset. Assumes actions range from 0 to A-1.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">max</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">t</span><span class="o">.</span><span class="n">a</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">τ</span><span class="p">)</span> <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>


<span class="n">State</span> <span class="o">=</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;...&quot;</span><span class="p">]</span>  <span class="c1"># arbitrary shape</span>

<span class="c1"># assume finite `A` actions and f outputs an array of Q-values</span>
<span class="c1"># i.e. Q(s, a, h) is implemented as f(s, h)[a]</span>
<span class="n">QFunction</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; A&quot;</span><span class="p">]]</span>


<span class="k">def</span> <span class="nf">Q_zero</span><span class="p">(</span><span class="n">A</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A Q-function that always returns zero.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>


<span class="c1"># a deterministic time-dependent policy</span>
<span class="n">Policy</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">State</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="nb">int</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">q_to_greedy</span><span class="p">(</span><span class="n">Q</span><span class="p">:</span> <span class="n">QFunction</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Policy</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the greedy policy for the given state-action value function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="introduction">
<h2><a class="toc-backref" href="#id1" role="doc-backlink"><span class="section-number">5.1. </span>Introduction</a><a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>The <a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Finite Markov Decision Processes</span></a> chapter discussed the case of <strong>finite</strong> MDPs, where the state and action spaces <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> were finite.
This gave us a closed-form expression for computing the r.h.s. of <a class="reference internal" href="mdps.html#bellman_consistency">the Bellman one-step consistency equation</a>.
In this chapter, we consider the case of <strong>large</strong> or <strong>continuous</strong> state spaces, where the state space is too large to be enumerated.
In this case, we need to <em>approximate</em> the value function and Q-function using methods from <strong>supervised learning</strong>.</p>
<p>We will first take a quick detour to introduce the <em>empirical risk minimization</em> framework for function approximation.
We will then see its application to <em>fitted</em> RL algorithms,
which attempt to learn the optimal value function (and the optimal policy) from a dataset of trajectories.</p>
</section>
<section id="empirical-risk-minimization">
<span id="erm"></span><h2><a class="toc-backref" href="#id2" role="doc-backlink"><span class="section-number">5.2. </span>Empirical risk minimization</a><a class="headerlink" href="#empirical-risk-minimization" title="Link to this heading">#</a></h2>
<p>The <strong>supervised learning</strong> task is as follows:
We seek to learn the relationship between some input variables <span class="math notranslate nohighlight">\(x\)</span> and some output variable <span class="math notranslate nohighlight">\(y\)</span>
(drawn from their joint distribution).
Precisely, we want to find a function <span class="math notranslate nohighlight">\(\hat f : x \mapsto y\)</span> that minimizes the
<em>squared error</em> of the prediction:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f} \E[(y - f(x))^2]
\]</div>
<p>An equivalent framing is that we seek to approximate the <em>conditional expectation</em> of <span class="math notranslate nohighlight">\(y\)</span> given <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="proof theorem admonition" id="conditional_expectation_minimizes_mse">
<p class="admonition-title"><span class="caption-number">Theorem 5.1 </span> (Conditional expectation minimizes mean squared error)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[
\arg\min_{f} \E[(y - f(x))^2] = (x \mapsto \E[y \mid x])
\]</div>
</section>
</div><div class="proof admonition" id="proof">
<p>Proof. We can decompose the mean squared error as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\E[(y - f(x))^2] &amp;= \E[ (y - \E[y \mid x] + \E[y \mid x] - f(x))^2 ] \\
&amp;= \E[ (y - \E[y \mid x])^2 ] + \E[ (\E[y \mid x] - f(x))^2 ] + 2 \E[ (y - \E[y \mid x])(\E[y \mid x] - f(x)) ] \\
\end{aligned}
\end{split}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Use the law of iterated expectations to show that the last term is zero.</p>
</div>
<p>The first term is the irreducible error, and the second term is the error due to the approximation,
which is minimized at <span class="math notranslate nohighlight">\(0\)</span> when <span class="math notranslate nohighlight">\(f(x) = \E[y \mid x]\)</span>.</p>
</div>
<p>In most applications, the joint distribution of <span class="math notranslate nohighlight">\(x, y\)</span> is unknown or extremely complex, and so we can’t
analytically evaluate <span class="math notranslate nohighlight">\(\E [y \mid x]\)</span>.
Instead, our strategy is to draw <span class="math notranslate nohighlight">\(N\)</span> samples <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> from the joint distribution of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>,
and then use the <em>sample average</em> <span class="math notranslate nohighlight">\(\sum_{i=1}^N (y_i - f(x_i))^2 / N\)</span> to approximate the mean squared error.
Then we use a <em>fitting method</em> to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that minimizes this objective
and thus approximates the conditional expectation.
This approach is called <strong>empirical risk minimization</strong>.</p>
<div class="proof definition admonition" id="empirical_risk_minimization">
<p class="admonition-title"><span class="caption-number">Definition 5.1 </span> (Empirical risk minimization)</p>
<section class="definition-content" id="proof-content">
<p>Given a dataset of samples <span class="math notranslate nohighlight">\((x_1, y_1), \dots, (x_N, y_N)\)</span>, empirical risk minimization seeks to find a function <span class="math notranslate nohighlight">\(f\)</span> (from some class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>) that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[
\hat f = \arg\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2
\]</div>
<p>We will cover the details of the minimization process in <a class="reference internal" href="supervised_learning.html#supervised-learning"><span class="std std-ref">the next section</span></a>.</p>
</section>
</div><div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Why is it important that we constrain our search to a class of functions <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>?</p>
<p>Hint: Consider the function <span class="math notranslate nohighlight">\(f(x) = \sum_{i=1}^N y_i \mathbb{1}_{\{ x = x_i \}}\)</span>. What is the empirical risk of this function? Would you consider it a good approximation of the conditional expectation?</p>
</div>
</section>
<section id="fitted-value-iteration">
<h2><a class="toc-backref" href="#id3" role="doc-backlink"><span class="section-number">5.3. </span>Fitted value iteration</a><a class="headerlink" href="#fitted-value-iteration" title="Link to this heading">#</a></h2>
<p>Let us apply ERM to the RL problem of computing the optimal policy / value function.</p>
<p>How did we compute the optimal value function in MDPs with <em>finite</em> state and action spaces?</p>
<ul class="simple">
<li><p>In a <a class="reference internal" href="mdps.html#id2"><span class="std std-ref">finite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#pi_star_dp">dynamic programming</a>, working backwards from the end of the time horizon, to compute the optimal value function exactly.</p></li>
<li><p>In an <a class="reference internal" href="mdps.html#infinite-horizon-mdps"><span class="std std-ref">infinite-horizon MDP</span></a>, we can use <a class="reference internal" href="mdps.html#value-iteration"><span class="std std-ref">value iteration</span></a>, which iterates the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(1.7)</a> to approximately compute the optimal value function.</p></li>
</ul>
<p>Our existing approaches represent the value function, and the MDP itself,
in matrix notation.
But what happens if the state space is extremely large, or even infinite (e.g. real-valued)?
Then computing a weighted sum over all possible next states, which is required to compute the Bellman operator,
becomes intractable.</p>
<p>Instead, we will need to use <em>function approximation</em> methods from supervised learning to solve for the value function in an alternative way.</p>
<p>In particular, suppose we have a dataset of <span class="math notranslate nohighlight">\(N\)</span> trajectories <span class="math notranslate nohighlight">\(\tau_1, \dots, \tau_N \sim \rho_{\pi}\)</span> from some policy <span class="math notranslate nohighlight">\(\pi\)</span> (called the <strong>data collection policy</strong>) acting in the MDP of interest.
Let us indicate the trajectory index in the superscript, so that</p>
<div class="math notranslate nohighlight">
\[
\tau_i = \{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \dots, s_{\hor-1}^i, a_{\hor-1}^i, r_{\hor-1}^i \}.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">collect_data</span><span class="p">(</span>
    <span class="n">env</span><span class="p">:</span> <span class="n">gym</span><span class="o">.</span><span class="n">Env</span><span class="p">,</span> <span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">H</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">key</span><span class="p">:</span> <span class="n">rand</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">,</span> <span class="n">π</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Collect a dataset of trajectories from the given policy (or a random one).&quot;&quot;&quot;</span>
    <span class="n">trajectories</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">seeds</span> <span class="o">=</span> <span class="p">[</span><span class="n">rand</span><span class="o">.</span><span class="n">bits</span><span class="p">(</span><span class="n">k</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">rand</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">N</span><span class="p">)]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)):</span>
        <span class="n">τ</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">s</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seeds</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">H</span><span class="p">):</span>
            <span class="c1"># sample from a random policy</span>
            <span class="n">a</span> <span class="o">=</span> <span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">if</span> <span class="n">π</span> <span class="k">else</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
            <span class="n">s_next</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">terminated</span><span class="p">,</span> <span class="n">truncated</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>
            <span class="n">τ</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Transition</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">terminated</span> <span class="ow">or</span> <span class="n">truncated</span><span class="p">:</span>
                <span class="k">break</span>
            <span class="n">s</span> <span class="o">=</span> <span class="n">s_next</span>
        <span class="n">trajectories</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">trajectories</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s2">&quot;LunarLander-v2&quot;</span><span class="p">)</span>
<span class="n">trajectories</span> <span class="o">=</span> <span class="n">collect_data</span><span class="p">(</span><span class="n">env</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">trajectories</span><span class="p">[</span><span class="mi">0</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>  <span class="c1"># show first five transitions from first trajectory</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                                                                                | 0/100 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 11%|████████████████████                                                                                                                                                                  | 11/100 [00:00&lt;00:00, 105.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 22%|████████████████████████████████████████▎                                                                                                                                              | 22/100 [00:00&lt;00:01, 77.61it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 32%|██████████████████████████████████████████████████████████▌                                                                                                                            | 32/100 [00:00&lt;00:00, 83.42it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 44%|████████████████████████████████████████████████████████████████████████████████▌                                                                                                      | 44/100 [00:00&lt;00:00, 93.45it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                              | 57/100 [00:00&lt;00:00, 102.16it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 71%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                    | 71/100 [00:00&lt;00:00, 112.76it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                             | 84/100 [00:00&lt;00:00, 115.60it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 98%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎   | 98/100 [00:00&lt;00:00, 122.12it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00&lt;00:00, 108.07it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[Transition(s=array([-0.00767412,  1.4020356 , -0.77731264, -0.39489663,  0.00889908,
         0.17607279,  0.        ,  0.        ], dtype=float32), a=2, r=0.4116697539428856),
 Transition(s=array([-0.015343  ,  1.3934507 , -0.7757339 , -0.3816226 ,  0.0176519 ,
         0.17507371,  0.        ,  0.        ], dtype=float32), a=2, r=0.521110008516058),
 Transition(s=array([-0.0229064 ,  1.3848708 , -0.7657348 , -0.38147786,  0.02694122,
         0.18580344,  0.        ,  0.        ], dtype=float32), a=0, r=-1.2505839552063946),
 Transition(s=array([-0.03047018,  1.3756915 , -0.7657622 , -0.40815678,  0.03622892,
         0.18577108,  0.        ,  0.        ], dtype=float32), a=3, r=-0.14553357267450792),
 Transition(s=array([-0.03795023,  1.3659201 , -0.7552518 , -0.4344905 ,  0.0433989 ,
         0.14341286,  0.        ,  0.        ], dtype=float32), a=1, r=-2.0988040694638515)]
</pre></div>
</div>
</div>
</div>
<p>Can we view the dataset of trajectories as a “labelled dataset” in order to apply supervised learning to approximate the optimal Q-function? Yes!
Recall that we can characterize the optimal Q-function using the <a class="reference internal" href="mdps.html#bellman_consistency_optimal">Bellman optimality equations</a>,
which don’t depend on an actual policy:</p>
<div class="math notranslate nohighlight">
\[
Q_\hi^\star(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [\max_{a'} Q_{\hi+1}^\star(s', a')]
\]</div>
<p>We can think of the arguments to the Q-function – i.e. the current state, action, and timestep <span class="math notranslate nohighlight">\(\hi\)</span> –
as the inputs <span class="math notranslate nohighlight">\(x\)</span>, and the r.h.s. of the above equation as the label <span class="math notranslate nohighlight">\(f(x)\)</span>. Note that the r.h.s. can also be expressed as a <strong>conditional expectation</strong>:</p>
<div class="math notranslate nohighlight">
\[
f(x) = \E [y \mid x] \quad \text{where} \quad y = r(s_\hi, a_\hi) + \max_{a'} Q^\star_{\hi + 1}(s', a').
\]</div>
<p>Approximating the conditional expectation is precisely the task that <a class="reference internal" href="#erm"><span class="std std-ref">empirical risk minimization</span></a> is suited for!</p>
<p>Our above dataset would give us <span class="math notranslate nohighlight">\(N \cdot \hor\)</span> samples in the dataset:</p>
<div class="math notranslate nohighlight">
\[
x_{i \hi} = (s_\hi^i, a_\hi^i, \hi) \qquad y_{i \hi} = r(s_\hi^i, a_\hi^i) + \max_{a'} Q^\star_{\hi + 1}(s_{\hi + 1}^i, a')
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    We pass the state and timestep as input to the Q-function</span>
<span class="sd">    and return an array of Q-values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rows</span> <span class="o">=</span> <span class="p">[(</span><span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">h</span><span class="p">)</span> <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span> <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">τ</span><span class="p">))]</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">ary</span><span class="p">)</span> <span class="k">for</span> <span class="n">ary</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">rows</span><span class="p">)]</span>


<span class="k">def</span> <span class="nf">get_y</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">f</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">π</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Transform the dataset of trajectories into a dataset for supervised learning.</span>
<span class="sd">    If `π` is None, instead estimates the optimal Q function.</span>
<span class="sd">    Otherwise, estimates the Q function of π.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span> <span class="o">=</span> <span class="n">f</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">τ</span> <span class="ow">in</span> <span class="n">trajectories</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">τ</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">r</span> <span class="o">=</span> <span class="n">τ</span><span class="p">[</span><span class="n">h</span><span class="p">]</span>
            <span class="n">Q_values</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
            <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">r</span> <span class="o">+</span> <span class="p">(</span><span class="n">Q_values</span><span class="p">[</span><span class="n">π</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span> <span class="k">if</span> <span class="n">π</span> <span class="k">else</span> <span class="n">Q_values</span><span class="o">.</span><span class="n">max</span><span class="p">()))</span>
        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">τ</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">r</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;states:&quot;</span><span class="p">,</span> <span class="n">s</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;actions:&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;timesteps:&quot;</span><span class="p">,</span> <span class="n">h</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>states: [[-0.00767412  1.4020356  -0.77731264 -0.39489663  0.00889908  0.17607279
   0.          0.        ]
 [-0.015343    1.3934507  -0.7757339  -0.3816226   0.0176519   0.17507371
   0.          0.        ]
 [-0.0229064   1.3848708  -0.7657348  -0.38147786  0.02694122  0.18580344
   0.          0.        ]
 [-0.03047018  1.3756915  -0.7657622  -0.40815678  0.03622892  0.18577108
   0.          0.        ]
 [-0.03795023  1.3659201  -0.7552518  -0.4344905   0.0433989   0.14341286
   0.          0.        ]]
actions: [2 2 0 3 1]
timesteps: [0 1 2 3 4]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">[:</span><span class="mi">1</span><span class="p">])[:</span><span class="mi">5</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([ 0.41166976,  0.52111   , -1.250584  , -0.14553358, -2.098804  ],      dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Then we can use empirical risk minimization to find a function <span class="math notranslate nohighlight">\(\hat f\)</span> that approximates the optimal Q-function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will see some examples of fitting methods in the next section</span>
<span class="n">FittingMethod</span> <span class="o">=</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;N D&quot;</span><span class="p">],</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; N&quot;</span><span class="p">]],</span> <span class="n">QFunction</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>But notice that the definition of <span class="math notranslate nohighlight">\(y_{i \hi}\)</span> depends on the Q-function itself!
How can we resolve this circular dependency?
Recall that we faced the same issue <a class="reference internal" href="mdps.html#iterative-pe"><span class="std std-ref">when evaluating a policy in an infinite-horizon MDP</span></a>. There, we iterated the <a class="reference internal" href="mdps.html#bellman_operator">Bellman operator</a> since we knew that the policy’s value function was a fixed point of the policy’s Bellman operator.
We can apply the same strategy here, using the <span class="math notranslate nohighlight">\(\hat f\)</span> from the previous iteration to compute the labels <span class="math notranslate nohighlight">\(y_{i \hi}\)</span>,
and then using this new dataset to fit the next iterate.</p>
<div class="proof algorithm admonition" id="fitted_q_iteration">
<p class="admonition-title"><span class="caption-number">Algorithm 5.1 </span> (Fitted Q-function iteration)</p>
<section class="algorithm-content" id="proof-content">
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the Bellman optimality operator <a class="reference internal" href="mdps.html#equation-bellman-optimality-operator">(1.7)</a></p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_q_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted Q-function iteration using the given dataset.</span>
<span class="sd">    Returns an estimate of the optimal Q-function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<p>We can also use this fixed-point interation to <em>evaluate</em> a policy using the dataset (not necessarily the one used to generate the trajectories):</p>
<div class="proof algorithm admonition" id="fitted_evaluation">
<p class="admonition-title"><span class="caption-number">Algorithm 5.2 </span> (Fitted policy evaluation)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Input:</strong> Policy <span class="math notranslate nohighlight">\(\pi : \mathcal{S} \times [H] \to \Delta(\mathcal{A})\)</span> to be evaluated.</p>
<p><strong>Output:</strong> An approximation of the value function <span class="math notranslate nohighlight">\(Q^\pi\)</span> of the policy.</p>
<ol class="arabic">
<li><p>Initialize some function <span class="math notranslate nohighlight">\(\hat f(s, a, h) \in \mathbb{R}\)</span>.</p></li>
<li><p>Iterate the following:</p>
<ol class="arabic">
<li><p>Generate a supervised learning dataset <span class="math notranslate nohighlight">\(X, y\)</span> from the trajectories and the current estimate <span class="math notranslate nohighlight">\(f\)</span>, where the labels come from the r.h.s. of the <a class="reference internal" href="mdps.html#bellman_consistency">Bellman consistency equation</a> for the given policy.</p></li>
<li><p>Set <span class="math notranslate nohighlight">\(\hat f\)</span> to the function that minimizes the empirical risk:</p>
<div class="math notranslate nohighlight">
\[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]</div>
</li>
</ol>
</li>
</ol>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_evaluation</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">π</span><span class="p">:</span> <span class="n">Policy</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">Q_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">QFunction</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">QFunction</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Run fitted policy evaluation using the given dataset.</span>
<span class="sd">    Returns an estimate of the Q-function of the given policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">Q_init</span> <span class="ow">or</span> <span class="n">Q_zero</span><span class="p">(</span><span class="n">get_num_actions</span><span class="p">(</span><span class="n">trajectories</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">get_X</span><span class="p">(</span><span class="n">trajectories</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">)):</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">get_y</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">Q_hat</span><span class="p">,</span> <span class="n">π</span><span class="p">)</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q_hat</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Spot the difference between <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> and <code class="docutils literal notranslate"><span class="pre">fitted_q_iteration</span></code>. (See the definition of <code class="docutils literal notranslate"><span class="pre">get_y</span></code>.)
How would you modify this algorithm to evaluate the data collection policy?</p>
</div>
<p>We can use this policy evaluation algorithm to adapt the <a class="reference internal" href="mdps.html#policy-iteration"><span class="std std-ref">policy iteration algorithm</span></a> to this new setting. The algorithm remains exactly the same – repeatedly make the policy greedy w.r.t. its own value function – except now we must evaluate the policy (i.e. compute its value function) using the iterative <code class="docutils literal notranslate"><span class="pre">fitted_evaluation</span></code> algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fitted_policy_iteration</span><span class="p">(</span>
    <span class="n">trajectories</span><span class="p">:</span> <span class="nb">list</span><span class="p">[</span><span class="n">Trajectory</span><span class="p">],</span>
    <span class="n">fit</span><span class="p">:</span> <span class="n">FittingMethod</span><span class="p">,</span>
    <span class="n">epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">evaluation_epochs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">π_init</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Policy</span><span class="p">]</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">,</span> <span class="n">h</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>  <span class="c1"># constant zero policy</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Run fitted policy iteration using the given dataset.&quot;&quot;&quot;</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">π_init</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
        <span class="n">Q_hat</span> <span class="o">=</span> <span class="n">fitted_evaluation</span><span class="p">(</span><span class="n">trajectories</span><span class="p">,</span> <span class="n">fit</span><span class="p">,</span> <span class="n">π</span><span class="p">,</span> <span class="n">evaluation_epochs</span><span class="p">)</span>
        <span class="n">π</span> <span class="o">=</span> <span class="n">q_to_greedy</span><span class="p">(</span><span class="n">Q_hat</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">π</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id4" role="doc-backlink"><span class="section-number">5.4. </span>Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="supervised_learning.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Supervised learning</p>
      </div>
    </a>
    <a class="right-next"
       href="pg.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Policy Optimization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">5.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#empirical-risk-minimization">5.2. Empirical risk minimization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fitted-value-iteration">5.3. Fitted value iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">5.4. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>