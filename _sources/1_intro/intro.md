# Introduction

Welcome to the study of reinforcement learning! This set of lecture
notes accompanies the undergraduate course CS/STAT 184 and is intended
to be a friendly yet rigorous introduction to this active
subfield of machine learning. Here are some questions you might have
before embarking on this journey:

**What is reinforcement learning (RL)?** Broadly speaking, RL is a subfield of machine learning that studies how
an agent can learn to make sequential decisions in an environment.

**Why study RL?** RL provides a powerful framework for attacking a wide variety of
problems, including robotic control, video games and board games,
resource management, language modelling, and more. It also provides an
interdisciplinary paradigm for studying animal and human behavior. Many
of the most stunning results in machine learning, ranging from AlphaGo
to ChatGPT, are built on top of RL.

**Is this book for me?** This book assumes familiarity with multivariable calculus, linear
algebra, and probability. For Harvard undergraduates, this would be
fulfilled by Math 21a, Math 21b, and Stat 110. Stat 111 is strongly
recommended but not required. Here is a non-comprehensive list of topics
you should be familiar with before starting this book:

-   **Linear Algebra:** Vectors and matrices, matrix multiplication, matrix
    inversion, eigenvalues and eigenvectors, and the Gram-Schmidt
    process.

-   **Multivariable Calculus:** Partial derivatives, gradients,
    directional derivatives, the chain rule, Taylor series.

-   **Probability:** Random variables, probability distributions,
    expectation, the law of iterated expectations (Adam's rule), variance, covariance,
    conditional probability, Bayes's rule, and the law of total probability.

How does reinforcement learning differ from other machine learning paradigms?
Here is a list of comparisons:

-   **Supervised learning.** Supervised learning concerns itself with
    learning a mapping from inputs to outputs (e.g. image
    classification). Typically the data takes the form of input-output
    pairs that are assumed to be sampled independently from some
    generating distribution. In RL, however, the data is generated by
    the agent interacting with the environment, meaning the observations
    depend on the agent's behaviour and are not independent from each
    other. This requires a more general set of tools.

    Conversely, supervised learning is a well-studied field that
    provides many useful tools for RL. For example, it may be useful to
    use supervised learning to predict how valuable a given state is, or
    to predict the probability of transitioning to a given state.

-  **Unsupervised learning.** Unsupervised learning deals with learning the
    structure of data without the use of labels.

## Overview

[Chapter 2](bandits) discusses **multi-armed bandits**, a simple model for
reinforcement learning. In this setting, there are multiple arms, each with their
own reward distribution. The agent must decide which arm to pull at each time step.

[Chapter 3](mdps) introduces (finite) **Markov Decision Processes**, the dominant
mathematical framework for studying RL. This general framework models an agent
interacting with a environment that in responds to its actions. Certain states
and actions will be rewarded, and the agent's goal is to maximize its total reward.

[Chapter 4](fitted_dp) introduces **fitted dynamic programming** algorithms
for solving MDPs when the state space is too large to be enumerated. These algorithms
borrow ideas from supervised learning to approximate the value function (discussed in {ref}`mdps`).

[Chapter 5](lqr) is a standalone chapter on the **linear quadratic regulator**,
an important tool for *continuous control*, in which the state space is no longer
finite (i.e. $|\S| < \infty$) but rather continuous (i.e. $|\mathcal{S}| = \mathbb{R}^{n_\st}$).


| Chapter | States | Actions | Rewards (or costs) |
|:-------:|:------:|:-------:|:-------:|
| {ref}`bandits` | N/A | Finite | Stochastic |
| {ref}`mdps` | Finite | Finite | Deterministic |
| {ref}`fitted_dp` | Large or continuous | Finite | Deterministic |
| {ref}`lqr` | Continuous | Continuous | Deterministic |


## Notation

We will use the following notation throughout the book. This notation is
inspired by Sutton and Barto and AJKS .

| Notation      | Definition                |
|:-------------:|:--------------------------|
|      $s$      | A state.                  |
|      $a$      | An action.                |
|      $r$      | A reward.                 |
|      $p$      | A probability.            |
|     $\pi$     | A policy.                 |
|      $V$      | A value function.         |
|      $Q$      | An action-value function. |
|      $A$      | An advantage function.    |
|   $\gamma$    | A discount factor.        |
|    $\tau$     | A trajectory.             |
| $\mathcal{S}$ | A state space.            |
| $\mathcal{A}$ | An action space.          |

## Challenges of reinforcement learning

**Exploration-exploitation tradeoff.** Should the agent try a new action or stick with the action that it knows
is good?

**Prediction.** The agent might want to predict the value of a state or state-action
pair.

**Policy computation (control).** In a complex environment, even if the dynamics are known, it can still
be challenging to compute the best policy.


## Programming

This is an interactive book built with [Jupyter Book](https://jupyterbook.org/en/stable/intro.html). It uses [Python 3.11](https://docs.python.org/3.11/contents.html).

It uses the [JAX](https://jax.readthedocs.io/en/latest/index.html) library for
numerical computing.
JAX was chosen for the clarity of its functional style
and due to its extensive RL ecosystem.

We use the standard [Gymnasium](https://gymnasium.farama.org/) library for
interfacing with RL environments.
