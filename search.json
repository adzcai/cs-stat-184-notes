[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "",
    "text": "Introduction\nWelcome to the study of reinforcement learning! This textbook accompanies the undergraduate course CS 1840/STAT 184 taught at Harvard. It is intended to be a friendly yet rigorous introduction to this active subfield of machine learning.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis book assumes the same prerequisites as the course: You should be familiar with multivariable calculus, linear algebra, and probability. For Harvard undergraduates, this is fulfilled by Math 21a, Math 21b, and Stat 110, or their equivalents. Stat 111 is strongly recommended but not required. Specifically, we will assume that you know the following topics. The italicized terms have brief re-introductions in the text or in the 10  Appendix: Background:\n\nLinear Algebra: Vectors and matrices, matrix multiplication, matrix inversion, eigenvalues and eigenvectors.\nMultivariable Calculus: Partial derivatives, the chain rule, Taylor series, gradients, directional derivatives, Lagrange multipliers.\nProbability: Random variables, probability distributions, expectation and variance, the law of iterated expectations (Adam’s rule), covariance, conditional probability, Bayes’s rule, and the law of total probability.\n\nYou should also be comfortable with programming in Python. See Programming for more about this textbook’s philosophy regarding programming.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#reinforcement-learning-in-a-nutshell",
    "href": "index.html#reinforcement-learning-in-a-nutshell",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Reinforcement learning in a nutshell",
    "text": "Reinforcement learning in a nutshell\nBroadly speaking, RL studies sequential decision-making in dynamic environments. An RL algorithm finds a strategy, called a policy, that maximizes the reward it obtains from the environment.\nRL provides a powerful framework for attacking a wide variety of problems, including robotic control, video games and board games, resource management, language modeling, and more. It also provides an interdisciplinary paradigm for studying animal and human behavior. Many of the most stunning results in machine learning, ranging from AlphaGo to ChatGPT, are built using RL algorithms.\nHow does RL compare to the other two core machine learning paradigms, supervised learning and unsupervised learning?\n\nSupervised learning (SL) concerns itself with learning a mapping from inputs to outputs. Typically the data takes the form of statistically independent input-output pairs. In RL, however, the data is generated by the agent interacting with the environment, meaning the sequential observations of the state are not independent from each other.\nConversely, SL is a well-studied field that provides many useful tools for RL.\nUnsupervised learning concerns itself with learning the structure of data without the use of outside feedback or labels. In RL, though, the agent receives a reward signal from the environment, which can be thought of as a sort of feedback.\nUnsupervised learning is crucial in many real-world applications of RL for dimensionality reduction and other purposes.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#core-tasks-of-reinforcement-learning",
    "href": "index.html#core-tasks-of-reinforcement-learning",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Core tasks of reinforcement learning",
    "text": "Core tasks of reinforcement learning\nWhat tasks, exactly, does RL comprise? An RL algorithm must typically solve two main subtasks:\n\nPolicy evaluation (prediction): How ‘good’ is a specific state, or state-action pair (under a given policy)? That is, how much reward does it lead to in the long run?\nPolicy optimization (control): Suppose we fully understand how the environment behaves. What is the best action to take in every scenario?",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#course-overview",
    "href": "index.html#course-overview",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Course overview",
    "text": "Course overview\nThe course will progress through the following units:\n1  Markov Decision Processes introduces Markov Decision Processes, the core mathematical framework for describing a large class of interactive environments.\n2  Linear Quadratic Regulators is a standalone chapter on the linear quadratic regulator (LQR), an important tool for continuous control, in which the state and action spaces are no longer finite but rather continuous. This has widespread applications in robotics.\n3  Multi-Armed Bandits introduces the multi-armed bandit (MAB) model for -stateless- sequential decision-making tasks. In exploring a number of algorithms, we will see how each of them strikes a different balance between exploring new options and exploiting known options. This exploration-exploitation tradeoff is a core consideration in RL algorithm design.\n4  Supervised learning is a standalone crash course on some tools from supervised learning that we will use in later chapters.\n5  Fitted Dynamic Programming Algorithms introduces fitted dynamic programming (fitted DP) algorithms for solving MDPs. These algorithms use supervised learning to approximately evaluate policies when they cannot be evaluated exactly.\n6  Policy Gradient Methods explores an important class of algorithms based on iteratively improving a policy. We will also encounter the use of deep neural networks to express more complicated policies and approximate complicated functions.\n7  Imitation Learning attempts to learn a good policy from expert demonstrations. At its most basic, this is an application of supervised learning to RL tasks.\n8  Tree Search Methods looks at ways to -explicitly- plan ahead when the environment’s dynamics are known. We will study the Monte Carlo Tree Search heuristic, which has been used to great success in the famous AlphaGo algorithm and its successors.\n9  Exploration in MDPs continues to investigate the exploration-exploitation tradeoff. We will extend ideas from multi-armed bandits to the MDP setting.\n10  Appendix: Background contains an overview of selected background mathematical content and programming content.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#notation",
    "href": "index.html#notation",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Notation",
    "text": "Notation\nWe will use the following notation throughout the book. This notation is inspired by Sutton and Barto (2018) and (Agarwal et al. 2022). We use \\([N]\\) as shorthand for the set \\(\\{ 0, 1, \\dots, N-1 \\}\\).\n\n\n\n\n\n\n\n\nElement\nSpace\nDefinition (of element)\n\n\n\n\n\\(s\\)\n\\(\\mathcal{S}\\)\nA state.\n\n\n\\(a\\)\n\\(\\mathcal{A}\\)\nAn action.\n\n\n\\(r\\)\n\nA reward.\n\n\n\\(\\gamma\\)\n\nA discount factor.\n\n\n\\(\\tau\\)\n\\(\\mathcal{T}\\)\nA trajectory.\n\n\n\\(\\pi\\)\n\\(\\Pi\\)\nA policy.\n\n\n\\(V^\\pi\\)\n\\(\\mathcal{S} \\to \\mathbb{R}\\)\nThe value function of policy \\(\\pi\\).\n\n\n\\(Q^\\pi\\)\n\\(\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\)\nThe action-value function (a.k.a. Q-function) of policy \\(\\pi\\).\n\n\n\\(A^\\pi\\)\n\\(\\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\)\nThe advantage function of policy \\(\\pi\\).\n\n\n\n\\(\\triangle(\\mathcal{X})\\)\nA distribution supported on \\(\\mathcal{X}\\).\n\n\n\\(h\\)\n\\([H]\\)\nTime horizon index of an MDP (subscript).\n\n\n\\(k\\)\n\\([K]\\)\nArm index of a multi-armed bandit (superscript).\n\n\n\\(t\\)\n\\([T]\\)\nIteration index of an algorithm (subscript).\n\n\n\\(\\theta\\)\n\\(\\Theta\\)\nA set of parameters.\n\n\n\nNote that throughout the text, certain symbols will stand for either random variables or fixed values. We aim to clarify in ambiguous settings. Be warned that notation in RL can appear quite complicated, since we often need to index across algorithm iterations, trajectories, and timesteps, so that certain values can have two or three indices attached to them.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#sec-programming",
    "href": "index.html#sec-programming",
    "title": "CS 1840: Introduction to Reinforcement Learning",
    "section": "Programming",
    "text": "Programming\nWhy include code in a textbook? We believe that implementing an algorithm is a strong test of your understanding of it; mathematical notation can often abstract away details, while a computer must be given every single instruction. We have sought to write readable Python code that is self-contained within each file. This approach is inspired by Sussman, Wisdom, and Farr (2013). There are some ways in which the code style differs from typical software projects:\n\nWe keep use of language features to a minimum, even if it leads to code that could otherwise be more concisely or idiomatically expressed.\nThe variable names used in the code match those used in the main text. For example, the variable s will be used instead of the more explicit state.\n\nWe also make extensive use of Python type annotations to explicitly specify variable types, including shapes of vectors and matrices using the jaxtyping library.\nThis is an interactive book built with Quarto. It uses Python 3.11. It uses the JAX library for numerical computing. JAX was chosen for the clarity of its functional style and due to its mature RL ecosystem, sustained in large part by the Google DeepMind research group and a large body of open-source contributors. We use the standard Gymnasium library for interfacing with RL environments.\n\n\n\n\nAgarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. Reinforcement Learning: Theory and Algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf.\n\n\nSussman, Gerald Jay, Jack Wisdom, and Will Farr. 2013. Functional Differential Geometry. Cambridge, MA: The MIT Press.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement Learning: An Introduction. Second edition. Adaptive Computation and Machine Learning Series. Cambridge, Massachusetts: The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "mdps.html",
    "href": "mdps.html",
    "title": "1  Markov Decision Processes",
    "section": "",
    "text": "1.1 Introduction\nThe field of RL studies how an agent can learn to make sequential decisions in an interactive environment. This is a very general problem! How can we formalize this task in a way that is both sufficiently general yet also tractable enough for fruitful analysis?\nLet’s consider some examples of sequential decision problems to identify the key common properties we’d like to capture:\nIn these environments and many others, the state transitions, the “rules” of the environment, only depend on the most recent state and action (generally speaking). For example, if you want to take a break while playing a game of chess, you could take a picture of the board, and later on reset the board to that state and continue playing; the past history of moves doesn’t matter (generally speaking). This is called the Markov property.\nEnvironments that satisfy the Markov property are called Markov decision processes (MDPs). This chapter will focus on introducing core vocabulary for MDPs that will be useful throughout the book.\nMDPs are usually classified as finite-horizon, where the interactions end after some finite number of time steps, or infinite-horizon, where the interactions can continue indefinitely. We’ll begin with the finite-horizon case and discuss the infinite-horizon case in the second half of the chapter.\nWe’ll describe how to evaluate different strategies, called policies, and how to compute (or approximate) the optimal policy for a given MDP. We’ll introduce the Bellman consistency condition, which allows us to analyze the whole sequence of interactions in terms of individual timesteps.\nCode\nfrom utils import NamedTuple, Float, Array, partial, jax, jnp, latexify, latex",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#introduction",
    "href": "mdps.html#introduction",
    "title": "1  Markov Decision Processes",
    "section": "",
    "text": "Board games and video games, where a player takes actions in a virtual environment.\nInventory management, where a company must efficiently move resources from producers to consumers.\nRobotic control, where a robot can move and interact with the real world to complete some task.\n\n\n\nDefinition 1.1 (Markov property) An interactive environment satisfies the Markov property if the probability of transitioning to a new state only depends on the current state and action:\n\\[\n\\pr(s_{\\hi+1} \\mid s_0, a_0, \\dots, s_\\hi, a_\\hi) = P(s_{\\hi+1} \\mid s_\\hi, a_\\hi)\n\\]\nwhere \\(P : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S})\\) describes the state transitions. (We’ll elaborate on this notation later in the chapter.)\n\n\n\nWhat information might be encoded in the state for each of the above examples? What might the valid set of actions be? Describe the state transitions heuristically and verify that they satisfy the Markov property.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#finite-horizon-mdps",
    "href": "mdps.html#finite-horizon-mdps",
    "title": "1  Markov Decision Processes",
    "section": "1.2 Finite-horizon MDPs",
    "text": "1.2 Finite-horizon MDPs\n\n1.2.1 Definition\n\nDefinition 1.2 (Finite-horizon Markov decision process) The components of a finite-horizon Markov decision process are:\n\nThe state that the agent interacts with. We use \\(\\mathcal{S}\\) to denote the set of possible states, called the state space.\nThe actions that the agent can take. We use \\(\\mathcal{A}\\) to denote the set of possible actions, called the action space.\nSome initial state distribution \\(\\mu \\in \\triangle(\\mathcal{S})\\).\nThe state transitions (a.k.a. dynamics) \\(P : \\mathcal{S} \\times \\mathcal{A} \\to \\triangle(\\mathcal{S})\\) that describe what state the agent transitions to after taking an action.\nThe reward signal. In this course we’ll take it to be a deterministic function on state-action pairs, \\(r : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\), but in general many results will extend to a stochastic reward signal.\nA time horizon \\(\\hor \\in \\mathbb{N}\\) that specifies the number of interactions in an episode.\n\nCombined together, these objects specify a finite-horizon Markov decision process:\n\\[\nM = (\\mathcal{S}, \\mathcal{A}, \\mu, P, r, \\hor).\n\\]\nWhen there are finitely many states and actions, i.e. \\(|\\mathcal{S}|, |\\mathcal{A}| &lt; \\infty\\), we can express the relevant quantities as vectors and matrices (i.e. tables of values):\n\\[\n\\begin{aligned}\n    \\mu &\\in [0, 1]^{|\\mathcal{S}|} &\n    P &\\in [0, 1]^{(|\\mathcal{S} \\times \\mathcal{A}|) \\times |\\mathcal{S}|} &\n    r &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}\n\\end{aligned}\n\\]\n\n\nVerify that the types and shapes provided above make sense!\n\n\n\nCode\nclass MDP(NamedTuple):\n    \"\"\"A description of a Markov decision process with finitely many states and actions.\"\"\"\n    S: int  # number of states\n    A: int  # number of actions\n    mu: Float[Array, \" S\"]\n    P: Float[Array, \"S A S\"]  # \"current\" state, \"current\" action, \"next\" state\n    r: Float[Array, \"S A\"]\n    H: int\n    γ: float = 1.0  # discount factor (used later)\n\n\n\nExample 1.1 (Tidying MDP) Let’s consider a simple decision problem throughout this chapter: the task of keeping your room tidy!\nYour room has the possible states \\(\\mathcal{S} = \\{ \\text{orderly}, \\text{messy} \\}.\\) You can take either of the actions \\(\\mathcal{A} = \\{ \\text{ignore}, \\text{tidy} \\}.\\) The room starts off orderly.\nThe state transitions are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it might become messy (see table below).\nThe rewards are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy your additional time). Tidying a messy room is a chore that gives no reward.\nThese are summarized in the following table:\n\\[\n\\begin{array}{ccccc}\n    s & a & P(\\text{orderly} \\mid s, a) & P(\\text{messy} \\mid s, a) & r(s, a) \\\\\n    \\text{orderly} & \\text{ignore} & 0.7 & 0.3 & 1 \\\\\n    \\text{orderly} & \\text{tidy} & 1 & 0 & -1 \\\\\n    \\text{messy} & \\text{ignore} & 0 & 1 & -1 \\\\\n    \\text{messy} & \\text{tidy} & 1 & 0 & 0 \\\\\n\\end{array}\n\\]\nConsider a time horizon of \\(\\hor = 7\\) days (one interaction per day). Let \\(t = 0\\) correspond to Monday and \\(t = 6\\) correspond to Sunday.\n\n\n\nCode\ntidy_mdp = MDP(\n    S=2,  # 0 = orderly, 1 = messy\n    A=2,  # 0 = ignore, 1 = tidy\n    mu=jnp.array([1.0, 0.0]),  # start in orderly state\n    P=jnp.array([\n        [\n            [0.7, 0.3],  # orderly, ignore\n            [1.0, 0.0],  # orderly, tidy\n        ],\n        [\n            [0.0, 1.0],  # messy, ignore\n            [1.0, 0.0],  # messy, tidy\n        ],\n    ]),\n    r=jnp.array([\n        [\n            1.0,   # orderly, ignore\n            -1.0,  # orderly, tidy\n        ],\n        [\n            -1.0,  # messy, ignore\n            0.0,   # messy, tidy\n        ]\n    ]),\n    H=7,\n)\n\n\n\n\n1.2.2 Policies\n\nDefinition 1.3 (Policies) A policy \\(\\pi\\) describes the agent’s strategy: which actions it takes in a given situation. A key goal of RL is to find the optimal policy that maximizes the total reward on average.\nThere are three axes along which policies can vary: their outputs, inputs, and time-dependence.\n\nDeterministic or stochastic. A deterministic policy outputs actions while a stochastic policy outputs distributions over actions.\n\n\n\n\n\n\n\n\n\nA deterministic policy, which produces a single action\n\n\n\n\n\n\n\n\n\nA stochastic policy, which produces a distribution over possible actions\n\n\n\n\n\n\nFigure 1.1\n\n\n\n\nState-dependent or history-dependent. A state-dependent (a.k.a. “Markovian”) policy only depends on the current state, while a history-dependent policy depends on the sequence of past states, actions, and rewards. We’ll only consider state-dependent policies in this course.\nStationary or time-dependent. A stationary (a.k.a. time-homogeneous) policy remains the same function at all time steps, while a time-dependent policy can depend on the current timestep. For consistency with states and actions, we will denote the timestep as a subscript, i.e. \\(\\pi = \\{ \\pi_0, \\dots, \\pi_{\\hor-1} \\}.\\)\n\n\nNote that for finite state and action spaces, we can represent a randomized mapping \\(\\mathcal{S} \\to \\Delta(\\mathcal{A})\\) as a matrix \\(\\pi \\in [0, 1]^{\\mathcal{S} \\times \\mathcal{A}}\\) where each row describes the policy’s distribution over actions for the corresponding state.\nA fascinating result is that every finite-horizon MDP has an optimal deterministic time-dependent policy! Intuitively, the Markov property implies that the current state contains all the information we need to make the optimal decision. We’ll prove this result constructively later in the chapter.\n\nExample 1.2 (Policies for the tidying MDP) Here are some possible policies for the tidying MDP Example 1.1:\n\nAlways tidy: \\(\\pi(s) = \\text{tidy}\\).\nOnly tidy on weekends: \\(\\pi_\\hi(s) = \\text{tidy}\\) if \\(\\hi \\in \\{ 5, 6 \\}\\) and \\(\\pi_\\hi(s) = \\text{ignore}\\) otherwise.\nOnly tidy if the room is messy: \\(\\pi_\\hi(\\text{messy}) = \\text{tidy}\\) and \\(\\pi_\\hi(\\text{orderly}) = \\text{ignore}\\) for all \\(\\hi\\).\n\n\n\n\nCode\n# arrays of shape (H, S, A) represent time-dependent policies\ntidy_policy_always_tidy = (\n    jnp.zeros((7, 2, 2))\n    .at[:, :, 1].set(1.0)\n)\ntidy_policy_weekends = (\n    jnp.zeros((7, 2, 2))\n    .at[5:7, :, 1].set(1.0)\n    .at[0:5, :, 0].set(1.0)\n)\ntidy_policy_messy_only = (\n    jnp.zeros((7, 2, 2))\n    .at[:, 1, 1].set(1.0)\n    .at[:, 0, 0].set(1.0)\n)\n\n\n\nRemark 1.1. Array objects in Jax are immutable, that is, they cannot be changed. This might seem inconvenient, but in larger projects, immutability makes code easier to reason about.\n\n\n\n1.2.3 Trajectories\n\nDefinition 1.4 (Trajectories) A sequence of states, actions, and rewards is called a trajectory:\n\\[\n\\tau = (s_0, a_0, r_0, \\dots, s_{H-1}, a_{H-1}, r_{H-1})\n\\]\nwhere \\(r_\\hi = r(s_\\hi, a_\\hi)\\). (Note that some sources omit the reward at the final time step. This is a minor detail.)\n\n\n\nCode\nclass Transition(NamedTuple):\n    \"\"\"A single state-action-reward interaction with the environment.\n\n    A trajectory comprises a sequence of transitions.\n    \"\"\"\n    s: int\n    a: int\n    r: float\n\n\nOnce we’ve chosen a policy, we can sample trajectories by repeatedly choosing actions according to the policy, transitioning according to the state transitions, and observing the rewards.\n\n\n\n\n\ngraph LR\n    S0($$s_0$$) -- $$\\pi_0$$ --&gt; A0{{$$a_0$$}}\n    S0 & A0 --&gt; R0[$$r_0$$]\n    A0 & S0 -- $$P$$ --&gt; S1($$s_1$$)\n    S1 -- $$\\pi_1$$ --&gt; A1{{$$a_1$$}}\n    S1 & A1 --&gt; R1[$$r_1$$]\n    A1 & S1 -- $$P$$ --&gt; S2($$s_2$$)\n    S2 -- $$\\pi_2$$ --&gt; A2{{$$a_2$$}}\n    S2 & A2 --&gt; R2[$$r_2$$]\n    A2 & S2 -- $$P$$ --&gt; S3($$s_3$$)\n\n\n\n\n\n\nThat is, a policy induces a distribution \\(\\rho^{\\pi}\\) over trajectories. (We assume that \\(\\mu\\) and \\(P\\) are clear from context.)\n\nExample 1.3 (Trajectories in the tidying environment) Here is a possible trajectory for the tidying example:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\hi\\)\n\\(0\\)\n\\(1\\)\n\\(2\\)\n\\(3\\)\n\\(4\\)\n\\(5\\)\n\\(6\\)\n\n\n\n\n\\(s\\)\norderly\norderly\norderly\nmessy\nmessy\norderly\norderly\n\n\n\\(a\\)\ntidy\nignore\nignore\nignore\ntidy\nignore\nignore\n\n\n\\(r\\)\n\\(-1\\)\n\\(1\\)\n\\(1\\)\n\\(-1\\)\n\\(0\\)\n\\(1\\)\n\\(1\\)\n\n\n\n\nCould any of the policies in Example 1.2 have generated this trajectory?\n\nNote that for a state-dependent policy, using the Markov property Definition 1.1, we can write down the likelihood function of this probability distribution in an autoregressive way (i.e. one timestep at a time):\n\nDefinition 1.5 (Autoregressive trajectory distribution) \\[\n\\rho^{\\pi}(\\tau) := \\mu(s_0) \\pi_0(a_0 \\mid s_0) P(s_1 \\mid s_0, a_0) \\cdots P(s_{\\hor-1} \\mid s_{\\hor-2}, a_{\\hor-2}) \\pi_{\\hor-1}(a_{\\hor-1} \\mid s_{\\hor-1})\n\\]\n\n\n\nCode\ndef trajectory_log_likelihood(\n    mdp: MDP,\n    trj: list[Transition],\n    pi: Float[Array, \"S A\"],\n) -&gt; float:\n    \"\"\"Compute the log-likelihood of a trajectory under a given MDP and policy.\"\"\"\n\n    # initial distribution and action\n    total = jnp.log(mdp.mu[trj[0].s])\n    total += jnp.log(pi[trj[0].s, trj[0].a])\n\n    # remaining state transitions and actions\n    for i in range(1, mdp.H):\n        total += jnp.log(mdp.P[trj[i - 1].s, trj[i - 1].a, trj[i].s])\n        total += jnp.log(pi[trj[i].s, trj[i].a])\n\n    return total\n\n\n\nHow would you modify this to include stochastic rewards?\n\nFor a deterministic policy \\(\\pi\\), we have that \\(\\pi_\\hi(a \\mid s) = \\mathbb{I}[a = \\pi_\\hi(s)]\\); that is, the probability of taking an action is \\(1\\) if it’s the unique action prescribed by the policy for that state and \\(0\\) otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution \\(\\mu\\) and the state transitions \\(P\\).\n\n\n1.2.4 Value functions\nThe main goal of RL is to find a policy that maximizes the expected total reward \\(\\E [r_0 + \\cdots + r_{\\hor-1}]\\).\n\nNote that \\(r_0 + \\cdots + r_{\\hor-1}\\) is a random variable. What sources of randomness does it depend on? Describe the generating process.\n\nLet’s introduce some notation for analyzing this quantity.\nA policy’s value function at time \\(\\hi\\) is its expected remaining reward from a given state:\n\nDefinition 1.6 (Value function) \\[\nV_\\hi^\\pi(s) := \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s]\n\\]\n\nSimilarly, we can define the action-value function (aka the Q-function) at time \\(h\\) as the expected remaining reward from a given state and taking a given action:\n\nDefinition 1.7 (Action-value function) \\[\nQ_\\hi^\\pi(s, a) := \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s, a_\\hi = a]\n\\]\n\n\n1.2.4.1 Relating the value function and action-value function\nNote that the value function is just the expected action-value over actions drawn from the policy:\n\\[\nV_\\hi^\\pi(s) = \\E_{a \\sim \\pi_\\hi(s)} [Q_\\hi^\\pi(s, a)]\n\\]\n\n\nCode\ndef q_to_v(\n    policy: Float[Array, \"S A\"],\n    q: Float[Array, \"S A\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"\n    Compute the value function for a given policy in a known finite MDP\n    at a single timestep from its action-value function.\n    \"\"\"\n    return jnp.average(q, weights=policy, axis=1)\n\n\nand the action-value is the sum of the immediate reward and the expected value of the following state:\n\\[\nQ_\\hi^\\pi(s, a) = r(s, a) + \\E_{s' \\sim P(s, a)} [V_{\\hi+1}^\\pi(s')]\n\\]\n\n\nCode\ndef v_to_q(\n    mdp: MDP,\n    v_next: Float[Array, \" S\"],\n) -&gt; Float[Array, \"S A\"]:\n    \"\"\"\n    Compute the action-value function in a known finite MDP\n    at a single timestep from the corresponding value function.\n    \"\"\"\n    # the discount factor is relevant later\n    return mdp.r + mdp.γ * mdp.P @ v_next\n\n\n# convert a list of v functions to a list of q functions\nv_ary_to_q_ary = jax.vmap(v_to_q, in_axes=(None, 0))\n\n\n\n\n1.2.4.2 Greedy policies\nFor any given \\(Q \\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}\\), we can define the greedy policy \\(\\hat \\pi_Q\\) as the deterministic policy that selects the action with the highest \\(Q\\)-value at each state:\n\\[\n\\hat \\pi_Q(s) = \\arg\\max_{a} Q_{sa}\n\\]\n\n\nCode\ndef q_to_greedy(q: Float[Array, \"S A\"]) -&gt; Float[Array, \"S A\"]:\n    \"\"\"\n    Get the (deterministic) greedy policy with respect to an action-value function.\n    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.\n    \"\"\"\n    A = q.shape[1]\n    a_ary = jnp.argmax(q, axis=1)\n    return jnp.eye(A)[a_ary]\n\n\ndef v_to_greedy(mdp: MDP, v: Float[Array, \" S\"]) -&gt; Float[Array, \"S A\"]:\n    \"\"\"Get the (deterministic) greedy policy with respect to a value function.\"\"\"\n    return q_to_greedy(v_to_q(mdp, v))\n\n\n\n\n\n1.2.5 The one-step (Bellman) consistency equation\nNote that by simply considering the cumulative reward as the sum of the current reward and the future cumulative reward, we can describe the value function recursively (in terms of itself). This is named the Bellman consistency equation after Richard Bellman (1920–1984), who is credited with introducing dynamic programming in 1953.\n\nTheorem 1.1 (Bellman consistency equation for the value function) \\[\nV_\\hi^\\pi(s) = \\E_{\\substack{a \\sim \\pi_\\hi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + V_{\\hi+1}^\\pi(s')]\n\\]\n\n\n\nCode\ndef check_bellman_consistency_v(\n    mdp: MDP,\n    policy: Float[Array, \"H S A\"],\n    v_ary: Float[Array, \"H S\"],\n) -&gt; bool:\n    \"\"\"\n    Check that the given (time-dependent) \"value function\"\n    satisfies the Bellman consistency equation.\n    \"\"\"\n    return all(\n        jnp.allclose(\n            # lhs\n            v_ary[h],\n            # rhs\n            jnp.sum(policy[h] * (mdp.r + mdp.γ * mdp.P @ v_ary[h + 1]), axis=1),\n        )\n        for h in range(mdp.H - 1)\n    )\n\n\n\nVerify that this equation holds by expanding \\(V_\\hi^\\pi(s)\\) and \\(V_{\\hi+1}^\\pi(s')\\).\n\nOne can analogously derive the Bellman consistency equation for the action-value function:\n\nTheorem 1.2 (Bellman consistency equation for action-values) \\[\nQ_\\hi^\\pi(s, a) = r(s, a) + \\E_{\\substack{s' \\sim P(s, a) \\\\ a' \\sim \\pi_{\\hi+1}(s')}} [Q_{\\hi+1}^\\pi(s', a')]\n\\]\n\n\nWrite a check_bellman_consistency_q function for the action-value function.\n\n\nRemark 1.2 (The Bellman consistency equation for deterministic policies). Note that for deterministic policies, the Bellman consistency equation simplifies to\n\\[\n\\begin{aligned}\n    V_\\hi^\\pi(s) &= r(s, \\pi_\\hi(s)) + \\E_{s' \\sim P(s, \\pi_\\hi(s))} [V_{\\hi+1}^\\pi(s')] \\\\\n    Q_\\hi^\\pi(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [Q_{\\hi+1}^\\pi(s', \\pi_{\\hi+1}(s'))]\n\\end{aligned}\n\\]\n\n\n\n1.2.6 The one-step Bellman operator\nFix a policy \\(\\pi\\). Consider the higher-order operator that takes in a “value function” \\(v : \\mathcal{S} \\to \\mathbb{R}\\) and returns the r.h.s. of the Bellman equation for that “value function”:\n\nDefinition 1.8 (Bellman operator) \\[\n[\\mathcal{J}^{\\pi}(v)](s) := \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + v(s')].\n\\]\nThis is a crucial tool for reasoning about MDPs. Intuitively, it answers the following question: if we evaluate the next state using \\(v\\), how good is the current state, according to the given policy?\n\n\n\nCode\ndef bellman_operator_looping(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"\n    Looping definition of the Bellman operator.\n    Concise version is below\n    \"\"\"\n    v_new = jnp.zeros(mdp.S)\n    for s in range(mdp.S):\n        for a in range(mdp.A):\n            for s_next in range(mdp.S):\n                v_new[s] += (\n                    policy[s, a]\n                    * mdp.P[s, a, s_next]\n                    * (mdp.r[s, a] + mdp.γ * v[s_next])\n                )\n    return v_new\n\n\nNote that we can concisely implement this using the q_to_v and v_to_q utilities from above:\n\n\nCode\ndef bellman_operator(\n    mdp: MDP,\n    policy: Float[Array, \"S A\"],\n    v: Float[Array, \" S\"],\n) -&gt; Float[Array, \" S\"]:\n    \"\"\"For a known finite MDP, the Bellman operator can be exactly evaluated.\"\"\"\n    return q_to_v(policy, v_to_q(mdp, v))  # equivalent\n    return jnp.sum(policy * (mdp.r + mdp.γ * mdp.P @ v), axis=1)\n\n\nWe’ll call \\(\\mathcal{J}^\\pi : \\mathbb{R}^{\\mathcal{S}} \\to \\mathbb{R}^{\\mathcal{S}}\\) the Bellman operator of \\(\\pi\\). Note that it’s defined on any “value function” mapping states to real numbers; \\(v\\) doesn’t have to be a well-defined value function for some policy (hence the lowercase notation). The Bellman operator also gives us a concise way to express Theorem 1.1 for the value function:\n\\[\nV_\\hi^\\pi = \\mathcal{J}^{\\pi}(V_{\\hi+1}^\\pi)\n\\]\nIntuitively, the output of the Bellman operator, a new “value function”, evaluates states as follows: from a given state, take one action according to \\(\\pi\\), observe the reward, and then evaluate the next state using the input “value function”.\nWhen we discuss infinite-horizon MDPs, the Bellman operator will turn out to be more than just a notational convenience: We’ll use it to construct algorithms for computing the optimal policy.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-finite-horizon-mdps",
    "href": "mdps.html#sec-finite-horizon-mdps",
    "title": "1  Markov Decision Processes",
    "section": "1.3 Solving finite-horizon MDPs",
    "text": "1.3 Solving finite-horizon MDPs\n\n1.3.1 Policy evaluation in finite-horizon MDPs\nHow can we actually compute the value function of a given policy? This is the task of policy evaluation.\n\n1.3.1.1 DP algorithm to evaluate a policy in a finite-horizon MDP\nThe Bellman consistency equation Theorem 1.1 gives us a convenient algorithm for evaluating stationary policies: it expresses the value function at timestep \\(\\hi\\) as a function of the value function at timestep \\(\\hi+1\\). This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman consistency equation to compute the value function at each time step.\n\n\n\nCode\ndef dp_eval_finite(mdp: MDP, policy: Float[Array, \"S A\"]) -&gt; Float[Array, \"H S\"]:\n    \"\"\"Evaluate a policy using dynamic programming.\"\"\"\n    V_ary = [None] * mdp.H + [jnp.zeros(mdp.S)]  # initialize to 0 at end of time horizon\n    for h in range(mdp.H - 1, -1, -1):\n        V_ary[h] = bellman_operator(mdp, policy[h], V_ary[h + 1])\n    return jnp.stack(V_ary[:-1])\n\nlatex(dp_eval_finite)\n\n\n&lt;latexify.ipython_wrappers.LatexifyWrapper at 0x11eb118d0&gt;\n\n\nThis runs in time \\(O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)\\) by counting the loops.\n\nDo you see where we compute \\(Q^\\pi_\\hi\\) along the way? Make this step explicit.\n\n\nExample 1.4 (Tidying policy evaluation) Let’s evaluate the policy from Example 1.2 in the tidying MDP that tidies if and only if the room is messy. We’ll use the Bellman consistency equation to compute the value function at each time step.\n\\[\n\\begin{aligned}\nV_{H-1}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) \\\\\n&= 1 \\\\\nV_{H-1}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) \\\\\n&= 0 \\\\\nV_{H-2}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\E_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-1}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1 + 0.3 \\cdot 0 \\\\\n&= 1.7 \\\\\nV_{H-2}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\E_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-1}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-1}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-1}^{\\pi}(\\text{messy}) \\\\\n&= 1 \\\\\nV_{H-3}^\\pi(\\text{orderly}) &= r(\\text{orderly}, \\text{ignore}) + \\E_{s' \\sim P(\\text{orderly}, \\text{ignore})} [V_{H-2}^\\pi(s')] \\\\\n&= 1 + 0.7 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0.3 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1 + 0.7 \\cdot 1.7 + 0.3 \\cdot 1 \\\\\n&= 2.49 \\\\\nV_{H-3}^\\pi(\\text{messy}) &= r(\\text{messy}, \\text{tidy}) + \\E_{s' \\sim P(\\text{messy}, \\text{tidy})} [V_{H-2}^\\pi(s')] \\\\\n&= 0 + 1 \\cdot V_{H-2}^{\\pi}(\\text{orderly}) + 0 \\cdot V_{H-2}^{\\pi}(\\text{messy}) \\\\\n&= 1.7\n\\end{aligned}\n\\]\netc. You may wish to repeat this computation for the other policies to get a better sense of this algorithm.\n\n\n\nCode\nV_messy = dp_eval_finite(tidy_mdp, tidy_policy_messy_only)\nV_messy\n\n\nArray([[5.5621696, 4.7927704],\n       [4.7927704, 4.0241003],\n       [4.0241003, 3.253    ],\n       [3.253    , 2.49     ],\n       [2.49     , 1.7      ],\n       [1.7      , 1.       ],\n       [1.       , 0.       ]], dtype=float32)\n\n\n\n\n1.3.2 Optimal policies in finite-horizon MDPs\nWe’ve just seen how to evaluate a given policy. But how can we find the optimal policy for a given environment?\n\nDefinition 1.9 (Optimal policies) We call a policy optimal, and denote it by \\(\\pi^\\star\\), if it does at least as well as any other policy \\(\\pi\\) (including stochastic and history-dependent ones) in all situations:\n\\[\n\\begin{aligned}\n    V_\\hi^{\\pi^\\star}(s) &= \\E_{\\tau \\sim \\rho^{\\pi^{\\star}}}[r_\\hi + \\cdots + r_{H-1} \\mid s_\\hi = s] \\\\\n    &\\ge \\E_{\\tau \\sim \\rho^{\\pi}}[r_\\hi + \\cdots + r_{H-1} \\mid \\tau_\\hi] \\quad \\forall \\pi, \\tau_\\hi, \\hi \\in [H]\n\\end{aligned}\n\\]\nwhere we condition on the trajectory up to time \\(\\hi\\), denoted \\(\\tau_\\hi = (s_0, a_0, r_0, \\dots, s_\\hi)\\), where \\(s_\\hi = s\\).\n\nConvince yourself that all optimal policies must have the same value function. We call this the optimal value function and denote it by \\(V_\\hi^\\star(s)\\). The same goes for the action-value function \\(Q_\\hi^\\star(s, a)\\).\nIt is a stunning fact that every finite-horizon MDP has an optimal policy that is time-dependent and deterministic. In particular, we can construct such a policy by acting greedily with respect to the optimal action-value function:\n\nTheorem 1.3 (It is optimal to be greedy with respect to the optimal value function) \\[\n\\pi_\\hi^\\star(s) = \\arg\\max_a Q_\\hi^\\star(s, a).\n\\]\n\n\nProof. Let \\(V^{\\star}\\) and \\(Q^{\\star}\\) denote the optimal value and action-value functions. Consider the greedy policy\n\\[\n\\hat \\pi_\\hi(s) := \\arg\\max_a Q_\\hi^{\\star}(s, a).\n\\]\nWe aim to show that \\(\\hat \\pi\\) is optimal; that is, \\(V^{\\hat \\pi} = V^{\\star}\\).\nFix an arbitrary state \\(s \\in \\mathcal{S}\\) and time \\(\\hi \\in [H]\\).\nFirstly, by the definition of \\(V^{\\star}\\), we already know \\(V_\\hi^{\\star}(s) \\ge V_\\hi^{\\hat \\pi}(s)\\). So for equality to hold we just need to show that \\(V_\\hi^{\\star}(s) \\le V_\\hi^{\\hat \\pi}(s)\\). We’ll first show that the Bellman operator \\(\\mathcal{J}^{\\hat \\pi}\\) never decreases \\(V_\\hi^{\\star}\\). Then we’ll apply this result recursively to show that \\(V^{\\star} = V^{\\hat \\pi}\\).\n\n1.3.2.1 The Bellman operator never decreases the optimal value function\n\\(\\mathcal{J}^{\\hat \\pi}\\) never decreases \\(V_\\hi^{\\star}\\) (elementwise):\n\\[\n[\\mathcal{J}^{\\hat \\pi} (V_{\\hi+1}^{\\star})](s) \\ge V_\\hi^{\\star}(s).\n\\]\nProof:\n\\[\n\\begin{aligned}\n    V_\\hi^{\\star}(s) &= \\max_{\\pi \\in \\Pi} V_\\hi^{\\pi}(s) \\\\\n    &= \\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{a \\sim \\pi(\\dots)}\\left[r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^\\pi(s') \\right] && \\text{Bellman consistency} \\\\\n    &\\le \\max_{\\pi \\in \\Pi} \\mathop{\\mathbb{E}}_{a \\sim \\pi(\\dots)}\\left[r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^{\\star}(s') \\right] && \\text{definition of } V^\\star \\\\\n    &= \\max_{a} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} V_{\\hi+1}^{\\star}(s') \\right] && \\text{only depends on } \\pi \\text{ via } a \\\\\n    &= [\\mathcal{J}^{\\hat \\pi}(V_{\\hi+1}^{\\star})](s).    \n\\end{aligned}\n\\]\nNote that the chosen action \\(a \\sim \\pi(\\dots)\\) above might depend on the past history; this isn’t shown in the notation and doesn’t affect our result (make sure you see why).\n\nWe can now apply this result recursively to get\n\\[\nV^{\\star}_t(s) \\le V^{\\hat \\pi}_t(s)\n\\]\nas follows. (Note that even though \\(\\hat \\pi\\) is deterministic, we’ll use the \\(a \\sim \\hat \\pi(s)\\) notation to make it explicit that we’re sampling a trajectory from it.)\n\\[\n\\begin{aligned}\n    V_{t}^{\\star}(s) &\\le [\\mathcal{J}^{\\hat \\pi}(V_{\\hi+1}^{\\star})](s) \\\\\n    &= \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} \\left[ {\\color{blue} V_{\\hi+1}^{\\star}(s')} \\right] \\right] && \\text{definition of } \\mathcal{J}^{\\hat \\pi} \\\\\n    &\\le \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} \\left[ {\\color{blue}[ \\mathcal{J}^{\\hat \\pi} (V_{t+2}^{\\star})] (s')} \\right] \\right] && \\text{above lemma} \\\\\n    &= \\mathop{\\mathbb{E}}_{a \\sim \\hat \\pi(s)} \\left[ r(s, a) + \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}{\\color{blue} \\left[ \\mathop{\\mathbb{E}}_{a' \\sim \\hat \\pi}  r(s', a') + \\mathop{\\mathbb{E}}_{s''} V_{t+2}^{\\star}(s'') \\right]} \\right] && \\text{definition of } \\mathcal{J}^{\\hat \\pi} \\\\\n    &\\le \\cdots && \\text{apply at all timesteps} \\\\\n    &= \\mathop{\\mathbb{E}}_{\\tau \\sim \\rho^{\\hat \\pi}} [G_{t} \\mid s_\\hi = s] && \\text{rewrite expectation} \\\\\n    &= V_{t}^{\\hat \\pi}(s) && \\text{definition}\n\\end{aligned}\n\\]\nAnd so we have \\(V^{\\star} = V^{\\hat \\pi}\\), making \\(\\hat \\pi\\) optimal.\n\nNote that this also gives simplified forms of the Theorem 1.1 for the optimal policy:\n\nTheorem 1.4 (Bellman consistency equations for the optimal policy) \\[\n\\begin{aligned}\n    V_\\hi^\\star(s) &= \\max_a Q_\\hi^\\star(s, a) \\\\\n    Q_\\hi^\\star(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [V_{\\hi+1}^\\star(s')]\n\\end{aligned}\n\\]\n\nNow that we’ve shown this particular greedy policy is optimal, all we need to do is compute the optimal value function and optimal policy. We can do this by working backwards in time using dynamic programming (DP).\n\nDefinition 1.10 (DP algorithm to compute an optimal policy in a finite-horizon MDP) Base case. At the end of the episode (time step \\(H-1\\)), we can’t take any more actions, so the \\(Q\\)-function is simply the reward that we obtain:\n\\[\nQ^\\star_{H-1}(s, a) = r(s, a)\n\\]\nso the best thing to do is just act greedily and get as much reward as we can!\n\\[\n\\pi^\\star_{H-1}(s) = \\arg\\max_a Q^\\star_{H-1}(s, a)\n\\]\nThen \\(V^\\star_{H-1}(s)\\), the optimal value of state \\(s\\) at the end of the trajectory, is simply whatever action gives the most reward.\n\\[\nV^\\star_{H-1} = \\max_a Q^\\star_{H-1}(s, a)\n\\]\nRecursion. Then, we can work backwards in time, starting from the end, using our consistency equations! i.e. for each \\(t = H-2, \\dots, 0\\), we set\n\\[\n\\begin{aligned}\n    Q^\\star_{t}(s, a) &= r(s, a) + \\E_{s' \\sim P(s, a)} [V^\\star_{\\hi+1}(s')] \\\\\n    \\pi^\\star_{t}(s) &= \\arg\\max_a Q^\\star_{t}(s, a) \\\\\n    V^\\star_{t}(s) &= \\max_a Q^\\star_{t}(s, a)\n\\end{aligned}\n\\]\n\n\n\nCode\ndef find_optimal_policy(mdp: MDP):\n    Q = [None] * mdp.H\n    pi = [None] * mdp.H\n    V = [None] * mdp.H + [jnp.zeros(mdp.S)]  # initialize to 0 at end of time horizon\n\n    for h in range(mdp.H - 1, -1, -1):\n        Q[h] = mdp.r + mdp.P @ V[h + 1]\n        pi[h] = jnp.eye(mdp.S)[jnp.argmax(Q[h], axis=1)]  # one-hot\n        V[h] = jnp.max(Q[h], axis=1)\n\n    Q = jnp.stack(Q)\n    pi = jnp.stack(pi)\n    V = jnp.stack(V[:-1])\n\n    return pi, V, Q\n\n\nAt each of the \\(H\\) timesteps, we must compute \\(Q^{\\star}\\) for each of the \\(|\\mathcal{S}| |\\mathcal{A}|\\) state-action pairs. Each computation takes \\(|\\mathcal{S}|\\) operations to evaluate the average value over \\(s'\\). This gives a total computation time of \\(O(H \\cdot |\\mathcal{S}|^2 \\cdot |\\mathcal{A}|)\\).\nNote that this algorithm is identical to the policy evaluation algorithm dp_eval_finite in Section 1.3.1, but instead of averaging over the actions chosen by a policy, we instead simply take a maximum over the action-values. We’ll see this relationship between policy evaluation and optimal policy computation show up again in the infinite-horizon setting.\n\n\nCode\npi_opt, V_opt, Q_opt = find_optimal_policy(tidy_mdp)\nassert jnp.allclose(pi_opt, tidy_policy_messy_only)\nassert jnp.allclose(V_opt, V_messy)\nassert jnp.allclose(Q_opt[:-1], v_ary_to_q_ary(tidy_mdp, V_messy)[1:])\n\"Assertions passed (the 'tidy when messy' policy is optimal)\"\n\n\n\"Assertions passed (the 'tidy when messy' policy is optimal)\"",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#sec-infinite-horizon-mdps",
    "href": "mdps.html#sec-infinite-horizon-mdps",
    "title": "1  Markov Decision Processes",
    "section": "1.4 Infinite-horizon MDPs",
    "text": "1.4 Infinite-horizon MDPs\nWhat happens if a trajectory is allowed to continue forever (i.e. \\(H = \\infty\\))? This is the setting of infinite horizon MDPs.\nIn this chapter, we’ll describe the necessary adjustments from the finite-horizon case to make the problem tractable. We’ll show that the Bellman operator in the discounted reward setting is a contraction mapping for any policy. We’ll discuss how to evaluate policies (i.e. compute their corresponding value functions). Finally, we’ll present and analyze two iterative algorithms, based on the Bellman operator, for computing the optimal policy: value iteration and policy iteration.\n\n1.4.1 Discounted rewards\nFirst of all, note that maximizing the cumulative reward \\(r_\\hi + r_{\\hi+1} + r_{\\hi+2} + \\cdots\\) is no longer a good idea since it might blow up to infinity. Instead of a time horizon \\(H\\), we now need a discount factor \\(\\gamma \\in [0, 1)\\) such that rewards become less valuable the further into the future they are:\n\\[\nr_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots = \\sum_{k=0}^\\infty \\gamma^k r_{\\hi+k}.\n\\]\nWe can think of \\(\\gamma\\) as measuring how much we care about the future: if it’s close to \\(0\\), we only care about the near-term rewards; it’s close to \\(1\\), we put more weight into future rewards.\nYou can also analyze \\(\\gamma\\) as the probability of continuing the trajectory at each time step. (This is equivalent to \\(H\\) being distributed by a First Success distribution with success probability \\(\\gamma\\).) This accords with the above interpretation: if \\(\\gamma\\) is close to \\(0\\), the trajectory will likely be very short, while if \\(\\gamma\\) is close to \\(1\\), the trajectory will likely continue for a long time.\n\nAssuming that \\(r_\\hi \\in [0, 1]\\) for all \\(\\hi \\in \\mathbb{N}\\), what is the maximum discounted cumulative reward? You may find it useful to review geometric series.\n\nThe other components of the MDP remain the same:\n\\[\nM = (\\mathcal{S}, \\mathcal{A}, \\mu, P, r, \\gamma).\n\\]\nCode-wise, we can reuse the MDP class from before Definition 1.2 and set mdp.H = float('inf').\n\n\nCode\ntidy_mdp_inf = tidy_mdp._replace(H=float(\"inf\"), γ=0.95)\n\n\n\n\n1.4.2 Stationary policies\nThe time-dependent policies from the finite-horizon case become difficult to handle in the infinite-horizon case. In particular, many of the DP approaches we saw required us to start at the end of the trajectory, which is no longer possible. We’ll shift to stationary policies \\(\\pi : \\mathcal{S} \\to \\mathcal{A}\\) (deterministic) or \\(\\Delta(\\mathcal{A})\\) (stochastic).\n\nWhich of the policies in Example 1.2 are stationary?\n\n\n\n1.4.3 Value functions and Bellman consistency\nWe also consider stationary value functions \\(V^\\pi : \\mathcal{S} \\to \\mathbb{R}\\) and \\(Q^\\pi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\). We need to insert a factor of \\(\\gamma\\) into the Bellman consistency equation Theorem 1.1 to account for the discounting:\n\\[\n\\begin{aligned}\n    V^\\pi(s) &= \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} \\cdots \\mid s_\\hi = s] && \\text{for any } \\hi \\in \\mathbb{N} \\\\\n    &= \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma V^\\pi(s')]\\\\\n    Q^\\pi(s, a) &= \\E_{\\tau \\sim \\rho^\\pi} [r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots \\mid s_\\hi = s, a_\\hi = a] && \\text{for any } \\hi \\in \\mathbb{N} \\\\\n    &= r(s, a) + \\gamma \\E_{\\substack{s' \\sim P(s, a) \\\\ a' \\sim \\pi(s')}} [Q^\\pi(s', a')]\n\\end{aligned}\n\\tag{1.1}\\]\n\nHeuristically speaking, why does it no longer matter which time step we condition on when defining the value function?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#solving-infinite-horizon-mdps",
    "href": "mdps.html#solving-infinite-horizon-mdps",
    "title": "1  Markov Decision Processes",
    "section": "1.5 Solving infinite-horizon MDPs",
    "text": "1.5 Solving infinite-horizon MDPs\n\n1.5.1 The Bellman operator is a contraction mapping\nRecall from Definition 1.8 that the Bellman operator \\(\\mathcal{J}^{\\pi}\\) for a policy \\(\\pi\\) takes in a “value function” \\(v : \\mathcal{S} \\to \\mathbb{R}\\) and returns the r.h.s. of the Bellman equation for that “value function”. In the infinite-horizon setting, this is\n\\[\n[\\mathcal{J}^{\\pi}(v)](s) := \\E_{\\substack{a \\sim \\pi(s) \\\\ s' \\sim P(s, a)}} [r(s, a) + \\gamma v(s')].\n\\]\nThe crucial property of the Bellman operator is that it is a contraction mapping for any policy. Intuitively, if we start with two “value functions” \\(v, u : \\mathcal{S} \\to \\mathbb{R}\\), if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate.\n\nDefinition 1.11 (Contraction mapping) Let \\(X\\) be some space with a norm \\(\\|\\cdot\\|\\). We call an operator \\(f: X \\to X\\) a contraction mapping if for any \\(x, y \\in X\\),\n\\[\n\\|f(x) - f(y)\\| \\le \\gamma \\|x - y\\|\n\\]\nfor some fixed \\(\\gamma \\in (0, 1)\\). Intuitively, this means that if two points are \\(\\delta\\) far apart, after applying the mapping,\n\n\nShow that for a contraction mapping \\(f\\) with coefficient \\(\\gamma\\), for all \\(t \\in \\mathbb{N}\\),\n\\[\n\\|f^{(t)}(x) - f^{(t)}(y)\\| \\le \\gamma^t \\|x - y\\|,\n\\]\ni.e. that any two points will be pushed closer by at least a factor of \\(\\gamma\\) at each iteration.\n\nIt is a powerful fact (known as the Banach fixed-point theorem) that every contraction mapping has a unique fixed point \\(x^\\star\\) such that \\(f(x^\\star) = x^\\star\\). This means that if we repeatedly apply \\(f\\) to any starting point, we will eventually converge to \\(x^\\star\\):\n\\[\n\\|f^{(t)}(x) - x^\\star\\| \\le \\gamma^t \\|x - x^\\star\\|.\n\\tag{1.2}\\]\nLet’s return to the RL setting and apply this result to the Bellman operator. How can we measure the distance between two “value functions” \\(v, u : \\mathcal{S} \\to \\mathbb{R}\\)? We’ll take the supremum norm as our distance metric:\n\\[\n\\| v - u \\|_{\\infty} := \\sup_{s \\in \\mathcal{S}} |v(s) - u(s)|,\n\\]\ni.e. we compare the “value functions” on the state that causes the biggest gap between them. Then Equation 1.2 implies that if we repeatedly apply \\(\\mathcal{J}^\\pi\\) to any starting “value function”, we will eventually converge to \\(V^\\pi\\):\n\\[\n\\|(\\mathcal{J}^\\pi)^{(t)}(v) - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v - V^\\pi\\|_{\\infty}.\n\\tag{1.3}\\]\nWe’ll use this useful fact to prove the convergence of several algorithms later on.\n\nTheorem 1.5 (The Bellman operator is a contraction mapping) \\[\n\\|\\mathcal{J}^{\\pi} (v) - \\mathcal{J}^{\\pi} (u) \\|_{\\infty} \\le \\gamma \\|v - u \\|_{\\infty}.\n\\]\n\n\nProof. For all states \\(s \\in \\mathcal{S}\\),\n\\[\n\\begin{aligned}\n|[\\mathcal{J}^{\\pi} (v)](s) - [\\mathcal{J}^{\\pi} (u)](s)|&= \\Big| \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[ r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} v(s') \\right] \\\\\n&\\qquad - \\mathop{\\mathbb{E}}_{a \\sim \\pi(s)} \\left[r(s, a) + \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} u(s') \\right] \\Big| \\\\\n&= \\gamma \\left|\\mathop{\\mathbb{E}}_{s' \\sim P(s, a)} [v(s') - u(s')] \\right| \\\\\n&\\le \\gamma \\mathop{\\mathbb{E}}_{s' \\sim P(s, a)}|v(s') - u(s')| \\qquad \\text{(Jensen's inequality)} \\\\\n&\\le \\gamma \\max_{s'} |v(s') - u(s')| \\\\\n&= \\gamma \\|v - u \\|_{\\infty}.\n\\end{aligned}\n\\]\n\n\n\n1.5.2 Policy evaluation in infinite-horizon MDPs\nThe backwards DP technique we used in the finite-horizon case (Section 1.3.1) no longer works since there is no “final timestep” to start from. We’ll need another approach to policy evaluation.\nThe Bellman consistency conditions yield a system of equations we can solve to evaluate a deterministic policy exactly. For a faster approximate solution, we can iterate the policy’s Bellman operator, since we know that it has a unique fixed point at the true value function.\n\n1.5.2.1 Matrix inversion for deterministic policies\nNote that when the policy \\(\\pi\\) is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:\n\\[\n\\begin{aligned}\n    r^{\\pi} &\\in \\mathbb{R}^{|\\mathcal{S}|} & P^{\\pi} &\\in [0, 1]^{|\\mathcal{S}| \\times |\\mathcal{S}|} & \\mu &\\in [0, 1]^{|\\mathcal{S}|} \\\\\n    \\pi &\\in \\mathcal{A}^{|\\mathcal{S}|} & V^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}|} & Q^\\pi &\\in \\mathbb{R}^{|\\mathcal{S}| \\times |\\mathcal{A}|}.\n\\end{aligned}\n\\]\nFor \\(P^\\pi\\), we’ll treat the rows as the states and the columns as the next states. Then \\(P^\\pi_{s, s'}\\) is the probability of transitioning from state \\(s\\) to state \\(s'\\) under policy \\(\\pi\\).\n\nExample 1.5 (Tidying MDP) The tabular MDP from before has \\(|\\mathcal{S}| = 2\\) and \\(|\\mathcal{A}| = 2\\). Let’s write down the quantities for the policy \\(\\pi\\) that tidies if and only if the room is messy:\n\\[\nr^{\\pi} = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}, \\quad\n        P^{\\pi} = \\begin{bmatrix} 0.7 & 0.3 \\\\ 1 & 0 \\end{bmatrix}, \\quad\n        \\mu = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\n\\]\nWe’ll see how to evaluate this policy in the next section.\n\nThe Bellman consistency equation for a deterministic policy can be written in tabular notation as\n\\[\nV^\\pi = r^\\pi + \\gamma P^\\pi V^\\pi.\n\\]\n(Unfortunately, this notation doesn’t simplify the expression for \\(Q^\\pi\\).) This system of equations can be solved with a matrix inversion:\n\\[\nV^\\pi = (I - \\gamma P^\\pi)^{-1} r^\\pi.\n\\tag{1.4}\\]\n\nNote we’ve assumed that \\(I - \\gamma P^\\pi\\) is invertible. Can you see why this is the case?\n(Recall that a linear operator, i.e. a square matrix, is invertible if and only if its null space is trivial; that is, it doesn’t map any nonzero vector to zero. In this case, we can see that \\(I - \\gamma P^\\pi\\) is invertible because it maps any nonzero vector to a vector with at least one nonzero element.)\n\n\n\nCode\ndef eval_deterministic_infinite(\n    mdp: MDP, policy: Float[Array, \"S A\"]\n) -&gt; Float[Array, \" S\"]:\n    pi = jnp.argmax(policy, axis=1)  # un-one-hot\n    P_pi = mdp.P[jnp.arange(mdp.S), pi]\n    r_pi = mdp.r[jnp.arange(mdp.S), pi]\n    return jnp.linalg.solve(jnp.eye(mdp.S) - mdp.γ * P_pi, r_pi)\n\n\n\nExample 1.6 (Tidying policy evaluation) Let’s use the same policy \\(\\pi\\) that tidies if and only if the room is messy. Setting \\(\\gamma = 0.95\\), we must invert\n\\[\nI - \\gamma P^{\\pi} = \\begin{bmatrix} 1 - 0.95 \\times 0.7 & - 0.95 \\times 0.3 \\\\ - 0.95 \\times 1 & 1 - 0.95 \\times 0 \\end{bmatrix} = \\begin{bmatrix} 0.335 & -0.285 \\\\ -0.95 & 1 \\end{bmatrix}.\n\\]\nThe inverse to two decimal points is\n\\[\n(I - \\gamma P^{\\pi})^{-1} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix}.\n\\]\nThus the value function is\n\\[\nV^{\\pi} = (I - \\gamma P^{\\pi})^{-1} r^{\\pi} = \\begin{bmatrix} 15.56 & 4.44 \\\\ 14.79 & 5.21 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix} = \\begin{bmatrix} 15.56 \\\\ 14.79 \\end{bmatrix}.\n\\]\nLet’s sanity-check this result. Since rewards are at most \\(1\\), the maximum cumulative return of a trajectory is at most \\(1/(1-\\gamma) = 20\\). We see that the value function is indeed slightly lower than this.\n\n\n\nCode\neval_deterministic_infinite(tidy_mdp_inf, tidy_policy_messy_only[0])\n\n\nArray([15.56419, 14.78598], dtype=float32)\n\n\n\n\n1.5.2.2 Iterative policy evaluation\nThe matrix inversion above takes roughly \\(O(|\\mathcal{S}|^3)\\) time. It also only works for deterministic policies. Can we trade off the requirement of finding the exact value function for a faster approximate algorithm that will also extend to stochastic policies?\nLet’s use the Bellman operator to define an iterative algorithm for computing the value function. We’ll start with an initial guess \\(v^{(0)}\\) with elements in \\([0, 1/(1-\\gamma)]\\) and then iterate the Bellman operator:\n\\[\nv^{(t+1)} = \\mathcal{J}^{\\pi}(v^{(t)}),\n\\]\ni.e. \\(v^{(t)} = (\\mathcal{J}^{\\pi})^{(t)} (v^{(0)})\\). Note that each iteration takes \\(O(|\\mathcal{S}|^2)\\) time for the matrix-vector multiplication.\n\n\nCode\ndef supremum_norm(v):\n    return jnp.max(jnp.abs(v))  # same as jnp.linalg.norm(v, jnp.inf)\n\n\ndef loop_until_convergence(op, v, ε=1e-6):\n    \"\"\"Repeatedly apply op to v until convergence (in supremum norm).\"\"\"\n    while True:\n        v_new = op(v)\n        if supremum_norm(v_new - v) &lt; ε:\n            return v_new\n        v = v_new\n\n\ndef iterative_evaluation(mdp: MDP, pi: Float[Array, \"S A\"], ε=1e-6) -&gt; Float[Array, \" S\"]:\n    op = partial(bellman_operator, mdp, pi)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\n\nThen, as we showed in Equation 1.3, by the Banach fixed-point theorem:\n\\[\n\\|v^{(t)} - V^\\pi \\|_{\\infty} \\le \\gamma^{t} \\| v^{(0)} - V^\\pi\\|_{\\infty}.\n\\]\n\n\nCode\niterative_evaluation(tidy_mdp_inf, tidy_policy_messy_only[0])\n\n\nArray([15.564166, 14.785956], dtype=float32)\n\n\n\nRemark 1.3 (Convergence of iterative policy evaluation). How many iterations do we need for an \\(\\epsilon\\)-accurate estimate? We can work backwards to solve for \\(t\\):\n\\[\n\\begin{aligned}\n    \\gamma^t \\|v^{(0)} - V^\\pi\\|_{\\infty} &\\le \\epsilon \\\\\n    t &\\ge \\frac{\\log (\\epsilon / \\|v^{(0)} - V^\\pi\\|_{\\infty})}{\\log \\gamma} \\\\\n    &= \\frac{\\log (\\|v^{(0)} - V^\\pi\\|_{\\infty} / \\epsilon)}{\\log (1 / \\gamma)},\n\\end{aligned}\n\\]\nand so the number of iterations required for an \\(\\epsilon\\)-accurate estimate is\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\\]\nNote that we’ve applied the inequalities \\(\\|v^{(0)} - V^\\pi\\|_{\\infty} \\le 1/(1-\\gamma)\\) and \\(\\log (1/x) \\ge 1-x\\).\n\n\n\n\n1.5.3 Optimal policies in infinite-horizon MDPs\nNow let’s move on to solving for an optimal policy in the infinite-horizon case. As in Definition 1.9, an optimal policy \\(\\pi^\\star\\) is one that does at least as well as any other policy in all situations. That is, for all policies \\(\\pi\\), states \\(s \\in \\mathcal{S}\\), times \\(\\hi \\in \\mathbb{N}\\), and initial trajectories \\(\\tau_\\hi = (s_0, a_0, r_0, \\dots, s_\\hi)\\) where \\(s_\\hi = s\\),\n\\[\n\\begin{aligned}\n    V^{\\pi^\\star}(s) &= \\E_{\\tau \\sim \\rho^{\\pi^{\\star}}}[r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2}  + \\cdots \\mid s_\\hi = s] \\\\\n    &\\ge \\E_{\\tau \\sim \\rho^{\\pi}}[r_\\hi + \\gamma r_{\\hi+1} + \\gamma^2 r_{\\hi+2} + \\cdots \\mid \\tau_\\hi]\n\\end{aligned}\n\\tag{1.5}\\]\nOnce again, all optimal policies share the same optimal value function \\(V^\\star\\), and the greedy policy with respect to this value function is optimal.\n\nVerify this by modifying the proof Theorem 1.3 from the finite-horizon case.\n\nSo how can we compute such an optimal policy? We can’t use the backwards DP approach from the finite-horizon case Definition 1.10 since there’s no “final timestep” to start from. Instead, we’ll exploit the fact that the Bellman consistency equation Equation 1.1 for the optimal value function doesn’t depend on any policy:\n\\[\nV^\\star(s) = \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} V^\\star(s'). \\right]\n\\tag{1.6}\\]\n\nVerify this by substituting the greedy policy into the Bellman consistency equation.\n\nAs before, thinking of the r.h.s. of Equation 1.6 as an operator on value functions gives the Bellman optimality operator\n\\[\n[\\mathcal{J}^{\\star}(v)](s) = \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} v(s') \\right]\n\\tag{1.7}\\]\n\n\nCode\ndef bellman_optimality_operator(mdp: MDP, v: Float[Array, \" S\"]) -&gt; Float[Array, \" S\"]:\n    return jnp.max(mdp.r + mdp.γ * mdp.P @ v, axis=1)\n\n\ndef check_optimal(v: Float[Array, \" S\"], mdp: MDP):\n    return jnp.allclose(v, bellman_optimality_operator(v, mdp))\n\n\n\n1.5.3.1 Value iteration\nSince the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as value iteration.\n\n\nCode\ndef value_iteration(mdp: MDP, ε: float = 1e-6) -&gt; Float[Array, \" S\"]:\n    \"\"\"Iterate the Bellman optimality operator until convergence.\"\"\"\n    op = partial(bellman_optimality_operator, mdp)\n    return loop_until_convergence(op, jnp.zeros(mdp.S), ε)\n\n\n\n\nCode\nvalue_iteration(tidy_mdp_inf)\n\n\nArray([15.564166, 14.785956], dtype=float32)\n\n\nNote that the runtime analysis for an \\(\\epsilon\\)-optimal value function is exactly the same as Section 1.5.2.2! This is because value iteration is simply the special case of applying iterative policy evaluation to the optimal value function.\nAs the final step of the algorithm, to return an actual policy \\(\\hat \\pi\\), we can simply act greedily with respect to the final iteration \\(v^{(T)}\\) of our above algorithm:\n\\[\n\\hat \\pi(s) = \\arg\\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} v^{(T)}(s') \\right].\n\\]\nWe must be careful, though: the value function of this greedy policy, \\(V^{\\hat \\pi}\\), is not the same as \\(v^{(T)}\\), which need not even be a well-defined value function for some policy!\nThe bound on the policy’s quality is actually quite loose: if \\(\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\epsilon\\), then the greedy policy \\(\\hat \\pi\\) satisfies \\(\\|V^{\\hat \\pi} - V^\\star\\|_{\\infty} \\le \\frac{2\\gamma}{1-\\gamma} \\epsilon\\), which might potentially be very large.\n\nTheorem 1.6 (Greedy policy value worsening) \\[\n\\|V^{\\hat \\pi} - V^\\star \\|_{\\infty} \\le \\frac{2 \\gamma}{1-\\gamma} \\|v - V^\\star\\|_{\\infty}\n\\]\nwhere \\(\\hat \\pi(s) = \\arg\\max_a q(s, a)\\) is the greedy policy with respect to\n\\[\nq(s, a) = r(s, a) + \\E_{s' \\sim P(s, a)} v(s').\n\\]\n\n\nProof. We first have\n\\[\n\\begin{aligned}\n        V^{\\star}(s) - V^{\\hat \\pi}(s) &= Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\\\\\n        &= [Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s))] + [Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))].\n\\end{aligned}\n\\]\nLet’s bound these two quantities separately.\nFor the first quantity, note that by the definition of \\(\\hat \\pi\\), we have\n\\[\nq(s, \\hat \\pi(s)) \\ge q(s,\\pi^\\star(s)).\n\\]\nLet’s add \\(q(s, \\hat \\pi(s)) - q(s,\\pi^\\star(s)) \\ge 0\\) to the first term to get\n\\[\n\\begin{aligned}\n        Q^{\\star}(s,\\pi^\\star(s)) - Q^{\\star}(s, \\hat \\pi(s)) &\\le [Q^{\\star}(s,\\pi^\\star(s))- q(s,\\pi^\\star(s))] + [q(s, \\hat \\pi(s)) - Q^{\\star}(s, \\hat \\pi(s))] \\\\\n        &= \\gamma \\E_{s' \\sim P(s, \\pi^{\\star}(s))} [ V^{\\star}(s') - v(s') ] + \\gamma \\E_{s' \\sim P(s, \\hat \\pi(s))} [ v(s') - V^{\\star}(s') ] \\\\\n        &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty}.\n\\end{aligned}\n\\]\nThe second quantity is bounded by\n\\[\n\\begin{aligned}\n        Q^{\\star}(s, \\hat \\pi(s)) - Q^{\\hat \\pi}(s, \\hat \\pi(s))\n        &=\n        \\gamma \\E_{s'\\sim P(s, \\hat \\pi(s))}\\left[ V^\\star(s') - V^{\\hat \\pi}(s') \\right] \\\\\n        & \\leq\n        \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty\n\\end{aligned}\n\\]\nand thus\n\\[\n\\begin{aligned}\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le 2 \\gamma \\|v - V^{\\star}\\|_{\\infty} + \\gamma \\|V^{\\star} - V^{\\hat \\pi}\\|_\\infty \\\\\n        \\|V^\\star - V^{\\hat \\pi}\\|_\\infty &\\le \\frac{2 \\gamma \\|v - V^{\\star}\\|_{\\infty}}{1-\\gamma}.\n\\end{aligned}\n\\]\n\nSo in order to compensate and achieve \\(\\|V^{\\hat \\pi} - V^{\\star}\\| \\le \\epsilon\\), we must have\n\\[\n\\|v^{(T)} - V^\\star\\|_{\\infty} \\le \\frac{1-\\gamma}{2 \\gamma} \\epsilon.\n\\]\nThis means, using Remark 1.3, we need to run value iteration for\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{\\gamma}{\\epsilon (1-\\gamma)^2}\\right) \\right)\n\\]\niterations to achieve an \\(\\epsilon\\)-accurate estimate of the optimal value function.\n\n\n1.5.3.2 Policy iteration\nCan we mitigate this “greedy worsening”? What if instead of approximating the optimal value function and then acting greedily by it at the very end, we iteratively improve the policy and value function together? This is the idea behind policy iteration. In each step, we simply set the policy to act greedily with respect to its own value function.\n\n\nCode\ndef policy_iteration(mdp: MDP, ε=1e-6) -&gt; Float[Array, \"S A\"]:\n    \"\"\"Iteratively improve the policy and value function.\"\"\"\n    def op(pi):\n        return v_to_greedy(mdp, eval_deterministic_infinite(mdp, pi))\n    pi_init = jnp.ones((mdp.S, mdp.A)) / mdp.A  # uniform random policy\n    return loop_until_convergence(op, pi_init, ε)\n\n\n\n\nCode\npolicy_iteration(tidy_mdp_inf)\n\n\nArray([[1., 0.],\n       [0., 1.]], dtype=float32)\n\n\nAlthough PI appears more complex than VI, we’ll use Theorem 1.5 to show convergence. This will give us the same runtime bound as value iteration and iterative policy evaluation for an \\(\\epsilon\\)-optimal value function Remark 1.3, although in practice, PI often converges much faster.\n\nTheorem 1.7 (Policy Iteration runtime and convergence) We aim to show that the number of iterations required for an \\(\\epsilon\\)-accurate estimate of the optimal value function is\n\\[\nT = O\\left( \\frac{1}{1-\\gamma} \\log\\left(\\frac{1}{\\epsilon (1-\\gamma)}\\right) \\right).\n\\]\nThis bound follows from the contraction property Equation 1.3:\n\\[\n\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.\n\\]\nWe’ll prove that the iterates of PI respect the contraction property by showing that the policies improve monotonically:\n\\[\nV^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s).\n\\]\nThen we’ll use this to show \\(V^{\\pi^{t+1}}(s) \\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s)\\). Note that\n\\[\n\\begin{aligned}\n(s) &= \\max_a \\left[ r(s, a) + \\gamma \\E_{s' \\sim P(s, a)} V^{\\pi^{t}}(s') \\right] \\\\\n    &= r(s, \\pi^{t+1}(s)) + \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} V^{\\pi^{t}}(s')\n\\end{aligned}\n\\]\nSince \\([\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s) \\ge V^{\\pi^{t}}(s)\\), we then have\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) \\\\\n    &= \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right].\n\\end{aligned}\n\\tag{1.8}\\]\nBut note that the expression being averaged is the same as the expression on the l.h.s. with \\(s\\) replaced by \\(s'\\). So we can apply the same inequality recursively to get\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - V^{\\pi^{t}}(s) &\\ge  \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge \\gamma^2 \\E_{\\substack{s' \\sim P(s, \\pi^{t+1}(s)) \\\\ s'' \\sim P(s', \\pi^{t+1}(s'))}} \\left[V^{\\pi^{t+1}}(s'') -  V^{\\pi^{t}}(s'') \\right]\\\\\n    &\\ge \\cdots\n\\end{aligned}\n\\]\nwhich implies that \\(V^{\\pi^{t+1}}(s) \\ge V^{\\pi^{t}}(s)\\) for all \\(s\\) (since the r.h.s. converges to zero). We can then plug this back into Equation 1.8 to get the desired result:\n\\[\n\\begin{aligned}\n    V^{\\pi^{t+1}}(s) - \\mathcal{J}^{\\star} (V^{\\pi^{t}})(s) &= \\gamma \\E_{s' \\sim P(s, \\pi^{t+1}(s))} \\left[V^{\\pi^{t+1}}(s') -  V^{\\pi^{t}}(s') \\right] \\\\\n    &\\ge 0 \\\\\n    V^{\\pi^{t+1}}(s) &\\ge [\\mathcal{J}^{\\star}(V^{\\pi^{t}})](s)\n\\end{aligned}\n\\]\nThis means we can now apply the Bellman convergence result Equation 1.3 to get\n\\[\n\\|V^{\\pi^{t+1}} - V^\\star \\|_{\\infty} \\le \\|\\mathcal{J}^{\\star} (V^{\\pi^{t}}) - V^{\\star}\\|_{\\infty} \\le \\gamma \\|V^{\\pi^{t}} - V^\\star \\|_{\\infty}.\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "mdps.html#summary",
    "href": "mdps.html#summary",
    "title": "1  Markov Decision Processes",
    "section": "1.6 Summary",
    "text": "1.6 Summary\n\nMarkov decision processes (MDPs) are a framework for sequential decision making under uncertainty. They consist of a state space \\(\\mathcal{S}\\), an action space \\(\\mathcal{A}\\), an initial state distribution \\(\\mu \\in \\Delta(\\mathcal{S})\\), a transition function \\(P(s' \\mid s, a)\\), and a reward function \\(r(s, a)\\). They can be finite-horizon (ends after \\(H\\) timesteps) or infinite-horizon (where rewards scale by \\(\\gamma \\in (0, 1)\\) at each timestep).\nOur goal is to find a policy \\(\\pi\\) that maximizes expected total reward. Policies can be deterministic or stochastic, state-dependent or history-dependent, stationary or time-dependent.\nA policy induces a distribution over trajectories.\nWe can evaluate a policy by computing its value function \\(V^\\pi(s)\\), which is the expected total reward starting from state \\(s\\) and following policy \\(\\pi\\). We can also compute the state-action value function \\(Q^\\pi(s, a)\\), which is the expected total reward starting from state \\(s\\), taking action \\(a\\), and then following policy \\(\\pi\\). In the finite-horizon setting, these also depend on the timestep \\(\\hi\\).\nThe Bellman consistency equation is an equation that the value function must satisfy. It can be used to solve for the value functions exactly. Thinking of the r.h.s. of this equation as an operator on value functions gives the Bellman operator.\nIn the finite-horizon setting, we can compute the optimal policy using dynamic programming.\nIn the infinite-horizon setting, we can compute the optimal policy using value iteration or policy iteration.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Markov Decision Processes</span>"
    ]
  },
  {
    "objectID": "control.html",
    "href": "control.html",
    "title": "2  Linear Quadratic Regulators",
    "section": "",
    "text": "2.1 Introduction\nmath: ‘: ’x’ ‘’: ‘u’\nUp to this point, we have considered decision problems with finitely many states and actions. However, in many applications, states and actions may take on continuous values. For example, consider autonomous driving, controlling a robot’s joints, and automated manufacturing. How can we teach computers to solve these kinds of problems? This is the task of continuous control.\nAside from the change in the state and action spaces, the general problem setup remains the same: we seek to construct an optimal policy that outputs actions to solve the desired task. We will see that many key ideas and algorithms, in particular dynamic programming algorithms, carry over to this new setting.\nThis chapter introduces a fundamental tool to solve a simple class of continuous control problems: the linear quadratic regulator. We will then extend this basic method to more complex settings.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#introduction",
    "href": "control.html#introduction",
    "title": "2  Linear Quadratic Regulators",
    "section": "",
    "text": "Solving a Rubik’s Cube with a robot hand\n\n\n\n\n\n\n\nBoston Dynamics’s Spot robot\n\n\n\n\n\n\n\n\nExample 2.1 (CartPole) Try to balance a pencil on its point on a flat surface. It’s much more difficult than it may first seem: the position of the pencil varies continuously, and the state transitions governing the system, i.e. the laws of physics, are highly complex. This task is equivalent to the classic control problem known as CartPole:\n\n\n\nCart pole\n\n\nThe state \\(s\\in \\mathbb{R}^4\\) can be described by:\n\nthe position of the cart;\nthe velocity of the cart;\nthe angle of the pole;\nthe angular velocity of the pole.\n\nWe can control the cart by applying a horizontal force \\(a\\in \\mathbb{R}\\).\nGoal: Stabilize the cart around an ideal state and action \\((s^\\star, a^\\star)\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#optimal-control",
    "href": "control.html#optimal-control",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.2 Optimal control",
    "text": "2.2 Optimal control\nRecall that an MDP is defined by its state space \\(\\mathcal{S}\\), action space \\(\\mathcal{A}\\), state transitions \\(P\\), reward function \\(r\\), and discount factor \\(\\gamma\\) or time horizon \\(H\\). These have equivalents in the control setting:\n\nThe state and action spaces are continuous rather than finite. That is, \\(\\mathcal{S} \\subseteq \\mathbb{R}^{n_s}\\) and \\(\\mathcal{A} \\subseteq \\mathbb{R}^{n_a}\\), where \\(n_s\\) and \\(n_a\\) are the corresponding dimensions of these spaces, i.e. the number of coordinates to specify a single state or action respectively.\nWe call the state transitions the dynamics of the system. In the most general case, these might change across timesteps and also include some stochastic noise \\(w_h\\) at each timestep. We denote these dynamics as the function \\(f_h\\) such that \\(s_{h+1} = f_h(s_h, a_h, w_h)\\). Of course, we can simplify to cases where the dynamics are deterministic/noise-free (no \\(w_h\\) term) and/or time-homogeneous (the same function \\(f\\) across timesteps).\nInstead of maximizing the reward function, we seek to minimize the cost function \\(c_h: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\). Often, the cost function describes how far away we are from a target state-action pair \\((s^\\star, a^\\star)\\). An important special case is when the cost is time-homogeneous; that is, it remains the same function \\(c\\) at each timestep \\(h\\).\nWe seek to minimize the undiscounted cost within a finite time horizon \\(H\\). Note that we end an episode at the final state \\(s_H\\) – there is no \\(a_H\\), and so we denote the cost for the final state as \\(c_H(s_H)\\).\n\nWith all of these components, we can now formulate the optimal control problem: compute a policy to minimize the expected undiscounted cost over \\(H\\) timesteps. In this chapter, we will only consider deterministic, time-dependent policies \\(\\pi = (\\pi_0, \\dots, \\pi_{H-1})\\) where \\(\\pi_h : \\mathcal{S} \\to \\mathcal{A}\\) for each \\(h\\in [H]\\).\n\nDefinition 2.1 (General optimal control problem) \\[\n\\begin{split}\n    \\min_{\\pi_0, \\dots, \\pi_{H-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\mathbb{E}\\left[\n        \\left( \\sum_{h=0}^{H-1} c_h(s_h, a_h) \\right) + c_H(s_H)\n        \\right] \\\\\n    \\text{where} \\quad & s_{h+1} = f_h(s_h, a_h, w_h), \\\\\n    & a_h= \\pi_h(s_h) \\\\\n    & s_0 \\sim \\mu_0 \\\\\n    & w_h\\sim \\text{noise}\n\\end{split}\n\\]\n\n\n2.2.1 A first attempt: Discretization\nCan we solve this problem using tools from the finite MDP setting? If \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) were finite, then we’d be able to work backwards using the DP algorithm for computing the optimal policy in an MDP (Definition 1.10). This inspires us to try discretizing the problem.\nSuppose \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) are bounded, that is, \\(\\max_{s\\in \\mathcal{S}} \\|s\\| \\le B_s\\) and \\(\\max_{a\\in \\mathcal{A}} \\|a\\| \\le B_a\\). To make \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) finite, let’s choose some small positive \\(\\epsilon\\), and simply round each coordinate to the nearest multiple of \\(\\epsilon\\). For example, if \\(\\epsilon = 0.01\\), then we round each element of \\(s\\) and \\(a\\) to two decimal spaces.\nHowever, the discretized \\(\\widetilde{\\mathcal{S}}\\) and \\(\\widetilde{\\mathcal{A}}\\) may be finite, but they may be infeasibly large: we must divide each dimension into intervals of length \\(\\varepsilon\\), resulting in \\(|\\widetilde{\\mathcal{S}}| = (B_s/\\varepsilon)^{n_s}\\) and \\(|\\widetilde{\\mathcal{A}}| = (B_a/\\varepsilon)^{n_a}\\). To get a sense of how quickly this grows, consider \\(\\varepsilon = 0.01, n_s= n_a= 10\\). Then the number of elements in the transition matrix would be \\(|\\widetilde{\\mathcal{S}}|^2 |\\widetilde{\\mathcal{A}}| = (100^{10})^2 (100^{10}) = 10^{60}\\)! (That’s a trillion trillion trillion trillion trillion.)\nWhat properties of the problem could we instead make use of? Note that by discretizing the state and action spaces, we implicitly assumed that rounding each state or action vector by some tiny amount \\(\\varepsilon\\) wouldn’t change the behavior of the system by much; namely, that the cost and dynamics were relatively continuous. Can we use this continuous structure in other ways? This leads us to the linear quadratic regulator.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-lqr",
    "href": "control.html#sec-lqr",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.3 The Linear Quadratic Regulator",
    "text": "2.3 The Linear Quadratic Regulator\nThe optimal control problem Definition 2.1 seems highly complex in general. Is there a relevant simplification that we can analyze? The linear quadratic regulator (LQR) is a solvable case and a fundamental tool in control theory.\n\nDefinition 2.2 (The linear quadratic regulator) The LQR problem is a special case of the Definition 2.1 with linear dynamics and an upward-curved quadratic cost function. Solving the LQR problem will additionally enable us to locally approximate more complex setups using Taylor approximations.\nLinear, time-homogeneous dynamics: for each timestep \\(h\\in [H]\\),\n\\[\n\\begin{aligned}\n    s_{h+1} &= f(s_h, a_h, w_h) = A s_h+ B a_h+ w_h\\\\\n    \\text{where } w_h&\\sim \\mathcal{N}(0, \\sigma^2 I).\n\\end{aligned}\n\\]\nHere, \\(w_h\\) is a spherical Gaussian noise term that makes the dynamics random. Setting \\(\\sigma = 0\\) gives us deterministic state transitions. We will find that the optimal policy actually does not depend on the noise, although the optimal value function and Q-function do.\nUpward-curved quadratic, time-homogeneous cost function:\n\\[\nc(s_h, a_h) = \\begin{cases}\n    s_h^\\top Q s_h+ a_h^\\top R a_h& h&lt; H\\\\\n    s_h^\\top Q s_h& h= H\n\\end{cases}.\n\\]\nThis cost function attempts to stabilize the state and action about \\((s^\\star, a^\\star) = (0, 0)\\). We require \\(Q \\in \\mathbb{R}^{n_s\\times n_s}\\) and \\(R \\in \\mathbb{R}^{n_a\\times n_a}\\) to both be positive definite matrices so that \\(c\\) has a well-defined unique minimum. We can furthermore assume without loss of generality that they are both symmetric (see exercise below).\nThis results in the LQR optimization problem:\n\\[\n\\begin{aligned}\n        \\min_{\\pi_0, \\dots, \\pi_{H-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\mathbb{E}\\left[ \\left( \\sum_{h=0}^{H-1} s_h^\\top Q s_h+ a_h^\\top R a_h\\right) + s_H^\\top Q s_H\\right] \\\\\n        \\textrm{where} \\quad                                & s_{h+1} = A s_h+ B a_h+ w_h\\\\\n                                                            & a_h= \\pi_h(s_h)                                                                                                        \\\\\n                                                            & w_h\\sim \\mathcal{N}(0, \\sigma^2 I)                                                                                               \\\\\n                                                            & s_0 \\sim \\mu_0.\n\\end{aligned}\n\\]\n\n\nExercise 2.1 (Symmetric \\(Q\\) and \\(R\\)) Here we’ll show that we don’t lose generality by assuming that \\(Q\\) and \\(R\\) are symmetric. Show that replacing \\(Q\\) and \\(R\\) with \\((Q + Q^\\top) / 2\\) and \\((R + R^\\top) / 2\\) (which are symmetric) yields the same cost function.\n\nWe will henceforth abbreviate “symmetric positive definite” as s.p.d. and “positive definite” as p.d.\nIt will be helpful to reintroduce the value function notation for a policy to denote the average cost it incurs. These will be instrumental in constructing the optimal policy via dynamic programming, as we did in Section 1.3.2 for MDPs.\n\nDefinition 2.3 (Value functions for LQR) Given a policy \\(\\mathbf{\\pi} = (\\pi_0, \\dots, \\pi_{H-1})\\), we can define its value function \\(V^\\pi_h: \\mathcal{S} \\to \\mathbb{R}\\) at time \\(h\\in [H]\\) as the average cost-to-go incurred by that policy:\n\\[\n\\begin{split}\n    V^\\pi_h(s) &= \\mathbb{E}\\left[ \\left( \\sum_{i=h}^{H-1} c(s_i, a_i) \\right) + c(s_H) \\mid s_h= s,  a_i = \\pi_i(s_i) \\quad \\forall h\\le i &lt; H \\right] \\\\\n    &= \\mathbb{E}\\left[ \\left( \\sum_{i=h}^{H-1} s_i^\\top Q s_i + a_i^\\top R a_i \\right) + s_H^\\top Q s_H\\mid s_h= s, a_i = \\pi_i(s_i) \\quad \\forall h\\le i &lt; H \\right] \\\\\n\\end{split}\n\\]\nThe Q-function additionally conditions on the first action we take:\n\\[\n\\begin{split}\n    Q^\\pi_h(s, a) &= \\mathbb{E}\\bigg[ \\left( \\sum_{i=h}^{H-1} c(s_i, a_i) \\right) + c(s_H) \\\\\n        &\\qquad\\qquad \\mid  (s_h, a_h) = (s, a), a_i = \\pi_i(s_i) \\quad \\forall h\\le i &lt; H \\bigg] \\\\\n    &= \\mathbb{E}\\bigg[ \\left( \\sum_{i=h}^{H-1} s_i^\\top Q s_i + a_i^\\top R a_i \\right) + s_H^\\top Q s_H\\\\\n        &\\qquad\\qquad \\mid (s_h, a_h) = (s, a), a_i = \\pi_i(s_i) \\quad \\forall h\\le i &lt; H \\bigg] \\\\\n\\end{split}\n\\]\nNote that since we use cost instead of reward, the best policies are the ones with smaller values of the value function.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-optimal-lqr",
    "href": "control.html#sec-optimal-lqr",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.4 Optimality and the Riccati Equation",
    "text": "2.4 Optimality and the Riccati Equation\nIn this section, we’ll compute the optimal value function \\(V^\\star_h\\), Q-function \\(Q^\\star_h\\), and policy \\(\\pi^\\star_h\\) in Definition 2.2 using dynamic programming in a very similar way to the DP algorithms in Section 1.3.1. Recall the definition of the optimal value function:\n\nDefinition 2.4 (Optimal value function in LQR) The optimal value function is the one that, at any time and in any state, achieves minimum cost across all policies:\n\\[\n\\begin{split}\n    V^\\star_h(s) &= \\min_{\\pi_h, \\dots, \\pi_{H-1}} V^\\pi_h(s) \\\\\n    &= \\min_{\\pi_{h}, \\dots, \\pi_{H-1}} \\mathbb{E}\\bigg[ \\left( \\sum_{i=h}^{H-1} s_h^\\top Q s_h+ a_h^\\top R a_h\\right) + s_H^\\top Q s_H\\\\\n        &\\hspace{8em} \\mid s_h= s, a_i = \\pi_i(s_i) \\quad \\forall h\\le i &lt; H \\bigg] \\\\\n\\end{split}\n\\]\nThe optimal Q-function is defined similarly, conditioned on the starting action as well:\n\\[\n\\begin{split}\n    Q^\\star_h(s, a) &= \\min_{\\pi_h, \\dots, \\pi_{H-1}} Q^\\pi_h(s, a) \\\\\n    &= \\min_{\\pi_{h}, \\dots, \\pi_{H-1}} \\mathbb{E}\\bigg[ \\left( \\sum_{i=h}^{H-1} s_h^\\top Q s_h+ a_h^\\top R a_h\\right) + s_H^\\top Q s_H\\\\\n        &\\hspace{8em} \\mid s_h= s, a_h= a, a_i = \\pi_i(s_i) \\quad \\forall h&lt; i &lt; H \\bigg] \\\\\n\\end{split}\n\\]\nBoth of the definitions above assume deterministic policies. Otherwise we would have to take an expectation over actions drawn from the policy, i.e. \\(a_h\\sim \\pi_h(s_h)\\).\n\nWe will prove the striking fact that the solution has very simple structure: \\(V_h^\\star\\) and \\(Q^\\star_h\\) are upward-curved quadratics and \\(\\pi_h^\\star\\) is linear and furthermore does not depend on the noise!\n\nTheorem 2.1 (Optimal value function in LQR is an upward-curved quadratic) At each timestep \\(h\\in [H]\\),\n\\[\nV^\\star_h(s) = s^\\top P_hs+ p_h\n\\]\nfor some s.p.d. matrix \\(P_h\\in \\mathbb{R}^{n_s\\times n_s}\\) and scalar \\(p_h\\in \\mathbb{R}\\).\n\n\nTheorem 2.2 (Optimal policy in LQR is linear) At each timestep \\(h\\in [H]\\),\n\\[\n\\pi^\\star_h(s) = - K_hs\n\\]\nfor some \\(K_h\\in \\mathbb{R}^{n_a\\times n_s}\\). (The negative is due to convention.)\n\nThe construction (and inductive proof) proceeds similarly to the one in the MDP setting (Section 1.3.1).\n\nWe’ll compute \\(V_H^\\star\\) (at the end of the horizon) as our base case.\nThen we’ll work step-by-step backwards in time, using \\(V_{h+1}^\\star\\) to compute \\(Q_h^\\star\\), \\(\\pi_{h}^\\star\\), and \\(V_h^\\star\\).\n\n\nBase case: At the final timestep, there are no possible actions to take, and so \\(V^\\star_H(s) = c(s) = s^\\top Q s\\). Thus \\(V_H^\\star(s) = s^\\top P_Hs+ p_H\\) where \\(P_H= Q\\) and \\(p_H= 0\\).\nInductive hypothesis: We seek to show that the inductive step holds for both theorems: If \\(V^\\star_{h+1}(s)\\) is an upward-curved quadratic, then \\(V^\\star_h(s)\\) must also be an upward-curved quadratic, and \\(\\pi^\\star_h(s)\\) must be linear. We’ll break this down into the following steps:\n\nShow that \\(Q^\\star_h(s, a)\\) is an upward-curved quadratic (in both \\(s\\) and \\(a\\)).\nDerive the optimal policy \\(\\pi^\\star_h(s) = \\arg \\min_aQ^\\star_h(s, a)\\) and show that it’s linear.\nShow that \\(V^\\star_h(s)\\) is an upward-curved quadratic.\n\nWe first assume the inductive hypothesis that our theorems are true at time \\(h+1\\). That is,\n\\[\nV^\\star_{h+1}(s) = s^\\top P_{h+1} s+ p_{h+1} \\quad \\forall s\\in \\mathcal{S}.\n\\]\n\n\n2.4.0.1 \\(Q^\\star_h(s, a)\\) is an upward-curved quadratic\nLet us decompose \\(Q^\\star_h: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}\\) into the immediate reward plus the expected cost-to-go:\n\\[\nQ^\\star_h(s, a) = c(s, a) + \\mathbb{E}_{s' \\sim f(s, a, w_{h+1})} [V^\\star_{h+1}(s')].\n\\]\nRecall \\(c(s, a) := s^\\top Q s+ a^\\top R a\\). Let’s consider the expectation over the next timestep. The only randomness in the dynamics comes from the noise \\(w_{h+1} \\sim \\mathcal{N}(0, \\sigma^2 I)\\), so we can expand the expectation as:\n\\[\n\\begin{aligned}\n            & \\mathbb{E}_{s'} [V^\\star_{h+1}(s')]                                                                                                         \\\\\n    {} = {} & \\mathbb{E}_{w_{h+1}} [V^\\star_{h+1}(A s+ B a+ w_{h+1})]                                             &  & \\text{definition of } f     \\\\\n    {} = {} & \\mathbb{E}_{w_{h+1}} [ (A s+ B a+ w_{h+1})^\\top P_{h+1} (A s+ B a+ w_{h+1}) + p_{h+1} ]. &  & \\text{inductive hypothesis}\n\\end{aligned}\n\\]\nSumming and combining like terms, we get\n\\[\n\\begin{aligned}\n    Q^\\star_h(s, a) & = s^\\top Q s+ a^\\top R a+ \\mathbb{E}_{w_{h+1}} [(A s+ B a+ w_{h+1})^\\top P_{h+1} (A s+ B a+ w_{h+1}) + p_{h+1}] \\\\\n                           & = s^\\top (Q + A^\\top P_{h+1} A)s+ a^\\top (R + B^\\top P_{h+1} B) a+ 2 s^\\top A^\\top P_{h+1} B a\\\\\n                           & \\qquad + \\mathbb{E}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] + p_{h+1}.\n\\end{aligned}\n\\]\nNote that the terms that are linear in \\(w_h\\) have mean zero and vanish. Now consider the remaining expectation over the noise. By expanding out the product and using linearity of expectation, we can write this out as\n\\[\n\\begin{aligned}\n    \\mathbb{E}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] & = \\sum_{i=1}^d \\sum_{j=1}^d (P_{h+1})_{ij} \\mathbb{E}_{w_{h+1}} [(w_{h+1})_i (w_{h+1})_j] \\\\\n    & = \\sigma^2 \\mathrm{Tr}(P_{h+ 1})\n\\end{aligned}\n\\]\n\n\n2.4.0.2 Quadratic forms\nWhen solving quadratic forms, i.e. expressions of the form \\(x^\\top A x\\), it’s often helpful to consider the terms on the diagonal (\\(i = j\\)) separately from those off the diagonal.\nIn this case, the expectation of each diagonal term becomes\n\\[\n(P_{h+1})_{ii} \\mathbb{E}(w_{h+1})_i^2 = \\sigma^2 (P_{h+1})_{ii}.\n\\]\nOff the diagonal, since the elements of \\(w_{h+1}\\) are independent, the expectation factors, and since each element has mean zero, the term vanishes:\n\\[\n(P_{h+1})_{ij} \\mathbb{E}[(w_{h+1})_i] \\mathbb{E}[(w_{h+1})_j] = 0.\n\\]\nThus, the only terms left are the ones on the diagonal, so the sum of these can be expressed as the trace of \\(\\sigma^2 P_{h+1}\\):\n\\[\n\\mathbb{E}_{w_{h+1}} [w_{h+1}^\\top P_{h+1} w_{h+1}] = \\sigma^2 \\mathrm{Tr}(P_{h+1}).\n\\]\n\nSubstituting this back into the expression for \\(Q^\\star_h\\), we have:\n\\[\n\\begin{aligned}\n    Q^\\star_h(s, a) & = s^\\top (Q + A^\\top P_{h+1} A) s+ a^\\top (R + B^\\top P_{h+1} B) a\n    + 2s^\\top A^\\top P_{h+1} B a\\\\\n                            & \\qquad + \\sigma^2 \\mathrm{Tr}(P_{h+1}) + p_{h+1}.\n\\end{aligned}\n\\]\nAs we hoped, this expression is quadratic in \\(s\\) and \\(a\\). Furthermore, we’d like to show that it also curves upwards with respect to \\(a\\) so that its minimum with respect to \\(a\\) is well-defined. We can do this by noting that the Hessian matrix of second derivatives is positive definite:\n\\[\n\\nabla_{aa} Q_h^\\star(s, a) = R + B^\\top P_{h+1} B\n\\]\nSince \\(R\\) is s.p.d. (Definition 2.2), and \\(P_{h+1}\\) is s.p.d. (by the inductive hypothesis), this sum must also be s.p.d., and so \\(Q^\\star_h\\) is indeed an upward-curved quadratic with respect to \\(a\\). (If this isn’t clear, try proving it as an exercise.) The proof of its upward curvature with respect to \\(s\\) is equivalent.\n\n\nLemma 2.1 (\\(\\pi^\\star_h\\) is linear) Since \\(Q^\\star_h\\) is an upward-curved quadratic, finding its minimum over \\(a\\) is easy: we simply set the gradient with respect to \\(a\\) equal to zero and solve for \\(a\\). First, we calculate the gradient:\n\\[\n\\begin{aligned}\n    \\nabla_aQ^\\star_h(s, a) & = \\nabla_a[ a^\\top (R + B^\\top P_{h+1} B) a+ 2 s^\\top A^\\top P_{h+1} B a] \\\\\n                                       & = 2 (R + B^\\top P_{h+1} B) a+ 2 (s^\\top A^\\top P_{h+1} B)^\\top\n\\end{aligned}\n\\]\nSetting this to zero, we get\n\\[\n\\begin{aligned}\n    0                  & = (R + B^\\top P_{h+1} B) \\pi^\\star_h(s) + B^\\top P_{h+1} A s\\nonumber \\\\\n    \\pi^\\star_h(s) & = (R + B^\\top P_{h+1} B)^{-1} (-B^\\top P_{h+1} A s) \\nonumber              \\\\\n                       & = - K_hs,\n\\end{aligned}\n\\]\nwhere\n\\[\nK_h= (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A.\n\\tag{2.1}\\]\nNote that this optimal policy doesn’t depend on the starting distribution \\(\\mu_0\\). It’s also fully deterministic and isn’t affected by the noise terms \\(w_0, \\dots, w_{H-1}\\).\n\n\nLemma 2.3 (The value function is an upward-curved quadratic) Using the identity \\(V^\\star_h(s) = Q^\\star_h(s, \\pi^\\star(s))\\), we have:\n\\[\n\\begin{aligned}\n    V^\\star_h(s) & = Q^\\star_h(s, \\pi^\\star(s))                                                                \\\\\n                     & = s^\\top (Q + A^\\top P_{h+1} A) s+ (-K_hs)^\\top (R + B^\\top P_{h+1} B) (-K_hs)\n    + 2s^\\top A^\\top P_{h+1} B (-K_hs)                                                                          \\\\\n                     & \\qquad + \\mathrm{Tr}(\\sigma^2 P_{h+1}) + p_{h+1}\n\\end{aligned}\n\\]\nNote that with respect to \\(s\\), this is the sum of a quadratic term and a constant, which is exactly what we were aiming for! The scalar term is clearly\n\\[\np_h= \\mathrm{Tr}(\\sigma^2 P_{h+1}) + p_{h+1}.\n\\]\nWe can simplify the quadratic term by substituting in \\(K_h\\) from Equation 2.1. Notice that when we do this, the \\((R+B^\\top P_{h+1} B)\\) term in the expression is cancelled out by its inverse, and the remaining terms combine to give the Riccati equation:\n\nDefinition 2.5 (Riccati equation) \\[\nP_h= Q + A^\\top P_{h+1} A - A^\\top P_{h+1} B (R + B^\\top P_{h+1} B)^{-1} B^\\top P_{h+1} A.\n\\]\n\nThere are several nice properties to note about the Riccati equation:\n\nIt’s defined recursively. Given the dynamics defined by \\(A\\) and \\(B\\), and the state cost matrix \\(Q\\), we can recursively calculate \\(P_h\\) across all timesteps starting from \\(P_H= Q\\).\n\\(P_h\\) often appears in calculations surrounding optimality, such as \\(V^\\star_h, Q^\\star_h\\), and \\(\\pi^\\star_h\\).\nTogether with the dynamics given by \\(A\\) and \\(B\\), and the action coefficients \\(R\\) in the lost function, it fully defines the optimal policy Lemma 2.1.\n\nIt remains to prove that \\(V^\\star_h\\) curves upwards, that is, that \\(P_h\\) is s.p.d. We will use the following fact about Schur complements:\n\nLemma 2.2 (Positive definiteness of Schur complements) Let\n\\[\nD = \\begin{pmatrix}\nA & B \\\\\nB^\\top & C\n\\end{pmatrix}\n\\]\nbe a symmetric \\((m+n) \\times (m+n)\\) block matrix, where \\(A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times n}, C \\in \\mathbb{R}^{n \\times n}\\). The Schur complement of \\(A\\) is denoted\n\\[\nD/A = C - B^\\top A^{-1} B.\n\\]\nSchur complements have various uses in linear algebra and numerical computation.\nA useful fact for us is that if \\(A\\) is positive definite, then \\(D\\) is positive semidefinite if and only if \\(D/A\\) is positive semidefinite.\n\nLet \\(P\\) denote \\(P_{h+ 1}\\) for brevity. We already know \\(Q\\) is p.d., so it suffices to show that\n\\[\nS = P - P B (R + B^\\top P B)^{-1} B^\\top P\n\\]\nis p.s.d. (positive semidefinite), since left- and right- multiplying by \\(A^\\top\\) and \\(A\\) respectively preserves p.s.d. We note that \\(S\\) is the Schur complement \\(D/(R + B^\\top P B)\\), where\n\\[\nD = \\begin{pmatrix}\nR + B^\\top P B & B^\\top P \\\\\nP B & P\n\\end{pmatrix}.\n\\]\nThus we must show that \\(D\\) is p.s.d.. This can be seen by computing\n\\[\n\\begin{aligned}\n\\begin{pmatrix}\ny^\\top & z^\\top\n\\end{pmatrix}\nD\n\\begin{pmatrix}\ny \\\\ z\n\\end{pmatrix}\n&= y^\\top R y + y^\\top B^\\top P B y + 2 y^\\top B^\\top P z + z^\\top P z \\\\\n&= y^\\top R y + (By + z)^\\top P (By + z) \\\\\n&&gt; 0.\n\\end{aligned}\n\\]\nSince \\(R + B^\\top P B\\) is p.d. and \\(D\\) is p.s.d., then \\(S = D / (R + B^\\top P B)\\) must be p.s.d., and \\(P_h= Q + A S A^\\top\\) must be p.d.\n\nNow we’ve shown that \\(V^\\star_h(s) = s^\\top P_hs+ p_h\\), where \\(P_h\\) is s.p.d., proving the inductive hypothesis and completing the proof of Theorem 2.2 and Theorem 2.1.\nIn summary, we just demonstrated that at each timestep \\(h\\in [H]\\), the optimal value function \\(V^\\star_h\\) and optimal Q-function \\(Q^\\star_h\\) are both upward-curved quadratics and the optimal policy \\(\\pi^\\star_h\\) is linear. We also showed that all of these quantities can be calculated using a sequence of s.p.d. matrices \\(P_0, \\dots, P_H\\) that can be defined recursively using Definition 2.5.\nBefore we move on to some extensions of LQR, let’s consider how the state at time \\(h\\) behaves when we act according to this optimal policy.\n\n2.4.1 Expected state at time \\(h\\)\nHow can we compute the expected state at time \\(h\\) when acting according to the optimal policy? Let’s first express \\(s_h\\) in a cleaner way in terms of the history. Note that having linear dynamics makes it easy to expand terms backwards in time:\n\\[\n\\begin{aligned}\n    s_h& = A s_{h-1} + B a_{h-1} + w_{h-1}                                 \\\\\n            & = A (As_{h-2} + B a_{h-2} + w_{h-2}) + B a_{h-1} + w_{h-1} \\\\\n            & = \\cdots                                                                     \\\\\n            & = A^hs_0 + \\sum_{i=0}^{h-1} A^i (B a_{h-i-1} + w_{h-i-1}).\n\\end{aligned}\n\\]\nLet’s consider the average state at this time, given all the past states and actions. Since we assume that \\(\\mathbb{E}[w_h] = 0\\) (this is the zero vector in \\(d\\) dimensions), when we take an expectation, the \\(w_h\\) term vanishes due to linearity, and so we’re left with\n\\[\n\\mathbb{E}[s_h\\mid s_{0:(h-1)}, a_{0:(h-1)}] = A^hs_0 + \\sum_{i=0}^{h-1} A^i B a_{h-i-1}.\n\\tag{2.2}\\]\n\nExercise 2.2 (Expected state) Show that if we choose actions according to the optimal policy Lemma 2.1, Equation 2.2 becomes\n\\[\n\\mathbb{E}[s_h\\mid s_0, a_i = \\pi^\\star_i(s_i)\\quad \\forall i \\le h] = \\left( \\prod_{i=0}^{h-1} (A - B K_i) \\right) s_0.\n\\]\n\nThis introdces the quantity \\(A - B K_i\\), which shows up frequently in control theory. For example, one important question is: will \\(s_h\\) remain bounded, or will it go to infinity as time goes on? To answer this, let’s imagine for simplicity that these \\(K_i\\)s are equal (call this matrix \\(K\\)). Then the expression above becomes \\((A-BK)^hs_0\\). Now consider the maximum eigenvalue \\(\\lambda_{\\max}\\) of \\(A - BK\\). If \\(|\\lambda_{\\max}| &gt; 1\\), then there’s some nonzero initial state \\(\\bar s_0\\), the corresponding eigenvector, for which\n\\[\n\\lim_{h\\to \\infty} (A - BK)^h\\bar s_0\n    = \\lim_{h\\to \\infty} \\lambda_{\\max}^h\\bar s_0\n    = \\infty.\n\\]\nOtherwise, if \\(|\\lambda_{\\max}| &lt; 1\\), then it’s impossible for your original state to explode as dramatically.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#extensions",
    "href": "control.html#extensions",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.5 Extensions",
    "text": "2.5 Extensions\nWe’ve now formulated an optimal solution for the time-homogeneous LQR and computed the expected state under the optimal policy. However, real world tasks rarely have such simple dynamics, and we may wish to design more complex cost functions. In this section, we’ll consider more general extensions of LQR where some of the assumptions we made above are relaxed. Specifically, we’ll consider:\n\nTime-dependency, where the dynamics and cost function might change depending on the timestep.\nGeneral quadratic cost, where we allow for linear terms and a constant term.\nTracking a goal trajectory rather than aiming for a single goal state-action pair.\n\nCombining these will allow us to use the LQR solution to solve more complex setups by taking Taylor approximations of the dynamics and cost functions.\n\n2.5.1 Time-dependent dynamics and cost function\nSo far, we’ve considered the time-homogeneous case, where the dynamics and cost function stay the same at every timestep. However, this might not always be the case. As an example, in many sports, the rules and scoring system might change during an overtime period. To address these sorts of problems, we can loosen the time-homogeneous restriction, and consider the case where the dynamics and cost function are time-dependent. Our analysis remains almost identical; in fact, we can simply add a time index to the matrices \\(A\\) and \\(B\\) that determine the dynamics and the matrices \\(Q\\) and \\(R\\) that determine the cost.\nThe modified problem is now defined as follows:\n\nDefinition 2.6 (Time-dependent LQR) \\[\n\\begin{aligned}\n        \\min_{\\pi_{0}, \\dots, \\pi_{H-1}} \\quad & \\mathbb{E}\\left[ \\left( \\sum_{h=0}^{H-1} (s_h^\\top Q_hs_h) + a_h^\\top R_ha_h\\right) + s_H^\\top Q_Hs_H\\right] \\\\\n        \\textrm{where} \\quad                      & s_{h+1} = f_h(s_h, a_h, w_h) = A_hs_h+ B_ha_h+ w_h\\\\\n                                                  & s_0 \\sim \\mu_0                                                                                                                                   \\\\\n                                                  & a_h= \\pi_h(s_h)                                                                                                                       \\\\\n                                                  & w_h\\sim \\mathcal{N}(0, \\sigma^2 I).\n\\end{aligned}\n\\]\n\nThe derivation of the optimal value functions and the optimal policy remains almost exactly the same, and we can modify the Riccati equation accordingly:\n\nDefinition 2.7 (Time-dependent Riccati Equation) \\[\nP_h= Q_h+ A_h^\\top P_{h+1} A_h- A_h^\\top P_{h+1} B_h(R_h+ B_h^\\top P_{h+1} B_h)^{-1} B_h^\\top P_{h+1} A_h.\n\\]\nNote that this is just the time-homogeneous Riccati equation (Definition 2.5), but with the time index added to each of the relevant matrices.\n\n\nExercise 2.3 (Time dependent LQR proof) Walk through the proof in Section 2.4 to verify that we can simply add \\(h\\) for the time-dependent case.\n\nAdditionally, by allowing the dynamics to vary across time, we gain the ability to locally approximate nonlinear dynamics at each timestep. We’ll discuss this later in the chapter.\n\n\n2.5.2 More general quadratic cost functions\nOur original cost function had only second-order terms with respect to the state and action, incentivizing staying as close as possible to \\((s^\\star, a^\\star) = (0, 0)\\). We can also consider more general quadratic cost functions that also have first-order terms and a constant term. Combining this with time-dependent dynamics results in the following expression, where we introduce a new matrix \\(M_h\\) for the cross term, linear coefficients \\(q_h\\) and \\(r_h\\) for the state and action respectively, and a constant term \\(c_h\\):\n\\[\nc_h(s_h, a_h) = ( s_h^\\top Q_hs_h+ s_h^\\top M_ha_h+ a_h^\\top R_ha_h) + (s_h^\\top q_h+ a_h^\\top r_h) + c_h.\n\\tag{2.3}\\]\nSimilarly, we can also include a constant term \\(v_h\\in \\mathbb{R}^{n_s}\\) in the dynamics (note that this is deterministic at each timestep, unlike the stochastic noise \\(w_h\\)):\n\\[\ns_{h+1} = f_h(s_h, a_h, w_h) = A_hs_h+ B_ha_h+ v_h+ w_h.\n\\]\n\nExercise 2.4 (General cost function) Derive the optimal solution. You will need to slightly modify the proof in Section 2.4.\n\n\n\n2.5.3 Tracking a predefined trajectory\nConsider applying LQR to a task like autonomous driving, where the target state-action pair changes over time. We might want the vehicle to follow a predefined trajectory of states and actions \\((s_h^\\star, a_h^\\star)_{h=0}^{H-1}\\). To express this as a control problem, we’ll need a corresponding time-dependent cost function:\n\\[\nc_h(s_h, a_h) = (s_h- s^\\star_h)^\\top Q (s_h- s^\\star_h) + (a_h- a^\\star_h)^\\top R (a_h- a^\\star_h).\n\\]\nNote that this punishes states and actions that are far from the intended trajectory. By expanding out these multiplications, we can see that this is actually a special case of the more general quadratic cost function above Equation 2.3:\n\\[\nM_h= 0, \\qquad q_h= -2Q s^\\star_h, \\qquad r_h= -2R a^\\star_h, \\qquad c_h= (s^\\star_h)^\\top Q (s^\\star_h) + (a^\\star_h)^\\top R (a^\\star_h).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#sec-approx-nonlinear",
    "href": "control.html#sec-approx-nonlinear",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.6 Approximating nonlinear dynamics",
    "text": "2.6 Approximating nonlinear dynamics\nThe LQR algorithm solves for the optimal policy when the dynamics are linear and the cost function is an upward-curved quadratic. However, real settings are rarely this simple! Let’s return to the CartPole example from the start of the chapter (Example 2.1). The dynamics (physics) aren’t linear. How can we approximate this by an LQR problem?\nConcretely, let’s consider a noise-free problem since, as we saw, the noise doesn’t factor into the optimal policy. Let’s assume the dynamics and cost function are stationary, and ignore the terminal state for simplicity:\n\nDefinition 2.8 (Nonlinear control problem) \\[\n\\begin{aligned}\n        \\min_{\\pi_0, \\dots, \\pi_{H-1} : \\mathcal{S} \\to \\mathcal{A}} \\quad & \\mathbb{E}_{s_0} \\left[ \\sum_{h=0}^{H-1} c(s_h, a_h) \\right] \\\\\n        \\text{where} \\quad                                  & s_{h+1} = f(s_h, a_h)                                   \\\\\n                                                            & a_h= \\pi_h(s_h)                                          \\\\\n                                                            & s_0 \\sim \\mu_0                                                     \\\\\n                                                            & c(s, a) = d(s, s^\\star) + d(a, a^\\star).\n\\end{aligned}\n\\]\nHere, \\(d\\) denotes a function that measures the “distance” between its two arguments.\n\nThis is now only slightly simplified from the general optimal control problem (see Definition 2.1). Here, we don’t know an analytical form for the dynamics \\(f\\) or the cost function \\(c\\), but we assume that we’re able to query/sample/simulate them to get their values at a given state and action. To clarify, consider the case where the dynamics are given by real world physics. We can’t (yet) write down an expression for the dynamics that we can differentiate or integrate analytically. However, we can still simulate the dynamics and cost function by running a real-world experiment and measuring the resulting states and costs. How can we adapt LQR to this more general nonlinear case?\n\n2.6.1 Local linearization\nHow can we apply LQR when the dynamics are nonlinear or the cost function is more complex? We’ll exploit the useful fact that we can take a function that’s locally continuous around \\((s^\\star, a^\\star)\\) and approximate it nearby with low-order polynomials (i.e. its Taylor approximation). In particular, as long as the dynamics \\(f\\) are differentiable around \\((s^\\star, a^\\star)\\) and the cost function \\(c\\) is twice differentiable at \\((s^\\star, a^\\star)\\), we can take a linear approximation of \\(f\\) and a quadratic approximation of \\(c\\) to bring us back to the regime of LQR.\nLinearizing the dynamics around \\((s^\\star, a^\\star)\\) gives:\n\\[\n\\begin{gathered}\n    f(s, a) \\approx f(s^\\star, a^\\star) + \\nabla_sf(s^\\star, a^\\star) (s- s^\\star) + \\nabla_af(s^\\star, a^\\star) (a- a^\\star) \\\\\n    (\\nabla_sf(s, a))_{ij} = \\frac{d f_i(s, a)}{d s_j}, \\quad i, j \\le n_s\\qquad (\\nabla_af(s, a))_{ij} = \\frac{d f_i(s, a)}{d a_j}, \\quad i \\le n_s, j \\le n_a\n\\end{gathered}\n\\]\nand quadratizing the cost function around \\((s^\\star, a^\\star)\\) gives:\n\\[\n\\begin{aligned}\n    c(s, a) & \\approx c(s^\\star, a^\\star) \\quad \\text{constant term}                                                                                      \\\\\n                 & \\qquad + \\nabla_sc(s^\\star, a^\\star) (s- s^\\star) + \\nabla_ac(s^\\star, a^\\star) (a - a^\\star) \\quad \\text{linear terms} \\\\\n                 & \\left. \\begin{aligned}\n                               & \\qquad + \\frac{1}{2} (s- s^\\star)^\\top \\nabla_{ss} c(s^\\star, a^\\star) (s- s^\\star)       \\\\\n                               & \\qquad + \\frac{1}{2} (a- a^\\star)^\\top \\nabla_{aa} c(s^\\star, a^\\star) (a- a^\\star) \\\\\n                               & \\qquad + (s- s^\\star)^\\top \\nabla_{sa} c(s^\\star, a^\\star) (a- a^\\star)\n                          \\end{aligned} \\right\\} \\text{quadratic terms}\n\\end{aligned}\n\\]\nwhere the gradients and Hessians are defined as\n\\[\n\\begin{aligned}\n    (\\nabla_sc(s, a))_{i}         & = \\frac{d c(s, a)}{d s_i}, \\quad i \\le n_s\n                                          & (\\nabla_ac(s, a))_{i}                                               & = \\frac{d c(s, a)}{d a_i}, \\quad i \\le n_a\\\\\n    (\\nabla_{ss} c(s, a))_{ij}  & = \\frac{d^2 c(s, a)}{d s_i d s_j}, \\quad i, j \\le n_s\n                                          & (\\nabla_{aa} c(s, a))_{ij}                                       & = \\frac{d^2 c(s, a)}{d a_i d a_j}, \\quad i, j \\le n_a\\\\\n    (\\nabla_{sa} c(s, a))_{ij} & = \\frac{d^2 c(s, a)}{d s_i d a_j}. \\quad i \\le n_s, j \\le n_a\n\\end{aligned}\n\\]\nExercise: Note that this cost can be expressed in the general quadratic form seen in Equation 2.3. Derive the corresponding quantities \\(Q, R, M, q, r, c\\).\n\n\n2.6.2 Finite differencing\nTo calculate these gradients and Hessians in practice, we use a method known as finite differencing for numerically computing derivatives. Namely, we can simply use the limit definition of the derivative, and see how the function changes as we add or subtract a tiny \\(\\delta\\) to the input.\n\\[\n\\frac{d}{dx} f(x) = \\lim_{\\delta \\to 0} \\frac{f(x + \\delta) - f(x)}{\\delta}\n\\]\nNote that this only requires us to be able to query the function, not to have an analytical expression for it, which is why it’s so useful in practice.\n\n\n2.6.3 Local convexification\nHowever, simply taking the second-order approximation of the cost function is insufficient, since for the LQR setup we required that the \\(Q\\) and \\(R\\) matrices were positive definite, i.e. that all of their eigenvalues were positive.\nOne way to naively force some symmetric matrix \\(D\\) to be positive definite is to set any non-positive eigenvalues to some small positive value \\(\\varepsilon &gt; 0\\). Recall that any real symmetric matrix \\(D \\in \\mathbb{R}^{n \\times n}\\) has an basis of eigenvectors \\(u_1, \\dots, u_n\\) with corresponding eigenvalues \\(\\lambda_1, \\dots, \\lambda_n\\) such that \\(D u_i = \\lambda_i u_i\\). Then we can construct the positive definite approximation by\n\\[\n\\widetilde{D} = \\left( \\sum_{i=1, \\dots, n \\mid \\lambda_i &gt; 0} \\lambda_i u_i u_i^\\top \\right) + \\varepsilon I.\n\\]\nExercise: Convince yourself that \\(\\widetilde{D}\\) is indeed positive definite.\nNote that Hessian matrices are generally symmetric, so we can apply this process to \\(Q\\) and \\(R\\) to obtain the positive definite approximations \\(\\widetilde{Q}\\) and \\(\\widetilde{R}\\). Now that we have an upward-curved quadratic approximation to the cost function, and a linear approximation to the state transitions, we can simply apply the time-homogenous LQR methods from Section 2.4.\nBut what happens when we enter states far away from \\(s^\\star\\) or want to use actions far from \\(a^\\star\\)? A Taylor approximation is only accurate in a local region around the point of linearization, so the performance of our LQR controller will degrade as we move further away. We’ll see how to address this in the next section using the iterative LQR algorithm.\n\n\n\nLocal linearization might only be accurate in a small region around the point of linearization\n\n\n\n\n2.6.4 Iterative LQR\nTo address these issues with local linearization, we’ll use an iterative approach, where we repeatedly linearize around different points to create a time-dependent approximation of the dynamics, and then solve the resulting time-dependent LQR problem to obtain a better policy. This is known as iterative LQR or iLQR:\n\nDefinition 2.9 (Iterative LQR) For each iteration of the algorithm:\n\nForm a time-dependent LQR problem around the current candidate trajectory using local linearization.\nCompute the optimal policy using Section 2.5.1.\nGenerate a new series of actions using this policy.\nCompute a better candidate trajectory by interpolating between the current and proposed actions.\n\n\nNow let’s go through the details of each step. We’ll use superscripts to denote the iteration of the algorithm. We’ll also denote \\(\\bar s_0 = \\mathbb{E}_{s_0 \\sim \\mu_0} [s_0]\\) as the expected initial state.\nAt iteration \\(i\\) of the algorithm, we begin with a candidate trajectory \\(\\bar \\tau^i = (\\bar s^i_0, \\bar a^i_0, \\dots, \\bar s^i_{H-1}, \\bar a^i_{H-1})\\).\nStep 1: Form a time-dependent LQR problem. At each timestep \\(h\\in [H]\\), we use the techniques from  to linearize the dynamics and quadratize the cost function around \\((\\bar s^i_h, \\bar a^i_h)\\):\n\\[\n\\begin{aligned}\n    f_h(s, a) & \\approx f(\\bar {s}^i_h, \\bar {a}^i_h) + \\nabla_{s} f(\\bar {s}^i_h, \\bar {a}^i_h)(s- \\bar {s}^i_h) + \\nabla_{a} f(\\bar {s}^i_h, \\bar {a}^i_h)(a- \\bar {a}^i_h)                         \\\\\n    c_h(s, a) & \\approx c(\\bar {s}^i_h, \\bar {a}^i_h) + \\begin{bmatrix}\n                                                              s- \\bar {s}^i_h& a- \\bar {a}^i_h\n                                                          \\end{bmatrix} \\begin{bmatrix}\n                                                                            \\nabla_{s} c(\\bar {s}^i_h, \\bar {a}^i_h)\\\\\n                                                                            \\nabla_{a} c(\\bar {s}^i_h, \\bar {a}^i_h)\n                                                                        \\end{bmatrix}                                                      \\\\\n                     & \\qquad + \\frac{1}{2} \\begin{bmatrix}\n                                                s- \\bar {s}^i_h& a- \\bar {a}^i_h\n                                            \\end{bmatrix} \\begin{bmatrix}\n                                                              \\nabla_{ss} c(\\bar {s}^i_h, \\bar {a}^i_h)  & \\nabla_{sa} c(\\bar {s}^i_h, \\bar {a}^i_h)  \\\\\n                                                              \\nabla_{as} c(\\bar {s}^i_h, \\bar {a}^i_h) & \\nabla_{aa} c(\\bar {s}^i_h, \\bar {a}^i_h)\n                                                          \\end{bmatrix}\n    \\begin{bmatrix}\n        s- \\bar {s}^i_h\\\\\n        a- \\bar {a}^i_h\n    \\end{bmatrix}.\n\\end{aligned}\n\\]\nStep 2: Compute the optimal policy. We can now solve the time-dependent LQR problem using the Riccati equation from Section 2.5.1 to compute the optimal policy \\(\\pi^i_0, \\dots, \\pi^i_{H-1}\\).\nStep 3: Generate a new series of actions. We can then generate a new sample trajectory by taking actions according to this optimal policy:\n\\[\n\\bar s^{i+1}_0 = \\bar s_0, \\qquad \\widetilde a_h= \\pi^i_h(\\bar s^{i+1}_h), \\qquad \\bar s^{i+1}_{h+1} = f(\\bar s^{i+1}_h, \\widetilde a_h).\n\\]\nNote that the states are sampled according to the true dynamics, which we assume we have query access to.\nStep 4: Compute a better candidate trajectory., Note that we’ve denoted these actions as \\(\\widetilde a_h\\) and aren’t directly using them for the next iteration \\(\\bar a^{i+1}_h\\). Rather, we want to interpolate between them and the actions from the previous iteration \\(\\bar a^i_0, \\dots, \\bar a^i_{H-1}\\). This is so that the cost will increase monotonically, since if the new policy turns out to actually be worse, we can stay closer to the previous trajectory. (Can you think of an intuitive example where this might happen?)\nFormally, we want to find \\(\\alpha \\in [0, 1]\\) to generate the next iteration of actions \\(\\bar a^{i+1}_0, \\dots, \\bar a^{i+1}_{H-1}\\) such that the cost is minimized:\n\\[\n\\begin{aligned}\n    \\min_{\\alpha \\in [0, 1]} \\quad & \\sum_{h=0}^{H-1} c(s_h, \\bar a^{i+1}_h)                     \\\\\n    \\text{where} \\quad             & s_{h+1} = f(s_h, \\bar a^{i+1}_h)                             \\\\\n                                   & \\bar a^{i+1}_h= \\alpha \\bar a^i_h+ (1-\\alpha) \\widetilde a_h\\\\\n                                   & s_0 = \\bar s_0.\n\\end{aligned}\n\\]\nNote that this optimizes over the closed interval \\([0, 1]\\), so by the Extreme Value Theorem, it’s guaranteed to have a global maximum.\nThe final output of this algorithm is a policy \\(\\pi^{n_\\text{steps}}\\) derived after \\(n_\\text{steps}\\) of the algorithm. Though the proof is somewhat complex, one can show that for many nonlinear control problems, this solution converges to a locally optimal solution (in the policy space).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "control.html#summary",
    "href": "control.html#summary",
    "title": "2  Linear Quadratic Regulators",
    "section": "2.7 Summary",
    "text": "2.7 Summary\nThis chapter introduced some approaches to solving different variants of the optimal control problem Definition 2.1. We began with the simple case of linear dynamics and an upward-curved quadratic cost. This model is called the LQR and we solved for the optimal policy using dynamic programming. We then extended these results to the more general nonlinear case via local linearization. We finally saw the iterative LQR algorithm for solving nonlinear control problems.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear Quadratic Regulators</span>"
    ]
  },
  {
    "objectID": "bandits.html",
    "href": "bandits.html",
    "title": "3  Multi-Armed Bandits",
    "section": "",
    "text": "3.1 Introduction\nThe multi-armed bandits (MAB) setting is a simple setting for studying the basic challenges of sequential decision-making. In this setting, an agent repeatedly chooses from a fixed set of actions, called arms, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period.\nIn particular, we’ll spend a lot of time discussing the Exploration-Exploitation Tradeoff: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?\nIn this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.\nCode\nfrom jaxtyping import Float, Array\nimport numpy as np\nimport latexify\nfrom typing import Callable, Union\nimport matplotlib.pyplot as plt\n\nimport solutions.bandits as solutions\n\nnp.random.seed(184)\n\ndef random_argmax(ary: Array) -&gt; int:\n    \"\"\"Take an argmax and randomize between ties.\"\"\"\n    max_idx = np.flatnonzero(ary == ary.max())\n    return np.random.choice(max_idx).item()\n\n\n# used as decorator\nlatex = latexify.algorithmic(\n    trim_prefixes={\"mab\"},\n    id_to_latex={\"arm\": \"a_t\", \"reward\": \"r\", \"means\": \"\\mu\"},\n    use_math_symbols=True,\n)\nLet \\(K\\) denote the number of arms. We’ll label them \\(0, \\dots, K-1\\) and use superscripts to indicate the arm index; since we seldom need to raise a number to a power, this won’t cause much confusion. In this chapter, we’ll consider the Bernoulli bandit setting from the examples above, where arm \\(k\\) either returns reward \\(1\\) with probability \\(\\mu^k\\) or \\(0\\) otherwise. The agent gets to pull an arm \\(T\\) times in total. We can formalize the Bernoulli bandit in the following Python code:\nCode\nclass MAB:\n    \"\"\"The Bernoulli multi-armed bandit environment.\n\n    :param means: the means (success probabilities) of the reward distributions for each arm\n    :param T: the time horizon\n    \"\"\"\n\n    def __init__(self, means: Float[Array, \" K\"], T: int):\n        assert all(0 &lt;= p &lt;= 1 for p in means)\n        self.means = means\n        self.T = T\n        self.K = self.means.size\n        self.best_arm = random_argmax(self.means)\n\n    def pull(self, k: int) -&gt; int:\n        \"\"\"Pull the `k`-th arm and sample from its (Bernoulli) reward distribution.\"\"\"\n        reward = np.random.rand() &lt; self.means[k].item()\n        return +reward\nCode\nmab = MAB(means=np.array([0.1, 0.8, 0.4]), T=100)\nIn pseudocode, the agent’s interaction with the MAB environment can be described by the following process:\nCode\n@latex\ndef mab_loop(mab: MAB, agent: \"Agent\") -&gt; int:\n    for t in range(mab.T):\n        arm = agent.choose_arm()  # in 0, ..., K-1\n        reward = mab.pull(arm)\n        agent.update_history(arm, reward)\n\n\nmab_loop\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{mab\\_loop}(\\mathrm{mab}: \\mathrm{MAB}, \\mathrm{agent}: \\textrm{\"Agent\"}) \\\\ \\hspace{1em} \\mathbf{for} \\ t \\in \\mathrm{range} \\mathopen{}\\left( T \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} a_t \\gets \\mathrm{agent}.\\mathrm{choose\\_arm} \\mathopen{}\\left( \\mathclose{}\\right) \\\\ \\hspace{2em} r \\gets \\mathrm{pull} \\mathopen{}\\left( a_t \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{agent}.\\mathrm{update\\_history} \\mathopen{}\\left( a_t, r \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\nThe Agent class stores the pull history and uses it to decide which arm to pull next. Since we are working with Bernoulli bandits, we can summarize the pull history concisely in a \\(\\mathbb{N}^{K \\times 2}\\) array.\nCode\nclass Agent:\n    def __init__(self, K: int, T: int):\n        \"\"\"The MAB agent that decides how to choose an arm given the past history.\"\"\"\n        self.K = K\n        self.T = T\n        self.rewards = []  # for plotting\n        self.choices = []\n        self.history = np.zeros((K, 2), dtype=int)\n\n    def choose_arm(self) -&gt; int:\n        \"\"\"Choose an arm of the MAB. Algorithm-specific.\"\"\"\n        ...\n\n    def count(self) -&gt; int:\n        \"\"\"The number of pulls made. Also the current step index.\"\"\"\n        return len(self.rewards)\n\n    def update_history(self, arm: int, reward: int):\n        self.rewards.append(reward)\n        self.choices.append(arm)\n        self.history[arm, reward] += 1\nWhat’s the optimal strategy for the agent, i.e. the one that achieves the highest expected reward? Convince yourself that the agent should try to always pull the arm with the highest expected reward:\n\\[\n\\mu^\\star := \\max_{k \\in [K]} \\mu^k.\n\\]\nThe goal, then, can be rephrased as to minimize the regret, defined below:\nCode\ndef regret_per_step(mab: MAB, agent: Agent):\n    \"\"\"Get the difference from the average reward of the optimal arm. The sum of these is the regret.\"\"\"\n    return [mab.means[mab.best_arm] - mab.means[arm] for arm in agent.choices]\nNote that this depends on the true means of the pulled arms, not the actual observed rewards. We typically think of this as a random variable where the randomness comes from the agent’s strategy (i.e. the sequence of actions \\(a_0, \\dots, a_{T-1}\\)).\nThroughout the chapter, we will try to upper bound the regret of various algorithms in two different senses:\nNote that these two different approaches say very different things about the regret. The first approach says that the average regret is at most \\(M_T\\). However, the agent might still achieve higher regret on many runs. The second approach says that, with high probability, the agent will achieve regret at most \\(M_{T, \\delta}\\). However, it doesn’t say anything about the regret in the remaining \\(\\delta\\) fraction of runs, which might be arbitrarily high.\nWe’d like to achieve sublinear regret in expectation, i.e. \\(\\mathbb{E}[\\text{Regret}_T] = o(T)\\). That is, as we learn more about the environment, we’d like to be able to exploit that knowledge to take the optimal arm as often as possible.\nThe rest of the chapter comprises a series of increasingly sophisticated MAB algorithms.\nCode\ndef plot_strategy(mab: MAB, agent: Agent):\n    plt.figure(figsize=(10, 6))\n\n    # plot reward and cumulative regret\n    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label=\"reward\")\n    cum_regret = np.cumsum(regret_per_step(mab, agent))\n    plt.plot(np.arange(mab.T), cum_regret, label=\"cumulative regret\")\n\n    # draw colored circles for arm choices\n    colors = [\"red\", \"green\", \"blue\"]\n    color_array = [colors[k] for k in agent.choices]\n    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c=color_array, label=\"arm\")\n\n    # labels and title\n    plt.xlabel(\"timestep\")\n    plt.legend()\n    plt.title(f\"{agent.__class__.__name__} reward and regret\")\n    plt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#introduction",
    "href": "bandits.html#introduction",
    "title": "3  Multi-Armed Bandits",
    "section": "",
    "text": "Example 3.1 (Online advertising) Let’s suppose you, the agent, are an advertising company. You have \\(K\\) different ads that you can show to users; For concreteness, let’s suppose there’s just a single user. You receive \\(1\\) reward if the user clicks the ad, and \\(0\\) otherwise. Thus, the unknown reward distribution associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.\n\n\nExample 3.2 (Clinical trials) Suppose you’re a pharmaceutical company, and you’re testing a new drug. You have \\(K\\) different dosages of the drug that you can administer to patients. You receive \\(1\\) reward if the patient recovers, and \\(0\\) otherwise. Thus, the unknown reward distribution associated to each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.\n\n\n\n\nRemark 3.1 (Namesake). The name “multi-armed bandits” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.\n\n\n\n\n\n\n\n\n\n\n\n\nDefinition 3.1 (Regret) The agent’s regret after \\(T\\) timesteps is defined as\n\\[\n\\text{Regret}_T := \\sum_{t=0}^{T-1} \\mu^\\star - \\mu^{a_t}.\n\\]\n\n\n\n\n\nUpper bound the expected regret, i.e. show \\(\\mathbb{E}[\\text{Regret}_T] \\le M_T\\).\nFind a high-probability upper bound on the regret, i.e. show \\(\\mathbb{P}(\\text{Regret}_T \\le M_{T, \\delta}) \\ge 1-\\delta\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-pure-exploration",
    "href": "bandits.html#sec-pure-exploration",
    "title": "3  Multi-Armed Bandits",
    "section": "3.2 Pure exploration",
    "text": "3.2 Pure exploration\nA trivial strategy is to always choose arms at random (i.e. “pure exploration”).\n\n\nCode\nclass PureExploration(Agent):\n    def choose_arm(self):\n        \"\"\"Choose an arm uniformly at random.\"\"\"\n        return solutions.pure_exploration_choose_arm(self)\n\n\nNote that\n\\[\n\\mathbb{E}_{a_t \\sim \\text{Unif}([K])}[\\mu^{a_t}] = \\bar \\mu = \\frac{1}{K} \\sum_{k=1}^K \\mu^k\n\\]\nso the expected regret is simply\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\text{Regret}_T] &= \\sum_{t=0}^{T-1} \\mathbb{E}[\\mu^\\star - \\mu^{a_t}] \\\\\n    &= T (\\mu^\\star - \\bar \\mu) &gt; 0.\n\\end{aligned}\n\\]\nThis scales as \\(\\Theta(T)\\), i.e. linear in the number of timesteps \\(T\\). There’s no learning here: the agent doesn’t use any information about the environment to improve its strategy. You can see that the distribution over its arm choices always appears “(uniformly) random”.\n\n\nCode\nagent = PureExploration(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-pure-greedy",
    "href": "bandits.html#sec-pure-greedy",
    "title": "3  Multi-Armed Bandits",
    "section": "3.3 Pure greedy",
    "text": "3.3 Pure greedy\nHow might we improve on pure exploration? Instead, we could try each arm once, and then commit to the one with the highest observed reward. We’ll call this the pure greedy strategy.\n\n\nCode\nclass PureGreedy(Agent):\n    def choose_arm(self):\n        \"\"\"Choose the arm with the highest observed reward on its first pull.\"\"\"\n        return solutions.pure_greedy_choose_arm(self)\n\n\nNote we’ve used superscripts \\(r^k\\) during the exploration phase to indicate that we observe exactly one reward for each arm. Then we use subscripts \\(r_t\\) during the exploitation phase to indicate that we observe a sequence of rewards from the chosen greedy arm \\(\\hat k\\).\nHow does the expected regret of this strategy compare to that of pure exploration? We’ll do a more general analysis in the following section. Now, for intuition, suppose there’s just \\(K=2\\) arms, with Bernoulli reward distributions with means \\(\\mu^0 &gt; \\mu^1\\).\nLet’s let \\(r^0\\) be the random reward from the first arm and \\(r^1\\) be the random reward from the second. If \\(r^0 &gt; r^1\\), then we achieve zero regret. Otherwise, we achieve regret \\(T(\\mu^0 - \\mu^1)\\). Thus, the expected regret is simply:\n\\[\n\\begin{aligned}\n    \\mathbb{E}[\\text{Regret}_T] &= \\mathbb{P}(r^0 &lt; r^1) \\cdot T(\\mu^0 - \\mu^1) + c \\\\\n    &= (1 - \\mu^0) \\mu^1 \\cdot T(\\mu^0 - \\mu^1) + c\n\\end{aligned}\n\\]\nWhich is still \\(\\Theta(T)\\), the same as pure exploration!\n\n\nCode\nagent = PureGreedy(mab.K, mab.T)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\n\nThe cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, if the greedy algorithm happens to get lucky on the first set of pulls, it may act entirely optimally for that episode! But its average regret is what measures its effectiveness.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-etc",
    "href": "bandits.html#sec-etc",
    "title": "3  Multi-Armed Bandits",
    "section": "3.4 Explore-then-commit",
    "text": "3.4 Explore-then-commit\nWe can improve the pure greedy algorithm as follows: let’s reduce the variance of the reward estimates by pulling each arm \\(N_{\\text{explore}}&gt; 1\\) times before committing. This is called the explore-then-commit strategy. Note that the “pure greedy” strategy above is just the special case where \\(N_{\\text{explore}}= 1\\).\n\n\nCode\nclass ExploreThenCommit(Agent):\n    def __init__(self, K: int, T: int, N_explore: int):\n        super().__init__(K, T)\n        self.N_explore = N_explore\n\n    def choose_arm(self):\n        return solutions.etc_choose_arm(self)\n\n\n\n\nCode\nagent = ExploreThenCommit(mab.K, mab.T, mab.T // 15)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\n\nNotice that now, the graphs are much more consistent, and the algorithm finds the true optimal arm and sticks with it much more frequently. We would expect ETC to then have a better (i.e. lower) average regret. Can we prove this?\n\n3.4.1 ETC regret analysis\nLet’s analyze the expected regret of the explore-then-commit strategy by splitting it up into the exploration and exploitation phases.\n\n3.4.1.1 Exploration phase.\nThis phase takes \\(N_{\\text{explore}}K\\) timesteps. Since at each step we incur at most \\(1\\) regret, the total regret is at most \\(N_{\\text{explore}}K\\).\n\n\n3.4.1.2 Exploitation phase.\nThis will take a bit more effort. We’ll prove that for any total time \\(T\\), we can choose \\(N_{\\text{explore}}\\) such that with arbitrarily high probability, the regret is sublinear.\nLet \\(\\hat k\\) denote the arm chosen after the exploration phase. We know the regret from the exploitation phase is\n\\[\nT_{\\text{exploit}} (\\mu^\\star - \\mu^{\\hat k}) \\qquad \\text{where} \\qquad T_{\\text{exploit}} := T - N_{\\text{explore}}K.\n\\]\nSo we’d like to bound \\(\\mu^\\star - \\mu^{\\hat k} = o(1)\\) (as a function of \\(T\\)) in order to achieve sublinear regret. How can we do this?\nLet’s define \\(\\Delta^k = \\hat \\mu^k - \\mu^k\\) to denote how far the mean estimate for arm \\(k\\) is from the true mean. How can we bound this quantity? We’ll use the following useful inequality for i.i.d. bounded random variables:\n\nTheorem 3.1 (Hoeffding’s inequality) Let \\(X_0, \\dots, X_{n-1}\\) be i.i.d. random variables with \\(X_i \\in [0, 1]\\) almost surely for each \\(i \\in [n]\\). Then for any \\(\\delta &gt; 0\\),\n\\[\n\\mathbb{P}\\left( \\left| \\frac{1}{n} \\sum_{i=1}^n (X_i - \\mathbb{E}[X_i]) \\right| &gt; \\sqrt{\\frac{\\ln(2/\\delta)}{2n}} \\right) \\le \\delta.\n\\]\n\nThe proof of this inequality is beyond the scope of this book. See Vershynin (2018) Chapter 2.2.\nWe can apply this directly to the rewards for a given arm \\(k\\), since the rewards from that arm are i.i.d.:\n\\[\n\\mathbb{P}\\left(|\\Delta^k | &gt; \\sqrt{\\frac{\\ln(2/\\delta)}{2N_{\\text{explore}}}} \\right) \\le \\delta.\n\\tag{3.1}\\]\nBut note that we can’t apply this to arm \\(\\hat k\\) directly since \\(\\hat k\\) is itself a random variable. Instead, we need to “uniform-ize” this bound across all the arms, i.e. bound the error across all the arms simultaneously, so that the resulting bound will apply no matter what \\(\\hat k\\) “crystallizes” to.\nThe union bound provides a simple way to do this:\n\nTheorem 3.2 (Union bound) Consider a set of events \\(A_0, \\dots, A_{n-1}\\). Then\n\\[\n\\mathbb{P}(\\exists i \\in [n]. A_i) \\le \\sum_{i=0}^{n-1} \\mathbb{P}(A_i).\n\\]\nIn particular, if \\(\\mathbb{P}(A_i) \\ge 1 - \\delta\\) for each \\(i \\in [n]\\), we have\n\\[\n\\mathbb{P}(\\forall i \\in [n]. A_i) \\ge 1 - n \\delta.\n\\]\n\nExercise: Prove the second statement above.\nApplying the union bound across the arms for the l.h.s. event of Equation 3.1, we have\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left( \\forall k \\in [K], |\\Delta^k | \\le \\sqrt{\\frac{\\ln(2/\\delta)}{2N_{\\text{explore}}}} \\right) &\\ge 1-K\\delta\n\\end{aligned}\n\\]\nThen to apply this bound to \\(\\hat k\\) in particular, we can apply the useful trick of “adding zero”:\n\\[\n\\begin{aligned}\n    \\mu^{k^\\star} - \\mu^{\\hat k} &= \\mu^{k^\\star} - \\mu^{\\hat k} + (\\hat \\mu^{k^\\star} - \\hat \\mu^{k^\\star}) + (\\hat \\mu^{\\hat k} - \\hat \\mu^{\\hat k}) \\\\\n    &= \\Delta^{\\hat k} - \\Delta^{k^*} + \\underbrace{(\\hat \\mu^{k^\\star} - \\hat \\mu^{\\hat k})}_{\\le 0 \\text{ by definition of } \\hat k} \\\\\n    &\\le 2 \\sqrt{\\frac{\\ln(2K/\\delta')}{2N_{\\text{explore}}}} \\text{ with probability at least } 1-\\delta'\n\\end{aligned}\n\\]\nwhere we’ve set \\(\\delta' = K\\delta\\). Putting this all together, we’ve shown that, with probability \\(1 - \\delta'\\),\n\\[\n\\text{Regret}_T \\le N_{\\text{explore}}K + T_{\\text{exploit}} \\cdot \\sqrt{\\frac{2\\ln(2K/\\delta')}{N_{\\text{explore}}}}.\n\\]\nNote that it suffices for \\(N_{\\text{explore}}\\) to be on the order of \\(\\sqrt{T}\\) to achieve sublinear regret. In particular, we can find the optimal \\(N_{\\text{explore}}\\) by setting the derivative of the r.h.s. to zero:\n\\[\n\\begin{aligned}\n    0 &= K - T_{\\text{exploit}} \\cdot \\frac{1}{2} \\sqrt{\\frac{2\\ln(2K/\\delta')}{N_{\\text{explore}}^3}} \\\\\n    N_{\\text{explore}}&= \\left( T_{\\text{exploit}} \\cdot \\frac{\\sqrt{\\ln(2K/\\delta')/2}}{K} \\right)^{2/3}\n\\end{aligned}\n\\]\nPlugging this into the expression for the regret, we have (still with probability \\(1-\\delta'\\))\n\\[\n\\begin{aligned}\n    \\text{Regret}_T &\\le 3 T^{2/3} \\sqrt[3]{K \\ln(2K/\\delta') / 2} \\\\\n    &= \\tilde{O}(T^{2/3} K^{1/3}).\n\\end{aligned}\n\\]\nThe ETC algorithm is rather “abrupt” in that it switches from exploration to exploitation after a fixed number of timesteps. In practice, it’s often better to use a more gradual transition, which brings us to the epsilon-greedy algorithm.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#epsilon-greedy",
    "href": "bandits.html#epsilon-greedy",
    "title": "3  Multi-Armed Bandits",
    "section": "3.5 Epsilon-greedy",
    "text": "3.5 Epsilon-greedy\nInstead of doing all of the exploration and then all of the exploitation separately – which additionally requires knowing the time horizon beforehand – we can instead interleave exploration and exploitation by, at each timestep, choosing a random action with some probability. We call this the epsilon-greedy algorithm.\n\n\nCode\nclass EpsilonGreedy(Agent):\n    def __init__(\n        self,\n        K: int,\n        T: int,\n        ε_array: Float[Array, \" T\"],\n    ):\n        super().__init__(K, T)\n        self.ε_array = ε_array\n\n    def choose_arm(self):\n        return solutions.epsilon_greedy_choose_arm(self)\n\n\n\n\nCode\nagent = EpsilonGreedy(mab.K, mab.T, np.full(mab.T, 0.1))\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\n\nNote that we let \\(\\epsilon\\) vary over time. In particular, we might want to gradually decrease \\(\\epsilon\\) as we learn more about the reward distributions and no longer need to spend time exploring.\n\nWhat is the expected regret of the algorithm if we set \\(\\epsilon\\) to be a constant?\n\nIt turns out that setting \\(\\epsilon_t = \\sqrt[3]{K \\ln(t)/t}\\) also achieves a regret of \\(\\tilde O(t^{2/3} K^{1/3})\\) (ignoring the logarithmic factors). (We will not prove this here; a proof can be found in (Agarwal et al. 2022).)\nIn ETC, we had to set \\(N_{\\text{explore}}\\) based on the total number of timesteps \\(T\\). But the epsilon-greedy algorithm actually handles the exploration automatically: the regret rate holds for any \\(t\\), and doesn’t depend on the final horizon \\(T\\).\nBut the way these algorithms explore is rather naive: we’ve been exploring uniformly across all the arms. But what if we could be smarter about it, and explore more for arms that we’re less certain about?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-ucb",
    "href": "bandits.html#sec-ucb",
    "title": "3  Multi-Armed Bandits",
    "section": "3.6 Upper Confidence Bound (UCB)",
    "text": "3.6 Upper Confidence Bound (UCB)\nTo quantify how certain we are about the mean of each arm, we’ll compute confidence intervals for our estimators, and then choose the arm with the highest upper confidence bound. This operates on the principle of the benefit of the doubt (i.e. optimism in the face of uncertainty): we’ll choose the arm that we’re most optimistic about.\nIn particular, for each arm \\(k\\) at time \\(t\\), we’d like to compute some upper confidence bound \\(M^k_t\\) such that \\(\\hat \\mu^k_t \\le M^k_t\\) with high probability, and then choose \\(a_t := \\arg \\max_{k \\in [K]} M^k_t\\). But how should we compute \\(M^k_t\\)?\nIn Section 3.4.1, we were able to compute this bound using Hoeffding’s inequality, which assumes that the number of samples is fixed. This was the case in ETC (where we pull each arm \\(N_{\\text{explore}}\\) times), but in UCB, the number of times we pull each arm depends on the agent’s actions, which in turn depend on the random rewards and are therefore stochastic. So we can’t use Hoeffding’s inequality directly.\nInstead, we’ll apply the same trick we used in the ETC analysis: we’ll use the union bound to compute a looser bound that holds uniformly across all timesteps and arms. Let’s introduce some notation to discuss this.\nLet \\(N^k_t\\) denote the (random) number of times arm \\(k\\) has been pulled within the first \\(t\\) timesteps, and \\(\\hat \\mu^k_t\\) denote the sample average of those pulls. That is,\n\\[\n\\begin{aligned}\n    N^k_t &:= \\sum_{\\tau=0}^{t-1} \\mathbf{1} \\{ a_\\tau = k \\} \\\\\n    \\hat \\mu^k_t &:= \\frac{1}{N^k_t} \\sum_{\\tau=0}^{t-1} \\mathbf{1} \\{ a_\\tau = k \\} r_\\tau.\n\\end{aligned}\n\\]\nTo achieve the “fixed sample size” assumption, we’ll need to shift our index from time to number of samples from each arm. In particular, we’ll define \\(\\tilde r^k_n\\) to be the \\(n\\)th sample from arm \\(k\\), and \\(\\tilde \\mu^k_n\\) to be the sample average of the first \\(n\\) samples from arm \\(k\\). Then, for a fixed \\(n\\), this satisfies the “fixed sample size” assumption, and we can apply Hoeffding’s inequality to get a bound on \\(\\tilde \\mu^k_n\\).\nSo how can we extend our bound on \\(\\tilde\\mu^k_n\\) to \\(\\hat \\mu^k_t\\)? Well, we know \\(N^k_t \\le t\\) (where equality would be the case if and only if we had pulled arm \\(k\\) every time). So we can apply the same trick as last time, where we uniform-ize across all possible values of \\(N^k_t\\):\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left( \\forall n \\le t, |\\tilde \\mu^k_n - \\mu^k | \\le \\sqrt{\\frac{\\ln(2/\\delta)}{2n}} \\right) &\\ge 1-t\\delta.\n\\end{aligned}\n\\]\nIn particular, since \\(N^k_t \\le t\\), and \\(\\tilde \\mu^k_{N^k_t} = \\hat \\mu^k_t\\) by definition, we have\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left( |\\hat \\mu^k_t - \\mu^k | \\le \\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}} \\right) &\\ge 1-\\delta' \\text{ where } \\delta' := t \\delta.\n\\end{aligned}\n\\]\nThis bound would then suffice for applying the UCB algorithm! That is, the upper confidence bound for arm \\(k\\) would be\n\\[\nM^k_t := \\hat \\mu^k_t + \\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}},\n\\]\nwhere we can choose \\(\\delta'\\) depending on how tight we want the interval to be.\n\nA smaller \\(\\delta'\\) would give us a larger and higher-confidence interval, emphasizing the exploration term.\nA larger \\(\\delta'\\) would give a tighter and lower-confidence interval, prioritizing the current sample averages.\n\nWe can now use this to define the UCB algorithm.\n\n\nCode\nclass UCB(Agent):\n    def __init__(self, K: int, T: int, delta: float):\n        super().__init__(K, T)\n        self.delta = delta\n\n    def choose_arm(self):\n        return solutions.ucb_choose_arm(self)\n\n\nIntuitively, UCB prioritizes arms where:\n\n\\(\\hat \\mu^k_t\\) is large, i.e. the arm has a high sample average, and we’d choose it for exploitation, and\n\\(\\sqrt{\\frac{\\ln(2t/\\delta')}{2N^k_t}}\\) is large, i.e. we’re still uncertain about the arm, and we’d choose it for exploration.\n\nAs desired, this explores in a smarter, adaptive way compared to the previous algorithms. Does it achieve lower regret?\n\n\nCode\nagent = UCB(mab.K, mab.T, 0.9)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\n\n\n3.6.1 UCB regret analysis\nFirst we’ll bound the regret incurred at each timestep. Then we’ll bound the total regret across timesteps.\nFor the sake of analysis, we’ll use a slightly looser bound that applies across the whole time horizon and across all arms. We’ll omit the derivation since it’s very similar to the above (walk through it yourself for practice).\n\\[\n\\begin{aligned}\n    \\mathbb{P}\\left(\\forall k \\le K, t &lt; T. |\\hat \\mu^k_t - \\mu^k | \\le B^k_t \\right) &\\ge 1-\\delta'' \\\\\n    \\text{where} \\quad B^k_t &:= \\sqrt{\\frac{\\ln(2TK/\\delta'')}{2N^k_t}}.\n\\end{aligned}\n\\]\nIntuitively, \\(B^k_t\\) denotes the width of the CI for arm \\(k\\) at time \\(t\\). Then, assuming the above uniform bound holds (which occurs with probability \\(1-\\delta''\\)), we can bound the regret at each timestep as follows:\n\\[\n\\begin{aligned}\n    \\mu^\\star - \\mu^{a_t} &\\le \\hat \\mu^{k^*}_t + B_t^{k^*} - \\mu^{a_t} && \\text{applying UCB to arm } k^\\star \\\\\n    &\\le \\hat \\mu^{a_t}_t + B^{a_t}_t - \\mu^{a_t} && \\text{since UCB chooses } a_t = \\arg \\max_{k \\in [K]} \\hat \\mu^k_t + B_t^{k} \\\\\n    &\\le 2 B^{a_t}_t && \\text{since } \\hat \\mu^{a_t}_t - \\mu^{a_t} \\le B^{a_t}_t \\text{ by definition of } B^{a_t}_t \\\\\n\\end{aligned}\n\\]\nSumming this across timesteps gives\n\\[\n\\begin{aligned}\n    \\text{Regret}_T &\\le \\sum_{t=0}^{T-1} 2 B^{a_t}_t \\\\\n    &= \\sqrt{2\\ln(2TK/\\delta'')} \\sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} \\\\\n    \\sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} &= \\sum_{t=0}^{T-1} \\sum_{k=1}^K \\mathbf{1}\\{ a_t = k \\} (N^k_t)^{-1/2} \\\\\n    &= \\sum_{k=1}^K \\sum_{n=1}^{N_T^k} n^{-1/2} \\\\\n    &\\le K \\sum_{n=1}^T n^{-1/2} \\\\\n    \\sum_{n=1}^T n^{-1/2} &\\le 1 + \\int_1^T x^{-1/2} \\ \\mathrm{d}x \\\\\n    &= 1 + (2 \\sqrt{x})_1^T \\\\\n    &= 2 \\sqrt{T} - 1 \\\\\n    &\\le 2 \\sqrt{T} \\\\\n\\end{aligned}\n\\]\nPutting everything together gives\n\\[\n\\begin{aligned}\n    \\text{Regret}_T &\\le 2 K \\sqrt{2T \\ln(2TK/\\delta'')} && \\text{with probability } 1-\\delta'' \\\\\n    &= \\tilde O(K\\sqrt{T})\n\\end{aligned}\n\\]\nIn fact, we can do a more sophisticated analysis to trim off a factor of \\(\\sqrt{K}\\) and show \\(\\text{Regret}_T = \\tilde O(\\sqrt{TK})\\).\n\n\n3.6.2 Lower bound on regret (intuition)\nIs it possible to do better than \\(\\Omega(\\sqrt{T})\\) in general? In fact, no! We can show that any algorithm must incur \\(\\Omega(\\sqrt{T})\\) regret in the worst case. We won’t rigorously prove this here, but the intuition is as follows.\nThe Central Limit Theorem tells us that with \\(T\\) i.i.d. samples from some distribution, we can only learn the mean of the distribution to within \\(\\Omega(1/\\sqrt{T})\\) (the standard deviation). Then, since we get \\(T\\) samples spread out across the arms, we can only learn each arm’s mean to an even looser degree.\nThat is, if two arms have means that are within about \\(1/\\sqrt{T}\\), we won’t be able to confidently tell them apart, and will sample them about equally. But then we’ll incur regret \\[\n\\Omega((T/2) \\cdot (1/\\sqrt{T})) = \\Omega(\\sqrt{T}).\n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#sec-thompson-sampling",
    "href": "bandits.html#sec-thompson-sampling",
    "title": "3  Multi-Armed Bandits",
    "section": "3.7 Thompson sampling and Bayesian bandits",
    "text": "3.7 Thompson sampling and Bayesian bandits\nSo far, we’ve treated the parameters \\(\\mu^0, \\dots, \\mu^{K-1}\\) of the reward distributions as fixed. Instead, we can take a Bayesian approach where we treat them as random variables from some prior distribution. Then, upon pulling an arm and observing a reward, we can simply condition on this observation to exactly describe the posterior distribution over the parameters. This fully describes the information we gain about the parameters from observing the reward.\nFrom this Bayesian perspective, the Thompson sampling algorithm follows naturally: just sample from the distribution of the optimal arm, given the observations!\n\n\nCode\nclass Distribution:\n    def sample(self) -&gt; Float[Array, \" K\"]:\n        \"\"\"Sample a vector of means for the K arms.\"\"\"\n        ...\n\n    def update(self, arm: int, reward: float):\n        \"\"\"Condition on obtaining `reward` from the given arm.\"\"\"\n        ...\n\n\n\n\nCode\nclass ThompsonSampling(Agent):\n    def __init__(self, K: int, T: int, prior: Distribution):\n        super().__init__(K, T)\n        self.distribution = prior\n\n    def choose_arm(self):\n        means = self.distribution.sample()\n        return random_argmax(means)\n\n    def update_history(self, arm: int, reward: int):\n        super().update_history(arm, reward)\n        self.distribution.update(arm, reward)\n\n\nIn other words, we sample each arm proportionally to how likely we think it is to be optimal, given the observations so far. This strikes a good exploration-exploitation tradeoff: we explore more for arms that we’re less certain about, and exploit more for arms that we’re more certain about. Thompson sampling is a simple yet powerful algorithm that achieves state-of-the-art performance in many settings.\n\nExample 3.3 (Bayesian Bernoulli bandit) We’ve been working in the Bernoulli bandit setting, where arm \\(k\\) yields a reward of \\(1\\) with probability \\(\\mu^k\\) and no reward otherwise. The vector of success probabilities \\(\\boldsymbol{\\mu} = (\\mu^1, \\dots, \\mu^K)\\) thus describes the entire MAB.\nUnder the Bayesian perspective, we think of \\(\\boldsymbol{\\mu}\\) as a random vector drawn from some prior distribution \\(\\pi(\\boldsymbol{\\mu})\\). For example, we might have \\(\\pi\\) be the Uniform distribution over the unit hypercube \\([0, 1]^K\\), that is,\n\\[\n\\pi(\\boldsymbol{\\mu}) = \\begin{cases}\n    1 & \\text{if } \\boldsymbol{\\mu}\\in [0, 1]^K \\\\\n    0 & \\text{otherwise}\n\\end{cases}\n\\]\nIn this case, upon viewing some reward, we can exactly calculate the posterior distribution of \\(\\boldsymbol{\\mu}\\) using Bayes’s rule (i.e. the definition of conditional probability):\n\\[\n\\begin{aligned}\n    \\mathbb{P}(\\boldsymbol{\\mu} \\mid a_0, r_0) &\\propto \\mathbb{P}(r_0 \\mid a_0, \\boldsymbol{\\mu}) \\mathbb{P}(a_0 \\mid \\boldsymbol{\\mu}) \\mathbb{P}(\\boldsymbol{\\mu}) \\\\\n    &\\propto (\\mu^{a_0})^{r_0} (1 - \\mu^{a_0})^{1-r_0}.\n\\end{aligned}\n\\]\nThis is the PDF of the \\(\\text{Beta}(1 + r_0, 1 + (1 - r_0))\\) distribution, which is a conjugate prior for the Bernoulli distribution. That is, if we start with a Beta prior on \\(\\mu^k\\) (note that \\(\\text{Unif}([0, 1]) = \\text{Beta}(1, 1)\\)), then the posterior, after conditioning on samples from \\(\\text{Bern}(\\mu^k)\\), will also be Beta. This is a very convenient property, since it means we can simply update the parameters of the Beta distribution upon observing a reward, rather than having to recompute the entire posterior distribution from scratch.\n\n\n\nCode\nclass Beta(Distribution):\n    def __init__(self, K: int, alpha: int = 1, beta: int = 1):\n        self.alphas = np.full(K, alpha)\n        self.betas = np.full(K, beta)\n\n    def sample(self):\n        return np.random.beta(self.alphas, self.betas)\n\n    def update(self, arm: int, reward: int):\n        self.alphas[arm] += reward\n        self.betas[arm] += 1 - reward\n\n\n\n\nCode\nbeta_distribution = Beta(mab.K)\nagent = ThompsonSampling(mab.K, mab.T, beta_distribution)\nmab_loop(mab, agent)\nplot_strategy(mab, agent)\n\n\n\n\n\n\n\n\n\nIt turns out that asymptotically, Thompson sampling is optimal in the following sense. Lai and Robbins (1985) prove an instance-dependent lower bound that says for any bandit algorithm,\n\\[\n\\liminf_{T \\to \\infty} \\frac{\\mathbb{E}[N_T^k]}{\\ln(T)} \\ge \\frac{1}{\\text{KL}(\\mu^k \\parallel \\mu^\\star)}\n\\]\nwhere\n\\[\n\\text{KL}(\\mu^k \\parallel \\mu^\\star) = \\mu^k \\ln \\frac{\\mu^k}{\\mu^\\star} + (1 - \\mu^k) \\ln \\frac{1 - \\mu^k}{1 - \\mu^\\star}\n\\]\nmeasures the Kullback-Leibler divergence from the Bernoulli distribution with mean \\(\\mu^k\\) to the Bernoulli distribution with mean \\(\\mu^\\star\\). It turns out that Thompson sampling achieves this lower bound with equality! That is, not only is the error rate optimal, but the constant factor is optimal as well.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#contextual-bandits",
    "href": "bandits.html#contextual-bandits",
    "title": "3  Multi-Armed Bandits",
    "section": "3.8 Contextual bandits",
    "text": "3.8 Contextual bandits\nIn the above MAB environment, the reward distributions of the arms remain constant. However, in many real-world settings, we might receive additional information that affects these distributions. For example, in the online advertising case where each arm corresponds to an ad we could show the user, we might receive information about the user’s preferences that changes how likely they are to click on a given ad. We can model such environments using contextual bandits.\n\nDefinition 3.2 (Contextual bandit) At each timestep \\(t\\), a new context \\(x_t\\) is drawn from some distribution \\(\\nu_{\\text{x}}\\). The learner gets to observe the context, and choose an action \\(a_t\\) according to some context-dependent policy \\(\\pi_t(x_t)\\). Then, the learner observes the reward from the chosen arm \\(r_t \\sim \\nu^{a_t}(x_t)\\). The reward distribution also depends on the context.\n\nAssuming our context is discrete, we can just perform the same algorithms, treating each context-arm pair as its own arm. This gives us an enlarged MAB of \\(K |\\mathcal{X}|\\) arms.\n\nWrite down the UCB algorithm for this enlarged MAB. That is, write an expression for \\(\\pi_t(x_t) = \\arg\\max_a \\dots\\).\n\nRecall that running UCB for \\(T\\) timesteps on an MAB with \\(K\\) arms achieves a regret bound of \\(\\tilde{O}(\\sqrt{TK})\\). So in this problem, we would achieve regret \\(\\tilde{O}(\\sqrt{TK|\\mathcal{X}|})\\) in the contextual MAB, which has a polynomial dependence on \\(|\\mathcal{X}|\\). But in a situation where we have large, or even infinitely many contexts, e.g. in the case where our context is a continuous value, this becomes intractable.\nNote that this “enlarged MAB” treats the different contexts as entirely unrelated to each other, while in practice, often contexts are related to each other in some way: for example, we might want to advertise similar products to users with similar preferences. How can we incorporate this structure into our solution?\n\n3.8.1 Linear contextual bandits\nWe want to model the mean reward of arm \\(k\\) as a function of the context, i.e. \\(\\mu^k(x)\\). One simple model is the linear one: \\(\\mu^k(x) = x^\\top \\theta^k\\), where \\(x \\in \\mathcal{X} = \\mathbb{R}^d\\) and \\(\\theta^k \\in \\mathbb{R}^d\\) describes a feature direction for arm \\(k\\). Recall that supervised learning gives us a way to estimate a conditional expectation from samples: We learn a least squares estimator from the timesteps where arm \\(k\\) was selected: \\[\n\\hat \\theta_t^k = \\arg\\min_{\\theta \\in \\mathbb{R}^d} \\sum_{\\{ i \\in [t] : a_i = k \\}} (r_i - x_i^\\top \\theta)^2.\n\\] This has the closed-form solution known as the ordinary least squares (OLS) estimator:\n\\[\n\\begin{aligned}\n    \\hat \\theta_t^k          & = (A_t^k)^{-1} \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i r_i \\\\\n    \\text{where} \\quad A_t^k & = \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i x_i^\\top.\n\\end{aligned}\n\\tag{3.2}\\]\nWe can now apply the UCB algorithm in this environment in order to balance exploration of new arms and exploitation of arms that we believe to have high reward. But how should we construct the upper confidence bound? Previously, we treated the pulls of an arm as i.i.d. samples and used Hoeffding’s inequality to bound the distance of the sample mean, our estimator, from the true mean. However, now our estimator is not a sample mean, but rather the OLS estimator above Equation 3.2. Instead, we’ll use Chebyshev’s inequality to construct an upper confidence bound.\n\nTheorem 3.3 (Chebyshev’s inequality) For a random variable \\(Y\\) such that \\(\\mathbb{E}Y = 0\\) and \\(\\mathbb{E}Y^2 = \\sigma^2\\), \\[\n|Y| \\le \\beta \\sigma \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\\]\n\nSince the OLS estimator is known to be unbiased (try proving this yourself), we can apply Chebyshev’s inequality to \\(x_t^\\top (\\hat \\theta_t^k - \\theta^k)\\):\n\\[\n\\begin{aligned}\n    x_t^\\top \\theta^k \\le x_t^\\top \\hat \\theta_t^k + \\beta \\sqrt{x_t^\\top (A_t^k)^{-1} x_t} \\quad \\text{with probability} \\ge 1 - \\frac{1}{\\beta^2}\n\\end{aligned}\n\\]\n\nWe haven’t explained why \\(x_t^\\top (A_t^k)^{-1} x_t\\) is the correct expression for the variance of \\(x_t^\\top \\hat \\theta_t^k\\). This result follows from some algebra on the definition of the OLS estimator Equation 3.2.\n\nThe first term is exactly our predicted reward \\(\\hat \\mu^k_t(x_t)\\). To interpret the second term, note that \\[\nx_t^\\top (A_t^k)^{-1} x_t = \\frac{1}{N_t^k} x_t^\\top (\\Sigma_t^k)^{-1} x_t,\n\\] where \\[\n\\Sigma_t^k = \\frac{1}{N_t^k} \\sum_{\\{ i \\in [t] : a_i = k \\}} x_i x_i^\\top\n\\] is the empirical covariance matrix of the contexts (assuming that the context has mean zero). That is, the learner is encouraged to choose arms when \\(x_t\\) is not aligned with the data seen so far, or if arm \\(k\\) has not been explored much and so \\(N_t^k\\) is small.\nWe can now substitute these quantities into UCB to get the LinUCB algorithm:\n\n\nCode\nclass LinUCBPseudocode(Agent):\n    def __init__(\n        self, K: int, T: int, D: int, lam: float, get_c: Callable[[int], float]\n    ):\n        super().__init__(K, T)\n        self.lam = lam\n        self.get_c = get_c\n        self.contexts = [None for _ in range(K)]\n        self.A = np.repeat(lam * np.eye(D)[...], K)  # regularization\n        self.targets = np.zeros(K, D)\n        self.w = np.zeros(K, D)\n\n    def choose_arm(self, context: Float[Array, \" D\"]):\n        c = self.get_c(self.count)\n        scores = self.w @ context + c * np.sqrt(\n            context.T @ np.linalg.solve(self.A, context)\n        )\n        return random_argmax(scores)\n\n    def update_history(self, context: Float[Array, \" D\"], arm: int, reward: int):\n        self.A[arm] += np.outer(context, context)\n        self.targets[arm] += context * reward\n        self.w[arm] = np.linalg.solve(self.A[arm], self.targets[arm])\n\n\n\nNote that the matrix \\(A_t^k\\) above might not be invertible. When does this occur? One way to address this is to include a \\(\\lambda I\\) regularization term to ensure that \\(A_t^k\\) is invertible. This is equivalent to solving a ridge regression problem instead of the unregularized least squares problem. Implement this solution.\n\n\\(c_t\\) is similar to the \\(\\log (2t/\\delta')\\) term of UCB: It controls the width of the confidence interval. Here, we treat it as a tunable parameter, though in a theoretical analysis, it would depend on \\(A_t^k\\) and the probability \\(\\delta\\) with which the bound holds.\nUsing similar tools for UCB, we can also prove an \\(\\tilde{O}(\\sqrt{T})\\) regret bound. The full details of the analysis can be found in Section 3 of (Agarwal et al. 2022).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "bandits.html#summary",
    "href": "bandits.html#summary",
    "title": "3  Multi-Armed Bandits",
    "section": "3.9 Summary",
    "text": "3.9 Summary\nIn this chapter, we explored the multi-armed bandit setting for analyzing sequential decision-making in an unknown environment. An MAB consists of multiple arms, each with an unknown reward distribution. The agent’s task is to learn about these through interaction, eventually minimizing the regret, which measures how suboptimal the chosen arms were.\nWe saw algorithms such as upper confidence bound and Thompson sampling that handle the tradeoff between exploration and exploitation, that is, the tradeoff between choosing arms that the agent is uncertain about and arms the agent already supposes are be good.\nWe finally discussed contextual bandits, in which the agent gets to observe some context that affects the reward distributions. We can approach these problems through supervised learning approaches.\n\n\n\n\nAgarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. Reinforcement Learning: Theory and Algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf.\n\n\nLai, T. L, and Herbert Robbins. 1985. “Asymptotically Efficient Adaptive Allocation Rules.” Advances in Applied Mathematics 6 (1): 4–22. https://doi.org/10.1016/0196-8858(85)90002-8.\n\n\nVershynin, Roman. 2018. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press. https://books.google.com?id=NDdqDwAAQBAJ.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Multi-Armed Bandits</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html",
    "href": "supervised_learning.html",
    "title": "4  Supervised learning",
    "section": "",
    "text": "4.1 Introduction\nCode\nfrom utils import Float, Array, Callable, plt, np, latex\nfrom torchvision.datasets import MNIST\nfrom config import MNIST_PATH\nSupervised learning (SL) is a core subfield of machine learning alongside RL and unsupervised learning. The typical SL task is to approximate an unknown function given a dataset of input-output pairs from that function.\nWhere might function approximation be useful in RL? There are many functions involved in the definition of an MDP (Definition 1.2), such as the state transitions \\(P\\) or the reward function \\(r\\), any of which might be unknown. We can plug in an SL algorithm to model these functions, and then solve the modeled environment using dynamic programming (Section 1.3.2). This approach is called fitted DP and will be covered in Chapter 5. In the rest of this chapter, we’ll formalize the SL task and examine some basic algorithms.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#introduction",
    "href": "supervised_learning.html#introduction",
    "title": "4  Supervised learning",
    "section": "",
    "text": "Example 4.1 (Image classification) One of the most common examples of an SL problem is the task of image classification: Given a dataset of images and their respective labels, construct a function that takes an image and outputs the correct label.\nFigure 4.1 illustrates two samples (that is, input-output pairs) from the MNIST database of handwritten digits (Deng 2012). This is a task that most humans can easily accomplish. By providing many samples of digits and their labels to a machine, SL algorithms can learn to solve this task as well.\n\n\nCode\ndata = MNIST(MNIST_PATH, train=True, download=True)\n\nplt.axis('off')\nplt.imshow(data.data[0], cmap='gray')\nplt.title(f\"Label: {data.targets[0]}\")\nplt.gcf().set_size_inches(2, 2)\nplt.show()\n\nplt.axis('off')\nplt.imshow(data.data[1], cmap='gray')\nplt.title(f\"Label: {data.targets[1]}\")\nplt.gcf().set_size_inches(2, 2)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) A handwritten digit.\n\n\n\n\n\n\n\n\n\n\n\n(b) Another handwritten digit.\n\n\n\n\n\n\n\nFigure 4.1: The MNIST image classification dataset of handwritten digits.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#the-supervised-learning-task",
    "href": "supervised_learning.html#the-supervised-learning-task",
    "title": "4  Supervised learning",
    "section": "4.2 The supervised learning task",
    "text": "4.2 The supervised learning task\nIn SL, we are given a dataset of labelled samples \\((x_1, y_1), \\dots, (x_N, y_N)\\) that are independently sampled from some joint distribution \\(p \\in \\triangle(X \\times Y)\\) known as the data generating process. Note that, by the chain rule of probability, this can be factored as \\(p(x, y) = p(y \\mid x) p(x)\\).\n\nExample 4.2 For example, in Example 4.1, the marginal distribution over \\(x\\) is assumed to be the distribution of handwritten digits by humans, scanned as \\(28 \\times 28\\) grayscale images, and the conditional distribution \\(y \\mid x\\) is assumed to be the distribution over \\(\\{ 0, \\dots, 9 \\}\\) that a human would assign to the image \\(x\\).\n\nOur task is to compute a “good” predictor \\(\\hat f : X \\to Y\\) that, as its name suggests, takes an input and tries to predict the corresponding output.\n\n4.2.1 Loss functions\nHow can we measure how “good” a predictor is? The most common way is to use a loss function \\(\\ell : Y \\times Y \\to \\mathbb{R}\\) that compares the guess \\(\\hat y := \\hat f(x)\\) with the true output \\(y\\). \\(\\ell(\\hat y, y)\\) should be low if the predictor accurately guessed the output, and high if the prediction was incorrect.\n\nExample 4.3 (Zero-one loss) In the image classification task Example 4.1, we have \\(X = [0, 1]^{28 \\times 28}\\) (the space of \\(28\\)-by-\\(28\\) grayscale images) and \\(Y = \\{ 0, \\dots, 9 \\}\\) (the image’s label). We could use the zero-one loss function,\n\\[\n\\ell(\\hat y, y) = \\begin{cases}\n0 & \\hat y = y \\\\\n1 & \\hat y \\ne y\n\\end{cases}\n\\]\nto measure the accuracy of the predictor. That is, if the predictor assigns the wrong label to an image, it incurs a loss of one for that sample.\n\n\nExample 4.4 (Square loss) For a continuous output (i.e. \\(Y \\subseteq \\mathbb{R}\\)), we typically use the squared difference as the loss function:\n\\[\n\\ell(\\hat y, y) = (\\hat y - y)^2\n\\]\nThe squared loss is nice to work with analytically since its derivative with respect to \\(\\hat y\\) is simply \\(2 (\\hat y - y)\\). (Sometimes authors define the square loss as half of the above value to cancel the factor of \\(2\\) in the derivative; generally speaking, scaling the loss by some constant scalar has no practical effect.)\n\n\n\nCode\nx = np.linspace(-1, 1, 20)\ny = x ** 2\nplt.plot(x, y)\nplt.xlabel(r\"$\\hat y - y$\")\nplt.ylabel(r\"$\\ell(\\hat y, y)$\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4.2: Squared loss.\n\n\n\n\n\n\n\n4.2.2 Model selection\nUltimately, we want a predictor that does well on new, unseen samples from the data generating process. We can thus ask, how much loss does the predictor incur in expectation? This is called the prediction’s generalization error or test error:\n\\[\n\\text{test error}(\\hat f) := \\mathbb{E}_{(x, y) \\sim p} [ \\ell(\\hat f(x), y) ]\n\\]\nOur goal is then to find the function \\(\\hat f\\) that minimizes the test error. For certain loss functions, this can be analytically computed, such as for squared error.\n\nTheorem 4.1 (The conditional expectation minimizes mean squared error) An important result is that, under the squared loss, the optimal predictor is the conditional expectation:\n\\[\n\\arg\\min_{f} \\mathbb{E}[(y - f(x))^2] = (x \\mapsto \\mathbb{E}[y \\mid x])\n\\]\n\n\nProof. We can decompose the mean squared error as\n\\[\n\\begin{aligned}\n\\mathbb{E}[(y - f(x))^2] &= \\mathbb{E}[ (y - \\mathbb{E}[y \\mid x] + \\mathbb{E}[y \\mid x] - f(x))^2 ] \\\\\n&= \\mathbb{E}[ (y - \\mathbb{E}[y \\mid x])^2 ] + \\mathbb{E}[ (\\mathbb{E}[y \\mid x] - f(x))^2 ] \\\\\n&\\quad {} + 2 \\mathbb{E}[ (y - \\mathbb{E}[y \\mid x])(\\mathbb{E}[y \\mid x] - f(x)) ] \\\\\n\\end{aligned}\n\\]\nWe leave it as an exercise to show that the last term is zero. (Hint: use the law of iterated expectations.) The first term is the noise, or irreducible error, that doesn’t depend on \\(f\\), and the second term is the error due to the approximation, which is minimized at \\(0\\) when \\(f(x) = \\mathbb{E}[y \\mid x]\\).\n\nIn most applications, such as in Example 4.2, the joint distribution of \\(x, y\\) is intractable to compute, and so we can’t evaluate \\(\\mathbb{E}[y \\mid x]\\) analytically. Instead, all we have are \\(N\\) samples from the joint distribution of \\(x\\) and \\(y\\). How might we use these to approximate the generalization error?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#sec-erm",
    "href": "supervised_learning.html#sec-erm",
    "title": "4  Supervised learning",
    "section": "4.3 Empirical risk minimization",
    "text": "4.3 Empirical risk minimization\nTo estimate the generalization error, we can simply take the sample average of the loss over the training data. This is called the training loss or empirical risk:\n\\[\n\\text{training loss}(\\hat f) := \\frac 1 N \\sum_{n=1}^N \\ell(\\hat f(x_n), y_n).\n\\]\nBy the law of large numbers, as \\(N\\) grows to infinity, the training loss converges to the generalization error.\nThe empirical risk minimization (ERM) approach is to find a predictor that minimizes the empirical risk. An ERM algorithm requires two ingredients to be chosen based on our domain knowledge about the DGP:\n\nA function class \\(\\mathcal{F}\\), that is, the space of functions to consider.\nA fitting method that uses the dataset to find the element of \\(\\mathcal{F}\\) that minimizes the training loss.\n\nThis allows us to compute the empirical risk minimizer:\n\\[\n\\begin{aligned}\n\\hat f_\\text{ERM} &:= \\arg\\min_{f \\in \\mathcal{F}} \\text{training loss}(f) \\\\\n&= \\arg\\min_{f \\in \\mathcal{F}}\\frac 1 N \\sum_{n=1}^N \\ell(f(x_n), y_n).\n\\end{aligned}\n\\tag{4.1}\\]\n\n4.3.1 Function classes\nHow should we choose the correct function class? In fact, why do we need to constrain our search at all?\n\nExercise 4.1 (Overfitting) Suppose we are trying to approximate a relationship between real-valued inputs and outputs using squared loss as our loss function. Consider the predictor (visualized in Figure 4.3)\n\\[\\hat f(x) = \\sum_{n=1}^N y_n \\mathbf{1}\\left\\{x = x_n\\right\\}.\\]\nWhat is the empirical risk of this function? How well does it perform on newly generated samples?\n\n\nCode\nn = 1000\nx_axis = np.linspace(-1, +1, n)\n\nfor _ in range(2):\n    x_train = np.random.uniform(-1, +1, 10)\n    y_train = np.sin(np.pi * x_train)\n    y_hat = np.where(np.isclose(x_axis[:, None], x_train, atol=2/n), y_train, 0).sum(axis=-1)\n\n    plt.plot(x_axis, y_hat, label=r'\\hat f(x)')\n    plt.scatter(x_train, y_train, color='red', marker='x', label='training data')\n    plt.legend()\n    plt.gcf().set_size_inches(3, 2)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) One training dataset.\n\n\n\n\n\n\n\n\n\n\n\n(b) Another training dataset.\n\n\n\n\n\n\n\nFigure 4.3: A pathological predictor.\n\n\n\nThe choice of \\(\\mathcal{F}\\) depends on our domain knowledge about the task. On one hand, \\(\\mathcal{F}\\) should be large enough to contain the true relationship, but on the other, it shouldn’t be too expressive; otherwise, it will overfit to random noise in the labels. The larger and more complex the function class, the more accurately we will be able to approximate any particular training dataset (i.e. smaller bias), but the more drastically the function will vary for different training datasets (i.e. larger variance). The mathematical details of the so-called bias-variance tradeoff can be found, for example, in Hastie, Tibshirani, and Friedman (2013, chap. 2.9).\n\nCode\nn_samples = 10\nx_axis = np.linspace(-1, +1, 50)\n\n\ndef generate_data(sigma=0.2):\n    x_train = np.random.uniform(-1, +1, n_samples)\n    y_train = np.sin(np.pi * x_train) + sigma * np.random.normal(size=n_samples)\n    return x_train, y_train\n\n\ndef transform(x: Float[Array, \" N\"], d: int):\n    return np.column_stack([\n        x ** d_\n        for d_ in range(d + 1)\n    ])\n\n\nfor d in [2, 5, 50]:\n    for _ in range(2):\n        x_train, y_train = generate_data()\n\n        x_features = transform(x_train, d)\n        w = np.linalg.lstsq(x_features, y_train)[0]\n        y_hat = transform(x_axis, d) @ w\n\n        color = 'blue' if _ == 0 else 'red'\n        plt.scatter(x_train, y_train, color=color, marker='x')\n        plt.plot(x_axis, y_hat, color=color)\n    plt.xlim(-1, +1)\n    plt.ylim(-1.2, 1.2)\n    plt.gcf().set_size_inches(2, 2)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Degree \\(2\\) polynomials\n\n\n\n\n\n\n\n\n\n\n\n(b) Degree \\(5\\) polynomials\n\n\n\n\n\n\n\n\n\n\n\n(c) Degree \\(50\\) polynomials\n\n\n\n\n\n\n\nFigure 4.4: Demonstrating the bias-variance tradeoff through polynomial regression. Increasing the degree increases the complexity of the polynomial function class.\n\n\n\nWe must also consider practical constraints on the function class. We need an efficient algorithm to actually compute the function in the class that minimizes the training error. This point should not be underestimated! The success of modern deep learning, for example, is in large part due to hardware developments that make certain parallelizable operations more efficient.\n\n\n4.3.2 Parameterized function classes\nBoth of the function classes we will consider, linear maps and neural networks, are finite-dimensional, a.k.a. parameterized. This means each function can be identified using some finite set of parameters, which we denote \\(\\theta \\in \\mathbb{R}^D\\).\n\nExample 4.5 (Quadratic functions) As a third example of a parameterized function class, consider the class of quadratic functions, i.e. polynomials of degree \\(2\\). This is a three-dimensional function space, since we can describe any quadratic \\(p\\) as\n\\[\np(x) = a x^2 + b x + c,\n\\]\nwhere \\(a, b, c\\) are the three parameters. We could also use a different parameterization:\n\\[\np(x) = a' (x - b')^2 + c'.\n\\]\nNote that the choice of parameterization can impact the performance of the chosen fitting method. What is the derivative of the first expression with respect to \\(a, b, c\\)? Compare this to the derivative of the second expression with respect to \\(a', b', c'\\). This shows that gradient-based fitting methods may change their behavior depending on the parameterization.\n\nUsing a parameterized function class allows us to reframe the ERM problem Equation 4.1 in terms of optimizing over the parameters instead of over the functions they represent:\n\\[\n\\begin{aligned}\n\\hat \\theta_\\text{ERM} &:= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\text{training loss}(f_\\theta) \\\\\n&= \\frac{1}{N} \\sum_{n=1}^N (y_n - f_\\theta(x_n))^2\n\\end{aligned}\n\\tag{4.2}\\]\nIn general, optimizing over a finite-dimensional space is much, much easier than optimizing over an infinite-dimensional space.\n\n\n4.3.3 Gradient descent\nOne widely applicable fitting method for parameterized function classes is gradient descent.\nLet \\(L(\\theta) = \\text{training loss}(f_\\theta)\\) denote the empirical risk in terms of the parameters. The gradient descent algorithm iteratively updates the parameters according to the rule\n\\[\n\\theta^{t+1} = \\theta^t - \\eta \\nabla_\\theta L(\\theta^t)\n\\]\nwhere \\(\\eta &gt; 0\\) is the learning rate and \\(\\nabla_\\theta L(\\theta^t)\\) indicates the gradient of \\(L\\) at the point \\(\\theta^t\\). Recall that the gradient of a function at a point is a vector in the direction that increases the function’s value the most within a neighborhood. So by taking small steps in the oppposite direction, we obtain a solution that achieves a slightly lower loss than the current one.\n\n\nCode\nParams = Float[Array, \" D\"]\n\n\ndef gradient_descent(\n    loss: Callable[[Params], float],\n    θ_init: Params,\n    η: float,\n    epochs: int,\n):\n    \"\"\"\n    Run gradient descent to minimize the given loss function\n    (expressed in terms of the parameters).\n    \"\"\"\n    θ = θ_init\n    for _ in range(epochs):\n        θ = θ - η * grad(loss)(θ)\n    return θ\n\n\nIn Section 6.2.1, we will discuss methods for implementing the grad function above, which takes in a function and returns its gradient, which can then be evaluated at a point.\nWhy do we need to scale down the step size by \\(\\eta\\)? The key word above is “neighborhood”. The gradient only describes the function within a local region around the point, whose size depends on the function’s smoothness. If we take a step that’s too large, we might end up with a worse solution by overshooting the region where the gradient is accurate. Note that, as a result, we can’t guarantee finding a global optimum of the function; we can only find local optima that are the best parameters within some neighborhood.\nAnother issue is that it’s often expensive to compute \\(\\nabla_\\theta L\\) when \\(N\\) is very large. Instead of calculating the gradient for every point in the dataset and averaging these, we can simply draw a batch of samples from the dataset and average the gradient across just these samples. Note that this is an unbiased random estimator of the true gradient. This algorithm is known as stochastic gradient descent. The added noise sometimes helps to jump to better solutions with a lower overall empirical risk.\nStepping for a moment back into the world of RL, you might wonder, why can’t we simply apply gradient descent (or rather, gradient ascent) to the total reward? It turns out that the gradient of the total reward with respect to the policy parameters known as the policy gradient, is challenging but possible to approximate. In Chapter 6, we will do exactly this.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#examples-of-parameterized-function-classes",
    "href": "supervised_learning.html#examples-of-parameterized-function-classes",
    "title": "4  Supervised learning",
    "section": "4.4 Examples of parameterized function classes",
    "text": "4.4 Examples of parameterized function classes\n\n4.4.1 Linear regression\nIn linear regression, we assume that the function \\(f\\) is linear in the parameters:\n\\[\n\\mathcal{F} = \\{ x \\mapsto \\theta^\\top x \\mid \\theta \\in \\mathbb{R}^D \\}\n\\]\nYou may already be familiar with linear regression from an introductory statistics course. This function class is extremely simple and only contains linear functions, whose graphs look like “lines of best fit” through the training data. It turns out that, when minimizing the squared error, the empirical risk minimizer has a closed-form solution, known as the ordinary least squares estimator. Let us write \\(Y = (y_1, \\dots, y_n)^\\top \\in \\mathbb{R}^N\\) and \\(X = (x_1, \\dots, x_N)^\\top \\in \\mathbb{R}^{N \\times D}\\). Then we can write\n\\[\n\\begin{aligned}\n\\hat \\theta &= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\frac{1}{2} \\sum_{n=1}^N (y_n - \\theta^\\top x_n)^2 \\\\\n&= \\arg\\min_{\\theta \\in \\mathbb{R}^D} \\frac 1 2 \\|Y - X \\theta \\|^2 \\\\\n&= (X^\\top X)^{-1} X^\\top Y,\n\\end{aligned}\n\\tag{4.3}\\]\nwhere we have assumed that the columns of \\(X\\) are linearly independent so that the matrix \\(X^\\top X\\) is invertible.\nWhat happens if the columns aren’t linearly independent? In this case, out of the possible solutions with the minimum empirical risk, we typically choose the one with the smallest norm.\n\nExercise 4.2 Gradient descent on the ERM problem (Equation 4.3), initialized at the origin and using a small enough step size, eventually finds the parameters with the smallest norm. In practice, since the squared error gradient is convenient to compute, running gradient descent can be faster than explicitly computing the inverse (or pseudoinverse) of a matrix.\nAssume that \\(N &lt; D\\) and that the data points are linearly independent.\n\nLet \\(\\hat{\\theta}\\) be the solution found by gradient descent. Show that \\(\\hat{\\theta}\\) is a linear combination of the data points, that is, \\(\\hat{\\theta} = X^\\top a\\), where \\(a \\in \\mathbb{R}^N\\).\nLet \\(w \\in \\mathbb{R}^D\\) be another empirical risk minimizer i.e. \\(X w = y\\). Show that \\(\\hat{\\theta}^\\top (w - \\hat{\\theta}) = 0\\).\nUse this to show that \\(\\|\\hat{\\theta}\\| \\le \\|w\\|\\), showing that the gradient descent solution has the smallest norm out of all solutions that fit the data. (No need for algebra; there is a nice geometric solution!)\n\n\nThough linear regression may appear trivially simple, it is a very powerful tool for more complex models to build upon. For instance, to expand the expressiveness of linear models, we can first transform the input \\(x\\) using some feature mapping \\(\\phi\\), i.e. \\(\\widetilde x = \\phi(x)\\), and then fit a linear model in the transformed space instead. By using domain knowledge to choose a useful feature mapping, we can obtain a powerful SL method for a particular task.\n\n\n4.4.2 Neural networks\nIn neural networks, we assume that the function \\(f\\) is a composition of linear functions (represented by matrices \\(W_i\\)) and non-linear activation functions (denoted by \\(\\sigma\\)):\n\\[\n\\mathcal{F} = \\{ x \\mapsto \\sigma(W_L \\sigma(W_{L-1} \\dots \\sigma(W_1 x + b_1) \\dots + b_{L-1}) + b_L) \\}\n\\]\nwhere \\(W_\\ell \\in \\mathbb{R}^{D_{\\ell+1} \\times D_\\ell}\\) and \\(b_\\ell \\in \\mathbb{R}^{D_{\\ell+1}}\\) are the parameters of the \\(i\\)-th layer, and \\(\\sigma\\) is the activation function.\nThis function class is highly expressive and allows for more parameters. This makes it more susceptible to overfitting on smaller datasets, but also allows it to represent more complex functions. In practice, however, neural networks exhibit interesting phenomena during training, and are often able to generalize well even with many parameters.\nAnother reason for their popularity is the efficient backpropagation algorithm for computing the gradient of the output with respect to the parameters. Essentially, the hierarchical structure of the neural network, i.e. computing the output of the network as a composition of functions, allows us to use the chain rule to compute the gradient of the output with respect to the parameters of each layer.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#summary",
    "href": "supervised_learning.html#summary",
    "title": "4  Supervised learning",
    "section": "4.5 Summary",
    "text": "4.5 Summary\nWe have now gotten a glimpse into supervised learning, which seeks to learn about some input-output relationship using a dataset of example points. In particular, we typically seek to compute a predictor that takes in an input value and returns a good guess for the corresponding output. We score predictors using a loss function that measures how incorrectly it guesses. We want to find a predictor that achieves low loss on unseen data points. We do this by searching over a class of functions to find one that minimizes the empirical risk over the training dataset. We finally saw two popular examples of parameterized function classes: linear regression and neural networks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "supervised_learning.html#references",
    "href": "supervised_learning.html#references",
    "title": "4  Supervised learning",
    "section": "4.6 References",
    "text": "4.6 References\nJames et al. (2023) provides an accessible introduction to supervised learning. Hastie, Tibshirani, and Friedman (2013) examines the subject in even further depth and covers many relevant supervised learning methods. Nielsen (2015) provides a comprehensive introduction to neural networks and backpropagation.\n\n\n\n\nDeng, Li. 2012. “The MNIST Database of Handwritten Digit Images for Machine Learning Research [Best of the Web].” IEEE Signal Processing Magazine 29 (6): 141–42. https://doi.org/10.1109/MSP.2012.2211477.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2013. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Springer Science & Business Media. https://books.google.com?id=yPfZBwAAQBAJ.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and Jonathan Taylor. 2023. An Introduction to Statistical Learning: With Applications in Python. Springer Texts in Statistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-38747-0.\n\n\nNielsen, Michael A. 2015. Neural Networks and Deep Learning. Determination Press. http://neuralnetworksanddeeplearning.com/.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Supervised learning</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html",
    "href": "fitted_dp.html",
    "title": "5  Fitted Dynamic Programming Algorithms",
    "section": "",
    "text": "5.1 Introduction\nWe borrow these definitions from the Chapter 1 chapter:\nCode\nfrom utils import gym, tqdm, rand, Float, Array, NamedTuple, Callable, Optional, np\n\nkey = rand.PRNGKey(184)\n\nclass Transition(NamedTuple):\n    s: int\n    a: int\n    r: float\n\n\nTrajectory = list[Transition]\n\n\ndef get_num_actions(trajectories: list[Trajectory]) -&gt; int:\n    \"\"\"Get the number of actions in the dataset. Assumes actions range from 0 to A-1.\"\"\"\n    return max(max(t.a for t in τ) for τ in trajectories) + 1\n\n\nState = Float[Array, \"...\"]  # arbitrary shape\n\n# assume finite `A` actions and f outputs an array of Q-values\n# i.e. Q(s, a, h) is implemented as f(s, h)[a]\nQFunction = Callable[[State, int], Float[Array, \" A\"]]\n\n\ndef Q_zero(A: int) -&gt; QFunction:\n    \"\"\"A Q-function that always returns zero.\"\"\"\n    return lambda s, a: np.zeros(A)\n\n\n# a deterministic time-dependent policy\nPolicy = Callable[[State, int], int]\n\n\ndef q_to_greedy(Q: QFunction) -&gt; Policy:\n    \"\"\"Get the greedy policy for the given state-action value function.\"\"\"\n    return lambda s, h: np.argmax(Q(s, h))\nThe Chapter 1 chapter discussed the case of finite MDPs, where the state and action spaces \\(\\mathcal{S}\\) and \\(\\mathcal{A}\\) were finite. This gave us a closed-form expression for computing the r.h.s. of Theorem 1.1. In this chapter, we consider the case of large or continuous state spaces, where the state space is too large to be enumerated. In this case, we need to approximate the value function and Q-function using methods from supervised learning.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#fitted-value-iteration",
    "href": "fitted_dp.html#fitted-value-iteration",
    "title": "5  Fitted Dynamic Programming Algorithms",
    "section": "5.2 Fitted value iteration",
    "text": "5.2 Fitted value iteration\nLet us apply ERM to the RL problem of computing the optimal policy / value function.\nHow did we compute the optimal value function in MDPs with finite state and action spaces?\n\nIn a Section 1.3, we can use Definition 1.10, working backwards from the end of the time horizon, to compute the optimal value function exactly.\nIn an Section 1.4, we can use Section 1.5.3.1, which iterates the Bellman optimality operator Equation 1.7 to approximately compute the optimal value function.\n\nOur existing approaches represent the value function, and the MDP itself, in matrix notation. But what happens if the state space is extremely large, or even infinite (e.g. real-valued)? Then computing a weighted sum over all possible next states, which is required to compute the Bellman operator, becomes intractable.\nInstead, we will need to use function approximation methods from supervised learning to solve for the value function in an alternative way.\nIn particular, suppose we have a dataset of \\(N\\) trajectories \\(\\tau_1, \\dots, \\tau_N \\sim \\rho_{\\pi}\\) from some policy \\(\\pi\\) (called the data collection policy) acting in the MDP of interest. Let us indicate the trajectory index in the superscript, so that\n\\[\n\\tau_i = \\{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \\dots, s_{H-1}^i, a_{H-1}^i, r_{H-1}^i \\}.\n\\]\n\n\nCode\ndef collect_data(\n    env: gym.Env, N: int, H: int, key: rand.PRNGKey, π: Optional[Policy] = None\n) -&gt; list[Trajectory]:\n    \"\"\"Collect a dataset of trajectories from the given policy (or a random one).\"\"\"\n    trajectories = []\n    seeds = [rand.bits(k).item() for k in rand.split(key, N)]\n    for i in tqdm(range(N)):\n        τ = []\n        s, _ = env.reset(seed=seeds[i])\n        for h in range(H):\n            # sample from a random policy\n            a = π(s, h) if π else env.action_space.sample()\n            s_next, r, terminated, truncated, _ = env.step(a)\n            τ.append(Transition(s, a, r))\n            if terminated or truncated:\n                break\n            s = s_next\n        trajectories.append(τ)\n    return trajectories\n\n\n\n\nCode\nenv = gym.make(\"LunarLander-v3\")\ntrajectories = collect_data(env, 100, 300, key)\ntrajectories[0][:5]  # show first five transitions from first trajectory\n\n\n  0%|          | 0/100 [00:00&lt;?, ?it/s] 60%|██████    | 60/100 [00:00&lt;00:00, 597.79it/s]100%|██████████| 100/100 [00:00&lt;00:00, 596.58it/s]\n\n\n[Transition(s=array([-0.00767412,  1.4020356 , -0.77731264, -0.3948966 ,  0.00889908,\n         0.17607284,  0.        ,  0.        ], dtype=float32), a=np.int64(2), r=np.float64(0.4116707418262536)),\n Transition(s=array([-0.015343  ,  1.3934507 , -0.7757339 , -0.38162258,  0.01765189,\n         0.17507371,  0.        ,  0.        ], dtype=float32), a=np.int64(3), r=np.float64(0.009265259820325583)),\n Transition(s=array([-0.02292128,  1.3842582 , -0.76436865, -0.40865484,  0.02411737,\n         0.12932152,  0.        ,  0.        ], dtype=float32), a=np.int64(3), r=np.float64(0.11140717727713081)),\n Transition(s=array([-0.03041582,  1.3744599 , -0.75385344, -0.43555325,  0.02847344,\n         0.08712957,  0.        ,  0.        ], dtype=float32), a=np.int64(0), r=np.float64(-0.7801783265419715)),\n Transition(s=array([-0.03791046,  1.364062  , -0.75386536, -0.46222275,  0.03282927,\n         0.08712444,  0.        ,  0.        ], dtype=float32), a=np.int64(3), r=np.float64(0.27157909778063183))]\n\n\nCan we view the dataset of trajectories as a “labelled dataset” in order to apply supervised learning to approximate the optimal Q-function? Yes! Recall that we can characterize the optimal Q-function using the Theorem 1.4, which don’t depend on an actual policy:\n\\[\nQ_h^\\star(s, a) = r(s, a) + \\mathbb{E}_{s' \\sim P(s, a)} [\\max_{a'} Q_{h+1}^\\star(s', a')]\n\\]\nWe can think of the arguments to the Q-function – i.e. the current state, action, and timestep \\(h\\) – as the inputs \\(x\\), and the r.h.s. of the above equation as the label \\(f(x)\\). Note that the r.h.s. can also be expressed as a conditional expectation:\n\\[\nf(x) = \\mathbb{E}[y \\mid x] \\quad \\text{where} \\quad y = r(s_h, a_h) + \\max_{a'} Q^\\star_{h+ 1}(s', a').\n\\]\nApproximating the conditional expectation is precisely the task that Section 4.3 is suited for!\nOur above dataset would give us \\(N \\cdot H\\) samples in the dataset:\n\\[\nx_{i h} = (s_h^i, a_h^i, h) \\qquad y_{i h} = r(s_h^i, a_h^i) + \\max_{a'} Q^\\star_{h+ 1}(s_{h+ 1}^i, a')\n\\]\n\n\nCode\ndef get_X(trajectories: list[Trajectory]):\n    \"\"\"\n    We pass the state and timestep as input to the Q-function\n    and return an array of Q-values.\n    \"\"\"\n    rows = [(τ[h].s, τ[h].a, h) for τ in trajectories for h in range(len(τ))]\n    return [np.stack(ary) for ary in zip(*rows)]\n\n\ndef get_y(\n    trajectories: list[Trajectory],\n    f: Optional[QFunction] = None,\n    π: Optional[Policy] = None,\n):\n    \"\"\"\n    Transform the dataset of trajectories into a dataset for supervised learning.\n    If `π` is None, instead estimates the optimal Q function.\n    Otherwise, estimates the Q function of π.\n    \"\"\"\n    f = f or Q_zero(get_num_actions(trajectories))\n    y = []\n    for τ in trajectories:\n        for h in range(len(τ) - 1):\n            s, a, r = τ[h]\n            Q_values = f(s, h + 1)\n            y.append(r + (Q_values[π(s, h + 1)] if π else Q_values.max()))\n        y.append(τ[-1].r)\n    return np.array(y)\n\n\n\n\nCode\ns, a, h = get_X(trajectories[:1])\nprint(\"states:\", s[:5])\nprint(\"actions:\", a[:5])\nprint(\"timesteps:\", h[:5])\n\n\nstates: [[-0.00767412  1.4020356  -0.77731264 -0.3948966   0.00889908  0.17607284\n   0.          0.        ]\n [-0.015343    1.3934507  -0.7757339  -0.38162258  0.01765189  0.17507371\n   0.          0.        ]\n [-0.02292128  1.3842582  -0.76436865 -0.40865484  0.02411737  0.12932152\n   0.          0.        ]\n [-0.03041582  1.3744599  -0.75385344 -0.43555325  0.02847344  0.08712957\n   0.          0.        ]\n [-0.03791046  1.364062   -0.75386536 -0.46222275  0.03282927  0.08712444\n   0.          0.        ]]\nactions: [2 3 3 0 3]\ntimesteps: [0 1 2 3 4]\n\n\n\n\nCode\nget_y(trajectories[:1])[:5]\n\n\narray([ 0.41167074,  0.00926526,  0.11140718, -0.78017833,  0.2715791 ])\n\n\nThen we can use empirical risk minimization to find a function \\(\\hat f\\) that approximates the optimal Q-function.\n\n\nCode\n# We will see some examples of fitting methods in the next section\nFittingMethod = Callable[[Float[Array, \"N D\"], Float[Array, \" N\"]], QFunction]\n\n\nBut notice that the definition of \\(y_{i h}\\) depends on the Q-function itself! How can we resolve this circular dependency? Recall that we faced the same issue when evaluating a policy in an infinite-horizon MDP (Section 1.5.2.2). There, we iterated the Definition 1.8 since we knew that the policy’s value function was a fixed point of the policy’s Bellman operator. We can apply the same strategy here, using the \\(\\hat f\\) from the previous iteration to compute the labels \\(y_{i h}\\), and then using this new dataset to fit the next iterate.\n\nDefinition 5.1 (Fitted Q-function iteration)  \n\nInitialize some function \\(\\hat f(s, a, h) \\in \\mathbb{R}\\).\nIterate the following:\n\nGenerate a supervised learning dataset \\(X, y\\) from the trajectories and the current estimate \\(f\\), where the labels come from the r.h.s. of the Bellman optimality operator Equation 1.7\nSet \\(\\hat f\\) to the function that minimizes the empirical risk:\n\n\n\\[\n\\hat f \\gets \\arg\\min_f \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2.\n\\]\n\n\n\nCode\ndef fitted_q_iteration(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    epochs: int,\n    Q_init: Optional[QFunction] = None,\n) -&gt; QFunction:\n    \"\"\"\n    Run fitted Q-function iteration using the given dataset.\n    Returns an estimate of the optimal Q-function.\n    \"\"\"\n    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))\n    X = get_X(trajectories)\n    for _ in range(epochs):\n        y = get_y(trajectories, Q_hat)\n        Q_hat = fit(X, y)\n    return Q_hat",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#sec-fitted-pi-eval",
    "href": "fitted_dp.html#sec-fitted-pi-eval",
    "title": "5  Fitted Dynamic Programming Algorithms",
    "section": "5.3 Fitted policy evaluation",
    "text": "5.3 Fitted policy evaluation\nWe can also use this fixed-point interation to evaluate a policy using the dataset (not necessarily the one used to generate the trajectories):\n\nDefinition 5.2 (Fitted policy evaluation) Input: Policy \\(\\pi : \\mathcal{S} \\times [H] \\to \\Delta(\\mathcal{A})\\) to be evaluated.\nOutput: An approximation of the value function \\(Q^\\pi\\) of the policy.\n\nInitialize some function \\(\\hat f(s, a, h) \\in \\mathbb{R}\\).\nIterate the following:\n\nGenerate a supervised learning dataset \\(X, y\\) from the trajectories and the current estimate \\(f\\), where the labels come from the r.h.s. of the Theorem 1.1 for the given policy.\nSet \\(\\hat f\\) to the function that minimizes the empirical risk:\n\n\n\\[\n\\hat f \\gets \\arg\\min_f \\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2.\n\\]\n\n\n\nCode\ndef fitted_evaluation(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    π: Policy,\n    epochs: int,\n    Q_init: Optional[QFunction] = None,\n) -&gt; QFunction:\n    \"\"\"\n    Run fitted policy evaluation using the given dataset.\n    Returns an estimate of the Q-function of the given policy.\n    \"\"\"\n    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))\n    X = get_X(trajectories)\n    for _ in tqdm(range(epochs)):\n        y = get_y(trajectories, Q_hat, π)\n        Q_hat = fit(X, y)\n    return Q_hat\n\n\n\nSpot the difference between fitted_evaluation and fitted_q_iteration. (See the definition of get_y.) How would you modify this algorithm to evaluate the data collection policy?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#fitted-policy-iteration",
    "href": "fitted_dp.html#fitted-policy-iteration",
    "title": "5  Fitted Dynamic Programming Algorithms",
    "section": "5.4 Fitted policy iteration",
    "text": "5.4 Fitted policy iteration\nWe can use this policy evaluation algorithm to adapt Section 1.5.3.2 to this new setting. The algorithm remains exactly the same – repeatedly make the policy greedy w.r.t. its own value function – except now we must evaluate the policy (i.e. compute its value function) using the iterative fitted-evaluation algorithm.\n\n\nCode\ndef fitted_policy_iteration(\n    trajectories: list[Trajectory],\n    fit: FittingMethod,\n    epochs: int,\n    evaluation_epochs: int,\n    π_init: Optional[Policy] = lambda s, h: 0,  # constant zero policy\n):\n    \"\"\"Run fitted policy iteration using the given dataset.\"\"\"\n    π = π_init\n    for _ in range(epochs):\n        Q_hat = fitted_evaluation(trajectories, fit, π, evaluation_epochs)\n        π = q_to_greedy(Q_hat)\n    return π",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "fitted_dp.html#summary",
    "href": "fitted_dp.html#summary",
    "title": "5  Fitted Dynamic Programming Algorithms",
    "section": "5.5 Summary",
    "text": "5.5 Summary",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Fitted Dynamic Programming Algorithms</span>"
    ]
  },
  {
    "objectID": "pg.html",
    "href": "pg.html",
    "title": "6  Policy Gradient Methods",
    "section": "",
    "text": "6.1 Introduction\nThe core task of RL is finding the optimal policy in a given environment. This is essentially an optimization problem: out of some space of policies, we want to find the one that achieves the maximum total reward (in expectation).\nIt’s typically intractable to compute the optimal policy exactly in some finite number of steps. Instead, policy optimization algorithms start from some randomly initialized policy, and then improve it step by step. We’ve already seen some examples of these, namely Section 1.5.3.2 for finite MDPs and Section 2.6.4 in continuous control.\nIn particular, we often use policies that can be described by some finite set of parameters. We will see some examples in Section 6.3.1. For such parameterized policies, we can approximate the policy gradient: the gradient of the expected total reward with respect to the parameters. This tells us the direction the parameters should be updated to achieve a higher expected total reward. Policy gradient methods are responsible for groundbreaking applications including AlphaGo, OpenAI Five, and large language models, many of which use policies parameterized as deep neural networks.\nCode\n%load_ext autoreload\n%autoreload 2\nCode\nfrom utils import plt, Array, Float, Callable, jax, jnp, latex, gym",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#introduction",
    "href": "pg.html#introduction",
    "title": "6  Policy Gradient Methods",
    "section": "",
    "text": "We begin the chapter with a short review of gradient ascent, a general optimization method.\nWe’ll then see how to estimate the policy gradient, enabling us to apply (stochastic) gradient ascent in the RL setting.\nThen we’ll explore some proximal optimization techniques that ensure the steps taken are “not too large”. This is helpful to stabilize training and widely used in practice.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#gradient-ascent",
    "href": "pg.html#gradient-ascent",
    "title": "6  Policy Gradient Methods",
    "section": "6.2 Gradient Ascent",
    "text": "6.2 Gradient Ascent\n\nYou may have previously heard of gradient descent for minimizing functions. Optimization problems are usually posed as minimization problems by convention. However, in RL, we usually talk about maximizing the expected total reward, and so we perform gradient ascent instead.\n\nGradient ascent is a general optimization algorithm for any differentiable function. A suitable analogy for this algorithm is hiking up a mountain, where you keep taking steps in the steepest direction upwards. Here, your vertical position \\(y\\) is the function being optimized, and your horizontal position \\((x, z)\\) is the input to the function. The slope of the mountain at your current position is given by the gradient, written \\(\\nabla y(x, z) \\in \\mathbb{R}^2\\).\n\n\nCode\ndef f(x, y):\n    \"\"\"Himmelblau's function\"\"\"\n    return (x**2 + y - 11)**2 + (x + y**2 - 7)**2\n\n# Create a grid of points\nx = jnp.linspace(-5, 5, 400)\ny = jnp.linspace(-5, 5, 400)\nX, Y = jnp.meshgrid(x, y)\nZ = f(X, Y)\n\nfig, ax = plt.subplots(figsize=(6, 6))\nimg = ax.imshow(Z, extent=[-5, 5, -5, 5], origin='lower')\nfig.colorbar(img, ax=ax)\n\ntx, ty = 1.0, 1.0\ngx, gy = jax.grad(f, argnums=(0, 1))(tx, ty)\n\nax.scatter(tx, ty, color='red', s=100)\nax.arrow(tx, ty, gx * 0.01, gy * 0.01, head_width=0.3, head_length=0.3, fc='blue', ec='blue')\n\nax.set_title(\"Gradient ascent example\")\nplt.show()\n\n\n\n\n\n\n\n\n\nFor differentiable functions, this can be thought of as the vector of partial derivatives,\n\\[\n\\nabla y(x, z) = \\begin{pmatrix}\n\\frac{\\partial y}{\\partial x} \\\\\n\\frac{\\partial y}{\\partial z}\n\\end{pmatrix}.\n\\]\nTo calculate the slope (aka “directional derivative”) of the mountain in a given direction \\((\\Delta x, \\Delta z)\\), you take the dot product of the difference vector with the gradient. This means that the direction with the highest slope is exactly the gradient itself, so we can describe the gradient ascent algorithm as follows:\n\nDefinition 6.1 (Gradient ascent) \\[\n\\begin{pmatrix}\nx^{k+1} \\\\ z^{k+1}\n\\end{pmatrix}\n=\n\\begin{pmatrix}\nx^{k} \\\\ z^{k}\n\\end{pmatrix}\n+\n\\eta \\nabla y(x^{k}, z^{k})\n\\]\n\nwhere \\(k\\) denotes the iteration of the algorithm and \\(\\eta &gt; 0\\) is a “step size” hyperparameter that controls the size of the steps we take. (Note that we could also vary the step size across iterations, that is, \\(\\eta^0, \\dots, \\eta^K\\).)\nThe case of a two-dimensional input is easy to visualize. But this idea can be straightforwardly extended to higher-dimensional inputs.\nFrom now on, we’ll use \\(J\\) to denote the function we’re trying to maximize, and \\(\\theta\\) to denote the parameters being optimized over. (In the above example, \\(\\theta = \\begin{pmatrix} x & z \\end{pmatrix}^\\top\\)).\nNotice that our parameters will stop changing once \\(\\nabla J(\\theta) = 0.\\) Once we reach this stationary point, our current parameters are ‘locally optimal’ in some sense; it’s impossible to increase the function by moving in any direction. If \\(J\\) is convex, then the only point where this happens is at the global optimum. Otherwise, if \\(J\\) is nonconvex, the best we can hope for is a local optimum.\n\n6.2.1 Computing derivatives\nHow does a computer compute the gradient of a function?\nOne way is symbolic differentiation, which is similar to the way you might compute it by hand: the computer applies a list of rules to transform the symbols involved. Python’s sympy package supports symbolic differentiation. However, functions implemented in code may not always have a straightforward symbolic representation.\nAnother way is numerical differentiation, which is based on the limit definition of a (directional) derivative:\n\\[\n\\nabla_{\\boldsymbol{u}} J(\\boldsymbol{x}) = \\lim_{\\varepsilon \\to 0}\n\\frac{J(\\boldsymbol{x} + \\varepsilon \\boldsymbol{u}) - J(\\boldsymbol{x})}{\\varepsilon}\n\\]\nThen, we can substitute a small value of \\(\\varepsilon\\) on the r.h.s. to approximate the directional derivative. How small, though? If we need an accurate estimate, we may need such a small value of \\(\\varepsilon\\) that typical computers will run into rounding errors. Also, to compute the full gradient, we would need to compute the r.h.s. once for each input dimension. This is an issue if computing \\(J\\) is expensive.\nAutomatic differentiation achieves the best of both worlds. Like symbolic differentiation, we manually implement the derivative rules for a few basic operations. However, instead of executing these on the symbols, we execute them on the values when the function gets called, like in numerical differentiation. This allows us to differentiate through programming constructs such as branches or loops, and doesn’t involve any arbitrarily small values. Baydin et al. (2018) provides an accessible survey of automatic differentiation.\n\n\n6.2.2 Stochastic gradient ascent\nIn real applications, computing the gradient of the target function is not so simple. As an example from supervised learning, \\(J(\\theta)\\) might be the sum of squared prediction errors across an entire training dataset. However, if our dataset is very large, it might not fit into our computer’s memory! Typically in these cases, we compute some estimate of the gradient at each step, and walk in that direction instead. This is called stochastic gradient ascent. In the SL example above, we might randomly choose a minibatch of samples and use them to estimate the true prediction error. (This approach is known as minibatch SGD.)\n\n\nCode\ndef sgd(\n    theta_init: Float[Array, \" D\"],\n    estimate_gradient: Callable[[Float[Array, \" D\"]], Float[Array, \" D\"]],\n    eta: float,\n    n_steps: int,\n):\n    # Perform `n_steps` steps of SGD.\n\n    # `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.\n    theta = theta_init\n    for step in range(n_steps):\n        theta += eta * estimate_gradient(theta)\n    return theta\n\nlatex(sgd)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{sgd}(\\theta_{\\mathrm{init}}: \\mathbb{R}^{D}, \\mathrm{estimate\\_gradient}: \\mathrm{Callable}_{\\mathopen{}\\left( \\mathopen{}\\left[ \\mathbb{R}^{D} \\mathclose{}\\right], \\mathbb{R}^{D} \\mathclose{}\\right)}, \\eta: \\mathrm{float}, n_{\\mathrm{steps}}: \\mathrm{int}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathrm{step} \\in \\mathrm{range} \\mathopen{}\\left( n_{\\mathrm{steps}} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\theta \\gets \\theta + \\eta \\cdot \\mathrm{estimate\\_gradient} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nWhat makes one gradient estimator better than another? Ideally, we want this estimator to be unbiased; that is, on average, it matches a single true gradient step:\n\\[\n\\mathbb{E}[\\tilde \\nabla J(\\theta)] = \\nabla J(\\theta).\n\\]\nWe also want the variance of the estimator to be low so that its performance doesn’t change drastically at each step.\nWe can actually show that, for many “nice” functions, in a finite number of steps, SGD will find a \\(\\theta\\) that is “close” to a stationary point. In another perspective, for such functions, the local “landscape” of \\(J\\) around \\(\\theta\\) becomes flatter and flatter the longer we run SGD.\n\n6.2.2.1 SGD convergence\nMore formally, suppose we run SGD for \\(K\\) steps, using an unbiased gradient estimator. Let the step size \\(\\eta^k\\) scale as \\(O(1/\\sqrt{k}).\\) Then if \\(J\\) is bounded and \\(\\beta\\)-smooth (see below), and the norm of the gradient estimator has a bounded second moment \\(\\sigma^2,\\)\n\\[\n\\|\\nabla J(\\theta^K)\\|^2 \\le O \\left( M \\beta \\sigma^2 / K\\right).\n\\]\nWe call a function \\(\\beta\\)-smooth if its gradient is Lipschitz continuous with constant \\(\\beta\\):\n\\[\n\\|\\nabla J(\\theta) - \\nabla J(\\theta')\\| \\le \\beta \\|\\theta - \\theta'\\|.\n\\]\n\nWe’ll now see a concrete application of gradient ascent in the context of policy optimization.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#policy-stochastic-gradient-ascent",
    "href": "pg.html#policy-stochastic-gradient-ascent",
    "title": "6  Policy Gradient Methods",
    "section": "6.3 Policy (stochastic) gradient ascent",
    "text": "6.3 Policy (stochastic) gradient ascent\nRemember that in RL, the primary goal is to find the optimal policy that achieves the maximimum total reward, which we can express using Definition 1.6:\n\\[\n\\begin{aligned}\n    J(\\pi) := \\mathbb{E}_{s_0 \\sim \\mu_0} V^{\\pi} (s_0) = & \\mathbb{E}_{\\tau \\sim \\rho^\\pi} \\sum_{h=0}^{H-1} r(s_h, a_h)\n\\end{aligned}\n\\tag{6.1}\\]\nwhere \\(\\rho^\\pi\\) is the distribution over trajectories induced by \\(\\pi\\) (see Definition 1.5).\n(Note that we’ll continue to work in the undiscounted, finite-horizon case. Analogous results hold for the discounted, infinite-horizon setup.)\nAs shown by the notation, this is exactly the function \\(J\\) that we want to maximize using gradient ascent. What variables are we optimizing over in this problem? Well, the objective function \\(J\\) is a function of the policy \\(\\pi\\), but in general, \\(\\pi\\) is a function, and optimizing over the entire space of arbitrary input-output mappings would be intractable. Instead, we need to describe \\(\\pi\\) in terms of some finite set of parameters \\(\\theta\\).\n\n6.3.1 Example policy parameterizations\nWhat are some ways we could parameterize our policy?\n\nExample 6.1 (Tabular representation) If both the state and action spaces are finite, perhaps we could simply learn a preference value \\(\\theta_{s,a}\\) for each state-action pair. Then to turn this into a valid distribution, we perform a softmax operation: we exponentiate each of them, and then normalize to form a valid distribution.\n\\[\n\\pi^\\text{softmax}_\\theta(a | s) = \\frac{\\exp(\\theta_{s,a})}{\\sum_{s,a'} \\exp (\\theta_{s,a'})}.\n\\]\nHowever, this doesn’t make use of any structure in the states or actions, so while this is flexible, it is also prone to overfitting.\n\n\nExample 6.2 (Linear in features) Another approach is to map each state-action pair into some feature space \\(\\phi(s, a) \\in \\mathbb{R}^p\\). Then, to map a feature vector to a probability, we take a linear combination of the features and take a softmax:\n\\[\n\\pi^\\text{linear in features}_{\\theta}(a|s) = \\frac{\\exp(\\theta^\\top \\phi(s, a))}{\\sum_{a'} \\exp(\\theta^\\top \\phi(s, a'))}.\n\\]\nAnother interpretation is that \\(\\theta\\) represents the feature vector of the “desired” state-action pair, as state-action pairs whose features align closely with \\(\\theta\\) are given higher probability.\n\n\nExample 6.3 (Neural policies) More generally, we could map states and actions to unnormalized scores via some parameterized function \\(f_\\theta : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R},\\) such as a neural network, and choose actions according to a softmax:\n\\[\n\\pi^\\text{general}_\\theta(a|s) = \\frac{\\exp(f_{\\theta}(s,a))}{\\sum_{a'} \\exp(f_{\\theta}(s,a'))}.\n\\]\n\n\nExample 6.4 (Diagonal Gaussian policies for continuous action spaces) Consider a continuous \\(n\\)-dimensional action space \\(\\mathcal{A} = \\mathbb{R}^n\\). Then for a stochastic policy, we could use a function to predict the mean action and then add some random noise about it. For example, we could use a neural network to predict the mean action \\(\\mu_\\theta(s)\\) and then add some noise \\(\\epsilon \\sim \\mathcal{N}(0, \\sigma^2 I)\\) to it:\n\\[\n\\pi_\\theta(a|s) = \\mathcal{N}(\\mu_\\theta(s), \\sigma^2 I).\n\\]\n\n\nNow that we have seen some examples of parameterized policies, we will write the total reward in terms of the parameters, overloading notation and letting \\(\\rho_\\theta := \\rho^{\\pi_\\theta}\\):\n\\[\nJ(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} R(\\tau)\n\\]\nwhere \\(R(\\tau) = \\sum_{h=0}^{H-1} r(s_h, a_h)\\) denotes the total reward in the trajectory.\nNow how do we maximize this function (the expected total reward) over the parameters? One simple idea would be to directly apply gradient ascent:\n\\[\n\\theta^{k+1} = \\theta^k + \\eta \\nabla J(\\theta^k).\n\\]\nIn order to apply this technique, we need to be able to evaluate the gradient \\(\\nabla J(\\theta).\\) But \\(J(\\theta)\\) is very difficult, or even intractable, to compute exactly, since it involves taking an expectation over all possible trajectories \\(\\tau.\\) Can we rewrite it in a form that’s more convenient to implement?\n\n\n6.3.2 Importance Sampling\nThere is a general trick called importance sampling for evaluating difficult expectations. Suppose we want to estimate \\(\\mathbb{E}_{x \\sim p}[f(x)]\\) where \\(p\\) is hard or expensive to sample from, but easy to evaluate the likelihood \\(p(x)\\) of. Suppose that we can easily sample from a different distribution \\(q\\). Since an expectation is just a weighted average, we can sample \\(x\\) from \\(q\\), compute \\(f(x)\\), and then reweight the results: if \\(x\\) is very likely under \\(p\\) but unlikely under \\(q\\), we should boost its weighting, and if it is common under \\(q\\) but uncommon under \\(p\\), we should lower its weighting. The reweighting factor is exactly the likelihood ratio between the target distribution \\(p\\) and the sampling distribution \\(q\\):\n\\[\n\\mathbb{E}_{x \\sim p}[f(x)] = \\sum_{x \\in \\mathcal{X}} f(x) p(x) = \\sum_{x \\in \\mathcal{X}} f(x) \\frac{p(x)}{q(x)} q(x) = \\mathbb{E}_{x \\sim q} \\left[ \\frac{p(x)}{q(x)} f(x) \\right].\n\\]\nDoesn’t this seem too good to be true? If there were no drawbacks, we could use this to estimate any expectation of any function on any arbitrary distribution! The drawback is that the variance may be very large due to the likelihood ratio term. If there are values of \\(x\\) that are very rare in the sampling distribution \\(q\\), but common under \\(p\\), then the likelihood ratio \\(p(x)/q(x)\\) will cause the variance to blow up.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#the-reinforce-policy-gradient",
    "href": "pg.html#the-reinforce-policy-gradient",
    "title": "6  Policy Gradient Methods",
    "section": "6.4 The REINFORCE policy gradient",
    "text": "6.4 The REINFORCE policy gradient\nReturning to RL, suppose there is some trajectory distribution \\(\\rho(\\tau)\\) that is easy to sample from, such as a database of existing trajectories. We can then rewrite \\(\\nabla J(\\theta)\\), a.k.a. the policy gradient, as follows. All gradients are being taken with respect to \\(\\theta\\).\n\\[\n\\begin{aligned}\n    \\nabla J(\\theta) & = \\nabla \\mathbb{E}_{\\tau \\sim \\rho_\\theta} [ R(\\tau) ]                                                                                         \\\\\n                     & = \\nabla \\mathbb{E}_{\\tau \\sim \\rho} \\left[ \\frac{\\rho_\\theta(\\tau)}{\\rho(\\tau)} R(\\tau) \\right] &  & \\text{likelihood ratio trick}             \\\\\n                     & = \\mathbb{E}_{\\tau \\sim \\rho} \\left[ \\frac{\\nabla \\rho_\\theta(\\tau)}{\\rho(\\tau)} R(\\tau) \\right] &  & \\text{switching gradient and expectation}\n\\end{aligned}\n\\]\nNote that for \\(\\rho = \\rho_\\theta\\), the inside term becomes\n\\[\n\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} [ \\nabla \\log \\rho_\\theta(\\tau) \\cdot R(\\tau)].\n\\]\n(The order of operations is \\(\\nabla (\\log \\rho_\\theta)(\\tau)\\).)\nRecall that when the state transitions are Markov (i.e. \\(s_{t}\\) only depends on \\(s_{t-1}, a_{t-1}\\)) and the policy is time-homogeneous (i.e. \\(a_h\\sim \\pi_\\theta (s_h)\\)), we can write out the likelihood of a trajectory under the policy \\(\\pi_\\theta\\) autoregressively, as in Definition 1.5. Taking the log of the trajectory likelihood turns it into a sum of terms:\n\\[\n\\log \\rho_\\theta(\\tau) = \\log \\mu(s_0) + \\sum_{h=0}^{H-1} \\log \\pi_\\theta(a_h\\mid s_h) + \\log P(s_{h+1} \\mid s_h, a_h)\n\\]\nWhen we take the gradient with respect to the parameters \\(\\theta\\), only the \\(\\pi_\\theta(a_h| s_h)\\) terms depend on \\(\\theta\\). This gives the following expression for the policy gradient, known as the “REINFORCE” policy gradient Williams (1992):\n\\[\n\\begin{aligned}\n    \\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{h=0}^{H-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_h| s_h) R(\\tau) \\right]\n\\end{aligned}\n\\tag{6.2}\\]\nThis expression allows us to estimate the gradient by sampling a few sample trajectories from \\(\\pi_\\theta,\\) calculating the likelihoods of the chosen actions, and substituting these into the expression inside the brackets of Equation 6.2. Then we can update the parameters \\(\\theta\\) in this direction to perform stochastic gradient ascent.\nThe rest of this chapter investigates ways to reduce the variance of this estimator by subtracting off certain correlated quantities.\n\nLemma 6.1 (Intuition behind REINFORCE) Intuitively speaking, we want to update the policy parameters to maximize the probability of taking optimal actions. That is, suppose we are in state \\(s\\), and \\(a^\\star\\) is an optimal action to take. Then we want to solve \\(\\theta = \\arg\\max_{\\theta'} \\pi_{\\theta'}(a^\\star \\mid s)\\), which would lead to the gradient ascent expression\n\\[\n\\theta \\gets \\theta + \\nabla \\pi_{\\theta}(a^\\star \\mid s).\n\\]\nHowever, we don’t know the optimal action \\(a^\\star\\) in practice. So instead, we must try many actions, and increase the probability of the “good” ones and decrease the probability of the “bad” ones. Suppose \\(A(s, a)\\) is a measure of how good action \\(a\\) is in state \\(s\\). Then we could write\n\\[\n\\theta \\gets \\theta + \\sum_a \\pi_{\\theta}(a \\mid s) A(s, a) \\nabla \\pi_{\\theta}(a \\mid s).\n\\]\nBut this has an issue: the size of each step doesn’t just depend on how good it is, but also how often the policy takes it already. This could lead to a positive feedback loop where likely actions become more and more likely, without respect to the quality of the action. So we divide by the likelihood to cancel out this factor:\n\\[\n\\theta \\gets \\theta + \\sum_a \\pi_{\\theta}(a \\mid s) A(s, a) \\frac{\\nabla \\pi_{\\theta}(a \\mid s)}{\\pi_{\\theta}(a \\mid s)}.\n\\]\nBut once we simplify, and sum across timesteps, this becomes almost exactly the gradient written above!\n\\[\n\\theta \\gets \\theta + \\mathbb{E}_{a \\sim \\pi_{\\theta}(\\cdot \\mid s)} [\\sum_{h=0}^{H-1} A(s_h, a_h) \\nabla \\log \\pi_{\\theta}(a_h\\mid s_h) ].\n\\]\nWe will see later on what \\(A\\) concretely corresponds to.\n\n\n\nCode\ndef estimate_gradient_reinforce_pseudocode(env: gym.Env, pi, theta: Float[Array, \" D\"]):\n    \"\"\"Estimate the policy gradient using REINFORCE.\"\"\"\n    tau = sample_trajectory(env, pi(theta))\n    nabla_hat = jnp.zeros_like(theta)\n    total_reward = sum(r for _s, _a, r in tau)\n    for s, a, r in tau:\n        def policy_log_likelihood(theta: Float[Array, \" D\"]) -&gt; float:\n            return log(pi(theta)(s, a))\n        nabla_hat += jax.grad(policy_log_likelihood)(theta) * total_reward\n    return nabla_hat\n\nlatex(estimate_gradient_reinforce_pseudocode, id_to_latex={\"jax.grad\": r\"\\nabla\"})\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{estimate\\_gradient\\_reinforce\\_pseudocode}(\\mathrm{env}: \\mathrm{gym}.\\mathrm{Env}, \\pi, \\theta: \\mathbb{R}^{D}) \\\\ \\hspace{1em} \\textrm{\"Estimate the policy gradient using REINFORCE.\"} \\\\ \\hspace{1em} \\tau \\gets \\mathrm{sample\\_trajectory} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\widehat{\\nabla} \\gets \\mathrm{jnp}.\\mathrm{zeros\\_like} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathrm{total\\_reward} \\gets \\sum_{\\mathopen{}\\left( \\mathrm{\\_s}, \\mathrm{\\_a}, r \\mathclose{}\\right) \\in \\tau}^{} \\mathopen{}\\left({r}\\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{for} \\ \\mathopen{}\\left( s, a, r \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{policy\\_log\\_likelihood}(\\theta: \\mathbb{R}^{D}) \\\\ \\hspace{3em} \\mathbf{return} \\ \\log \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\widehat{\\nabla} \\gets \\widehat{\\nabla} + \\nabla \\mathopen{}\\left( \\mathrm{policy\\_log\\_likelihood} \\mathclose{}\\right) \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\cdot \\mathrm{total\\_reward} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\widehat{\\nabla} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nFor some intuition into how this method works, recall that we update our parameters according to\n\\[\n\\begin{aligned}\n    \\theta_{t+1} &= \\theta_t + \\eta \\nabla J(\\theta_t) \\\\\n    &= \\theta_t + \\eta \\mathbb{E}_{\\tau \\sim \\rho_{\\theta_t}} [\\nabla \\log \\rho_{\\theta_t}(\\tau) \\cdot R(\\tau)].\n\\end{aligned}\n\\]\nConsider the “good” trajectories where \\(R(\\tau)\\) is large. Then \\(\\theta\\) gets updated so that these trajectories become more likely. To see why, recall that \\(\\rho_{\\theta}(\\tau)\\) is the likelihood of the trajectory \\(\\tau\\) under the policy \\(\\pi_\\theta,\\) so the gradient points in the direction that makes \\(\\tau\\) more likely.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#baselines-and-advantages",
    "href": "pg.html#baselines-and-advantages",
    "title": "6  Policy Gradient Methods",
    "section": "6.5 Baselines and advantages",
    "text": "6.5 Baselines and advantages\nA central idea from supervised learning is the bias-variance decomposition, which shows that the mean squared error of an estimator is the sum of its squared bias and its variance. The REINFORCE gradient estimator Equation 6.2 is already unbiased, meaning that its expectation over trajectories is the true policy gradient. Can we find ways to reduce its variance as well?\nAs a first step, consider that the action taken at step \\(t\\) does not affect the reward from previous timesteps, since they’re already in the past. You can also show rigorously that this is the case, and that we only need to consider the present and future rewards to calculate the policy gradient:\n\\[\n\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{h=0}^{H-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_h| s_h) \\sum_{h' = h}^{H-1} r(s_{h'}, a_{h'}) \\right]\n\\]\nFurthermore, by a conditioning argument, we can replace the inner sum over remaining rewards with the policy’s Q-function, evaluated at the current state:\n\\[\n\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{h=0}^{H-1} \\nabla_\\theta \\log \\pi_{\\theta}(a_h| s_h) Q^{\\pi_\\theta}(s_{h}, a_{h}) \\right]\n\\tag{6.3}\\]\nExercise: Prove that this is equivalent to the previous definitions. What modification to the expression must be made for the discounted, infinite-horizon setting?\nWe can further reduce variance by subtracting a baseline function \\(b_h: \\mathcal{S} \\to \\mathbb{R}\\) at each timestep \\(h\\). This modifies the policy gradient as follows:\n\\[\n\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[\n    \\sum_{h=0}^{H-1} \\nabla \\log \\pi_\\theta (a_h| s_h) \\left(\n    Q^{\\pi_\\theta}(s_h, a_h)\n    - b_h(s_h)\n    \\right)\n    \\right].\n\\tag{6.4}\\]\n(Again, you should try to prove that this equality still holds.) For example, we might want \\(b_h\\) to estimate the average reward-to-go at a given timestep:\n\\[\nb_h^\\theta = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} R_h(\\tau).\n\\]\nAs a better baseline, we could instead choose the value function. Note that the random variable \\(Q^\\pi_h(s, a) - V^\\pi_h(s),\\) where the randomness is taken over the actions, is centered around zero. (Recall \\(V^\\pi_h(s) = \\mathbb{E}_{a \\sim \\pi} Q^\\pi_h(s, a).\\)) This quantity matches the intuition given in Lemma 6.1: it is positive for actions that are better than average (in state \\(s\\)), and negative for actions that are worse than average. In fact, this quantity has a particular name: the advantage function.\n\nDefinition 6.2 (Advantage function) \\[\nA^\\pi_h(s, a) = Q^\\pi_h(s, a) - V^\\pi_h(s)\n\\]\n\nThis measures how much better this action does than the average for that policy. (Note that for an optimal policy \\(\\pi^\\star,\\) the advantage of a given state-action pair is always zero or negative.)\nWe can now express the policy gradient as follows. Note that the advantage function effectively replaces the \\(Q\\)-function from Equation 6.3:\n\\[\n\\nabla J(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[\n        \\sum_{h=0}^{H-1} \\nabla \\log \\pi_\\theta(a_h| s_h) A^{\\pi_\\theta}_h(s_h, a_h)\n\\right].\n\\tag{6.5}\\]\n\nExample 6.5 (Policy gradient for the linear-in-features parameterization) The gradient-log-likelihood for the linear-in-features parameterization Example 6.2 is also quite elegant:\n\\[\n\\begin{aligned}\n        \\nabla \\log \\pi_\\theta(a|s) &= \\nabla \\left( \\theta^\\top \\phi(s, a) - \\log \\left( \\sum_{a'} \\exp(\\theta^\\top \\phi(s, a')) \\right) \\right) \\\\\n        &= \\phi(s, a) - \\mathbb{E}_{a' \\sim \\pi_\\theta(s)} \\phi(s, a')\n\\end{aligned}\n\\]\nPlugging this into our policy gradient expression, we get\n\\[\n\\begin{aligned}\n    \\nabla J(\\theta) & = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[\n    \\sum_{t=0}^{T-1} \\nabla \\log \\pi_\\theta(a_h| s_h) A_h^{\\pi_\\theta}\n    \\right]                                                                                                                    \\\\\n                     & = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[\n    \\sum_{t=0}^{T-1} \\left( \\phi(s_h, a_h) - \\mathbb{E}_{a' \\sim \\pi(s_h)} \\phi(s_h, a') \\right) A_h^{\\pi_\\theta}(s_h, a_h)\n    \\right]                                                                                                                    \\\\\n                     & = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{t=0}^{T-1} \\phi(s_h, a_h) A_h^{\\pi_\\theta} (s_h, a_h) \\right]\n\\end{aligned}\n\\]\nWhy can we drop the \\(\\mathbb{E}\\phi(s_h, a')\\) term? By linearity of expectation, consider the dropped term at a single timestep: \\(\\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\left( \\mathbb{E}_{a' \\sim \\pi(s_h)} \\phi(s, a') \\right) A_h^{\\pi_\\theta}(s_h, a_h) \\right].\\) By Adam’s Law, we can wrap the advantage term in a conditional expectation on the state \\(s_h.\\) Then we already know that \\(\\mathbb{E}_{a \\sim \\pi(s)} A_h^{\\pi}(s, a) = 0,\\) and so this entire term vanishes.\n\nNote that to avoid correlations between the gradient estimator and the value estimator (i.e. baseline), we must estimate them with independently sampled trajectories:\n\n\n\nCode\ndef pg_with_learned_baseline(env: gym.Env, pi, eta: float, theta_init, K: int, N: int) -&gt; Float[Array, \" D\"]:\n    theta = theta_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, pi(theta), N)\n        V_hat = fit_value(trajectories)\n        tau = sample_trajectories(env, pi(theta), 1)\n        nabla_hat = jnp.zeros_like(theta)  # gradient estimator\n\n        for h, (s, a) in enumerate(tau):\n            def log_likelihood(theta_opt):\n                return jnp.log(pi(theta_opt)(s, a))\n            nabla_hat = nabla_hat + jax.grad(log_likelihood)(theta) * (return_to_go(tau, h) - V_hat(s))\n        \n        theta = theta + eta * nabla_hat\n    return theta\n\nlatex(pg_with_learned_baseline)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{pg\\_with\\_learned\\_baseline}(\\mathrm{env}: \\mathrm{gym}.\\mathrm{Env}, \\pi, \\eta: \\mathrm{float}, \\theta_{\\mathrm{init}}, K: \\mathrm{int}, N: \\mathrm{int}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( K \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), N \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{V} \\gets \\mathrm{fit\\_value} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\tau \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), 1 \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{\\nabla} \\gets \\mathrm{jnp}.\\mathrm{zeros\\_like} \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ \\mathopen{}\\left( h, \\mathopen{}\\left( s, a \\mathclose{}\\right) \\mathclose{}\\right) \\in \\mathrm{enumerate} \\mathopen{}\\left( \\tau \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathbf{function} \\ \\mathrm{log\\_likelihood}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{4em} \\mathbf{return} \\ \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ function} \\\\ \\hspace{3em} \\widehat{\\nabla} \\gets \\widehat{\\nabla} + \\mathrm{jax}.\\mathrm{grad} \\mathopen{}\\left( \\mathrm{log\\_likelihood} \\mathclose{}\\right) \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\cdot \\mathopen{}\\left( \\mathrm{return\\_to\\_go} \\mathopen{}\\left( \\tau, h \\mathclose{}\\right) - \\widehat{V} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\theta \\gets \\theta + \\eta \\widehat{\\nabla} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nNote that you could also generalize this by allowing the learning rate \\(\\eta\\) to vary across steps, or take multiple trajectories \\(\\tau\\) and compute the sample average of the gradient estimates.\nThe baseline estimation step fit_value can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#comparing-policy-gradient-algorithms-to-policy-iteration",
    "href": "pg.html#comparing-policy-gradient-algorithms-to-policy-iteration",
    "title": "6  Policy Gradient Methods",
    "section": "6.6 Comparing policy gradient algorithms to policy iteration",
    "text": "6.6 Comparing policy gradient algorithms to policy iteration\n\nWhat advantages does the policy gradient algorithm have over the policy iteration algorithms covered in Section 1.5.3.2?\n\nRemark 6.1 (Policy iteration review). Recall that policy iteration is an algorithm for MDPs with unknown state transitions where we alternate between these two steps:\n\nEstimating the \\(Q\\)-function (or advantage function) of the current policy;\nUpdating the policy to be greedy with respect to this approximate \\(Q\\)-function (or advantage function).\n\n\nTo analyze the difference between them, we’ll make use of the performance difference lemma, which provides an expression for comparing the difference between two value functions.\n\nTheorem 6.1 (Performance difference lemma) Suppose Alice is playing a game (an MDP). Bob is spectating, and can evaluate how good an action is compared to his own strategy. (That is, Bob can compute his advantage function \\(A_h^{\\text{Bob}}(s_h, a_h)\\)). The performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:\n\\[\nV_0^{\\text{Alice}}(s) - V_0^{\\text{Bob}}(s) = \\mathbb{E}_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\sum_{h=0}^{H-1} A_h^{\\text{Bob}} (s_h, a_h) \\right]\n\\tag{6.6}\\]\nwhere \\(\\rho_{\\text{Alice}, s}\\) denotes the distribution over trajectories starting in state \\(s\\) when Alice is playing.\nTo see why, consider a specific step \\(h\\) in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average. But this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!\nFormally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that\n\\[\n\\begin{aligned}\nA^\\pi_h(s_h, a_h) &= Q^\\pi_h(s_h, a_h) - V^\\pi_h(s_h) \\\\\n&= r_h(s_h, a_h) + \\mathbb{E}_{s_{h+1} \\sim P(s_h, a_h)} [V^\\pi_{h+1}(s_{h+1})] - V^\\pi_h(s_h)\n\\end{aligned}\n\\]\nso expanding out the r.h.s. expression of Equation 6.6 and grouping terms together gives\n\\[\n\\begin{aligned}\n\\mathbb{E}_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\sum_{h=0}^{H-1} A_h^{\\text{Bob}} (s_h, a_h) \\right] &= \\mathbb{E}_{\\tau \\sim \\rho_{\\text{Alice}, s}} \\left[ \\left( \\sum_{h=0}^{H-1} r_h(s_h, a_h) \\right) + \\left( V^{\\text{Bob}}_1(s_1) + \\cdots + V^{\\text{Bob}}_H(s_H) \\right) - \\left( V^{\\text{Bob}_0}(s_0) + \\cdots + V^{\\text{Bob}}_{H-1}(s_{H-1}) \\right) \\right] \\\\\n&= V^{\\text{Alice}}_0(s) - V^{\\text{Bob}}_0(s)\n\\end{aligned}\n\\]\nas desired. (Note that the “inner” expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid.)\n\nThe PDL gives insight into why fitted approaches such as PI don’t work as well in the “full” RL setting. To see why, let’s consider a single iteration of policy iteration, where policy \\(\\pi\\) gets updated to \\(\\tilde \\pi\\). We’ll assume these policies are deterministic. Suppose the new policy \\(\\tilde \\pi\\) chooses some action with a negative advantage with respect to \\(\\pi\\). That is, when acting according to \\(\\pi\\), taking the action from \\(\\tilde \\pi\\) would perform worse than expected. Define \\(\\Delta_\\infty\\) to be the most negative advantage, that is, \\(\\Delta_\\infty = \\min_{s \\in \\mathcal{S}} A^{\\pi}_h(s, \\tilde \\pi(s))\\). Plugging this into the Theorem 6.1 gives\n\\[\n\\begin{aligned}\nV_0^{\\tilde \\pi}(s) - V_0^{\\pi}(s) &= \\mathbb{E}_{\\tau \\sim \\rho_{\\tilde \\pi, s}} \\left[\n\\sum_{h=0}^{H-1} A_h^{\\pi}(s_h, a_h)\n\\right] \\\\\n&\\ge H \\Delta_\\infty \\\\\nV_0^{\\tilde \\pi}(s) &\\ge V_0^{\\pi}(s) - H|\\Delta_\\infty|.\n\\end{aligned}\n\\]\nThat is, for some state \\(s\\), the lower bound on the performance of \\(\\tilde \\pi\\) is lower than the performance of \\(\\pi\\). This doesn’t state that \\(\\tilde \\pi\\) will necessarily perform worse than \\(\\pi\\), only suggests that it might be possible. If these worst case states do exist, though, PI does not avoid situations where the new policy often visits them; It does not enforce that the trajectory distributions \\(\\rho_\\pi\\) and \\(\\rho_{\\tilde \\pi}\\) be close to each other. In other words, the “training distribution” that our prediction rule is fitted on, \\(\\rho_\\pi\\), may differ significantly from the “evaluation distribution” \\(\\rho_{\\tilde \\pi}\\).\n\nOn the other hand, policy gradient methods do, albeit implicitly, encourage \\(\\rho_\\pi\\) and \\(\\rho_{\\tilde \\pi}\\) to be similar. Suppose that the mapping from policy parameters to trajectory distributions is relatively smooth. Then, by adjusting the parameters only a small distance, the new policy will also have a similar trajectory distribution. But this is not very rigorous, and in practice the parameter-to-distribution mapping may not be so smooth. Can we constrain the distance between the resulting distributions more explicitly?\nThis brings us to the next three methods: - trust region policy optimization (TRPO), which explicitly constrains the difference between the distributions before and after each step; - the natural policy gradient (NPG), a first-order approximation of TRPO; - proximal policy optimization (PPO), a “soft relaxation” of TRPO.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-trpo",
    "href": "pg.html#sec-trpo",
    "title": "6  Policy Gradient Methods",
    "section": "6.7 Trust region policy optimization",
    "text": "6.7 Trust region policy optimization\nWe saw above that policy gradient methods are effective because they implicitly constrain how much the policy changes at each iteration. Can we design an algorithm that explicitly constrains the “step size”? That is, we want to improve the policy as much as possible, measured in terms of the r.h.s. of the Theorem 6.1, while ensuring that its trajectory distribution does not change too much:\n\\[\n\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{\\theta^{\\text{opt}}} \\mathbb{E}_{s_0, \\dots, s_{H-1} \\sim \\pi^{k}} \\left[ \\sum_{h=0}^{H-1} \\mathbb{E}_{a_h\\sim \\pi^{\\theta^\\text{opt}}(s_h)} A^{\\pi^{k}}(s_h, a_h) \\right] \\\\\n& \\text{where } \\text{distance}(\\rho_{\\theta^{\\text{opt}}}, \\rho_{\\theta^k}) &lt; \\delta\n\\end{aligned}\n\\]\nNote that we have made a small change to the r.h.s. expression: we use the states sampled from the old policy, and only use the actions from the new policy. It would be computationally infeasible to sample entire trajectories from \\(\\pi_\\theta\\) as we are optimizing over \\(\\theta\\). On the other hand, if \\(\\pi_\\theta\\) returns a vector representing a probability distribution over actions, then evaluating the expected advantage with respect to this distribution only requires taking a dot product. This approximation also matches the r.h.s. of the PDL to first order in \\(\\theta\\). (We will elaborate more on this later.)\nHow do we describe the distance between \\(\\rho_{\\theta^{\\text{opt}}}\\) and \\(\\rho_{\\theta^k}\\)? We’ll use the Kullback-Leibler divergence (KLD):\n\nDefinition 6.3 (Kullback-Leibler divergence) For two PDFs \\(p, q\\),\n\\[\n\\mathrm{KL}\\left(p\\parallel q\\right) := \\mathbb{E}_{x \\sim p} \\left[ \\log \\frac{p(x)}{q(x)} \\right]\n\\]\nThis can be interpreted in many different ways, many stemming from information theory. One such interpretation is that \\(\\mathrm{KL}\\left(p\\parallel q\\right)\\) describes my average “surprise” if I think data is being generated by \\(q\\) but it’s actually generated by \\(p\\). (The surprise of an event with probability \\(p\\) is \\(- \\log_2 p\\).) Note that \\(\\mathrm{KL}\\left(p\\parallel q\\right) = 0\\) if and only if \\(p = q\\). Also note that it is generally not symmetric.\n\nBoth the objective function and the KLD constraint involve a weighted average over the space of all trajectories. This is intractable in general, so we need to estimate the expectation. As before, we can do this by taking an empirical average over samples from the trajectory distribution. This gives us the following pseudocode:\n\n\nCode\ndef kl_div_trajectories(pi, theta_1, theta_2, trajectories):\n    # Assume trajectories are sampled from pi(theta_1)\n    kl_div = 0\n    for tau in trajectories:\n        for s, a, _r in tau:\n            kl_div += jnp.log(pi(theta_1)(s, a)) - jnp.log(pi(theta_2)(s, a))\n    return kl_div / len(trajectories)\n\nlatex(kl_div_trajectories)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{kl\\_div\\_trajectories}(\\pi, \\theta_{\\mathrm{1}}, \\theta_{\\mathrm{2}}, \\mathrm{trajectories}) \\\\ \\hspace{1em} \\mathrm{kl\\_div} \\gets 0 \\\\ \\hspace{1em} \\mathbf{for} \\ \\tau \\in \\mathrm{trajectories} \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathbf{for} \\ \\mathopen{}\\left( s, a, \\mathrm{\\_r} \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathrm{kl\\_div} \\gets \\mathrm{kl\\_div} + \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{1}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) - \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{2}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\frac{\\mathrm{kl\\_div}}{\\mathrm{len} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right)} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\n\n\nCode\ndef trpo(env, δ, theta_init, n_interactions):\n    theta = theta_init\n    for k in range(K):\n        trajectories = sample_trajectories(env, pi(theta), n_interactions)\n        A_hat = fit_advantage(trajectories)\n        \n        def approximate_gain(theta_opt):\n            A_total = 0\n            for tau in trajectories:\n                for s, _a, _r in tau:\n                    for a in env.action_space:\n                        A_total += pi(theta)(s, a) * A_hat(s, a)\n            return A_total\n        \n        def constraint(theta_opt):\n            return kl_div_trajectories(pi, theta, theta_opt, trajectories) &lt;= δ\n        \n        theta = optimize(approximate_gain, constraint)\n\n    return theta\n\nlatex(trpo)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{trpo}(\\mathrm{env}, δ, \\theta_{\\mathrm{init}}, n_{\\mathrm{interactions}}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( K \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), n_{\\mathrm{interactions}} \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{A} \\gets \\mathrm{fit\\_advantage} \\mathopen{}\\left( \\mathrm{trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{approximate\\_gain}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} A_{\\mathrm{total}} \\gets 0 \\\\ \\hspace{3em} \\mathbf{for} \\ \\tau \\in \\mathrm{trajectories} \\ \\mathbf{do} \\\\ \\hspace{4em} \\mathbf{for} \\ \\mathopen{}\\left( s, \\mathrm{\\_a}, \\mathrm{\\_r} \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{5em} \\mathbf{for} \\ a \\in \\mathrm{env}.\\mathrm{action\\_space} \\ \\mathbf{do} \\\\ \\hspace{6em} A_{\\mathrm{total}} \\gets A_{\\mathrm{total}} + \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\cdot \\widehat{A} \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{5em} \\mathbf{end \\ for} \\\\ \\hspace{4em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{return} \\ A_{\\mathrm{total}} \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{constraint}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} \\mathbf{return} \\ \\mathrm{kl\\_div\\_trajectories} \\mathopen{}\\left( \\pi, \\theta, \\theta_{\\mathrm{opt}}, \\mathrm{trajectories} \\mathclose{}\\right) \\le δ \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\theta \\gets \\mathrm{optimize} \\mathopen{}\\left( \\mathrm{approximate\\_gain}, \\mathrm{constraint} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\n\nThe above isn’t entirely complete: we still need to solve the actual optimization problem at each step. Unless we know additional properties of the problem, this might be an intractable optimization. Do we need to solve it exactly, though? Instead, if we assume that both the objective function and the constraint are somewhat smooth in terms of the policy parameters, we can use their Taylor expansions to give us a simpler optimization problem with a closed-form solution. This brings us to the natural policy gradient algorithm.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#natural-policy-gradient",
    "href": "pg.html#natural-policy-gradient",
    "title": "6  Policy Gradient Methods",
    "section": "6.8 Natural policy gradient",
    "text": "6.8 Natural policy gradient\nWe take a linear (first-order) approximation to the objective function and a quadratic (second-order) approximation to the KL divergence constraint about the current estimate \\(\\theta^k\\). This results in the optimization problem\n\\[\n\\begin{gathered}\n    \\max_\\theta \\nabla_\\theta J(\\pi_{\\theta^k})^\\top (\\theta - \\theta^k) \\\\\n    \\text{where } \\frac{1}{2} (\\theta - \\theta^k)^\\top F_{\\theta^k} (\\theta - \\theta^k) \\le \\delta\n\\end{gathered}\n\\tag{6.7}\\]\nwhere \\(F_{\\theta^k}\\) is the Fisher information matrix defined below.\n\nDefinition 6.4 (Fisher information matrix) Let \\(p_\\theta\\) denote a parameterized distribution. Its Fisher information matrix \\(F_\\theta\\) can be defined equivalently as:\n\\[\n\\begin{aligned}\n        F_{\\theta} & = \\mathbb{E}_{x \\sim p_\\theta} \\left[ (\\nabla_\\theta \\log p_\\theta(x)) (\\nabla_\\theta \\log p_\\theta(x))^\\top \\right] & \\text{covariance matrix of the Fisher score}          \\\\\n                   & = \\mathbb{E}_{x \\sim p_{\\theta}} [- \\nabla_\\theta^2 \\log p_\\theta(x)]                                                & \\text{average Hessian of the negative log-likelihood}\n\\end{aligned}\n\\]\nRecall that the Hessian of a function describes its curvature: for a vector \\(\\delta \\in \\Theta\\), the quantity \\(\\delta^\\top F_\\theta \\delta\\) describes how rapidly the negative log-likelihood changes if we move by \\(\\delta\\). The Fisher information matrix is precisely the Hessian of the KL divergence (with respect to either one of the parameters).\nIn particular, when \\(p_\\theta = \\rho_{\\theta}\\) denotes a trajectory distribution, we can further simplify the expression:\n\\[\nF_{\\theta} = \\mathbb{E}_{\\tau \\sim \\rho_\\theta} \\left[ \\sum_{h=0}^{H-1} (\\nabla \\log \\pi_\\theta (a_h\\mid s_h)) (\\nabla \\log \\pi_\\theta(a_h\\mid s_h))^\\top \\right]\n\\tag{6.8}\\]\nNote that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.\n\nThis is a convex optimization problem with a closed-form solution. To see why, it helps to visualize the case where \\(\\theta\\) is two-dimensional: the constraint describes the inside of an ellipse, and the objective function is linear, so we can find the extreme point on the boundary of the ellipse. We recommend Boyd and Vandenberghe (2004) for a comprehensive treatment of convex optimization.\nMore generally, for a higher-dimensional \\(\\theta\\), we can compute the global optima by setting the gradient of the Lagrangian to zero:\n\\[\n\\begin{aligned}\n    \\mathcal{L}(\\theta, \\alpha)                     & = \\nabla J(\\pi_{\\theta^k})^\\top (\\theta - \\theta^k) - \\alpha \\left[ \\frac{1}{2} (\\theta - \\theta^k)^\\top F_{\\theta^k} (\\theta - \\theta^k) - \\delta \\right] \\\\\n    \\nabla \\mathcal{L}(\\theta^{k+1}, \\alpha) & := 0                                                                                                                                                             \\\\\n    \\implies \\nabla J(\\pi_{\\theta^k})        & = \\alpha F_{\\theta^k} (\\theta^{k+1} - \\theta^k)                                                                                                                   \\\\\n    \\theta^{k+1}                           & = \\theta^k + \\eta F_{\\theta^k}^{-1} \\nabla J(\\pi_{\\theta^k})                                                                                             \\\\\n    \\text{where } \\eta                     & = \\sqrt{\\frac{2 \\delta}{\\nabla J(\\pi_{\\theta^k})^\\top F_{\\theta^k}^{-1} \\nabla J(\\pi_{\\theta^k})}}\n\\end{aligned}\n\\]\nThis gives us the closed-form update. Now the only challenge is to estimate the Fisher information matrix, since, as with the KL divergence constraint, it is an expectation over trajectories, and computing it exactly is therefore typically intractable.\n\nDefinition 6.5 (Natural policy gradient) How many trajectory samples do we need to accurately estimate the Fisher information matrix? As a rule of thumb, the sample complexity should scale with the dimension of the parameter space. This makes this approach intractable in the deep learning setting where we might have a very large number of parameters.\n\nAs you can see, the NPG is the “basic” policy gradient algorithm we saw above, but with the gradient transformed by the inverse Fisher information matrix. This matrix can be understood as accounting for the geometry of the parameter space. The typical gradient descent algorithm implicitly measures distances between parameters using the typical Euclidean distance. Here, where the parameters map to a distribution, using the natural gradient update is equivalent to optimizing over distribution space rather than parameter space, where distance between distributions is measured by the Definition 6.3.\n\nExample 6.6 (Natural gradient on a simple problem) Let’s step away from RL and consider the following optimization problem over Bernoulli distributions \\(\\pi \\in \\Delta(\\{ 0, 1 \\})\\):\n\\[\n\\begin{aligned}\n        J(\\pi) & = 100 \\cdot \\pi(1) + 1 \\cdot \\pi(0)\n\\end{aligned}\n\\]\nWe can think of the space of such distributions as the line between \\((0, 1)\\) to \\((1, 0)\\) on the Cartesian plane:\n\n\nCode\nx = jnp.linspace(0, 1, 50)\ny = 1 - x\nplt.plot(x, y)\nplt.xlabel(r\"$\\pi(0)$\")\nplt.ylabel(r\"$\\pi(1)$\")\nplt.title(\"Space of Bernoulli distributions\")\nplt.show()\n\n\n\n\n\nA line from (0, 1) to (1, 0)\n\n\n\n\nClearly the optimal distribution is the constant one \\(\\pi(1) = 1\\). Suppose we optimize over the parameterized family \\(\\pi_\\theta(1) = \\frac{\\exp(\\theta)}{1+\\exp(\\theta)}\\). Then our optimization algorithm should set \\(\\theta\\) to be unboundedly large. Then the “vanilla” gradient is\n\\[\n\\nabla_\\theta J(\\pi_\\theta) = \\frac{99 \\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\\]\nNote that as \\(\\theta \\to \\infty\\) that the increments get closer and closer to \\(0\\); the rate of increase becomes exponentially slow.\nHowever, if we compute the Fisher information “matrix” (which is just a scalar in this case), we can account for the geometry induced by the parameterization.\n\\[\n\\begin{aligned}\n        F_\\theta & = \\mathbb{E}_{x \\sim \\pi_\\theta} [ (\\nabla_\\theta \\log \\pi_\\theta(x))^2 ] \\\\\n                 & = \\frac{\\exp(\\theta)}{(1 + \\exp(\\theta))^2}.\n\\end{aligned}\n\\]\nThis gives the natural gradient update\n\\[\n\\begin{aligned}\n        \\theta^{k+1} & = \\theta^k + \\eta F_{\\theta^k}^{-1} \\nabla_ \\theta J(\\theta^k) \\\\\n                     & = \\theta^k + 99 \\eta\n\\end{aligned}\n\\]\nwhich increases at a constant rate, i.e. improves the objective more quickly than “vanilla” gradient ascent.\n\nThough the NPG now gives a closed-form optimization step, it requires computing the inverse Fisher information matrix, which typically scales as \\(O((\\dim \\Theta)^3)\\). This can be expensive if the parameter space is large. Can we find an algorithm that works in linear time with respect to the dimension of the parameter space?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#sec-ppo",
    "href": "pg.html#sec-ppo",
    "title": "6  Policy Gradient Methods",
    "section": "6.9 Proximal policy optimization",
    "text": "6.9 Proximal policy optimization\nWe can relax the TRPO optimization problem in a different way: Rather than imposing a hard constraint on the KL distance, we can instead impose a soft constraint by incorporating it into the objective and penalizing parameter values that drastically change the trajectory distribution.\n\\[\n\\begin{aligned}\n\\theta^{k+1} &\\gets \\arg\\max_{\\theta} \\mathbb{E}_{s_0, \\dots, s_{H-1} \\sim \\rho_{\\pi^{k}}} \\left[ \\sum_{h=0}^{H-1} \\mathbb{E}_{a_h\\sim \\pi_{\\theta}(s_h)} A^{\\pi^{k}}(s_h, a_h) \\right] - \\lambda \\mathrm{KL}\\left(\\rho_{\\theta}\\parallel\\rho_{\\theta^k}\\right)\n\\end{aligned}\n\\]\nHere \\(\\lambda\\) is a regularization hyperparameter that controls the tradeoff between the two terms. This is the objective of the proximal policy optimization algorithm (Schulman et al. (2017)).\nHow do we solve this optimization? Let us begin by simplifying the \\(\\mathrm{KL}\\left(\\rho_{\\pi^k}\\parallel\\rho_{\\pi_{\\theta}}\\right)\\) term. Expanding gives\n\\[\n\\begin{aligned}\n    \\mathrm{KL}\\left(\\rho_{\\pi^k}\\parallel\\rho_{\\pi_{\\theta}}\\right) & = \\mathbb{E}_{\\tau \\sim \\rho_{\\pi^k}} \\left[\\log \\frac{\\rho_{\\pi^k}(\\tau)}{\\rho_{\\pi_{\\theta}}(\\tau)}\\right]                                                       \\\\\n                                           & = \\mathbb{E}_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{\\pi^k(a_h\\mid s_h)}{\\pi_{\\theta}(a_h\\mid s_h)}\\right] & \\text{state transitions cancel} \\\\\n                                           & = \\mathbb{E}_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_h\\mid s_h)}\\right] + c\n\\end{aligned}\n\\]\nwhere \\(c\\) is some constant with respect to \\(\\theta\\), and can be ignored. This gives the objective\n\\[\n\\ell^k(\\theta)\n=\n\\mathbb{E}_{s_0, \\dots, s_{H-1} \\sim \\rho_{\\pi^{k}}} \\left[ \\sum_{h=0}^{H-1} \\mathbb{E}_{a_h\\sim \\pi_{\\theta}(s_h)} A^{\\pi^{k}}(s_h, a_h) \\right] - \\lambda \\mathbb{E}_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\log \\frac{1}{\\pi_{\\theta}(a_h\\mid s_h)}\\right]\n\\]\nOnce again, this takes an expectation over trajectories. But here we cannot directly sample trajectories from \\(\\pi^k\\), since in the first term, the actions actually come from \\(\\pi_\\theta\\). To make this term line up with the other expectation, we would need the actions to also come from \\(\\pi^k\\).\nThis should sound familiar: we want to estimate an expectation over one distribution by sampling from another. We can once again use Section 6.3.2 to rewrite the inner expectation:\n\\[\n\\mathbb{E}_{a_h\\sim \\pi_{\\theta}(s_h)} A^{\\pi^{k}}(s_h, a_h)\n=\n\\mathbb{E}_{a_h\\sim \\pi^k(s_h)} \\frac{\\pi_\\theta(a_h\\mid s_h)}{\\pi^k(a_h\\mid s_h)} A^{\\pi^{k}}(s_h, a_h)\n\\]\nNow we can combine the expectations together to get the objective\n\\[\n\\ell^k(\\theta) = \\mathbb{E}_{\\tau \\sim \\rho_{\\pi^k}} \\left[ \\sum_{h=0}^{H-1} \\left( \\frac{\\pi_\\theta(a_h\\mid s_h)}{\\pi^k(a_h\\mid s_h)} A^{\\pi^k}(s_h, a_h) - \\lambda \\log \\frac{1}{\\pi_\\theta(a_h\\mid s_h)} \\right) \\right]\n\\]\nNow we can estimate this function by a sample average over trajectories from \\(\\pi^k\\). Remember that to complete a single iteration of PPO, we execute\n\\[\n\\theta^{k+1} \\gets \\arg\\max_{\\theta} \\ell^k(\\theta).\n\\]\nIf \\(\\ell^k\\) is differentiable, we can optimize it by gradient ascent, completing a single iteration of PPO.\n\n\nCode\nfrom typing import TypeVar\n\nState = TypeVar(\"State\")\nAction = TypeVar(\"Action\")\n\ndef ppo(\n    env,\n    pi: Callable[[Float[Array, \" D\"]], Callable[[State, Action], float]],\n    λ: float,\n    theta_init: Float[Array, \" D\"],\n    n_iters: int,\n    n_fit_trajectories: int,\n    n_sample_trajectories: int,\n):\n    theta = theta_init\n    for k in range(n_iters):\n        fit_trajectories = sample_trajectories(env, pi(theta), n_fit_trajectories)\n        A_hat = fit(fit_trajectories)\n\n        sample_trajectories = sample_trajectories(env, pi(theta), n_sample_trajectories)\n        \n        def objective(theta_opt):\n            total_objective = 0\n            for tau in sample_trajectories:\n                for s, a, _r in tau:\n                    total_objective += pi(theta_opt)(s, a) / pi(theta)(s, a) * A_hat(s, a) + λ * jnp.log(pi(theta_opt)(s, a))\n            return total_objective / n_sample_trajectories\n        \n        theta = optimize(objective, theta)\n\n    return theta\n\nlatex(ppo)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{ppo}(\\mathrm{env}, \\pi: \\mathrm{Callable}_{\\mathopen{}\\left( \\mathopen{}\\left[ \\mathbb{R}^{D} \\mathclose{}\\right], \\mathrm{Callable}_{\\mathopen{}\\left( \\mathopen{}\\left[ \\mathrm{State}, \\mathrm{Action} \\mathclose{}\\right], \\mathrm{float} \\mathclose{}\\right)} \\mathclose{}\\right)}, λ: \\mathrm{float}, \\theta_{\\mathrm{init}}: \\mathbb{R}^{D}, n_{\\mathrm{iters}}: \\mathrm{int}, \\mathrm{n\\_fit\\_trajectories}: \\mathrm{int}, \\mathrm{n\\_sample\\_trajectories}: \\mathrm{int}) \\\\ \\hspace{1em} \\theta \\gets \\theta_{\\mathrm{init}} \\\\ \\hspace{1em} \\mathbf{for} \\ k \\in \\mathrm{range} \\mathopen{}\\left( n_{\\mathrm{iters}} \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{2em} \\mathrm{fit\\_trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), \\mathrm{n\\_fit\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\widehat{A} \\gets \\mathrm{fit} \\mathopen{}\\left( \\mathrm{fit\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathrm{sample\\_trajectories} \\gets \\mathrm{sample\\_trajectories} \\mathopen{}\\left( \\mathrm{env}, \\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right), \\mathrm{n\\_sample\\_trajectories} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{function} \\ \\mathrm{objective}(\\theta_{\\mathrm{opt}}) \\\\ \\hspace{3em} \\mathrm{total\\_objective} \\gets 0 \\\\ \\hspace{3em} \\mathbf{for} \\ \\tau \\in \\mathrm{sample\\_trajectories} \\ \\mathbf{do} \\\\ \\hspace{4em} \\mathbf{for} \\ \\mathopen{}\\left( s, a, \\mathrm{\\_r} \\mathclose{}\\right) \\in \\tau \\ \\mathbf{do} \\\\ \\hspace{5em} \\mathrm{total\\_objective} \\gets \\mathrm{total\\_objective} + \\frac{\\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right)}{\\pi \\mathopen{}\\left( \\theta \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right)} \\widehat{A} \\mathopen{}\\left( s, a \\mathclose{}\\right) + λ \\cdot \\log \\pi \\mathopen{}\\left( \\theta_{\\mathrm{opt}} \\mathclose{}\\right) \\mathopen{}\\left( s, a \\mathclose{}\\right) \\\\ \\hspace{4em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{end \\ for} \\\\ \\hspace{3em} \\mathbf{return} \\ \\frac{\\mathrm{total\\_objective}}{\\mathrm{n\\_sample\\_trajectories}} \\\\ \\hspace{2em} \\mathbf{end \\ function} \\\\ \\hspace{2em} \\theta \\gets \\mathrm{optimize} \\mathopen{}\\left( \\mathrm{objective}, \\theta \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ for} \\\\ \\hspace{1em} \\mathbf{return} \\ \\theta \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "pg.html#summary",
    "href": "pg.html#summary",
    "title": "6  Policy Gradient Methods",
    "section": "6.10 Summary",
    "text": "6.10 Summary\nPolicy gradient methods are a powerful family of algorithms that directly optimize the expected total reward by iteratively updating the policy parameters. Precisely, we estimate the gradient of the expected total reward (with respect to the parameters), and update the parameters in that direction. But estimating the gradient is a tricky task! We saw many ways to reduce the variance of the gradient estimator, culminating in the advantage-based expression Equation 6.5.\nBut updating the parameters doesn’t entirely solve the problem: Sometimes, a small step in the parameters might lead to a big step in the policy. To avoid changing the policy too much at each step, we must account for the curvature in the parameter space. We first did this explicitly with Section 6.7, and then saw ways to relax the constraint in Definition 6.5 and Section 6.9.\nThese are still popular methods to this day, especially because they efficiently integrate with deep neural networks for representing complex functions.\n\n\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. “Automatic Differentiation in Machine Learning: A Survey.” February 5, 2018. https://doi.org/10.48550/arXiv.1502.05767.\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2004. Convex Optimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/.\n\n\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms.” August 28, 2017. https://doi.org/10.48550/arXiv.1707.06347.\n\n\nWilliams, Ronald J. 1992. “Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.” Machine Learning 8 (3): 229–56. https://doi.org/10.1007/BF00992696.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Policy Gradient Methods</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html",
    "href": "imitation_learning.html",
    "title": "7  Imitation Learning",
    "section": "",
    "text": "7.1 Introduction\nImagine you are tasked with learning how to drive. How do, or did, you go about it? At first, this task might seem insurmountable: there are a vast array of controls, and the cost of making a single mistake could be extremely high, making it hard to explore by trial and error. Luckily, there are already people in the world who know how to drive who can get you started. In almost every challenge we face, we “stand on the shoulders of giants” and learn skills from experts who have already mastered them.\nNow in machine learning, we are often trying to teach machines to accomplish tasks that humans are already proficient at. In such cases, the machine learning algorithm is the one learning the new skill, and humans are the “experts” that can demonstrate how to perform the task. Imitation learning is an approach to reinforcement learning where we aim to learn a policy that performs at least as well as the expert. It is often used as a first step for complex tasks where it is impractical to learn from scratch.\nWe’ll see that the most naive form of imitation learning, called behavioral cloning (or “behavior cloning”), is really an application of supervised learning to interactive tasks. We’ll then explore dataset aggregation (DAgger) as a way to query an expert and learn even more effectively.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#introduction",
    "href": "imitation_learning.html#introduction",
    "title": "7  Imitation Learning",
    "section": "",
    "text": "a robot imitating the pose of a young child (Photo by Pavel Danilyuk: https://www.pexels.com/photo/a-robot-imitating-a-girl-s-movement-8294811/)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#behavioral-cloning",
    "href": "imitation_learning.html#behavioral-cloning",
    "title": "7  Imitation Learning",
    "section": "7.2 Behavioral cloning",
    "text": "7.2 Behavioral cloning\nThis notion of “learning from human-provided data” may remind you of the basic premise of Chapter 4. In supervised learning, there is some mapping from inputs to outputs, such as the task of assigning the correct label to an image, that humans can implicitly compute. To teach a machine to calculate this mapping, we first collect a large training dataset by getting people to label a lot of inputs, and then use some optimization algorithm to produce a predictor that maps from the inputs to the outputs as closely as possible.\nHow does this relate to interactive tasks? Here, the input is the observation seen by the agent and the output is the action it selects, so the mapping is the agent’s policy. What’s stopping us from applying supervised learning techniques to mimic the expert’s policy? In principle, nothing! This is called behavioral cloning.\n\nDefinition 7.1 (Behavioral cloning)  \n\nCollect a training dataset of trajectories \\(\\mathcal{D} = (s^n, a^n)_{n=1}^{N}\\) generated by an expert policy \\(\\pi_\\text{expert}\\). (For example, if the dataset contains \\(M\\) trajectories, each with a finite horizon \\(H\\), then \\(N = M \\times H\\).)\nUse a supervised learning algorithm \\(\\texttt{fit} : \\mathcal{D} \\mapsto \\widetilde{\\pi}\\) to extract a policy \\(\\widetilde{\\pi}\\) that approximates the expert policy.\n\n\nTypically, this second task can be framed as empirical risk minimization (which we previously saw in Section 4.3.2:\n\\[\n\\widetilde{\\pi} = \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=0}^{N-1} \\text{loss}(\\pi(s^n), a^n)\n\\]\nwhere \\(\\Pi\\) is some class of possible policies, \\(\\text{loss}\\) is the loss function to measure how different the policy’s prediction is from the true observed action, and the supervised learning algorithm itself, also known as the fitting method, tells us how to compute this \\(\\arg\\min\\).\nHow should we choose the loss function? In supervised learning, we saw that the mean squared error is a good choice for continuous outputs. However, how should we measure the difference between two actions in a discrete action space? In this setting, the policy acts more like a classifier that picks the best action in a given state. Rather than considering a deterministic policy that just outputs a single action, we’ll consider a stochastic policy \\(\\pi\\) that outputs a distribution over actions. This allows us to assign a likelihood to observing the entire dataset \\(\\mathcal{D}\\) under the policy \\(\\pi\\), as if the state-action pairs are independent:\n\\[\n\\mathbb{P}_\\pi(\\mathcal{D}) = \\prod_{n=1}^{N} \\pi(a_n \\mid s_n)\n\\]\nNote that the states and actions are not, however, actually independent! A key property of interactive tasks is that the agent’s output – the action that it takes – may influence its next observation. We want to find a policy under which the training dataset \\(\\mathcal{D}\\) is the most likely. This is called the maximum likelihood estimate of the policy that generated the dataset:\n\\[\n\\widetilde{\\pi} = \\arg\\max_{\\pi \\in \\Pi} \\mathbb{P}_{\\pi}(\\mathcal{D})\n\\]\nThis is also equivalent to doing empirical risk minimization with the negative log likelihood as the loss function:\n\\[\n\\begin{aligned}\n\\widetilde{\\pi} &= \\arg\\min_{\\pi \\in \\Pi} - \\log \\mathbb{P}_\\pi(\\mathcal{D}) \\\\\n&= \\arg\\min_{\\pi \\in \\Pi} \\sum_{n=1}^N - \\log \\pi(a_n \\mid s_n)\n\\end{aligned}\n\\]\n\n7.2.1 Performance of behavioral cloning\nCan we quantify how well this algorithm works? For simplicity, let’s consider the case where the action space is finite and both the expert policy and learned policy are deterministic. Suppose the learned policy obtains \\(\\varepsilon\\) classification error. That is, for trajectories drawn from the expert policy, the learned policy chooses a different action at most \\(\\varepsilon\\) of the time:\n\\[\n\\mathbb{E}_{\\tau \\sim \\rho_{\\pi_{\\text{expert}}}} \\left[ \\frac 1 H\\sum_{h=0}^{H-1} \\mathbf{1}\\left\\{ \\widetilde{\\pi}(s_h) \\ne \\pi_{\\text{expert}} (s_h) \\right\\} \\right] \\le \\varepsilon\n\\]\nThen, their value functions differ by\n\\[\n| V^{\\pi_{\\text{expert}}} - V^{\\widetilde{\\pi}} | \\le H^2 \\varepsilon\n\\]\nwhere \\(H\\) is the horizon.\n\n7.2.1.1 Performance of behavioral cloning\nRecall the Performance Difference Lemma (Theorem 6.1) allows us to express the difference between \\(\\pi-{\\text{expert}}\\) and \\(\\widetilde{\\pi}\\) as\n\\[\nV_0^{\\pi_{\\text{expert}}}(s) - V_0^{\\widetilde{\\pi}} (s) = \\mathbb{E}_{\\tau \\sim \\rho^{\\pi_{\\text{expert}}} \\mid s_0 = s} \\left[ \\sum_{h=0}^{H-1} A_h^{\\widetilde{\\pi}} (s_h, a_h) \\right].\n\\tag{7.1}\\]\nNow since the expert policy is deterministic, we can substitute \\(a_h= \\pi_{\\text{expert}}(s_h)\\). This allows us to make a further simplification: since \\(\\pi_{\\text{expert}}\\) is deterministic, the advantage of the chosen action is exactly zero:\n\\[\nA^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) = Q^{\\pi_{\\text{expert}}}(s, \\pi_{\\text{expert}}(s)) - V^{\\pi_{\\text{expert}}}(s) = 0.\n\\]\nBut the right-hand-side of Equation 7.1 uses \\(A^{\\widetilde{\\pi}}\\), not \\(A^{\\pi-{\\text{expert}}}\\). To bridge this gap, we now use the assumption that \\(\\widetilde{\\pi}\\) obtains \\(\\varepsilon\\) classification error. Note that \\(A_h^{\\widetilde{\\pi}}(s_h, \\pi_{\\text{expert}}(s_h)) = 0\\) when \\(\\pi_{\\text{expert}}(s_h) = \\widetilde{\\pi}(s_h)\\). In the case where the two policies differ on \\(s_h\\), which occurs with probability \\(\\varepsilon\\), the advantage is naively upper bounded by \\(H\\) (assuming rewards are bounded between \\(0\\) and \\(1\\)). Taking the final sum gives the desired bound.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#distribution-shift",
    "href": "imitation_learning.html#distribution-shift",
    "title": "7  Imitation Learning",
    "section": "7.3 Distribution shift",
    "text": "7.3 Distribution shift\nLet us return to the driving analogy. Suppose you have taken some driving lessons and now feel comfortable in your neighbourhood. But today you have to travel to an area you haven’t visited before, such as a highway, where it would be dangerous to try and apply the techniques you’ve already learned. This is the issue of distribution shift: a policy learned under a certain distribution of states may not perform well if this distribution changes.\nThis is already a common issue in supervised learning, where the training dataset for a model might not resemble the environment where it gets deployed. In interactive environments, this issue is further exacerbated by the dependency between the observations and the agent’s behavior; if you take a wrong turn early on, it may be difficult or impossible to recover in that trajectory.\nHow could you learn a strategy for these new settings? In the driving example, you might decide to install a dashcam to record the car’s surroundings. That way, once you make it back to safety, you can show the recording to an expert, who can provide feedback at each step of the way. Then the next time you go for a drive, you can remember the expert’s advice, and take a safer route. You could then repeat this training as many times as desired, thereby collecting the expert’s feedback over a diverse range of locations. This is the key idea behind dataset aggregation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#dataset-aggregation-dagger",
    "href": "imitation_learning.html#dataset-aggregation-dagger",
    "title": "7  Imitation Learning",
    "section": "7.4 Dataset aggregation (DAgger)",
    "text": "7.4 Dataset aggregation (DAgger)\nThe DAgger algorithm (Ross, Gordon, and Bagnell (2010)) assumes that we have query access to the expert policy. That is, for a given state \\(s\\), we can ask for the expert’s action \\(\\pi_{\\text{expert}}(s)\\) in that state. We also need access to the environment for rolling out policies. This makes DAgger an online algorithm, as opposed to pure behavioral cloning, which is offline since we don’t need to act in the environment at all.\nYou can think of DAgger as a specific way of collecting the dataset \\(\\mathcal{D}\\).\n\nDefinition 7.2 (DAgger algorithm) Inputs: \\(\\pi_{\\text{expert}}\\), an initial policy \\(\\pi_{\\text{init}}\\), the number of iterations \\(T\\), and the number of trajectories \\(N\\) to collect per iteration.\n\nInitialize \\(\\mathcal{D} = \\{\\}\\) (the empty set) and \\(\\pi = \\pi_{\\text{init}}\\).\nFor \\(t = 1, \\dots, T\\):\n\nCollect \\(N\\) trajectories \\(\\tau_1, \\dots, \\tau_N\\) using the current policy \\(\\pi\\).\nFor each trajectory \\(\\tau_n\\):\n\nReplace each action \\(a_h\\) in \\(\\tau_n\\) with the expert action \\(\\pi_{\\text{expert}}(s_h)\\).\nCall the resulting trajectory \\(\\tau^{\\text{expert}}_n\\).\n\n\\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{ \\tau^{\\text{expert}}_1, \\dots, \\tau^{\\text{expert}}_n \\}\\).\nLet \\(\\pi \\gets \\texttt{fit}(\\mathcal{D})\\), where \\(\\texttt{fit}\\) is a behavioral cloning algorithm.\n\nReturn \\(\\pi\\).\n\n\nWe leave the implementation as an exercise. How well does DAgger perform? A full proof can be found in Ross, Gordon, and Bagnell (2010) that under certain assumptions, the DAgger algorithm can better approximate the expert policy:\n\\[\n|V^{\\pi_{\\text{expert}}} - V^{\\pi_{\\text{DAgger}}}| \\le H \\varepsilon\n\\]\nwhere \\(\\varepsilon\\) is the “classification error” guaranteed by the supervised learning algorithm.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "imitation_learning.html#summary",
    "href": "imitation_learning.html#summary",
    "title": "7  Imitation Learning",
    "section": "7.5 Summary",
    "text": "7.5 Summary\nFor tasks where it is too difficult or expensive to learn from scratch, we can instead start off with a collection of expert demonstrations. Then we can use supervised learning techniques to find a policy that imitates the expert demonstrations.\nThe simplest way to do this is to apply a supervised learning algorithm to an already-collected dataset of expert state-action pairs. This is called behavioral cloning. However, given query access to the expert policy, we can do better by integrating its feedback in an online loop. The DAgger algorithm is one way of doing this, where we use the expert policy to augment trajectories and then learn from this augmented dataset using behavioral cloning.\n\n\n\n\nRoss, Stéphane, Geoffrey J. Gordon, and J. Bagnell. 2010. “A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning.” In. https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Imitation Learning</span>"
    ]
  },
  {
    "objectID": "planning.html",
    "href": "planning.html",
    "title": "8  Tree Search Methods",
    "section": "",
    "text": "8.1 Introduction\nHave you ever lost a strategy game against a skilled opponent? It probably seemed like they were ahead of you at every turn. They might have been planning ahead and anticipating your actions, then formulating their strategy to counter yours. If this opponent was a computer, they might have been using one of the strategies that we are about to explore.\nCode\n%load_ext autoreload\n%autoreload 2\nCode\nfrom utils import Int, Array, latex, jnp, NamedTuple\nfrom enum import IntEnum",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#deterministic-zero-sum-fully-observable-two-player-games",
    "href": "planning.html#deterministic-zero-sum-fully-observable-two-player-games",
    "title": "8  Tree Search Methods",
    "section": "8.2 Deterministic, zero sum, fully observable two-player games",
    "text": "8.2 Deterministic, zero sum, fully observable two-player games\nIn this chapter, we will focus on games that are:\n\ndeterministic,\nzero sum (one player wins and the other loses),\nfully observable, that is, the state of the game is perfectly known by both players,\nfor two players that alternate turns,\n\nWe can represent such a game as a complete game tree. Each possible state is a node in the tree, and since we only consider deterministic games, we can represent actions as edges leading from the current state to the next. Each path through the tree, from root to leaf, represents a single game.\n\n\n\nThe first two layers of the complete game tree of tic-tac-toe.\n\n\nIf you could store the complete game tree on a computer, you would be able to win every potentially winnable game by searching all paths from your current state and taking a winning move. We will see an explicit algorithm for this in Section 8.3. However, as games become more complex, it becomes computationally impossible to search every possible path.\nFor instance, a chess player has roughly 30 actions to choose from at each turn, and each game takes roughly 40 moves per player, so trying to solve chess exactly using minimax would take somewhere on the order of \\(30^{80} \\approx 10^{118}\\) operations. That’s 10 billion billion billion billion billion billion billion billion billion billion billion billion billion operations. As of the time of writing, the fastest processor can achieve almost 10 GHz (10 billion operations per second), so to fully solve chess using minimax is many, many orders of magnitude out of reach.\nIt is thus intractable, in any realistic setting, to solve the complete game tree exactly. Luckily, only a small fraction of those games ever occur in reality; Later in this chapter, we will explore ways to prune away parts of the tree that we know we can safely ignore. We can also approximate the value of a state without fully evaluating it. Using these approximations, we can no longer guarantee winning the game, but we can come up with strategies that will do well against most opponents.\n\n8.2.1 Notation\nLet us now describe these games formally. We’ll call the first player Max and the second player Min. Max seeks to maximize the final game score, while Min seeks to minimize the final game score.\n\nWe’ll use \\(\\mathcal{S}\\) to denote the set of all possible game states.\nThe game begins in some initial state \\(s_0 \\in \\mathcal{S}\\).\nMax moves on even turn numbers \\(h = 2n\\), and Min moves on odd turn numbers \\(h = 2n+1\\), where \\(n\\) is a natural number.\nThe space of possible actions, \\(\\mathcal{A}_h(s)\\), depends on the state itself, as well as whose turn it is. (For example, in tic-tac-toe, Max can only play Xs while Min can only play Os.)\nThe game ends after \\(H\\) total moves (which might be even or odd). We call the final state a terminal state.\n\\(P\\) denotes the state transitions, that is, \\(P(s, a)\\) denotes the resulting state when taking action \\(a \\in \\mathcal{A}(s)\\) in state \\(s\\). We’ll assume that this function is time-homogeneous (a.k.a. stationary) and doesn’t change across timesteps.\n\\(r(s)\\) denotes the game score of the terminal state \\(s\\). Note that this is some positive or negative value seen by both players: A positive value indicates Max winning, a negative value indicates Min winning, and a value of \\(0\\) indicates a tie.\n\nWe also call the sequence of states and actions a trajectory.\n\nAbove, we suppose that the game ends after \\(H\\) total moves. But most real games have a variable length. How would you describe this?\n\n\nExample 8.1 (Tic-tac-toe) Let us frame tic-tac-toe in this setting.\n\nEach of the \\(9\\) squares is either empty, marked X, or marked O. So there are \\(|\\mathcal{S}| = 3^9\\) potential states. Not all of these may be reachable!\nThe initial state \\(s_0\\) is the empty board.\nThe set of possible actions for Max in state \\(s\\), \\(\\mathcal{A}_{2n}(s)\\), is the set of tuples \\((\\text{``X''}, i)\\) where \\(i\\) refers to an empty square in \\(s\\). Similarly, \\(\\mathcal{A}_{2n+1}(s)\\) is the set of tuples \\((\\text{``O''}, i)\\) where \\(i\\) refers to an empty square in \\(s\\).\nWe can take \\(H = 9\\) as the longest possible game length.\n\\(P(s, a)\\) for a nonterminal state \\(s\\) is simply the board with the symbol and square specified by \\(a\\) marked into \\(s\\). Otherwise, if \\(s\\) is a terminal state, i.e. it already has three symbols in a row, the state no longer changes.\n\\(r(s)\\) at a terminal state is \\(+1\\) if there are three Xs in a row, \\(-1\\) if there are three Os in a row, and \\(0\\) otherwise.\n\n\nOur notation may remind you of Chapter 1. Given that these games also involve a sequence of states and actions, can we formulate them as finite-horizon MDPs? The two settings are not exactly analogous, since in MDPs we only consider a single policy, while these games involve two distinct players with opposite objectives. Since we want to analyze the behavior of both players at the same time, describing such a game as an MDP is more trouble than it’s worth.\n\n\nCode\nclass Player(IntEnum):\n    EMPTY = 0\n    X = 1\n    O = 2\n\n\nif False:\n    class TicTacToeEnv(gym.Env):\n        metadata = {\"render.modes\": [\"human\"]}\n\n        def __init__(self):\n            super().__init__()\n            self.action_space = spaces.Discrete(9)\n            self.observation_space = spaces.Box(\n                low=0, high=2, shape=(3, 3), dtype=jnp.int32\n            )\n            self.board = None\n            self.current_player = None\n            self.done = None\n\n        def reset(self, seed=None, options=None):\n            super().reset(seed=seed)\n            self.board = jnp.zeros((3, 3), dtype=jnp.int32)\n            self.current_player = Player.X\n            self.done = False\n            return self.board, {}\n\n        def step(self, action: jnp.int32) -&gt; Int[Array, \"3 3\"]:\n            \"\"\"Take the action a in state s.\"\"\"\n            if self.done:\n                raise ValueError(\"The game is already over. Call `env.reset()` to reset the environment.\")\n            \n            row, col = divmod(action, 3)\n            if self.board[row, col] != Player.EMPTY:\n                return self.board, -10\n            return s.at[row, col].set(player)\n\n        @staticmethod\n        def is_terminal(s: Int[Array, \"3 3\"]):\n            \"\"\"Check if the game is over.\"\"\"\n            return is_winner(s, Player.X) or is_winner(s, Player.O) or jnp.all(s == Player.EMPTY)\n\n        @staticmethod\n        def is_winner(board: Int[Array, \"3 3\"], player: Player):\n            \"\"\"Check if the given player has won.\"\"\"\n            return any(\n                jnp.all(board[i, :] == player) or\n                jnp.all(board[:, i] == player)\n                for i in range(3)\n            ) or jnp.all(jnp.diag(board) == player) or jnp.all(jnp.diag(jnp.fliplr(board)) == player)\n\n        @staticmethod\n        def show(s: Int[Array, \"3 3\"]):\n            \"\"\"Print the board.\"\"\"\n            for row in range(3):\n                print(\" | \".join(\" XO\"[s[row, col]] for col in range(3)))\n                if row &lt; 2:\n                    print(\"-\" * 5)",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-min-max-search",
    "href": "planning.html#sec-min-max-search",
    "title": "8  Tree Search Methods",
    "section": "8.3 Min-max search",
    "text": "8.3 Min-max search\nIn the introduction, we claimed that we could win any potentially winnable game by looking ahead and predicting the opponent’s actions. This would mean that each nonterminal state already has some predetermined game score, that is, in each state, it is already “obvious” which player is going to win.\nLet \\(V_\\hi^\\star(s)\\) denote the game score under optimal play from both players starting in state \\(s\\) at time \\(\\hi\\).\n\nDefinition 8.1 (Min-max search algorithm) \\[\nV_\\hi^{\\star}(s) = \\begin{cases}\nr(s) & \\hi = \\hor \\\\\n\\max_{a \\in \\mathcal{A}_\\hi(s)} V_{\\hi+1}^{\\star}(P(s, a)) & \\hi \\text{ is even and } \\hi &lt; H \\\\\n\\min_{a \\in \\mathcal{A}_\\hi(s)} V_{\\hi+1}^{\\star}(P(s, a)) & \\hi \\text{ is odd and } \\hi &lt; H \\\\\n\\end{cases}\n\\]\n\nWe can compute this by starting at the terminal states, when the game’s outcome is known, and working backwards, assuming that Max chooses the action that leads to the highest score and Min chooses the action that leads to the lowest score.\nThis translates directly into a recursive depth-first search algorithm for searching the complete game tree.\n\n\nCode\ndef minimax_search(s, player) -&gt; tuple[\"Action\", \"Value\"]:\n    # Return the value of the state (for Max) and the best action for Max to take.\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in env.action_space(s):\n            _, v = minimax_search(env.step(s, a), min)\n            if v &gt; v_max:\n                a_max, v_max = a, v\n        return a_max, v_max\n    else:\n        a_min, v_min = None, None\n        for a in env.action_space(s):\n            _, v = minimax_search(env.step(s, a), max)\n            if v &lt; v_min:\n                a_min, v_min = a, v\n        return a_min, v_min\n\nlatex(minimax_search, id_to_latex={\"env.step\": \"P\", \"env.action_space\": r\"\\mathcal{A}\"})\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{minimax\\_search}(s, \\mathrm{player}) \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{env}.\\mathrm{is\\_terminal} \\mathopen{}\\left( s \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( \\mathrm{None}, \\mathrm{env}.\\mathrm{winner} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{player} \\equiv \\mathrm{max} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathcal{A} \\mathopen{}\\left( s \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( P \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{min} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &gt; v_{\\mathrm{max}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{else} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathcal{A} \\mathopen{}\\left( s \\mathclose{}\\right) \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( P \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{max} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &lt; v_{\\mathrm{min}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\n\nExample 8.2 (Min-max search for a simple game) Consider a simple game with just two steps: Max chooses one of three possible actions (A, B, C), and then Min chooses one of three possible actions (D, E, F). The combination leads to a certain integer outcome, shown in the table below:\n\n\n\n\nD\nE\nF\n\n\n\n\nA\n4\n-2\n5\n\n\nB\n-3\n3\n1\n\n\nC\n0\n3\n-1\n\n\n\nWe can visualize this as the following complete game tree, where each box contains the value \\(V_\\hi^\\star(s)\\) of that node. The min-max values of the terminal states are already known:\n\nWe begin min-max search at the root, exploring each of Max’s actions. Suppose Max chooses action A. Then Min will choose action E to minimize the game score, making the value of this game node \\(\\min(4, -2, 5) = -2\\).\n\nSimilarly, if Max chooses action B, then Min will choose action D, and if Max chooses action C, then Min will choose action F. We can fill in the values of these nodes accordingly:\n\nThus, Max’s best move is to take action C, resulting in a game score of \\(\\max(-2, -3, -1) = -1\\).\n\n\n\n8.3.1 Complexity of min-max search\nAt each of the \\(\\hor\\) timesteps, this algorithm iterates through the entire action space at that state, and therefore has a time complexity of \\(\\hor^{n_A}\\) (where \\(n_A\\) is the largest number of actions possibly available at once). This makes the min-max algorithm impractical for even moderately sized games.\nBut do we need to compute the exact value of every possible state? Instead, is there some way we could “ignore” certain actions and their subtrees if we already know of better options? The alpha-beta search makes use of this intuition.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-alpha-beta-search",
    "href": "planning.html#sec-alpha-beta-search",
    "title": "8  Tree Search Methods",
    "section": "8.4 Alpha-beta search",
    "text": "8.4 Alpha-beta search\nThe intuition behind alpha-beta search is as follows: Suppose Max is in state \\(s\\), and considering whether to take action \\(a\\) or \\(a'\\). If at any point they find out that action \\(a'\\) is definitely worse than (or equal to) action \\(a\\), they don’t need to evaluate action \\(a'\\) any further.\nConcretely, we run min-max search as above, except now we keep track of two additional parameters \\(\\alpha(s)\\) and \\(\\beta(s)\\) while evaluating each state:\n\nStarting in state \\(s\\), Max can achieve a game score of at least \\(\\alpha(s)\\) assuming Min plays optimally. That is, \\(V^\\star_\\hi(s) \\ge \\alpha(s)\\) at all points.\nAnalogously, starting in state \\(s\\), Min can ensure a game score of at most \\(\\beta(s)\\) assuming Max plays optimally. That is, \\(V^\\star_\\hi(s) \\le \\beta(s)\\) at all points.\n\nSuppose we are evaluating \\(V^\\star_\\hi(s)\\), where it is Max’s turn (\\(\\hi\\) is even). We update \\(\\alpha(s)\\) to be the highest minimax value achievable from \\(s\\) so far. That is, the value of \\(s\\) is at least \\(\\alpha(s)\\). Suppose Max chooses action \\(a\\), which leads to state \\(s'\\), in which it is Min’s turn. If any of Min’s actions in \\(s'\\) achieve a value \\(V^\\star_{\\hi+1}(s') \\le \\alpha(s)\\), we know that Max would not choose action \\(a\\), since they know that it is worse than whichever action gave the value \\(\\alpha(s)\\). Similarly, to evaluate a state on Min’s turn, we update \\(\\beta(s)\\) to be the lowest value achievable from \\(s\\) so far. That is, the value of \\(s\\) is at most \\(\\beta(s)\\). Suppose Min chooses action \\(a\\), which leads to state \\(s'\\) for Max. If Max has any actions that do better than \\(\\beta(s)\\), they would take it, making action \\(a\\) a suboptimal choice for Min.\n\nExample 8.3 (Alpha-beta search for a simple game) Let us use the same simple game from Example 8.2. We list the values of \\(\\alpha(s), \\beta(s)\\) in each node throughout the algorithm. These values are initialized to \\(-\\infty, +\\infty\\) respectively. We shade any squares that have not been visited by the algorithm, and we assume that actions are evaluated from left to right.\n\nSuppose Max takes action A. Let \\(s'\\) be the resulting game state. The values of \\(\\alpha(s')\\) and \\(\\beta(s')\\) are initialized at the same values as the root state, since we want to prune a subtree if there exists a better action at any step higher in the tree.\n\nThen we iterate through Min’s possible actions, updating the value of \\(\\beta(s')\\) as we go.\n \nOnce the value of state \\(s'\\) is fully evaluated, we know that Max can achieve a value of at least \\(-2\\) starting from the root, and so we update \\(\\alpha(s)\\), where \\(s\\) is the root state:\n\nThen Max imagines taking action B. Again, let \\(s'\\) denote the resulting game state. We initialize \\(\\alpha(s')\\) and \\(\\beta(s')\\) from the root:\n\nNow suppose Min takes action D, resulting in a value of \\(-3\\). We see that \\(V^\\star_\\hi(s') = \\min(-3, x, y)\\), where \\(x\\) and \\(y\\) are the values of the remaining two actions. But since \\(\\min(-3, x, y) \\le -3\\), we know that the value of \\(s'\\) is at most \\(-3\\). But Max can achieve a better value of \\(\\alpha(s') = -2\\) by taking action A, and so Max will never take action B, and we can prune the search here. We will use dotted lines to indicate states that have been ruled out from the search:\n\nFinally, suppose Max takes action C. For Min’s actions D and E, there is still a chance that action C might outperform action A, so we continue expanding:\n \nFinally, we see that Min taking action F achieves the minimum value at this state. This shows that optimal play is for Max to take action C, and Min to take action F.\n\n\n\n\nCode\ndef alpha_beta_search(s, player, alpha, beta) -&gt; tuple[\"Action\", \"Value\"]:\n    # Return the value of the state (for Max) and the best action for Max to take.\n    if env.is_terminal(s):\n        return None, env.winner(s)\n\n    if player is max:\n        a_max, v_max = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), min, alpha, beta)\n            if v &gt; v_max:\n                a_max, v_max = a, v\n                alpha = max(alpha, v)\n            if v_max &gt;= beta:\n                # we know Min will not choose the action that leads to this state\n                return a_max, v_max\n        return a_max, v_max\n\n    else:\n        a_min, v_min = None, None\n        for a in actions:\n            _, v = minimax_search(env.step(s, a), max)\n            if v &lt; v_min:\n                a_min, v_min = a, v\n                beta = min(beta, v)\n            if v_min &lt;= alpha:\n                # we know Max will not choose the action that leads to this state\n                return a_min, v_min\n        return a_min, v_min\n\n\nlatex(alpha_beta_search)\n\n\n$\n\\[\\begin{array}{l} \\mathbf{function} \\ \\mathrm{alpha\\_beta\\_search}(s, \\mathrm{player}, \\alpha, \\beta) \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{env}.\\mathrm{is\\_terminal} \\mathopen{}\\left( s \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( \\mathrm{None}, \\mathrm{env}.\\mathrm{winner} \\mathopen{}\\left( s \\mathclose{}\\right) \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\hspace{1em} \\mathbf{if} \\ \\mathrm{player} \\equiv \\mathrm{max} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathrm{actions} \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( \\mathrm{env}.\\mathrm{step} \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{min}, \\alpha, \\beta \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &gt; v_{\\mathrm{max}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{4em} \\alpha \\gets \\mathrm{max} \\mathopen{}\\left( \\alpha, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{3em} \\mathbf{if} \\ v_{\\mathrm{max}} \\ge \\beta \\\\ \\hspace{4em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{max}}, v_{\\mathrm{max}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{else} \\\\ \\hspace{2em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( \\mathrm{None}, \\mathrm{None} \\mathclose{}\\right) \\\\ \\hspace{2em} \\mathbf{for} \\ a \\in \\mathrm{actions} \\ \\mathbf{do} \\\\ \\hspace{3em} \\mathopen{}\\left( \\mathrm{\\_}, v \\mathclose{}\\right) \\gets \\mathrm{minimax\\_search} \\mathopen{}\\left( \\mathrm{env}.\\mathrm{step} \\mathopen{}\\left( s, a \\mathclose{}\\right), \\mathrm{max} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{if} \\ v &lt; v_{\\mathrm{min}} \\\\ \\hspace{4em} \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\gets \\mathopen{}\\left( a, v \\mathclose{}\\right) \\\\ \\hspace{4em} \\beta \\gets \\mathrm{min} \\mathopen{}\\left( \\beta, v \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{3em} \\mathbf{if} \\ v_{\\mathrm{min}} \\le \\alpha \\\\ \\hspace{4em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{3em} \\mathbf{end \\ if} \\\\ \\hspace{2em} \\mathbf{end \\ for} \\\\ \\hspace{2em} \\mathbf{return} \\ \\mathopen{}\\left( a_{\\mathrm{min}}, v_{\\mathrm{min}} \\mathclose{}\\right) \\\\ \\hspace{1em} \\mathbf{end \\ if} \\\\ \\mathbf{end \\ function} \\end{array}\\]\n$\n\n\nHow do we choose what order to explore the branches? As you can tell, this significantly affects the efficiency of the pruning algorithm. If Max explores the possible actions in order from worst to best, they will not be able to prune any branches at all! Additionally, to verify that an action is suboptimal, we must run the search recursively from that action, which ultimately requires traversing the tree all the way to a leaf node. The longer the game might possibly last, the more computation we have to run.\nIn practice, we can often use background information about the game to develop a heuristic for evaluating possible actions. If a technique is based on background information or intuition, especially if it isn’t rigorously justified, we call it a heuristic.\nCan we develop heuristic methods for tree exploration that works for all sorts of games?",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#sec-mcts",
    "href": "planning.html#sec-mcts",
    "title": "8  Tree Search Methods",
    "section": "8.5 Monte Carlo Tree Search",
    "text": "8.5 Monte Carlo Tree Search\nThe task of evaluating actions in a complex environment might seem familiar. We’ve encountered this problem before in both the Chapter 3 setting and the Chapter 1 setting. Now we’ll see how to combine concepts from these to form a more general and efficient tree search heuristic called Monte Carlo Tree Search (MCTS).\nWhen a problem is intractable to solve exactly, we often turn to approximate algorithms that sacrifice some accuracy in exchange for computational efficiency. MCTS also improves on alpha-beta search in this sense. As the name suggests, MCTS uses Monte Carlo simulation, that is, collecting random samples and computing the sample statistics, in order to approximate the value of each action.\nAs before, we imagine a complete game tree in which each path represents an entire game. The goal of MCTS is to assign values to only the game states that are relevant to the current game; We gradually expand the tree at each move. For comparison, in alpha-beta search, the entire tree only needs to be solved once, and from then on, choosing an action is as simple as taking a maximum over the previously computed values.\nThe crux of MCTS is approximating the win probability of a state by a sample probability. In practice, MCTS is used for games with binary outcomes where \\(r(s) \\in \\{ +1, -1 \\}\\), and so this is equivalent to approximating the final game score. To approximate the win probability from state \\(s\\), MCTS samples random games starting in \\(s\\) and computes the sample proportion of those that the player wins.\nNote that, for a given state \\(s\\), choosing the best action \\(a\\) can be framed as a Chapter 3 problem, where each action corresponds to an arm, and the reward distribution of arm \\(k\\) is the distribution of the game score over random games after choosing that arm. The most commonly used bandit algorithm in practice for MCTS is the Section 3.6 algorithm.\n\nRemark 8.1 (Summary of UCB). Let us quickly review the UCB bandit algorithm. For each arm \\(k\\), we track the sample mean \\[\n\\hat \\mu^k_t = \\frac{1}{N_t^k} \\sum_{\\tau=0}^{t-1} \\ind{a_\\tau = k} r_\\tau\n\\] of all rewards from that arm up to time \\(t\\). Then we construct a confidence interval \\[\nC_t^k = [\\hat \\mu^k_t - B_t^k, \\hat \\mu^k_t + B_t^k],\n\\] where \\(B_t^k = \\sqrt{\\frac{\\ln(2 t / \\delta)}{2 N_t^k}}\\) is given by Hoeffding’s inequality, so that with probability \\(\\delta\\) (some fixed parameter we choose), the true mean \\(\\mu^k\\) lies within \\(C_t^k\\). Note that \\(B_t^k\\) scales like \\(\\sqrt{1/N^k_t}\\), i.e. the more we have visited that arm, the more confident we get about it, and the narrower the confidence interval.\nTo select an arm, we pick the arm with the highest upper confidence bound.\n\nThis means that, for each edge (corresponding to a state-action pair \\((s, a)\\)) in the game tree, we keep track of the statistics required to compute its UCB:\n\nHow many times it has been “visited” (\\(N_t^{s, a}\\))\nHow many of those visits resulted in victory (\\(\\sum_{\\tau=0}^{t-1} \\ind{(s_\\tau, a_\\tau) = (s, a)} r_\\tau\\)). Let us call this latter value \\(W^{s, a}_t\\) (for number of “wins”).\n\nWhat does \\(t\\) refer to in the above expressions? Recall \\(t\\) refers to the number of time steps elapsed in the bandit environment. As mentioned above, each state \\(s\\) corresponds to its own bandit environment, and so \\(t\\) refers to \\(N^s\\), that is, how many actions have been taken from state \\(s\\). This term, \\(N^s\\), gets incremented as the algorithm runs; for simplicity, we won’t introduce another index to track how it changes.\n\nDefinition 8.2 (Monte Carlo tree search algorithm) Inputs: - \\(T\\), the number of iterations per move - \\(\\pi_{\\text{rollout}}\\), the rollout policy for randomly sampling games - \\(c\\), a positive value that encourages exploration\nTo choose a single move starting at state \\(s_{\\text{start}}\\), MCTS first tries to estimate the UCB values for each of the possible actions \\(\\mathcal{A}(s_\\text{start})\\), and then chooses the best one. To estimate the UCB values, it repeats the following four steps \\(T\\) times:\n\nSelection: We start at \\(s = s_{\\text{start}}\\). Let \\(\\tau\\) be an empty list that we will use to track states and actions.\n\nUntil \\(s\\) has at least one action that hasn’t been taken:\n\nChoose \\(a \\gets \\arg\\max_k \\text{UCB}^{s, k}\\), where \\[\n\\text{UCB}^{s, a} = \\frac{W^{s, a}}{N^{s, a}} + c \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\\tag{8.1}\\]\nAppend \\((s, a)\\) to \\(\\tau\\)\nSet \\(s \\gets P(s, a)\\)\n\n\nExpansion: Let \\(s_\\text{new}\\) denote the final state in \\(\\tau\\) (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from \\(s_\\text{new}\\). Call it \\(a_{\\text{new}}\\). Add it to \\(\\tau\\).\nSimulation: Simulate a complete game episode by starting with the action \\(a_{\\text{new}}\\) and then playing according to \\(\\pi_\\text{rollout}\\). This results in the outcome \\(r \\in \\{ +1, -1 \\}\\).\nBackup: For each \\((s, a) \\in \\tau\\):\n\nSet \\(N^{s, a} \\gets N^{s, a} + 1\\)\n\\(W^{s, a} \\gets W^{s, a} + r\\)\nSet \\(N^s \\gets N^s + 1\\)\n\n\nAfter \\(T\\) repeats of the above, we return the action with the highest UCB value Equation 8.1. Then play continues.\nBetween turns, we can keep the subtree whose statistics we have visited so far. However, the rest of the tree for the actions we did not end up taking gets discarded.\n\nThe application which brought the MCTS algorithm to fame was DeepMind’s AlphaGo Silver et al. (2016). Since then, it has been used in numerous applications ranging from games to automated theorem proving.\nHow accurate is this Monte Carlo estimation? It depends heavily on the rollout policy \\(\\pi_\\text{rollout}\\). If the distribution \\(\\pi_\\text{rollout}\\) induces over games is very different from the distribution seen during real gameplay, we might end up with a poor value approximation.\n\n8.5.1 Incorporating value functions and policies\nTo remedy this, we might make use of a value function \\(v : \\mathcal{S} \\to \\mathbb{R}\\) that more efficiently approximates the value of a state. Then, we can replace the simulation step of Definition 8.2 with evaluating \\(r = v(s-\\text{next})\\), where \\(s-\\text{next} = P(s-\\text{new}, a-\\text{new})\\).\nWe might also make use of a “guiding” policy \\(\\pi_\\text{guide} : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\) that provides “intuition” as to which actions are more valuable in a given state. We can scale the exploration term of Equation 8.1 according to the policy’s outputs.\nPutting these together, we can describe an updated version of MCTS that makes use of these value functions and policy:\n\nDefinition 8.3 (Monte Carlo tree search with policy and value functions) Inputs: - \\(T\\), the number of iterations per move - \\(v\\), a value function that evaluates how good a state is - \\(\\pi_\\text{guide}\\), a guiding policy that encourages certain actions - \\(c\\), a positive value that encourages exploration\nTo select a move in state \\(s_\\text{start}\\), we repeat the following four steps \\(T\\) times:\n\nSelection: We start at \\(s = s_{\\text{start}}\\). Let \\(\\tau\\) be an empty list that we will use to track states and actions.\n\nUntil \\(s\\) has at least one action that hasn’t been taken:\n\nChoose \\(a \\gets \\arg\\max_k \\text{UCB}^{s, k}\\), where \\[\n\\text{UCB}^{s, a} = \\frac{W^{s, a}}{N^s} + c \\cdot \\pi_\\text{guide}(a \\mid s) \\sqrt{\\frac{\\ln N^s}{N^{s, a}}}\n\\tag{8.2}\\]\nAppend \\((s, a)\\) to \\(\\tau\\)\nSet \\(s \\gets P(s, a)\\)\n\n\nExpansion: Let \\(s_\\text{new}\\) denote the final state in \\(\\tau\\) (that has at least one action that hasn’t been taken). Choose one of these unexplored actions from \\(s_\\text{new}\\). Call it \\(a_{\\text{new}}\\). Add it to \\(\\tau\\).\nSimulation: Let \\(s_\\text{next} = P(s_\\text{new}, a_\\text{new})\\). Evaluate \\(r = v(s_\\text{next})\\). This approximates the value of the game after taking the action \\(a_\\text{new}\\).\nBackup: For each \\((s, a) \\in \\tau\\):\n\n\\(N^{s, a} \\gets N^{s, a} + 1\\)\n\\(W^{s, a} \\gets W^{s, a} + r\\)\n\\(N^s \\gets N^s + 1\\)\n\n\nWe finally return the action with the highest UCB value Equation 8.2. Then play continues. As before, we can reuse the tree across timesteps.\n\n\n\nCode\nclass EdgeStatistics(NamedTuple):\n    wins: int = 0\n    visits: int = 0\n\nclass MCTSTree:\n    \"\"\"A representation of the search tree.\n\n    Maps each state-action pair to its number of wins and the number of visits.\n    \"\"\"\n\n    edges: dict[tuple[\"State\", \"Action\"], EdgeStatistics]\n\ndef mcts_iter(tree, s_init):\n    s = s_init\n    # while all((s, a) in tree for a in env.action_state(s)):\n\n\nHow do we actually compute a useful \\(\\pi_\\text{guide}\\) and \\(v\\)? If we have some existing dataset of trajectories, we could use Chapter 7 (that is, imitation learning) to generate a policy \\(\\pi_\\text{guide}\\) via behavioral cloning and learn \\(v\\) by regressing the game outcomes onto states. Then, plugging these into Definition 8.3 results in a stronger policy by using tree search to “think ahead”.\nBut we don’t have to stop at just one improvement step; we could iterate this process via self-play.\n\n\n8.5.2 Self-play\nRecall the Section 1.5.3.2 algorithm from the Chapter 1. Policy iteration alternates between policy evaluation (taking \\(\\pi\\) and computing \\(V^\\pi\\)) and policy improvement (setting \\(\\pi\\) to be greedy with respect to \\(V^\\pi\\)). Above, we saw how MCTS can be thought of as a “policy improvement” operation: for a given policy \\(\\pi^0\\), we can use it to guide MCTS, resulting in an algorithm that is itself a policy \\(\\pi^0_\\text{MCTS}\\) that maps from states to actions. Now, we can use Chapter 7 to obtain a new policy \\(\\pi^1\\) that imitates \\(\\pi^0_\\text{MCTS}\\). We can now use \\(\\pi^1\\) to guide MCTS, and repeat.\n\nDefinition 8.4 (MCTS with self-play) Input:\n\nA parameterized policy class \\(\\pi_\\theta : \\mathcal{S} \\to \\triangle(\\mathcal{A})\\)\nA parameterized value function class \\(v_\\lambda : \\mathcal{S} \\to \\mathbb{R}\\)\nA number of trajectories \\(M\\) to generate\nThe initial parameters \\(\\theta^0, \\lambda^0\\)\n\nFor \\(t = 0, \\dots, T-1\\):\n\nPolicy improvement: Let \\(\\pi^t_\\text{MCTS}\\) denote the policy obtained by Definition 8.3 with \\(\\pi-{\\theta^t}\\) and \\(v-{\\lambda^t}\\). We use \\(\\pi^t-\\text{MCTS}\\) to play against itself \\(M\\) times. This generates \\(M\\) trajectories \\(\\tau-0, \\dots, \\tau-{M-1}\\).\nPolicy evaluation: Use behavioral cloning to find a set of policy parameters \\(\\theta^{t+1}\\) that mimic the behavior of \\(\\pi^t_\\text{MCTS}\\) and a set of value function parameters \\(\\lambda^{t+1}\\) that approximate its value function. That is, \\[\n\\begin{aligned}\n\\theta^{t+1} &\\gets \\arg\\min_\\theta \\sum_{m=0}^{M-1} \\sum_{\\hi=0}^{H-1} - \\log \\pi_\\theta(a^m_\\hi \\mid s^m_\\hi) \\\\\n\\lambda^{t+1} &\\gets \\arg\\min_\\lambda \\sum_{m=0}^{M-1} \\sum_{\\hi=0}^{H-1} (v_\\lambda(s^m_\\hi) - R(\\tau_m))^2\n\\end{aligned}\n\\]\n\nNote that in implementation, the policy and value are typically both returned by a single deep neural network, that is, with a single set of parameters, and the two loss functions are added together.\n\nThis algorithm was brought to fame by AlphaGo Zero Silver et al. (2017).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#summary",
    "href": "planning.html#summary",
    "title": "8  Tree Search Methods",
    "section": "8.6 Summary",
    "text": "8.6 Summary\nIn this chapter, we explored tree search-based algorithms for deterministic, zero sum, fully observable two-player games. We began with Section 8.3, an algorithm for exactly solving the game value of every possible state. However, this is impossible to execute in practice, and so we must resort to various ways to reduce the number of states and actions that we must explore. Section 8.4 does this by -pruning- away states that we already know to be suboptimal, and Section 8.5 -approximates- the value of states instead of evaluating them exactly.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "planning.html#references",
    "href": "planning.html#references",
    "title": "8  Tree Search Methods",
    "section": "8.7 References",
    "text": "8.7 References\nChapter 5 of Russell and Norvig (2021) provides an excellent overview of search methods in games. The original AlphaGo paper Silver et al. (2016) was a groundbreaking application of these technologies. Silver et al. (2017) removed the imitation learning phase, learning from scratch. AlphaZero Silver et al. (2018) then extended to other games beyond Go, namely shogi and chess, also learning from scratch. In MuZero Schrittwieser et al. (2020), this was further extended by learning a model of the game dynamics.\n\n\n\n\nRussell, Stuart J., and Peter Norvig. 2021. Artificial Intelligence: A Modern Approach. Fourth edition. Pearson Series in Artificial Intelligence. Hoboken: Pearson.\n\n\nSchrittwieser, Julian, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, et al. 2020. “Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model.” Nature 588 (7839, 7839): 604–9. https://doi.org/10.1038/s41586-020-03051-4.\n\n\nSilver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, et al. 2016. “Mastering the Game of Go with Deep Neural Networks and Tree Search.” Nature 529 (7587, 7587): 484–89. https://doi.org/10.1038/nature16961.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou, Matthew Lai, Arthur Guez, Marc Lanctot, et al. 2018. “A General Reinforcement Learning Algorithm That Masters Chess, Shogi, and Go Through Self-Play.” Science 362 (6419): 1140–44. https://doi.org/10.1126/science.aar6404.\n\n\nSilver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the Game of Go Without Human Knowledge.” Nature 550 (7676, 7676): 354–59. https://doi.org/10.1038/nature24270.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Tree Search Methods</span>"
    ]
  },
  {
    "objectID": "exploration.html",
    "href": "exploration.html",
    "title": "9  Exploration in MDPs",
    "section": "",
    "text": "9.1 Introduction\nOne of the key challenges of reinforcement learning is the exploration-exploitation tradeoff. Should we exploit actions we know will give high reward, or should we explore different actions to discover potentially better strategies? An algorithm that doesn’t explore effectively might easily overfit to certain areas of the state space, and fail to generalize once they enter a region they haven’t yet seen. The algorithms we saw in the chapter on fitted DP Chapter 5 suffer from this issue.\nIn Chapter 3, where the state never changes so all we care about are the actions, we saw algorithms like Section 3.6 and Section 3.7 that incentivize the learner to explore arms that it is uncertain about. In this chapter, we will see how to generalize these ideas to the MDP setting.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#introduction",
    "href": "exploration.html#introduction",
    "title": "9  Exploration in MDPs",
    "section": "",
    "text": "Definition 9.1 (Per-episode regret) To quantify the performance of a learning algorithm, we will consider its per-episode regret over \\(T\\) timesteps/episodes:\n\\[\n\\text{Regret}_T = \\mathbb{E}\\left[ \\sum_{t=0}^{T-1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right]\n\\]\nwhere \\(\\pi^t\\) is the policy generated by the algorithm at the \\(t\\)th iteration.\n\n\n9.1.1 Sparse reward\nExploration is crucial in unknown sparse reward problems where reward doesn’t come until after many steps, and algorithms which do not systematically explore new states may fail to learn anything meaningful (within a reasonable amount of time). We can see this by considering the following simple MDP:\n\nExample 9.1 (Sparse Reward MDP) Here’s a simple example of an MDP with sparse reward:\n\n\n\nimage\n\n\nThere are \\(|\\mathcal{S}|\\) states. The agent starts in the leftmost state. In every state, there are three possible actions, two of which move the agent left and one which moves the agent right. The reward function assigns \\(r=1\\) to the rightmost cell.\n\nHow well would the algorithms we’ve covered so far do on this problem? For example, Chapter 6 require the gradient to be nonzero in order to learn. If we never observe any reward, the gradient will always be zero, and the policy will never change or improve. Chapter 5 will run into a similar issue: as we randomly interact with the environment, we will never observe any reward, and so the reward model simply gives zero for every state-action pair. In expectation, it would take a computationally infeasible number of rollouts to observe the reward by chance.\nThe rest of this chapter will consider ways to explicitly add exploration to these algorithms.\n\n\n9.1.2 Exploration in deterministic MDPs\nLet us address the exploration problem in a deterministic MDP, that is, where taking action \\(a\\) in state \\(s\\) always leads to the state \\(P(s, a) \\in \\mathcal{S}\\). How can we methodically visit every single state-action pair? In the bandit setting, it was trivial to visit every arm: just pull each arm once. How might I navigate to a particular state \\(s\\) in the MDP setting? Doing so requires planning out a path from the original state. In fact, solving navigation itself is a complex RL problem!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#explore-then-exploit-for-deterministic-mdps",
    "href": "exploration.html#explore-then-exploit-for-deterministic-mdps",
    "title": "9  Exploration in MDPs",
    "section": "9.2 Explore-then-exploit (for deterministic MDPs)",
    "text": "9.2 Explore-then-exploit (for deterministic MDPs)\nIn this simple deterministic setting, the environment never randomly takes us to unseen states, so our strategy must actively explore new states.\n\nDefinition 9.2 We’ll keep a set \\(\\mathcal{D}\\) of all the \\((s, a, r, s')\\) pairs we’ve observed. Each episode, we’ll use \\(\\mathcal{D}\\) to construct a fully known MDP, \\(M_{\\mathcal{D}}\\), in which unseen state-action pairs are rewarded. We’ll then use the planning algorithms from Section 1.3.2 to reach the unknown states in \\(M-{\\mathcal{D}}\\).\nWe assume that every state can be reached from the initial state within a single episode, and that the action space \\(\\mathcal{A}\\) is fixed.\n\n\\(\\mathcal{D} \\gets \\emptyset\\)\nFor \\(T = 0, 1, 2, \\dots\\) (until the entire MDP has been explored):\n\nConstruct \\(M_{\\mathcal{D}}\\) using \\(\\mathcal{D}\\). That is, the state transitions are set to those observed in \\(\\mathcal{D}\\), and the reward is set to \\(0\\) for all state-action pairs in \\(\\mathcal{D}\\), and \\(1\\) otherwise.\nExecute Section 1.3.2 on the known MDP \\(M-{\\mathcal{D}}\\) to compute policy \\(\\pi^\\star_{\\mathcal{D}}\\).\nExecute \\(\\pi^\\star_{\\mathcal{D}}\\). This will visit some \\((s, a)\\) not yet observed in \\(\\mathcal{D}\\).\n\\(\\mathcal{D} \\gets \\mathcal{D} \\cup \\{ (s, a, r, s') \\}\\), where \\(s' = P(s, a), r = r(s, a)\\) are the observed state transition and reward.\n\n\n\n\n\nTheorem 9.1 (Performance of explore-then-exploit) As long as every state can be reached from \\(s_0\\) within a single episode, i.e. \\(|\\mathcal{S}| \\le H\\), this will eventually be able to explore all \\(|\\mathcal{S}| |\\mathcal{A}|\\) state-action pairs, adding one new transition per episode. We know it will take at most \\(|\\mathcal{S}| |\\mathcal{A}|\\) iterations to explore the entire MDP, after which \\(\\pi^t = \\pi^\\star\\), incurring no additional regret. For each \\(\\pi^t\\) up until then, corresponding to the shortest-path policies \\(\\tilde \\pi\\), the value of policy \\(\\pi^t\\) will differ from that of \\(\\pi^\\star\\) by at most \\(H\\), since the policies will differ by at most \\(1\\) reward at each timestep. So,\n\\[\n\\sum_{t=0}^{T-1} V^\\star_0 - V_0^{\\pi^t} \\le |\\mathcal{S}||\\mathcal{A}| H.\n\\]\n(Note that this MDP and algorithm are deterministic, so the regret is not random.)",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#sec-mdp-mab",
    "href": "exploration.html#sec-mdp-mab",
    "title": "9  Exploration in MDPs",
    "section": "9.3 Treating an unknown MDP as a MAB",
    "text": "9.3 Treating an unknown MDP as a MAB\nWe also explored the exploration-exploitation tradeoff in Chapter 3. Recall tthat in the MAB setting, we have \\(K\\) arms, each of which has an unknown reward distribution, and we want to learn which of the arms is optimal, i.e. has the highest mean reward.\nOne algorithm that struck a good balance between exploration and exploitation was the upper confidence bound algorithm Section 3.6: For each arm, we construct a confidence interval for its true mean award, and then choose the arm with the highest upper confidence bound. In summary,\n\\[\nk_{t+1} \\gets \\arg\\max_{k \\in [K]} \\frac{R^{k}_t}{N^{k}_t} + \\sqrt{\\frac{\\ln(2t/\\delta)}{2 N^{k}_t}}\n\\]\nwhere \\(N_t^k\\) indicates the number of times arm \\(k\\) has been pulled up until time \\(t\\), \\(R_t^k\\) indicates the total reward obtained by pulling arm \\(k\\) up until time \\(t\\), and \\(\\delta &gt; 0\\) controls the width of the confidence interval. How might we extend UCB to the MDP case?\nLet us formally describe an unknown MDP as an MAB problem. In an unknown MDP, we want to learn which policy is optimal. So if we want to apply MAB techniques to solving an MDP, it makes sense to think of arms as policies. There are \\(K = (|\\mathcal{A}|^{|\\mathcal{S}|})^H\\) deterministic policies in a finite MDP. Then, “pulling” arm \\(\\pi\\) corresponds to using \\(\\pi\\) to act through a trajectory in the MDP, and observing the total reward.\n\nExercise 9.1 (Notation for expected reward) Which quantity that we have seen so far equals the expected reward from arm \\(\\pi\\)?\n\nRecall that UCB incurs regret \\(\\tilde{O}(\\sqrt{TK})\\), where \\(T\\) is the number of pulls and \\(K\\) is the number of arms. So in the MDP-as-MAB problem, using UCB for \\(T\\) episodes would achieve regret\n\\[\n\\tilde{O}(\\sqrt{|\\mathcal{A}|^{|\\mathcal{S}|H} T})\n\\tag{9.1}\\]\nThis scales exponentially in \\(|\\mathcal{S}|\\) and \\(H\\), which quickly becomes intractable. Notably, this method doesn’t consider the information that we gain across different policies. We can illustrate this with the following example:\n\nExample 9.2 (Treating an MDP as a MAB) Consider a “coin MDP” with two states “heads” and “tails”, two actions “Y” and “N”, and a time horizon of \\(H=2\\). The state transition flips the coin, and doesn’t depend on the action. The reward only depends on the action: Taking action Y gives reward \\(1\\), and taking action N gives reward \\(0\\).\nSuppose we collect data from the two constant policies \\(\\pi_{\\text{Y}}(s) = \\text{Y}\\) and \\(\\pi_{\\text{N}}(s) = \\text{N}\\). Now we want to learn about the policy \\(\\tilde{\\pi}\\) that takes action Y and then N. Do we need to collect data from \\(\\tilde{\\pi}\\) to evaluate it? No: Since the reward only depends on the action, we can infer its value from our data on the policies \\(\\pi_{\\text{Y}}\\) and \\(\\pi_{\\text{N}}\\). However, if we treat the MDP as a bandit in which \\(\\tilde{\\pi}\\) is a new, unknown arm, we ignore the known correlation between the action and the reward.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#ucb-vi",
    "href": "exploration.html#ucb-vi",
    "title": "9  Exploration in MDPs",
    "section": "9.4 UCB-VI",
    "text": "9.4 UCB-VI\nThe approach above is inefficient: We shouldn’t need to consider all \\(|\\mathcal{A}|^{|\\mathcal{S}| H}\\) deterministic policies to achieve low regret. Rather, all we need to describe the optimal policy is \\(Q^\\star\\), which has \\(H |\\mathcal{S}||\\mathcal{A}|\\) entries to be learned. Can we borrow ideas from UCB to reduce the regret to a polynomial in \\(|\\mathcal{S}|\\), \\(|\\mathcal{A}|\\), and \\(H\\)?\nOne way to frame the UCB algorithm is that, when choosing arms, we optimize over a proxy reward that is the sum of the estimated mean reward and an exploration term. In the UCB-VI algorithm, we will extend this idea to the case of an unknown MDP \\(\\mathcal{M}^{?}\\) by modeling a proxy MDP \\(\\widetilde{\\mathcal{M}}\\) with a reward function that encourages exploration. Then, we will use DP to solve for the optimal policy in \\(\\widetilde{\\mathcal{M}}\\).\nAssumptions: For simplicity, here we assume the reward function of \\(\\mathcal{M}^{?}\\) is known, so we only need to model the state transitions, though the rewards can be modelled similarly. We will also consider the more general case of a time-varying MDP, where the transition and reward functions may change over time. We take the convention that \\(P_h\\) is the distribution of \\(s_{h+1} \\mid s_{h}, a_{h}\\) and \\(r_h\\) is applied to \\(s_h, a_h\\).\nAt a high level, the UCB-VI algorithm can be described as follows:\n\nmodeling: Use previous data to model the transitions \\(\\widehat{P}_0, \\dots, \\widehat{P}_{H-1}\\).\nReward bonus: Design a reward bonus \\(b_h(s, a) \\in \\mathbb{R}\\) to encourage exploration, analogous to the UCB term.\nOptimistic planning: Use DP to compute the optimal policy \\(\\widehat \\pi_h(s)\\) in the modelled MDP\n\n\\[\n\\tilde{\\mathcal{M}} = (\\mathcal{S}, \\mathcal{A}, \\{ \\widehat{P}_h\\}_{h \\in [H]}, \\{ r_h+ b_h\\}_{h \\in [H]}, H).\n\\]\n\nExecution: Use \\(\\widehat \\pi_h(s)\\) to collect a new trajectory, and repeat.\n\nWe detail each of these steps below. The full definition follows in Definition 9.3.\n\n9.4.1 modeling the transitions\nWe seek to approximate \\(P_h(s_{h+1} \\mid s_h, a_h) = \\frac{\\mathbb{P}(s_h, a_h, s_{h+1})}{\\mathbb{P}(s_h, a_h)}\\). We can estimate these using their sample probabilities from the dataset. That is, define\n\\[\n\\begin{aligned}\n    N_h^t(s, a, s') & := \\sum_{i=0}^{t-1} \\mathbf{1}\\left\\{ (s_h^i, a_h^i, s_{h+1}^i) = (s, a, s') \\right\\} \\\\\n    N_h^t(s, a)     & := \\sum_{i=0}^{t-1} \\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\}                \\\\\n\\end{aligned}\n\\]\nThen we can model\n\\[\n\\widehat{P}_h^t(s' \\mid s, a) = \\frac{N_h^t(s, a, s')}{N_h^t(s, a)}.\n\\]\n\nNote that this is also a fairly naive, nonparametric estimator that doesn’t assume any underlying structure of the MDP. We’ll see how to incorporate assumptions about the MDP in the following section.\n\n\n\n9.4.2 Reward bonus\nTo motivate the reward bonus term \\(b_h^t(s, a)\\), recall how we designed the reward bonus term for UCB:\n\nWe used Hoeffding’s inequality to bound, with high probability, how far the sample mean \\(\\widehat \\mu_t^k\\) deviated from the true mean \\(\\mu^k\\).\nBy inverting this inequality, we obtained a \\((1-\\delta)\\)-confidence interval for the true mean, centered at our estimate.\nTo make this bound uniform across all timesteps \\(t \\in [T]\\), we applied the union bound and multiplied \\(\\delta\\) by a factor of \\(T\\).\n\nWe’d like to do the same for UCB-VI, and construct the bonus term such that \\(V^\\star_h(s) \\le \\widehat{V}_h^t(s)\\) with high probability. However, our construction will be more complex than the MAB case, since \\(\\widehat{V}_h^t(s)\\) depends on the bonus \\(b_h^t(s, a)\\) implicitly via DP. We claim that the bonus term that gives the proper bound is\n\\[\nb_h^t(s, a) = 2 H \\sqrt{\\frac{\\log( |\\mathcal{S}||\\mathcal{A}|H T/\\delta )}{N_h^t(s, a)}}.\n\\tag{9.2}\\]\nWe will only provide a heuristic sketch of the proof; see (Agarwal et al. 2022) (Section 7.3) for a full proof.\n\nRemark 9.1 (UCB-VI reward bonus construction). We aim to show that, with high probability,\n\\[\nV_h^\\star(s) \\le \\widehat{V}_h^t(s) \\quad \\forall t \\in [T], h \\in [H], s \\in \\mathcal{S}.\n\\]\nWe’ll do this by bounding the error incurred at each step of DP. Recall that DP solves for \\(\\widehat{V}_h^t(s)\\) recursively as follows:\n\\[\n\\widehat{V}_h^t(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\tilde r^t_h(s, a) + \\mathbb{E}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ \\widehat{V}_{h+1}^t(s') \\right] \\right]\n\\]\nwhere \\(\\tilde r^t_h(s, a) = r_h(s, a) + b_h^t(s, a)\\) is the reward function of our modelled MDP \\(\\tilde{\\mathcal{M}}^t\\). On the other hand, we know that \\(V^\\star\\) must satisfy\n\\[\nV^\\star_h(s) = \\max_{a \\in \\mathcal{A}} \\left[ \\tilde r^t_h(s, a) + \\mathbb{E}_{s' \\sim P^?_h(\\cdot \\mid s, a)} [V^\\star_{h+1}(s')] \\right]\n\\]\nso it suffices to bound the difference between the two inner expectations. There are two sources of error:\n\nThe value functions \\(\\widehat{V}^t_{h+1}\\) v.s. \\(V^\\star_{h+1}\\)\nThe transition probabilities \\(\\widehat{P}_h^t\\) v.s. \\(P^?_h\\).\n\nWe can bound these individually, and then combine them by the triangle inequality. For the former, we can simply bound the difference by \\(H\\), assuming that the rewards are within \\([0, 1]\\). Now, all that is left is to bound the error from the transition probabilities:\n\\[\n\\text{error} = \\left| \\mathbb{E}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] - \\mathbb{E}_{s' \\sim P^?_h(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]. \\right|\n\\tag{9.3}\\]\nLet us bound this term for a fixed \\(s, a, h, t\\). (Later we can make this uniform across \\(s, a, h, t\\) using the union bound.) Note that expanding out the definition of \\(\\widehat{P}_h^t\\) gives\n\\[\n\\begin{aligned}\n        \\mathbb{E}_{s' \\sim \\widehat{P}_h^t(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right] & = \\sum_{s' \\in \\mathcal{S}} \\frac{N^t_h(s, a, s')}{N^t_h(s, a)} V^\\star_{h+1}(s')                                                     \\\\\n                                                                                   & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\sum_{s' \\in \\mathcal{S}} \\mathbf{1}\\left\\{ (s_h^i, a_h^i, s_{h+1}^i) = (s, a, s') \\right\\} V^\\star_{h+1}(s') \\\\\n                                                                                   & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\underbrace{\\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\} V^\\star_{h+1}(s_{h+1}^i)}_{X^i}\n\\end{aligned}\n\\]\nsince the terms where \\(s' \\neq s_{h+1}^i\\) vanish.\nNow, in order to apply Hoeffding’s inequality, we would like to express the second term in Equation 9.3 as a sum over \\(t\\) random variables as well. We will do this by redundantly averaging over all desired trajectories (i.e. where we visit state \\(s\\) and action \\(a\\) at time \\(h\\)):\n\\[\n\\begin{aligned}\n        \\mathbb{E}_{s' \\sim P^?_h(\\cdot \\mid s, a)} \\left[ V^\\star_{h+1}(s') \\right]\n         & = \\sum_{s' \\in \\mathcal{S}} P^?_h(s' \\mid s, a) V^\\star_{h+1}(s')                                                                              \\\\\n         & = \\sum_{s' \\in \\mathcal{S}} \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\mathbf{1}\\left\\{ (s_h^i, a_h^i) = (s, a) \\right\\} P^?_h(s' \\mid s, a) V^\\star_{h+1}(s') \\\\\n         & = \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\mathbb{E}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i.\n\\end{aligned}\n\\]\nNow we can apply Hoeffding’s inequality to \\(X^i - \\mathbb{E}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i\\), which is bounded by \\(H\\), to obtain that, with probability at least \\(1-\\delta\\),\n\\[\n\\text{error} = \\left| \\frac{1}{N^t_h(s, a)} \\sum_{i=0}^{t-1} \\left(X^i - \\mathbb{E}_{s_{h+1}^i \\sim P^?_{h}(\\cdot \\mid s_h^i, a_h^i)} X^i \\right) \\right| \\le 2 H \\sqrt{\\frac{\\ln(1/\\delta)}{N_h^t(s, a)}}.\n\\]\nApplying a union bound over all \\(s \\in \\mathcal{S}, a \\in \\mathcal{A}, t \\in [T], h \\in [H]\\) gives the \\(b_h^t(s, a)\\) term above.\n\nPutting these parts together, we can define the algorithm as follows:\n\nDefinition 9.3 (UCB-VI algorithm) TODO\n\n\n\n9.4.3 Performance of UCB-VI\nHow exactly does UCB-VI strike a good balance between exploration and exploitation? In UCB for MABs, the bonus exploration term is simple to interpret: It encourages the learner to take actions with a high exploration term. Here, the policy depends on the bonus term indirectly: The policy is obtained by planning in an MDP where the bonus term is added to the reward function. Note that the bonuses propagate backwards in DP, effectively enabling the learner to plan to explore unknown states. This effect takes some further interpretation.\nRecall we constructed \\(b^t_h\\) so that, with high probability, \\(V^\\star_h(s) \\le \\widehat{V}_h^t(s)\\) and so\n\\[\nV^\\star_h(s) - V^{\\pi^t}_h(s) \\le \\widehat{V}_h^t(s) - V^{\\pi^t}_h(s).\n\\]\nThat is, the l.h.s. measures how suboptimal policy \\(\\pi^t\\) is in the true environment, while the r.h.s. is the difference in the policy’s value when acting in the modelled MDP \\(\\tilde{\\mathcal{M}}^t\\) instead of the true one \\(\\mathcal{M}^{?}\\).\nIf the r.h.s. is small, this implies that the l.h.s. difference is also small, i.e. that \\(\\pi^t\\) is exploiting actions that are giving high reward.\nIf the r.h.s. is large, then we have overestimated the value: \\(\\pi^t\\), the optimal policy of \\(\\tilde{\\mathcal{M}}^t\\), does not perform well in the true environment \\(\\mathcal{M}^{?}\\). This indicates that one of the \\(b_h^t(s, a)\\) terms must be large, or some \\(\\widehat P^t_h(\\cdot \\mid s, a)\\) must be inaccurate, indicating a state-action pair with a low visit count \\(N^t_h(s, a)\\) that the learner was encouraged to explore.\nIt turns out that UCB-VI achieves a regret of\n\nTheorem 9.2 (UCB-VI regret) \\[\n\\mathbb{E}\\left[ \\sum_{t=0}^{T-1} \\left(V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right) \\right] = \\tilde{O}(H^2 \\sqrt{|\\mathcal{S}| |\\mathcal{A}| T})\n\\]\n\nComparing this to the UCB regret bound \\(\\tilde{O}(\\sqrt{T K})\\), where \\(K\\) is the number of arms of the MAB, we see that we’ve reduced the number of effective arms from \\(|\\mathcal{A}|^{|\\mathcal{S}|H}\\) (in Equation 9.1) to \\(H^4 |\\mathcal{S}||\\mathcal{A}|\\), which is indeed polynomial in \\(|\\mathcal{S}|\\), \\(|\\mathcal{A}|\\), and \\(H\\), as desired. This is also roughly the number of episodes it takes to achieve constant-order average regret:\n\\[\n\\frac{1}{T} \\mathbb{E}[\\text{Regret}_T] = \\tilde{O}\\left(\\sqrt{\\frac{H^4 |\\mathcal{S}||\\mathcal{A}|}{T}}\\right)\n\\]\nNote that the time-dependent transition matrix has \\(H |\\mathcal{S}|^2 |\\mathcal{A}|\\) entries. Assuming \\(H \\ll |\\mathcal{S}|\\), this shows that it’s possible to achieve low regret, and achieve a near-optimal policy, while only understanding a \\(1/|\\mathcal{S}|\\) fraction of the world’s dynamics.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#linear-mdps",
    "href": "exploration.html#linear-mdps",
    "title": "9  Exploration in MDPs",
    "section": "9.5 Linear MDPs",
    "text": "9.5 Linear MDPs\nA polynomial dependency on \\(|\\mathcal{S}|\\) and \\(|\\mathcal{A}|\\) is manageable when the state and action spaces are small. But for large or continuous state and action spaces, even this polynomial factor will become intractable. Can we find algorithms that don’t depend on \\(|\\mathcal{S}|\\) or \\(|\\mathcal{A}|\\) at all, effectively reducing the dimensionality of the MDP? In this section, we’ll explore linear MDPs: an example of a parameterized MDP where the rewards and state transitions depend only on some parameter space of dimension \\(d\\) that is independent from \\(|\\mathcal{S}|\\) or \\(|\\mathcal{A}|\\).\n\nDefinition 9.4 (Linear MDP) We assume that the transition probabilities and rewards are linear in some feature vector\n\\(\\phi(s, a) \\in \\mathbb{R}^d\\):\n\\[\n\\begin{aligned}\n        P_h(s' \\mid s, a) & = \\phi(s, a)^\\top \\mu^\\star_h(s') \\\\\n        r_h(s, a)         & = \\phi(s, a)^\\top \\theta_h^\\star\n\\end{aligned}\n\\]\nNote that we can also think of \\(P_h(\\cdot \\mid s, a) = \\mu_h^\\star\\) as an \\(|\\mathcal{S}| \\times d\\) matrix, and think of \\(\\mu^\\star_h(s')\\) as indexing into the \\(s'\\)-th row of this matrix (treating it as a column vector). Thinking of \\(V^\\star_{h+1}\\) as an \\(|\\mathcal{S}|\\)-dimensional vector, this allows us to write\n\\[\n\\mathbb{E}_{s' \\sim P_h(\\cdot \\mid s, a)}[V^\\star_{h+1}(s)] = (\\mu^\\star_h\\phi(s, a))^\\top V^\\star_{h+1}.\n\\]\nThe \\(\\phi\\) feature mapping can be designed to capture interactions between the state \\(s\\) and action \\(a\\). In this book, we’ll assume that the feature map \\(\\phi : \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}^d\\) and the reward function (described by \\(\\theta_h^\\star\\)) are known to the learner.\n\n\n9.5.1 Planning in a linear MDP\nIt turns out that \\(Q^\\star_h\\) is also linear with respect to this feature mapping. We can prove this by simply computing it using DP. We initialize the value function at the end of the time horizon by setting \\(V_{H}^\\star(s) = 0\\) for all states \\(s\\). Then we iterate:\n\\[\n\\begin{aligned}\n    Q^\\star_h(s, a)  & = r_h(s, a) + \\mathbb{E}_{s' \\sim P_h(\\cdot \\mid s, a)} [V^\\star_{h+1}(s')]                          \\\\\n                     & = \\phi(s, a)^\\top \\theta_h^\\star + (\\mu_h^\\star \\phi(s, a))^\\top V^\\star_{h+1}               \\\\\n                     & = \\phi(s, a)^\\top \\underbrace{( \\theta_h^\\star + (\\mu_h^\\star)^\\top  V^\\star_{h+1})}_{w_h} \\\\\n    V^\\star_h(s)     & = \\max_a Q^\\star_h(s, a)                                                                       \\\\\n    \\pi^\\star_h(s) & = \\arg\\max_a Q^\\star_h(s, a)\n\\end{aligned}\n\\]\n\nShow that \\(Q^\\pi_h\\) is also linear with respect to \\(\\phi(s, a)\\) for any policy \\(\\pi\\).\n\n\n\n9.5.2 UCB-VI in a linear MDP\n\n9.5.2.1 modeling the transitions\nThis linear assumption on the MDP will also allow us to model the unknown dynamics \\(P^?_h(s' \\mid s, a)\\) with techniques from supervised learning (SL). Recall that SL is useful for estimating conditional expectations by minimizing mean squared error. We can rephrase the estimation of \\(P^?_h(s' \\mid s, a)\\) as a least-squares problem as follows: Write \\(\\delta_s\\) to denote a one-hot vector in \\(\\mathbb{R}^{|\\mathcal{S}|}\\), with a \\(1\\) in the \\(s\\)-th entry and \\(0\\) everywhere else. Note that\n\\[\n\\mathbb{E}_{s' \\sim P_h(\\cdot \\mid s, a)} [\\delta_{s'}] = P_h(\\cdot \\mid s, a) = \\mu_h^\\star \\phi(s, a).\n\\]\nFurthermore, since the expectation here is linear with respect to \\(\\phi(s, a)\\), we can directly apply least-squares multi-target linear regression to construct the estimate\n\\[\n\\widehat \\mu = \\arg\\min_{\\mu \\in \\mathbb{R}^{|\\mathcal{S}| \\times d}} \\sum_{t=0}^{T-1} \\|\\mu \\phi(s_h^i, a_h^i) - \\delta_{s_{h+1}^i} \\|_2^2.\n\\]\nThis has a well-known closed-form solution:\n\\[\n\\begin{aligned}\n    \\widehat \\mu^\\top            & = (A_h^t)^{-1} \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\delta_{s_{h+1}^i}^\\top \\\\\n    \\text{where} \\quad A_h^t & = \\sum_{i=0}^{t-1} \\phi(s_h^i, a_h^i) \\phi(s_h^i, a_h^i)^\\top + \\lambda I\n\\end{aligned}\n\\]\nwhere we include a \\(\\lambda I\\) term to ensure that the matrix \\(A^t_h\\) is invertible. (This can also be derived by adding a \\(\\lambda \\|\\mu\\|_{\\text{F}}^2\\) regularization term to the objective.) We can directly plug in this estimate into \\(\\widehat{P}^t_h(\\cdot \\mid s, a) = \\widehat \\mu^t_h \\phi(s, a)\\).\n\n\n9.5.2.2 Reward bonus\nNow, to design the reward bonus, we can’t apply Hoeffding’s inequality anymore, since the terms no longer involve sample means of bounded random variables; Instead, we’re incorporating information across different states and actions. Rather, we can construct an upper bound using Chebyshev’s inequality in the same way we did for the LinUCB algorithm in the MAB setting Section 3.8.1:\n\\[\nb^t_h(s, a) = \\beta \\sqrt{\\phi(s, a)^\\top (A^t_h)^{-1} \\phi(s, a)}, \\quad \\beta = \\tilde O(d H).\n\\]\nNote that this isn’t explicitly inversely proportional to \\(N_h^t(s, a)\\) as in the original UCB-VI bonus term Equation 9.2. Rather, it is inversely proportional to the amount that the direction \\(\\phi(s, a)\\) has been explored in the history. That is, if \\(A-h^t\\) has a large component in the direction \\(\\phi(s, a)\\), implying that this direction is well explored, then the bonus term will be small, and vice versa.\nWe can now plug in these transition estimates and reward bonuses into the UCB-VI algorithm Definition 9.3.\n\nTheorem 9.3 (LinUCB-VI regret) The LinUCB-VI algorithm achieves expected regret\n\\[\n\\mathbb{E}[\\text{Regret}_T] = \\mathbb{E}\\left[\\sum_{t=0}^{T-1} V^\\star_0(s_0) - V^{\\pi^t}_0(s_0) \\right] \\le \\tilde O(H^2 d^{1.5} \\sqrt{T})\n\\]\n\nComparing this to our bound for UCB-VI in an environment without this linear assumption, we see that we go from a sample complexity of \\(\\tilde \\Omega(H^4 |\\mathcal{S}||\\mathcal{A}|)\\) to \\(\\tilde \\Omega(H^4 d^{3})\\). This new sample complexity only depends on the feature dimension and not on the state or action space of the MDP!",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "exploration.html#summary",
    "href": "exploration.html#summary",
    "title": "9  Exploration in MDPs",
    "section": "9.6 Summary",
    "text": "9.6 Summary\nIn this chapter, we’ve explored how to explore in an unknown MDP.\n\nWe first discussed the explore-then-exploit algorithm Definition 9.2, a simple way to explore a deterministic MDP by visiting all state-action pairs.\nWe then discussed how to treat an unknown MDP as a MAB Section 9.3, and how this approach is inefficient since it doesn’t make use of relationships between policies.\nWe then introduced the UCB-VI algorithm Definition 9.3, which models the unknown MDP by a proxy MDP with a reward bonus term that encourages exploration.\nFinally, assuming that the transitions and rewards are linear with respect to a feature transformation of the state and action, we introduced the LinUCB-VI algorithm Section 9.5.2, which has a sample complexity independent of the size of the state and action spaces.\n\n\n\n\n\nAgarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. Reinforcement Learning: Theory and Algorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Exploration in MDPs</span>"
    ]
  },
  {
    "objectID": "background.html",
    "href": "background.html",
    "title": "10  Appendix: Background",
    "section": "",
    "text": "10.1 O notation\nThroughout this chapter and the rest of the book, we will describe the asymptotic behavior of a function using \\(O\\) notation.\nFor two functions \\(f(t)\\) and \\(g(t)\\), we say that \\(f(t) \\le O(g(t))\\) if \\(f\\) is asymptotically upper bounded by \\(g\\). Formally, this means that there exists some constant \\(C &gt; 0\\) such that \\(f(t) \\le C \\cdot g(t)\\) for all \\(t\\) past some point \\(t_0\\).\nWe say \\(f(t) &lt; o(g(t))\\) if asymptotically \\(f\\) grows strictly slower than \\(g\\). Formally, this means that for any scalar \\(C &gt; 0\\), there exists some \\(t_0\\) such that \\(f(t) \\le C \\cdot g(t)\\) for all \\(t &gt; t_0\\). Equivalently, we say \\(f(t) &lt; o(g(t))\\) if \\(\\lim_{t \\to \\infty} f(t)/g(t) = 0\\).\n\\(f(t) = \\Theta(g(t))\\) means that \\(f\\) and \\(g\\) grow at the same rate asymptotically. That is, \\(f(t) \\le O(g(t))\\) and \\(g(t) \\le O(f(t))\\).\nFinally, we use \\(f(t) \\ge \\Omega(g(t))\\) to mean that \\(g(t) \\le O(f(t))\\), and \\(f(t) &gt; \\omega(g(t))\\) to mean that \\(g(t) &lt; o(f(t))\\).\nWe also use the notation \\(\\tilde O(g(t))\\) to hide logarithmic factors. That is, \\(f(t) = \\tilde O(g(t))\\) if there exists some constant \\(C\\) such that \\(f(t) \\le C \\cdot g(t) \\cdot \\log^k(t)\\) for some \\(k\\) and all \\(t\\).\nOccasionally, we will also use \\(O(f(t))\\) (or one of the other symbols) as shorthand to manipulate function classes. For example, we might write \\(O(f(t)) + O(g(t)) = O(f(t) + g(t))\\) to mean that the sum of two functions in \\(O(f(t))\\) and \\(O(g(t))\\) is in \\(O(f(t) + g(t))\\).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Appendix: Background</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022.\nReinforcement Learning: Theory and\nAlgorithms. https://rltheorybook.github.io/rltheorybook_AJKS.pdf.\n\n\nBaydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul,\nand Jeffrey Mark Siskind. 2018. “Automatic Differentiation in\nMachine Learning: A Survey.” February 5, 2018. https://doi.org/10.48550/arXiv.1502.05767.\n\n\nBoyd, Stephen, and Lieven Vandenberghe. 2004. Convex\nOptimization. Cambridge University Press. https://web.stanford.edu/~boyd/cvxbook/.\n\n\nDeng, Li. 2012. “The MNIST Database of\nHandwritten Digit Images for Machine Learning\nResearch [Best of the Web].”\nIEEE Signal Processing Magazine 29 (6): 141–42. https://doi.org/10.1109/MSP.2012.2211477.\n\n\nHastie, Trevor, Robert Tibshirani, and Jerome Friedman. 2013. The\nElements of Statistical Learning: Data\nMining, Inference, and Prediction.\nSpringer Science & Business Media. https://books.google.com?id=yPfZBwAAQBAJ.\n\n\nJames, Gareth, Daniela Witten, Trevor Hastie, Robert Tibshirani, and\nJonathan Taylor. 2023. An Introduction to\nStatistical Learning: With Applications in\nPython. Springer Texts in\nStatistics. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-031-38747-0.\n\n\nLai, T. L, and Herbert Robbins. 1985. “Asymptotically Efficient\nAdaptive Allocation Rules.” Advances in Applied\nMathematics 6 (1): 4–22. https://doi.org/10.1016/0196-8858(85)90002-8.\n\n\nNielsen, Michael A. 2015. Neural Networks and\nDeep Learning. Determination Press. http://neuralnetworksanddeeplearning.com/.\n\n\nRoss, Stéphane, Geoffrey J. Gordon, and J. Bagnell. 2010. “A\nReduction of Imitation Learning and\nStructured Prediction to No-Regret Online\nLearning.” In. https://www.semanticscholar.org/paper/A-Reduction-of-Imitation-Learning-and-Structured-to-Ross-Gordon/79ab3c49903ec8cb339437ccf5cf998607fc313e.\n\n\nRussell, Stuart J., and Peter Norvig. 2021. Artificial Intelligence:\nA Modern Approach. Fourth edition. Pearson Series in Artificial\nIntelligence. Hoboken: Pearson.\n\n\nSchrittwieser, Julian, Ioannis Antonoglou, Thomas Hubert, Karen\nSimonyan, Laurent Sifre, Simon Schmitt, Arthur Guez, et al. 2020.\n“Mastering Atari, Go, Chess and Shogi by\nPlanning with a Learned Model.” Nature 588 (7839, 7839):\n604–9. https://doi.org/10.1038/s41586-020-03051-4.\n\n\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg\nKlimov. 2017. “Proximal Policy Optimization\nAlgorithms.” August 28, 2017. https://doi.org/10.48550/arXiv.1707.06347.\n\n\nSilver, David, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre,\nGeorge van den Driessche, Julian Schrittwieser, et al. 2016.\n“Mastering the Game of Go with Deep Neural Networks\nand Tree Search.” Nature 529 (7587, 7587): 484–89. https://doi.org/10.1038/nature16961.\n\n\nSilver, David, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,\nMatthew Lai, Arthur Guez, Marc Lanctot, et al. 2018. “A General\nReinforcement Learning Algorithm That Masters Chess, Shogi, and\nGo Through Self-Play.” Science 362 (6419):\n1140–44. https://doi.org/10.1126/science.aar6404.\n\n\nSilver, David, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou,\nAja Huang, Arthur Guez, Thomas Hubert, et al. 2017. “Mastering the\nGame of Go Without Human Knowledge.” Nature\n550 (7676, 7676): 354–59. https://doi.org/10.1038/nature24270.\n\n\nSussman, Gerald Jay, Jack Wisdom, and Will Farr. 2013. Functional\nDifferential Geometry. Cambridge, MA: The MIT Press.\n\n\nSutton, Richard S., and Andrew G. Barto. 2018. Reinforcement\nLearning: An Introduction. Second edition. Adaptive Computation and\nMachine Learning Series. Cambridge, Massachusetts: The MIT Press. http://incompleteideas.net/book/RLbook2020trimmed.pdf.\n\n\nVershynin, Roman. 2018. High-Dimensional Probability:\nAn Introduction with Applications in\nData Science. Cambridge University Press. https://books.google.com?id=NDdqDwAAQBAJ.\n\n\nWilliams, Ronald J. 1992. “Simple Statistical Gradient-Following\nAlgorithms for Connectionist Reinforcement Learning.” Machine\nLearning 8 (3): 229–56. https://doi.org/10.1007/BF00992696.",
    "crumbs": [
      "References"
    ]
  }
]