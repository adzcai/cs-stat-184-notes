\providecommand{\main}{..}

\documentclass[\main/main]{subfiles}


\begin{document}

Welcome to the study of reinforcement learning! This set of lecture notes accompanies the undergraduate course CS/STAT 184 and is intended to be a friendly yet rigorous introduction to this exciting and active subfield of machine learning. Here are some questions you might have before embarking on this journey:

\textbf{What is reinforcement learning (RL)?} Broadly speaking, RL is a subfield of machine learning that studies how an agent can learn to make sequential decisions in an environment.

\textbf{Why study RL?} RL provides a powerful framework for attacking a wide variety of problems, including robotic control, video games and board games, resource management, language modelling, and more. It also provides an interdisciplinary paradigm for studying animal and human behavior. Many of the most stunning results in machine learning, ranging from AlphaGo to ChatGPT, are built on top of RL.

\textbf{Is this book for me?} This book assumes familiarity with multivariable calculus, linear algebra, and probability. For Harvard undergraduates, this would be fulfilled by Math 21a, Math 21b, and Stat 110. Stat 111 is strongly recommended but not required. Here is a non-comprehensive list of topics of which this book will assume knowledge:

\begin{itemize}
    \item \textbf{Linear Algebra:} Vectors, matrices, matrix multiplication, matrix inversion, eigenvalues and eigenvectors, and the Gram-Schmidt process.
    \item \textbf{Multivariable Calculus:} Partial derivatives, gradient, directional derivative, and the chain rule.
    \item \textbf{Probability:} Random variables, probability distributions, expectation, variance, covariance, conditional probability, Bayes' rule, and the law of total probability.
\end{itemize}

\textbf{How does reinforcement learning differ from other machine learning paradigms?} Here is a list of comparisons:

\begin{itemize}
    \item \textbf{Supervised learning.} Supervised learning concerns itself with learning a mapping from inputs to outputs (e.g. image classification). Typically the data takes the form of input-output pairs that are assumed to be sampled independently from some generating distribution. In RL, however, the data is generated by the agent interacting with the environment, meaning the observations depend on the agent's behaviour and are not independent from each other. This requires a more general set of tools.

    Conversely, supervised learning is a well-studied field that provides many useful tools for RL. For example, it may be useful to use supervised learning to predict how valuable a given state is, or to predict the probability of transitioning to a given state.
\end{itemize}

\section{Overview}

Chapter 1 introduces \textbf{Markov Decision Processes}, the dominant mathematical framework for studying RL. We'll discuss \textbf{dynamic programming} algorithms for solving MDPs, including \textbf{policy evaluation}, \textbf{policy iteration}, and \textbf{value iteration}.

Chapter 2 then discusses \textbf{multi-armed bandits}, a simpler problem that is often used as a warm-up to RL.

\section{Notation}

We will use the following notation throughout the book. This notation is inspired by Sutton and Barto \todo{cite} and AJKS \todo{cite}.

We will use \emph{lowercase letters} to index over \emph{uppercase letters}.

\todo{Add notation.}

\subsection{Multi-Armed Bandits}

\begin{tabular}{cl}
    $[N]$ & The set $\{0, 1, \dots, N-1\}$. \\
    $K$ & The number of arms \\
    $T$ & The number of time steps (i.e. algorithm iterations). \\
\end{tabular}

\subsection{MDPs}

\begin{tabular}{cl}
$\S$ & The state space. \\
$\A$ & The action space. \\
$s \in \S$ & A state. \\
$a \in \A$ & An action. \\
$r : \S \times \A \to \R$ & The reward function. \\
$P : \S times \A \to \triangle(\S)$ & The transition probabilities. \\
$\gamma \in (0, 1)$ & The discount factor. \\
$\pi : \S \to \A$ & A policy. \\
$V^\pi(s) \in \R$ & The value function of policy $\pi$. \\
$Q^\pi(s, a) \in \R$ & The action-value function of policy $\pi$. \\
$\pi^\star, \Vopt, \Qopt$ & The optimal policy, value function, and action-value function. \\
\end{tabular}

\subsection{Vectors}

Note that a vector $v \in \R^d$ can be thought of as a function from $v : [d] \to \R : i \mapsto v_i$.
Similarly, we will often identify a real-valued function on states, such as the transition matrix for a given state-action pair,
with the corresponding vector. That is, we will treat $P(\cdot \mid s, a) \in \R^{|\S|}$.

\section{Challenges of reinforcement learning}

\textbf{Exploration-exploitation tradeoff.} Should the agent try a new action or stick with the action that it knows is good?

\textbf{Prediction.} The agent might want to predict the value of a state or state-action pair.

\textbf{Policy computation (control).} In a complex environment, even if the dynamics are known, it can still be challenging to compute the best policy.


\section{Resources}

Inspired by the Stat 110 textbook and Stat 111 lecture notes.

This book seeks to provide an intuitive understanding before technical treatment.

\todo{Refer to AJKS, Sutton and Barto, online resources.}


\end{document}
