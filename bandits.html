
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. Multi-Armed Bandits &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'bandits';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="3. Finite Markov Decision Processes" href="mdps.html" />
    <link rel="prev" title="1. Introduction" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">1. Introduction</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="mdps.html">3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="fitted_dp.html">4. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">5. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="pg.html">6. Policy Gradient Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">7. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">8. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/bandits.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Multi-Armed Bandits</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">2.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pure-exploration-random-guessing">2.2. Pure exploration (random guessing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pure-greedy">2.3. Pure greedy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explore-then-commit">2.4. Explore-then-commit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#etc-regret-analysis">2.4.1. ETC regret analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-phase">2.4.1.1. Exploration phase.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploitation-phase">2.4.1.2. Exploitation phase.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">2.5. Epsilon-greedy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">2.6. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ucb-regret-analysis">2.6.1. UCB regret analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lower-bound-on-regret-intuition">2.6.2. Lower bound on regret (intuition)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling-and-bayesian-bandits">2.7. Thompson sampling and Bayesian bandits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextual-bandits">2.8. Contextual bandits</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-contextual-bandits">2.8.1. Linear contextual bandits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">2.9. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="multi-armed-bandits">
<span id="bandits"></span><h1><span class="section-number">2. </span>Multi-Armed Bandits<a class="headerlink" href="#multi-armed-bandits" title="Link to this heading">#</a></h1>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># from bokeh.plotting import figure, show, output_notebook</span>
<span class="kn">import</span> <span class="nn">latexify</span>
<span class="kn">from</span> <span class="nn">abc</span> <span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>  <span class="c1"># &quot;Abstract Base Class&quot;</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Union</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="kn">import</span> <span class="nn">solutions.bandits</span> <span class="k">as</span> <span class="nn">solutions</span>

<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">184</span><span class="p">)</span>

<span class="c1"># output_notebook()  # set up bokeh</span>

<span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s2">&quot;fivethirtyeight&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">random_argmax</span><span class="p">(</span><span class="n">ary</span><span class="p">:</span> <span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">max_idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">flatnonzero</span><span class="p">(</span><span class="n">ary</span> <span class="o">==</span> <span class="n">ary</span><span class="o">.</span><span class="n">max</span><span class="p">())</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">max_idx</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>


<span class="n">latex</span> <span class="o">=</span> <span class="n">latexify</span><span class="o">.</span><span class="n">algorithmic</span><span class="p">(</span>
    <span class="n">prefixes</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;mab&quot;</span><span class="p">},</span>
    <span class="n">identifiers</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;arm&quot;</span><span class="p">:</span> <span class="s2">&quot;a_t&quot;</span><span class="p">,</span> <span class="s2">&quot;reward&quot;</span><span class="p">:</span> <span class="s2">&quot;r&quot;</span><span class="p">,</span> <span class="s2">&quot;means&quot;</span><span class="p">:</span> <span class="s2">&quot;mu&quot;</span><span class="p">},</span>
    <span class="n">use_math_symbols</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">escape_underscores</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>The <strong>multi-armed bandits</strong> (MAB) setting is a simple setting for studying the basic challenges of RL. In this setting, an agent repeatedly chooses from a fixed set of actions, called <strong>arms</strong>, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>States</p></th>
<th class="head text-center"><p>Actions</p></th>
<th class="head text-center"><p>Rewards</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p>None</p></td>
<td class="text-center"><p>Finite</p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{A} \to \triangle([0, 1])\)</span></p></td>
</tr>
</tbody>
</table>
</div>
<p>In particular, we’ll spend a lot of time discussing the <strong>Exploration-Exploitation Tradeoff</strong>: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?</p>
<div class="proof example admonition" id="advertising">
<p class="admonition-title"><span class="caption-number">Example 2.1 </span> (Online advertising)</p>
<section class="example-content" id="proof-content">
<p>Let’s suppose you, the agent, are an advertising company. You have <span class="math notranslate nohighlight">\(K\)</span> different ads that you can show to users; For concreteness, let’s suppose there’s just a single user. You receive <span class="math notranslate nohighlight">\(1\)</span> reward if the user clicks the ad, and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Thus, the unknown <em>reward distribution</em> associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.</p>
</section>
</div><div class="proof example admonition" id="clinical_trials">
<p class="admonition-title"><span class="caption-number">Example 2.2 </span> (Clinical trials)</p>
<section class="example-content" id="proof-content">
<p>Suppose you’re a pharmaceutical company, and you’re testing a new drug. You have <span class="math notranslate nohighlight">\(K\)</span> different dosages of the drug that you can administer to patients. You receive <span class="math notranslate nohighlight">\(1\)</span> reward if the patient recovers, and <span class="math notranslate nohighlight">\(0\)</span> otherwise. Thus, the unknown <em>reward distribution</em> associated to each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.</p>
</section>
</div><p>In this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.</p>
<section id="introduction">
<h2><span class="section-number">2.1. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<div class="proof remark admonition" id="multi-armed">
<p class="admonition-title"><span class="caption-number">Remark 2.1 </span> (Namesake)</p>
<section class="remark-content" id="proof-content">
<p>The name “multi-armed bandits” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and take money from the player.</p>
</section>
</div><p>Let <span class="math notranslate nohighlight">\(K\)</span> denote the number of arms. We’ll label them <span class="math notranslate nohighlight">\(0, \dots, K-1\)</span> and use <em>superscripts</em> to indicate the arm index; since we seldom need to raise a number to a power, this won’t cause much confusion. In this chapter, we’ll consider the <strong>Bernoulli bandit</strong> setting from the examples above, where arm <span class="math notranslate nohighlight">\(k\)</span> either returns reward <span class="math notranslate nohighlight">\(1\)</span> with probability <span class="math notranslate nohighlight">\(\mu^k\)</span> or <span class="math notranslate nohighlight">\(0\)</span> otherwise. The agent gets to pull an arm <span class="math notranslate nohighlight">\(T\)</span> times in total. We can formalize the Bernoulli bandit in the following Python code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MAB</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The Bernoulli multi-armed bandit environment.</span>

<span class="sd">    :param means: the means (success probabilities) of the reward distributions for each arm</span>
<span class="sd">    :param T: the time horizon</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">means</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; K&quot;</span><span class="p">],</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">all</span><span class="p">(</span><span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">p</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">means</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">means</span> <span class="o">=</span> <span class="n">means</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="o">.</span><span class="n">size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">best_arm</span> <span class="o">=</span> <span class="n">random_argmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">pull</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">k</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Pull the `k`-th arm and sample from its (Bernoulli) reward distribution.&quot;&quot;&quot;</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        <span class="k">return</span> <span class="o">+</span><span class="n">reward</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mab</span> <span class="o">=</span> <span class="n">MAB</span><span class="p">(</span><span class="n">means</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]),</span> <span class="n">T</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In pseudocode, the agent’s interaction with the MAB environment can be
described by the following process:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@latex</span>
<span class="k">def</span> <span class="nf">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">:</span> <span class="n">MAB</span><span class="p">,</span> <span class="n">agent</span><span class="p">:</span> <span class="s2">&quot;Agent&quot;</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">choose_arm</span><span class="p">()</span>  <span class="c1"># in 0, ..., K-1</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">mab</span><span class="o">.</span><span class="n">pull</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">update_history</span><span class="p">(</span><span class="n">arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>


<span class="n">mab_loop</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_latex math notranslate nohighlight">
\[\begin{split} \begin{array}{l} \mathbf{function} \ \mathrm{\mathrm{mab}_\mathrm{loop}}(\mathrm{mab}, \mathrm{agent}) \\ \hspace{1em} \mathbf{for} \ t \in \mathrm{range} \mathopen{}\left( T \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{a_t} \gets \mathrm{agent}.\mathrm{\mathrm{choose}_\mathrm{arm}} \mathopen{}\left( \mathclose{}\right) \\ \hspace{2em} r \gets \mathrm{pull} \mathopen{}\left( \mathrm{a_t} \mathclose{}\right) \\ \hspace{2em} \mathrm{agent}.\mathrm{\mathrm{update}_\mathrm{history}} \mathopen{}\left( \mathrm{a_t}, r \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \mathbf{end \ function} \end{array} \end{split}\]</div>
</div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">Agent</span></code> class stores the pull history and uses it to decide which arm to pull next. Since we are working with Bernoulli bandits, we can summarize the pull history concisely in a <span class="math notranslate nohighlight">\(\mathbb{N}^{K \times 2}\)</span> array.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Agent</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The MAB agent that decides how to choose an arm given the past history.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">K</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">T</span> <span class="o">=</span> <span class="n">T</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># for plotting</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">K</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Choose an arm of the MAB. Algorithm-specific.&quot;&quot;&quot;</span>
        <span class="o">...</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">count</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;The number of pulls made.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">choices</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="n">arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
</pre></div>
</div>
</div>
</div>
<p>What’s the <em>optimal</em> strategy for the agent, i.e. the one that achieves
the highest expected reward? Convince yourself that the agent should try
to always pull the arm with the highest expected reward:</p>
<div class="math notranslate nohighlight">
\[\mu^\star := \max_{k \in [K]} \mu^k.\]</div>
<p>The goal, then, can be rephrased as to minimize the <strong>regret</strong>, defined
below:</p>
<div class="proof definition admonition" id="regret">
<p class="admonition-title"><span class="caption-number">Definition 2.1 </span> (Regret)</p>
<section class="definition-content" id="proof-content">
<p>The agent’s <strong>regret</strong> after <span class="math notranslate nohighlight">\(T\)</span> timesteps is defined as</p>
<div class="math notranslate nohighlight">
\[
\text{Regret}_T := \sum_{t=0}^{T-1} \mu^\star - \mu^{a_t}.
\]</div>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">regret_per_step</span><span class="p">(</span><span class="n">mab</span><span class="p">:</span> <span class="n">MAB</span><span class="p">,</span> <span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the difference from the average reward of the optimal arm. The sum of these is the regret.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">mab</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">mab</span><span class="o">.</span><span class="n">best_arm</span><span class="p">]</span> <span class="o">-</span> <span class="n">mab</span><span class="o">.</span><span class="n">means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="k">for</span> <span class="n">arm</span> <span class="ow">in</span> <span class="n">agent</span><span class="o">.</span><span class="n">choices</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Note that this depends on the <em>true means</em> of the pulled arms, <em>not</em> the actual
observed rewards.
We typically think of this as a random variable where
the randomness comes from the agent’s strategy (i.e. the sequence of
actions <span class="math notranslate nohighlight">\(a_0, \dots, a_{T-1}\)</span>).</p>
<p>Throughout the chapter, we will try to upper bound the regret of various
algorithms in two different senses:</p>
<ol class="arabic simple">
<li><p>Upper bound the <em>expected regret,</em> i.e. show
<span class="math notranslate nohighlight">\(\E[\text{Regret}_T] \le M_T\)</span>.</p></li>
<li><p>Find a <em>high-probability</em> upper bound on the regret, i.e. show
<span class="math notranslate nohighlight">\(\P(\text{Regret}_T \le M_{T, \delta}) \ge 1-\delta\)</span>.</p></li>
</ol>
<p>Note that these two different approaches say very different things about the regret. The first approach says that the <em>average</em> regret is at most <span class="math notranslate nohighlight">\(M_T\)</span>. However, the agent might still achieve higher regret on many runs. The second approach says that, <em>with high probability</em>, the agent will achieve regret at most <span class="math notranslate nohighlight">\(M_{T, \delta}\)</span>. However, it doesn’t say anything about the regret in the remaining <span class="math notranslate nohighlight">\(\delta\)</span> fraction of runs, which might be arbitrarily high.</p>
<p>We’d like to achieve <strong>sublinear regret</strong> in expectation, i.e. <span class="math notranslate nohighlight">\(\E[\text{Regret}_T] = o(T)\)</span>. That is, as we learn more about the environment, we’d like to be able to exploit that knowledge to take the optimal arm as often as possible.</p>
<p>The rest of the chapter comprises a series of increasingly sophisticated
MAB algorithms.</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">:</span> <span class="n">MAB</span><span class="p">,</span> <span class="n">agent</span><span class="p">:</span> <span class="n">Agent</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

    <span class="c1"># plot reward and cumulative regret</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">agent</span><span class="o">.</span><span class="n">rewards</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;reward&quot;</span><span class="p">)</span>
    <span class="n">cum_regret</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">regret_per_step</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">cum_regret</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;cumulative regret&quot;</span><span class="p">)</span>

    <span class="c1"># draw colored circles for arm choices</span>
    <span class="n">colors</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="s2">&quot;blue&quot;</span><span class="p">]</span>
    <span class="n">color_array</span> <span class="o">=</span> <span class="p">[</span><span class="n">colors</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">agent</span><span class="o">.</span><span class="n">choices</span><span class="p">]</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="n">color_array</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;arm&quot;</span><span class="p">)</span>

    <span class="c1"># labels and title</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;timestep&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">agent</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2"> reward and regret&quot;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="pure-exploration-random-guessing">
<h2><span class="section-number">2.2. </span>Pure exploration (random guessing)<a class="headerlink" href="#pure-exploration-random-guessing" title="Link to this heading">#</a></h2>
<p>A trivial strategy is to always choose arms at random (i.e. “pure
exploration”).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PureExploration</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Choose an arm uniformly at random.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">solutions</span><span class="o">.</span><span class="n">pure_exploration_choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note that</p>
<div class="math notranslate nohighlight">
\[
\E_{a_t \sim \text{Unif}([K])}[\mu^{a_t}] = \bar \mu = \frac{1}{K} \sum_{k=1}^K \mu^k
\]</div>
<p>so the expected regret is simply</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \E[\text{Regret}_T] &amp;= \sum_{t=0}^{T-1} \E[\mu^\star - \mu^{a_t}] \\
    &amp;= T (\mu^\star - \bar \mu) &gt; 0.
\end{aligned}
\end{split}\]</div>
<p>This scales as <span class="math notranslate nohighlight">\(\Theta(T)\)</span>, i.e. <em>linear</em> in the number of timesteps <span class="math notranslate nohighlight">\(T\)</span>. There’s no learning here: the agent doesn’t use any information about the environment to improve its strategy. You can see that the distribution over its arm choices always appears “(uniformly) random”.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">PureExploration</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/58938b3295ffe4b5ac7521efd6784ba93977317658e7c7999e54199a2348dbf4.png" src="_images/58938b3295ffe4b5ac7521efd6784ba93977317658e7c7999e54199a2348dbf4.png" />
</div>
</div>
</section>
<section id="pure-greedy">
<h2><span class="section-number">2.3. </span>Pure greedy<a class="headerlink" href="#pure-greedy" title="Link to this heading">#</a></h2>
<p>How might we improve on pure exploration? Instead, we could try each arm
once, and then commit to the one with the highest observed reward. We’ll
call this the <strong>pure greedy</strong> strategy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PureGreedy</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Choose the arm with the highest observed reward on its first pull.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">solutions</span><span class="o">.</span><span class="n">pure_greedy_choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Note we’ve used superscripts <span class="math notranslate nohighlight">\(r^k\)</span> during the exploration phase to
indicate that we observe exactly one reward for each arm. Then we use
subscripts <span class="math notranslate nohighlight">\(r_t\)</span> during the exploitation phase to indicate that we
observe a sequence of rewards from the chosen greedy arm <span class="math notranslate nohighlight">\(\hat k\)</span>.</p>
<p>How does the expected regret of this strategy compare to that of pure
exploration? We’ll do a more general analysis in the following section.
Now, for intuition, suppose there’s just <span class="math notranslate nohighlight">\(K=2\)</span> arms, with Bernoulli
reward distributions with means <span class="math notranslate nohighlight">\(\mu^0 &gt; \mu^1\)</span>.</p>
<p>Let’s let <span class="math notranslate nohighlight">\(r^0\)</span> be the random reward from the first arm and <span class="math notranslate nohighlight">\(r^1\)</span> be the
random reward from the second. If <span class="math notranslate nohighlight">\(r^0 &gt; r^1\)</span>, then we achieve zero
regret. Otherwise, we achieve regret <span class="math notranslate nohighlight">\(T(\mu^0 - \mu^1)\)</span>. Thus, the
expected regret is simply:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \E[\text{Regret}_T] &amp;= \P(r^0 &lt; r^1) \cdot T(\mu^0 - \mu^1) + c \\
    &amp;= (1 - \mu^0) \mu^1 \cdot T(\mu^0 - \mu^1) + c
\end{aligned}
\end{split}\]</div>
<p>Which is still <span class="math notranslate nohighlight">\(\Theta(T)\)</span>, the same as pure exploration!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">PureGreedy</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/2b5866bf76b1b1282bf3dd9a636c2edb08747f425b3ec6990a8a4f49970d9199.png" src="_images/2b5866bf76b1b1282bf3dd9a636c2edb08747f425b3ec6990a8a4f49970d9199.png" />
</div>
</div>
<p>The cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, if the greedy algorithm happens to get lucky on the first set of pulls, it may act entirely optimally for that episode! But its <em>average</em> regret is what measures its effectiveness.</p>
</section>
<section id="explore-then-commit">
<span id="etc"></span><h2><span class="section-number">2.4. </span>Explore-then-commit<a class="headerlink" href="#explore-then-commit" title="Link to this heading">#</a></h2>
<p>We can improve the pure greedy algorithm as follows: let’s reduce the variance of the reward estimates by pulling each arm <span class="math notranslate nohighlight">\(N_{\text{explore}}&gt; 1\)</span> times before committing. This is called the <strong>explore-then-commit</strong> strategy. Note that the “pure greedy” strategy above is just the special case where
<span class="math notranslate nohighlight">\(N_{\text{explore}}= 1\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ExploreThenCommit</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">N_explore</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_explore</span> <span class="o">=</span> <span class="n">N_explore</span>

    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">solutions</span><span class="o">.</span><span class="n">etc_choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">ExploreThenCommit</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span> <span class="o">//</span> <span class="mi">15</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/28e29def663ccd6cdc50809928467bc4218df307aaaa1ff759bbaaa1f2e290db.png" src="_images/28e29def663ccd6cdc50809928467bc4218df307aaaa1ff759bbaaa1f2e290db.png" />
</div>
</div>
<p>Notice that now, the graphs are much more consistent, and the algorithm finds the true optimal arm and sticks with it much more frequently. We would expect ETC to then have a better (i.e. lower) average regret. Can we prove this?</p>
<section id="etc-regret-analysis">
<span id="id1"></span><h3><span class="section-number">2.4.1. </span>ETC regret analysis<a class="headerlink" href="#etc-regret-analysis" title="Link to this heading">#</a></h3>
<p>Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up
into the exploration and exploitation phases.</p>
<section id="exploration-phase">
<h4><span class="section-number">2.4.1.1. </span>Exploration phase.<a class="headerlink" href="#exploration-phase" title="Link to this heading">#</a></h4>
<p>This phase takes <span class="math notranslate nohighlight">\(N_{\text{explore}}K\)</span> timesteps. Since at each step we
incur at most <span class="math notranslate nohighlight">\(1\)</span> regret, the total regret is at most
<span class="math notranslate nohighlight">\(N_{\text{explore}}K\)</span>.</p>
</section>
<section id="exploitation-phase">
<h4><span class="section-number">2.4.1.2. </span>Exploitation phase.<a class="headerlink" href="#exploitation-phase" title="Link to this heading">#</a></h4>
<p>This will take a bit more effort. We’ll prove that for any total time <span class="math notranslate nohighlight">\(T\)</span>, we can choose <span class="math notranslate nohighlight">\(N_{\text{explore}}\)</span> such that with arbitrarily high probability, the regret is sublinear.</p>
<p>Let <span class="math notranslate nohighlight">\(\hat k\)</span> denote the arm chosen after the exploration phase. We know the regret from the
exploitation phase is</p>
<div class="math notranslate nohighlight">
\[T_{\text{exploit}} (\mu^\star - \mu^{\hat k}) \qquad \text{where} \qquad T_{\text{exploit}} := T - N_{\text{explore}}K.\]</div>
<p>So we’d like to bound <span class="math notranslate nohighlight">\(\mu^\star - \mu^{\hat k} = o(1)\)</span> (as a function
of <span class="math notranslate nohighlight">\(T\)</span>) in order to achieve sublinear regret. How can we do this?</p>
<p>Let’s define <span class="math notranslate nohighlight">\(\Delta^k = \hat \mu^k - \mu^k\)</span> to denote how far the mean
estimate for arm <span class="math notranslate nohighlight">\(k\)</span> is from the true mean. How can we bound this
quantity? We’ll use the following useful inequality for i.i.d. bounded
random variables:</p>
<div class="proof theorem admonition" id="hoeffding">
<p class="admonition-title"><span class="caption-number">Theorem 2.1 </span> (Hoeffding’s inequality)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X_0, \dots, X_{n-1}\)</span> be i.i.d. random variables with
<span class="math notranslate nohighlight">\(X_i \in [0, 1]\)</span> almost surely for each <span class="math notranslate nohighlight">\(i \in [n]\)</span>. Then for any
<span class="math notranslate nohighlight">\(\delta &gt; 0\)</span>,</p>
<div class="math notranslate nohighlight">
\[\P\left( \left| \frac{1}{n} \sum_{i=1}^n (X_i - \E[X_i]) \right| &gt; \sqrt{\frac{\ln(2/\delta)}{2n}} \right) \le \delta.\]</div>
</section>
</div><p>The proof of this inequality is beyond the scope of this book. See <span id="id2">[<a class="reference internal" href="bibliography.html#id17" title="Roman Vershynin. High-Dimensional Probability: An Introduction with Applications in Data Science. Cambridge University Press, September 2018. ISBN 978-1-108-41519-4.">Vershynin, 2018</a>]</span> Chapter 2.2.</p>
<p>We can apply this directly to the rewards for a given arm <span class="math notranslate nohighlight">\(k\)</span>, since the rewards from that arm are i.i.d.:</p>
<div class="math notranslate nohighlight" id="equation-hoeffding-etc">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-hoeffding-etc" title="Link to this equation">#</a></span>\[\P\left(|\Delta^k | &gt; \sqrt{\frac{\ln(2/\delta)}{2N_{\text{explore}}}} \right) \le \delta.\]</div>
<p>But note that we can’t apply this to arm <span class="math notranslate nohighlight">\(\hat k\)</span> directly since
<span class="math notranslate nohighlight">\(\hat k\)</span> is itself a random variable. Instead, we need to “uniform-ize”
this bound across <em>all</em> the arms, i.e. bound the error across all the
arms simultaneously, so that the resulting bound will apply <em>no matter
what</em> <span class="math notranslate nohighlight">\(\hat k\)</span> “crystallizes” to.</p>
<p>The <strong>union bound</strong> provides a simple way to do this:</p>
<div class="proof theorem admonition" id="union_bound">
<p class="admonition-title"><span class="caption-number">Theorem 2.2 </span> (Union bound)</p>
<section class="theorem-content" id="proof-content">
<p>Consider a set of events <span class="math notranslate nohighlight">\(A_0, \dots, A_{n-1}\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[\P(\exists i \in [n]. A_i) \le \sum_{i=0}^{n-1} \P(A_i).\]</div>
<p>In
particular, if <span class="math notranslate nohighlight">\(\P(A_i) \ge 1 - \delta\)</span> for each <span class="math notranslate nohighlight">\(i \in [n]\)</span>, we have</p>
<div class="math notranslate nohighlight">
\[\P(\forall i \in [n]. A_i) \ge 1 - n \delta.\]</div>
</section>
</div><p><strong>Exercise:</strong> Prove the second statement above.</p>
<p>Applying the union bound across the arms for the l.h.s. event of <a class="reference internal" href="#equation-hoeffding-etc">(2.1)</a>, we have</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    \P\left( \forall k \in [K], |\Delta^k | \le \sqrt{\frac{\ln(2/\delta)}{2N_{\text{explore}}}} \right) &amp;\ge 1-K\delta
\end{aligned}
\]</div>
<p>Then to apply this bound to <span class="math notranslate nohighlight">\(\hat k\)</span> in particular, we
can apply the useful trick of “adding zero”:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mu^{k^\star} - \mu^{\hat k} &amp;= \mu^{k^\star} - \mu^{\hat k} + (\hat \mu^{k^\star} - \hat \mu^{k^\star}) + (\hat \mu^{\hat k} - \hat \mu^{\hat k}) \\
    &amp;= \Delta^{\hat k} - \Delta^{k^*} + \underbrace{(\hat \mu^{k^\star} - \hat \mu^{\hat k})}_{\le 0 \text{ by definition of } \hat k} \\
    &amp;\le 2 \sqrt{\frac{\ln(2K/\delta')}{2N_{\text{explore}}}} \text{ with probability at least } 1-\delta'
\end{aligned}
\end{split}\]</div>
<p>where we’ve set <span class="math notranslate nohighlight">\(\delta' = K\delta\)</span>. Putting this all
together, we’ve shown that, with probability <span class="math notranslate nohighlight">\(1 - \delta'\)</span>,</p>
<div class="math notranslate nohighlight">
\[\text{Regret}_T \le N_{\text{explore}}K + T_{\text{exploit}} \cdot \sqrt{\frac{2\ln(2K/\delta')}{N_{\text{explore}}}}.\]</div>
<p>Note that it suffices for <span class="math notranslate nohighlight">\(N_{\text{explore}}\)</span> to be on the order of
<span class="math notranslate nohighlight">\(\sqrt{T}\)</span> to achieve sublinear regret. In particular, we can find the
optimal <span class="math notranslate nohighlight">\(N_{\text{explore}}\)</span> by setting the derivative of the r.h.s. to
zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    0 &amp;= K - T_{\text{exploit}} \cdot \frac{1}{2} \sqrt{\frac{2\ln(2K/\delta')}{N_{\text{explore}}^3}} \\
    N_{\text{explore}}&amp;= \left( T_{\text{exploit}} \cdot \frac{\sqrt{\ln(2K/\delta')/2}}{K} \right)^{2/3}
\end{aligned}
\end{split}\]</div>
<p>Plugging this into the expression for the regret, we
have (still with probability <span class="math notranslate nohighlight">\(1-\delta'\)</span>)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \text{Regret}_T &amp;\le 3 T^{2/3} \sqrt[3]{K \ln(2K/\delta') / 2} \\
    &amp;= \tilde{O}(T^{2/3} K^{1/3}).
\end{aligned}
\end{split}\]</div>
<p>The ETC algorithm is rather “abrupt” in that it switches from
exploration to exploitation after a fixed number of timesteps. In
practice, it’s often better to use a more gradual transition, which
brings us to the <em>epsilon-greedy</em> algorithm.</p>
</section>
</section>
</section>
<section id="epsilon-greedy">
<h2><span class="section-number">2.5. </span>Epsilon-greedy<a class="headerlink" href="#epsilon-greedy" title="Link to this heading">#</a></h2>
<p>Instead of doing all of the exploration and then all of the exploitation
separately – which additionally requires knowing the time horizon
beforehand – we can instead interleave exploration and exploitation by,
at each timestep, choosing a random action with some probability. We
call this the <strong>epsilon-greedy</strong> algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">EpsilonGreedy</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">get_epsilon</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_epsilon</span> <span class="o">=</span> <span class="n">get_epsilon</span>

    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">solutions</span><span class="o">.</span><span class="n">epsilon_greedy_choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">EpsilonGreedy</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="k">lambda</span> <span class="n">t</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/08611d502939ddee47b7dc77f96c805eb7d02ae190c2fd1b79fa139a35d8f67f.png" src="_images/08611d502939ddee47b7dc77f96c805eb7d02ae190c2fd1b79fa139a35d8f67f.png" />
</div>
</div>
<p>Note that we let <span class="math notranslate nohighlight">\(\epsilon\)</span> vary over time. In particular, we might want to gradually <em>decrease</em> <span class="math notranslate nohighlight">\(\epsilon\)</span> as we learn more about the reward distributions and no longer need to spend time exploring.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>What is the expected regret of the algorithm if we set <span class="math notranslate nohighlight">\(\epsilon\)</span> to be a constant?</p>
</div>
<p>It turns out that setting <span class="math notranslate nohighlight">\(\epsilon_t = \sqrt[3]{K \ln(t)/t}\)</span> also achieves a regret of <span class="math notranslate nohighlight">\(\tilde O(t^{2/3} K^{1/3})\)</span> (ignoring the logarithmic factors). (We will not prove this here.) TODO ADD PROOF CITATION</p>
<p>In ETC, we had to set <span class="math notranslate nohighlight">\(N_{\text{explore}}\)</span> based on the total number of timesteps <span class="math notranslate nohighlight">\(T\)</span>. But the epsilon-greedy algorithm actually handles the exploration <em>automatically</em>: the regret rate holds for <em>any</em> <span class="math notranslate nohighlight">\(t\)</span>, and doesn’t depend on the final horizon <span class="math notranslate nohighlight">\(T\)</span>.</p>
<p>But the way these algorithms explore is rather naive: we’ve been exploring <em>uniformly</em> across all the arms. But what if we could be smarter about it, and explore <em>more</em> for arms that we’re less certain about?</p>
</section>
<section id="upper-confidence-bound-ucb">
<span id="ucb"></span><h2><span class="section-number">2.6. </span>Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Link to this heading">#</a></h2>
<p>To quantify how <em>certain</em> we are about the mean of each arm, we’ll
compute <em>confidence intervals</em> for our estimators, and then choose the
arm with the highest <em>upper confidence bound</em>. This operates on the
principle of <strong>the benefit of the doubt (i.e. optimism in the face of
uncertainty)</strong>: we’ll choose the arm that we’re most optimistic about.</p>
<p>In particular, for each arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, we’d like to compute some
upper confidence bound <span class="math notranslate nohighlight">\(M^k_t\)</span> such that <span class="math notranslate nohighlight">\(\hat \mu^k_t \le M^k_t\)</span> with
high probability, and then choose <span class="math notranslate nohighlight">\(a_t := \arg \max_{k \in [K]} M^k_t\)</span>.
But how should we compute <span class="math notranslate nohighlight">\(M^k_t\)</span>?</p>
<p>In <a class="reference internal" href="#etc-regret-analysis"><span class="std std-ref">ETC regret analysis</span></a>, we were able to compute this bound
using Hoeffding’s inequality, which assumes that the number of samples
is <em>fixed</em>. This was the case in ETC (where we pull each arm
<span class="math notranslate nohighlight">\(N_{\text{explore}}\)</span> times), but in UCB, the number of times we pull
each arm depends on the agent’s actions, which in turn depend on the
random rewards and are therefore stochastic. So we <em>can’t</em> use
Hoeffding’s inequality directly.</p>
<p>Instead, we’ll apply the same trick we used in the ETC analysis: we’ll
use the <strong>union bound</strong> to compute a <em>looser</em> bound that holds
<em>uniformly</em> across all timesteps and arms. Let’s introduce some notation
to discuss this.</p>
<p>Let <span class="math notranslate nohighlight">\(N^k_t\)</span> denote the (random) number of times arm <span class="math notranslate nohighlight">\(k\)</span> has been pulled
within the first <span class="math notranslate nohighlight">\(t\)</span> timesteps, and <span class="math notranslate nohighlight">\(\hat \mu^k_t\)</span> denote the sample
average of those pulls. That is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    N^k_t &amp;:= \sum_{\tau=0}^{t-1} \mathbf{1} \{ a_\tau = k \} \\
    \hat \mu^k_t &amp;:= \frac{1}{N^k_t} \sum_{\tau=0}^{t-1} \mathbf{1} \{ a_\tau = k \} r_\tau.
\end{aligned}
\end{split}\]</div>
<p>To achieve the “fixed sample size” assumption, we’ll
need to shift our index from <em>time</em> to <em>number of samples from each
arm</em>. In particular, we’ll define <span class="math notranslate nohighlight">\(\tilde r^k_n\)</span> to be the <span class="math notranslate nohighlight">\(n\)</span>th sample
from arm <span class="math notranslate nohighlight">\(k\)</span>, and <span class="math notranslate nohighlight">\(\tilde \mu^k_n\)</span> to be the sample average of the first
<span class="math notranslate nohighlight">\(n\)</span> samples from arm <span class="math notranslate nohighlight">\(k\)</span>. Then, for a fixed <span class="math notranslate nohighlight">\(n\)</span>, this satisfies the
“fixed sample size” assumption, and we can apply Hoeffding’s inequality
to get a bound on <span class="math notranslate nohighlight">\(\tilde \mu^k_n\)</span>.</p>
<p>So how can we extend our bound on <span class="math notranslate nohighlight">\(\tilde\mu^k_n\)</span> to <span class="math notranslate nohighlight">\(\hat \mu^k_t\)</span>?
Well, we know <span class="math notranslate nohighlight">\(N^k_t \le t\)</span> (where equality would be the case if and
only if we had pulled arm <span class="math notranslate nohighlight">\(k\)</span> every time). So we can apply the same
trick as last time, where we uniform-ize across all possible values of
<span class="math notranslate nohighlight">\(N^k_t\)</span>:</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    \P\left( \forall n \le t, |\tilde \mu^k_n - \mu^k | \le \sqrt{\frac{\ln(2/\delta)}{2n}} \right) &amp;\ge 1-t\delta.
\end{aligned}
\]</div>
<p>In particular, since <span class="math notranslate nohighlight">\(N^k_t \le t\)</span>, and <span class="math notranslate nohighlight">\(\tilde \mu^k_{N^k_t} = \hat \mu^k_t\)</span> by definition, we have</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    \P\left( |\hat \mu^k_t - \mu^k | \le \sqrt{\frac{\ln(2t/\delta')}{2N^k_t}} \right) &amp;\ge 1-\delta' \text{ where } \delta' := t \delta.
\end{aligned}
\]</div>
<p>This bound would then suffice for applying the UCB algorithm! That is, the upper confidence bound for arm <span class="math notranslate nohighlight">\(k\)</span> would be</p>
<div class="math notranslate nohighlight">
\[M^k_t := \hat \mu^k_t + \sqrt{\frac{\ln(2t/\delta')}{2N^k_t}},\]</div>
<p>where we can choose <span class="math notranslate nohighlight">\(\delta'\)</span> depending on how tight we want the interval to be. A smaller <span class="math notranslate nohighlight">\(\delta'\)</span> would give us a larger and higher-confidence interval, and vice versa. We can now use this to define the UCB algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">UCB</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">delta</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">delta</span> <span class="o">=</span> <span class="n">delta</span>

    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">solutions</span><span class="o">.</span><span class="n">ucb_choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Intuitively, UCB prioritizes arms where:</p>
<ol class="arabic simple">
<li><p><span class="math notranslate nohighlight">\(\hat \mu^k_t\)</span> is large, i.e. the arm has a high sample average, and
we’d choose it for <em>exploitation</em>, and</p></li>
<li><p><span class="math notranslate nohighlight">\(\sqrt{\frac{\ln(2t/\delta')}{2N^k_t}}\)</span> is large, i.e. we’re still
uncertain about the arm, and we’d choose it for <em>exploration</em>.</p></li>
</ol>
<p>As desired, this explores in a smarter, <em>adaptive</em> way compared to the
previous algorithms. Does it achieve lower regret?</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span> <span class="o">=</span> <span class="n">UCB</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/4fd1308a04944fd0b6525506ce0e14fa7692941199ade9f5d1196cedb64eba83.png" src="_images/4fd1308a04944fd0b6525506ce0e14fa7692941199ade9f5d1196cedb64eba83.png" />
</div>
</div>
<section id="ucb-regret-analysis">
<h3><span class="section-number">2.6.1. </span>UCB regret analysis<a class="headerlink" href="#ucb-regret-analysis" title="Link to this heading">#</a></h3>
<p>First we’ll bound the regret incurred at each timestep. Then we’ll bound
the <em>total</em> regret across timesteps.</p>
<p>For the sake of analysis, we’ll use a slightly looser bound that applies
across the whole time horizon and across all arms. We’ll omit the
derivation since it’s very similar to the above (walk through it
yourself for practice).</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \P\left(\forall k \le K, t &lt; T. |\hat \mu^k_t - \mu^k | \le B^k_t \right) &amp;\ge 1-\delta'' \\
    \text{where} \quad B^k_t &amp;:= \sqrt{\frac{\ln(2TK/\delta'')}{2N^k_t}}.
\end{aligned}
\end{split}\]</div>
<p>Intuitively, <span class="math notranslate nohighlight">\(B^k_t\)</span> denotes the <em>width</em> of the CI for arm <span class="math notranslate nohighlight">\(k\)</span> at time
<span class="math notranslate nohighlight">\(t\)</span>. Then, assuming the above uniform bound holds (which occurs with
probability <span class="math notranslate nohighlight">\(1-\delta''\)</span>), we can bound the regret at each timestep as
follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mu^\star - \mu^{a_t} &amp;\le \hat \mu^{k^*}_t + B_t^{k^*} - \mu^{a_t} &amp;&amp; \text{applying UCB to arm } k^\star \\
    &amp;\le \hat \mu^{a_t}_t + B^{a_t}_t - \mu^{a_t} &amp;&amp; \text{since UCB chooses } a_t = \arg \max_{k \in [K]} \hat \mu^k_t + B_t^{k} \\
    &amp;\le 2 B^{a_t}_t &amp;&amp; \text{since } \hat \mu^{a_t}_t - \mu^{a_t} \le B^{a_t}_t \text{ by definition of } B^{a_t}_t \\
\end{aligned}
\end{split}\]</div>
<p>Summing this across timesteps gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \text{Regret}_T &amp;\le \sum_{t=0}^{T-1} 2 B^{a_t}_t \\
    &amp;= \sqrt{2\ln(2TK/\delta'')} \sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} \\
    \sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} &amp;= \sum_{t=0}^{T-1} \sum_{k=1}^K \mathbf{1}\{ a_t = k \} (N^k_t)^{-1/2} \\
    &amp;= \sum_{k=1}^K \sum_{n=1}^{N_T^k} n^{-1/2} \\
    &amp;\le K \sum_{n=1}^T n^{-1/2} \\
    \sum_{n=1}^T n^{-1/2} &amp;\le 1 + \int_1^T x^{-1/2} \ \mathrm{d}x \\
    &amp;= 1 + (2 \sqrt{x})_1^T \\
    &amp;= 2 \sqrt{T} - 1 \\
    &amp;\le 2 \sqrt{T} \\
\end{aligned}
\end{split}\]</div>
<p>Putting everything together gives</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \text{Regret}_T &amp;\le 2 K \sqrt{2T \ln(2TK/\delta'')} &amp;&amp; \text{with probability } 1-\delta'' \\
    &amp;= \tilde O(K\sqrt{T})
\end{aligned}
\end{split}\]</div>
<p>In fact, we can do a more sophisticated analysis to trim off a factor of
<span class="math notranslate nohighlight">\(\sqrt{K}\)</span> and show <span class="math notranslate nohighlight">\(\text{Regret}_T = \tilde O(\sqrt{TK})\)</span>.</p>
</section>
<section id="lower-bound-on-regret-intuition">
<h3><span class="section-number">2.6.2. </span>Lower bound on regret (intuition)<a class="headerlink" href="#lower-bound-on-regret-intuition" title="Link to this heading">#</a></h3>
<p>Is it possible to do better than <span class="math notranslate nohighlight">\(\Omega(\sqrt{T})\)</span> in general? In fact,
no! We can show that any algorithm must incur <span class="math notranslate nohighlight">\(\Omega(\sqrt{T})\)</span> regret
in the worst case. We won’t rigorously prove this here, but the
intuition is as follows.</p>
<p>The Central Limit Theorem tells us that with <span class="math notranslate nohighlight">\(T\)</span> i.i.d. samples from
some distribution, we can only learn the mean of the distribution to
within <span class="math notranslate nohighlight">\(\Omega(1/\sqrt{T})\)</span> (the standard deviation). Then, since we get
<span class="math notranslate nohighlight">\(T\)</span> samples spread out across the arms, we can only learn each arm’s
mean to an even looser degree.</p>
<p>That is, if two arms have means that are within about <span class="math notranslate nohighlight">\(1/\sqrt{T}\)</span>, we
won’t be able to confidently tell them apart, and will sample them about
equally. But then we’ll incur regret
$<span class="math notranslate nohighlight">\(\Omega((T/2) \cdot (1/\sqrt{T})) = \Omega(\sqrt{T}).\)</span>$</p>
</section>
</section>
<section id="thompson-sampling-and-bayesian-bandits">
<span id="thompson-sampling"></span><h2><span class="section-number">2.7. </span>Thompson sampling and Bayesian bandits<a class="headerlink" href="#thompson-sampling-and-bayesian-bandits" title="Link to this heading">#</a></h2>
<p>So far, we’ve treated the parameters <span class="math notranslate nohighlight">\(\mu^0, \dots, \mu^{K-1}\)</span> of the
reward distributions as <em>fixed</em>. Instead, we can take a <strong>Bayesian</strong>
approach where we treat them as random variables from some <strong>prior
distribution</strong>. Then, upon pulling an arm and observing a reward, we can
simply <em>condition</em> on this observation to exactly describe the
<strong>posterior distribution</strong> over the parameters. This fully describes the
information we gain about the parameters from observing the reward.</p>
<p>From this Bayesian perspective, the <strong>Thompson sampling</strong> algorithm
follows naturally: just sample from the distribution of the optimal arm,
given the observations!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Distribution</span><span class="p">(</span><span class="n">ABC</span><span class="p">):</span>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; K&quot;</span><span class="p">]:</span> <span class="o">...</span>

    <span class="nd">@abstractmethod</span>
    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span> <span class="o">...</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">ThompsonSampling</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">prior</span><span class="p">:</span> <span class="n">Distribution</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span> <span class="o">=</span> <span class="n">prior</span>

    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">means</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">random_argmax</span><span class="p">(</span><span class="n">means</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">update_history</span><span class="p">(</span><span class="n">arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">distribution</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">arm</span><span class="p">,</span> <span class="n">reward</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In other words, we sample each arm proportionally to how likely we think
it is to be optimal, given the observations so far. This strikes a good
exploration-exploitation tradeoff: we explore more for arms that we’re
less certain about, and exploit more for arms that we’re more certain
about. Thompson sampling is a simple yet powerful algorithm that
achieves state-of-the-art performance in many settings.</p>
<div class="proof example admonition" id="bayesian_bernoulli">
<p class="admonition-title"><span class="caption-number">Example 2.3 </span> (Bayesian Bernoulli bandit)</p>
<section class="example-content" id="proof-content">
<p>We’ve been working in the Bernoulli bandit setting, where arm <span class="math notranslate nohighlight">\(k\)</span> yields a reward of <span class="math notranslate nohighlight">\(1\)</span> with probability <span class="math notranslate nohighlight">\(\mu^k\)</span> and no reward otherwise. The vector of success probabilities <span class="math notranslate nohighlight">\(\boldsymbol{\mu} = (\mu^1, \dots, \mu^K)\)</span> thus describes the entire MAB.</p>
<p>Under the Bayesian perspective, we think of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> as a <em>random</em> vector drawn from some prior distribution <span class="math notranslate nohighlight">\(\pi(\boldsymbol{\mu})\)</span>. For example, we might have <span class="math notranslate nohighlight">\(\pi\)</span> be the Uniform distribution over the unit hypercube <span class="math notranslate nohighlight">\([0, 1]^K\)</span>, that is,</p>
<div class="math notranslate nohighlight">
\[\begin{split}\pi(\boldsymbol{\mu}) = \begin{cases}
    1 &amp; \text{if } \boldsymbol{\mu}\in [0, 1]^K \\
    0 &amp; \text{otherwise}
\end{cases}\end{split}\]</div>
<p>In this case, upon viewing some reward, we can exactly calculate the <strong>posterior</strong> distribution of <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> using Bayes’s rule (i.e. the definition of conditional probability):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \P(\boldsymbol{\mu} \mid a_0, r_0) &amp;\propto \P(r_0 \mid a_0, \boldsymbol{\mu}) \P(a_0 \mid \boldsymbol{\mu}) \P(\boldsymbol{\mu}) \\
    &amp;\propto (\mu^{a_0})^{r_0} (1 - \mu^{a_0})^{1-r_0}.
\end{aligned}
\end{split}\]</div>
<p>This is the PDF of the
<span class="math notranslate nohighlight">\(\text{Beta}(1 + r_0, 1 + (1 - r_0))\)</span> distribution, which is a conjugate
prior for the Bernoulli distribution. That is, if we start with a Beta
prior on <span class="math notranslate nohighlight">\(\mu^k\)</span> (note that <span class="math notranslate nohighlight">\(\text{Unif}([0, 1]) = \text{Beta}(1, 1)\)</span>),
then the posterior, after conditioning on samples from
<span class="math notranslate nohighlight">\(\text{Bern}(\mu^k)\)</span>, will also be Beta. This is a very convenient
property, since it means we can simply update the parameters of the Beta
distribution upon observing a reward, rather than having to recompute
the entire posterior distribution from scratch.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Beta</span><span class="p">(</span><span class="n">Distribution</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">beta</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">alpha</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">betas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">sample</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">arm</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alphas</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">betas</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reward</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">beta_distribution</span> <span class="o">=</span> <span class="n">Beta</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">)</span>
<span class="n">agent</span> <span class="o">=</span> <span class="n">ThompsonSampling</span><span class="p">(</span><span class="n">mab</span><span class="o">.</span><span class="n">K</span><span class="p">,</span> <span class="n">mab</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">beta_distribution</span><span class="p">)</span>
<span class="n">mab_loop</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
<span class="n">plot_strategy</span><span class="p">(</span><span class="n">mab</span><span class="p">,</span> <span class="n">agent</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/08771a16f85ed1bc7e00735e31aa7db1281614db46c5398717b90065f831ab0e.png" src="_images/08771a16f85ed1bc7e00735e31aa7db1281614db46c5398717b90065f831ab0e.png" />
</div>
</div>
<p>It turns out that asymptotically, Thompson sampling is optimal in the
following sense. <span id="id3">[<a class="reference internal" href="bibliography.html#id10" title="T. L Lai and Herbert Robbins. Asymptotically efficient adaptive allocation rules. Advances in Applied Mathematics, 6(1):4–22, March 1985. doi:10.1016/0196-8858(85)90002-8.">Lai and Robbins, 1985</a>]</span> prove an
<em>instance-dependent</em> lower bound that says for <em>any</em> bandit algorithm,</p>
<div class="math notranslate nohighlight">
\[\liminf_{T \to \infty} \frac{\E[N_T^k]}{\ln(T)} \ge \frac{1}{\text{KL}(\mu^k \parallel \mu^\star)}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\text{KL}(\mu^k \parallel \mu^\star) = \mu^k \ln \frac{\mu^k}{\mu^\star} + (1 - \mu^k) \ln \frac{1 - \mu^k}{1 - \mu^\star}\]</div>
<p>measures the <strong>Kullback-Leibler divergence</strong> from the Bernoulli
distribution with mean <span class="math notranslate nohighlight">\(\mu^k\)</span> to the Bernoulli distribution with mean
<span class="math notranslate nohighlight">\(\mu^\star\)</span>. It turns out that Thompson sampling achieves this lower
bound with equality! That is, not only is the error <em>rate</em> optimal, but
the <em>constant factor</em> is optimal as well.</p>
</section>
<section id="contextual-bandits">
<h2><span class="section-number">2.8. </span>Contextual bandits<a class="headerlink" href="#contextual-bandits" title="Link to this heading">#</a></h2>
<p>In the above MAB environment, the reward distributions of the arms
remain constant. However, in many real-world settings, we might receive
additional information that affects these distributions. For example, in
the online advertising case where each arm corresponds to an ad we could
show the user, we might receive information about the user’s preferences
that changes how likely they are to click on a given ad. We can model
such environments using <strong>contextual bandits</strong>.</p>
<div class="proof definition admonition" id="contextual_bandit">
<p class="admonition-title"><span class="caption-number">Definition 2.2 </span> (Contextual bandit)</p>
<section class="definition-content" id="proof-content">
<p>At each timestep <span class="math notranslate nohighlight">\(t\)</span>, a new <em>context</em>
<span class="math notranslate nohighlight">\(x_t\)</span> is drawn from some distribution <span class="math notranslate nohighlight">\(\nu_{\text{x}}\)</span>. The learner gets
to observe the context, and choose an action <span class="math notranslate nohighlight">\(a_t\)</span> according to some
context-dependent policy <span class="math notranslate nohighlight">\(\pi_t(x_t)\)</span>. Then, the learner observes the
reward from the chosen arm <span class="math notranslate nohighlight">\(r_t \sim \nu^{a_t}(x_t)\)</span>. The reward
distribution also depends on the context.</p>
</section>
</div><p>Assuming our context is <em>discrete</em>, we can just perform the same
algorithms, treating each context-arm pair as its own arm. This gives us
an enlarged MAB of <span class="math notranslate nohighlight">\(K |\mathcal{X}|\)</span> arms.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Write down the UCB algorithm for this enlarged MAB. That is, write an
expression for <span class="math notranslate nohighlight">\(\pi_t(x_t) = \argmax_a \dots\)</span>.</p>
</div>
<p>Recall that running UCB for <span class="math notranslate nohighlight">\(T\)</span> timesteps on an MAB with <span class="math notranslate nohighlight">\(K\)</span> arms
achieves a regret bound of <span class="math notranslate nohighlight">\(\tilde{O}(\sqrt{TK})\)</span>. So in this problem,
we would achieve regret <span class="math notranslate nohighlight">\(\tilde{O}(\sqrt{TK|\mathcal{X}|})\)</span> in the
contextual MAB, which has a polynomial dependence on <span class="math notranslate nohighlight">\(|\mathcal{X}|\)</span>.
But in a situation where we have large, or even infinitely many
contexts, e.g. in the case where our context is a continuous value, this
becomes intractable.</p>
<p>Note that this “enlarged MAB” treats the different contexts as entirely
unrelated to each other, while in practice, often contexts are <em>related</em>
to each other in some way: for example, we might want to advertise
similar products to users with similar preferences. How can we
incorporate this structure into our solution?</p>
<section id="linear-contextual-bandits">
<span id="lin-ucb"></span><h3><span class="section-number">2.8.1. </span>Linear contextual bandits<a class="headerlink" href="#linear-contextual-bandits" title="Link to this heading">#</a></h3>
<p>We want to model the <em>mean reward</em> of arm <span class="math notranslate nohighlight">\(k\)</span> as a function of the
context, i.e. <span class="math notranslate nohighlight">\(\mu^k(x)\)</span>. One simple model is the <em>linear</em> one:
<span class="math notranslate nohighlight">\(\mu^k(x) = x^\top \theta^k\)</span>, where <span class="math notranslate nohighlight">\(x \in \mathcal{X} = \mathbb{R}^d\)</span> and
<span class="math notranslate nohighlight">\(\theta^k \in \mathbb{R}^d\)</span> describes a <em>feature direction</em> for arm <span class="math notranslate nohighlight">\(k\)</span>. Recall
that <strong>supervised learning</strong> gives us a way to estimate a conditional
expectation from samples: We learn a <em>least squares</em> estimator from the
timesteps where arm <span class="math notranslate nohighlight">\(k\)</span> was selected:
$<span class="math notranslate nohighlight">\(\hat \theta_t^k = \argmin_{\theta \in \mathbb{R}^d} \sum_{\{ i \in [t] : a_i = k \}} (r_i - x_i^\top \theta)^2.\)</span>$
This has the closed-form solution known as the <em>ordinary least squares</em>
(OLS) estimator:</p>
<div class="math notranslate nohighlight" id="equation-ols-bandit">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-ols-bandit" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    \hat \theta_t^k          &amp; = (A_t^k)^{-1} \sum_{\{ i \in [t] : a_i = k \}} x_i r_i \\
    \text{where} \quad A_t^k &amp; = \sum_{\{ i \in [t] : a_i = k \}} x_i x_i^\top.
\end{aligned}\end{split}\]</div>
<p>We can now apply the UCB algorithm in this environment in order to
balance <em>exploration</em> of new arms and <em>exploitation</em> of arms that we
believe to have high reward. But how should we construct the upper
confidence bound? Previously, we treated the pulls of an arm as i.i.d.
samples and used Hoeffding’s inequality to bound the distance of the
sample mean, our estimator, from the true mean. However, now our
estimator is not a sample mean, but rather the OLS estimator above <a class="reference internal" href="#equation-ols-bandit">(2.2)</a>. Instead, we’ll use <strong>Chebyshev’s
inequality</strong> to construct an upper confidence bound.</p>
<div class="proof theorem admonition" id="chebyshev">
<p class="admonition-title"><span class="caption-number">Theorem 2.3 </span> (Chebyshev’s inequality)</p>
<section class="theorem-content" id="proof-content">
<p>For a random variable <span class="math notranslate nohighlight">\(Y\)</span> such that
<span class="math notranslate nohighlight">\(\E Y = 0\)</span> and <span class="math notranslate nohighlight">\(\E Y^2 = \sigma^2\)</span>,
$<span class="math notranslate nohighlight">\(|Y| \le \beta \sigma \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}\)</span>$</p>
</section>
</div><p>Since the OLS estimator is known to be unbiased (try proving this
yourself), we can apply Chebyshev’s inequality to
<span class="math notranslate nohighlight">\(x_t^\top (\hat \theta_t^k - \theta^k)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{aligned}
    x_t^\top \theta^k \le x_t^\top \hat \theta_t^k + \beta \sqrt{x_t^\top (A_t^k)^{-1} x_t} \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\end{aligned}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>We haven’t explained why <span class="math notranslate nohighlight">\(x_t^\top (A_t^k)^{-1} x_t\)</span> is the correct
expression for the variance of <span class="math notranslate nohighlight">\(x_t^\top \hat \theta_t^k\)</span>. This result
follows from some algebra on the definition of the OLS estimator <a class="reference internal" href="#equation-ols-bandit">(2.2)</a>.</p>
</div>
<p>The first term is exactly our predicted reward <span class="math notranslate nohighlight">\(\hat \mu^k_t(x_t)\)</span>. To
interpret the second term, note that
$<span class="math notranslate nohighlight">\(x_t^\top (A_t^k)^{-1} x_t = \frac{1}{N_t^k} x_t^\top (\Sigma_t^k)^{-1} x_t,\)</span><span class="math notranslate nohighlight">\(
where
\)</span><span class="math notranslate nohighlight">\(\Sigma_t^k = \frac{1}{N_t^k} \sum_{\{ i \in [t] : a_i = k \}} x_i x_i^\top\)</span><span class="math notranslate nohighlight">\(
is the empirical covariance matrix of the contexts (assuming that the
context has mean zero). That is, the learner is encouraged to choose
arms when \)</span>x_t<span class="math notranslate nohighlight">\( is *not aligned* with the data seen so far, or if arm
\)</span>k<span class="math notranslate nohighlight">\( has not been explored much and so \)</span>N_t^k$ is small.</p>
<p>We can now substitute these quantities into UCB to get the <strong>LinUCB</strong>
algorithm:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LinUCBPseudocode</span><span class="p">(</span><span class="n">Agent</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">K</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">T</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">D</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">lam</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">get_c</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">int</span><span class="p">],</span> <span class="nb">float</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lam</span> <span class="o">=</span> <span class="n">lam</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">get_c</span> <span class="o">=</span> <span class="n">get_c</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">contexts</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">K</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">lam</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">D</span><span class="p">)[</span><span class="o">...</span><span class="p">],</span> <span class="n">K</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">D</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">choose_arm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; D&quot;</span><span class="p">]):</span>
        <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_c</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">count</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">@</span> <span class="n">context</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
            <span class="n">context</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">random_argmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">update_history</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">context</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot; D&quot;</span><span class="p">],</span> <span class="n">arm</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">reward</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">outer</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">context</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">context</span> <span class="o">*</span> <span class="n">reward</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="n">arm</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">targets</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note that the matrix <span class="math notranslate nohighlight">\(A_t^k\)</span> above might not be invertible. When does this occur? One way to address this is to include a <span class="math notranslate nohighlight">\(\lambda I\)</span> regularization term to ensure that <span class="math notranslate nohighlight">\(A_t^k\)</span> is invertible. This is equivalent to solving a <em>ridge regression</em> problem instead of the unregularized least squares problem. Implement this solution. TODO SOLUTION CURRENTLY SHOWN</p>
</div>
<p><span class="math notranslate nohighlight">\(c_t\)</span> is similar to the <span class="math notranslate nohighlight">\(\log (2t/\delta')\)</span> term of UCB: It controls the
width of the confidence interval. Here, we treat it as a tunable
parameter, though in a theoretical analysis, it would depend on <span class="math notranslate nohighlight">\(A_t^k\)</span>
and the probability <span class="math notranslate nohighlight">\(\delta\)</span> with which the bound holds.</p>
<p>Using similar tools for UCB, we can also prove an <span class="math notranslate nohighlight">\(\tilde{O}(\sqrt{T})\)</span>
regret bound. The full details of the analysis can be found in Section 3 of <span id="id4">[<a class="reference internal" href="bibliography.html#id3">Agarwal <em>et al.</em>, 2022</a>]</span>.</p>
</section>
</section>
<section id="summary">
<h2><span class="section-number">2.9. </span>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Introduction</p>
      </div>
    </a>
    <a class="right-next"
       href="mdps.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Finite Markov Decision Processes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">2.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pure-exploration-random-guessing">2.2. Pure exploration (random guessing)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pure-greedy">2.3. Pure greedy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explore-then-commit">2.4. Explore-then-commit</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#etc-regret-analysis">2.4.1. ETC regret analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-phase">2.4.1.1. Exploration phase.</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#exploitation-phase">2.4.1.2. Exploitation phase.</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">2.5. Epsilon-greedy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">2.6. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ucb-regret-analysis">2.6.1. UCB regret analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lower-bound-on-regret-intuition">2.6.2. Lower bound on regret (intuition)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling-and-bayesian-bandits">2.7. Thompson sampling and Bayesian bandits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#contextual-bandits">2.8. Contextual bandits</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-contextual-bandits">2.8.1. Linear contextual bandits</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">2.9. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>