<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Multi-Armed Bandits – rlbook</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./supervised_learning.html" rel="next">
<link href="./control.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./bandits.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">rlbook</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">3.1</span> Introduction</a></li>
  <li><a href="#sec-pure-exploration" id="toc-sec-pure-exploration" class="nav-link" data-scroll-target="#sec-pure-exploration"><span class="header-section-number">3.2</span> Pure exploration</a></li>
  <li><a href="#sec-pure-greedy" id="toc-sec-pure-greedy" class="nav-link" data-scroll-target="#sec-pure-greedy"><span class="header-section-number">3.3</span> Pure greedy</a></li>
  <li><a href="#sec-etc" id="toc-sec-etc" class="nav-link" data-scroll-target="#sec-etc"><span class="header-section-number">3.4</span> Explore-then-commit</a>
  <ul class="collapse">
  <li><a href="#sec-etc-regret-analysis" id="toc-sec-etc-regret-analysis" class="nav-link" data-scroll-target="#sec-etc-regret-analysis"><span class="header-section-number">3.4.1</span> ETC regret analysis</a></li>
  </ul></li>
  <li><a href="#epsilon-greedy" id="toc-epsilon-greedy" class="nav-link" data-scroll-target="#epsilon-greedy"><span class="header-section-number">3.5</span> Epsilon-greedy</a></li>
  <li><a href="#sec-ucb" id="toc-sec-ucb" class="nav-link" data-scroll-target="#sec-ucb"><span class="header-section-number">3.6</span> Upper Confidence Bound (UCB)</a>
  <ul class="collapse">
  <li><a href="#ucb-regret-analysis" id="toc-ucb-regret-analysis" class="nav-link" data-scroll-target="#ucb-regret-analysis"><span class="header-section-number">3.6.1</span> UCB regret analysis</a></li>
  <li><a href="#lower-bound-on-regret-intuition" id="toc-lower-bound-on-regret-intuition" class="nav-link" data-scroll-target="#lower-bound-on-regret-intuition"><span class="header-section-number">3.6.2</span> Lower bound on regret (intuition)</a></li>
  </ul></li>
  <li><a href="#sec-thompson-sampling" id="toc-sec-thompson-sampling" class="nav-link" data-scroll-target="#sec-thompson-sampling"><span class="header-section-number">3.7</span> Thompson sampling and Bayesian bandits</a></li>
  <li><a href="#contextual-bandits" id="toc-contextual-bandits" class="nav-link" data-scroll-target="#contextual-bandits"><span class="header-section-number">3.8</span> Contextual bandits</a>
  <ul class="collapse">
  <li><a href="#sec-lin-ucb" id="toc-sec-lin-ucb" class="nav-link" data-scroll-target="#sec-lin-ucb"><span class="header-section-number">3.8.1</span> Linear contextual bandits</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">3.9</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-bandits" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<hr>
<hr>
<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">3.1</span> Introduction</h2>
<p>The <strong>multi-armed bandits</strong> (MAB) setting is a simple setting for studying the basic challenges of sequential decision-making. In this setting, an agent repeatedly chooses from a fixed set of actions, called <strong>arms</strong>, each of which has an associated reward distribution. The agent’s goal is to maximize the total reward it receives over some time period.</p>
<!-- 
| States | Actions | Rewards                             |
| :----: | :-----: | :---------------------------------: |
| None   | Finite  | $\mathcal{A} \to \triangle([0, 1])$ |
-->
<p>In particular, we’ll spend a lot of time discussing the <strong>Exploration-Exploitation Tradeoff</strong>: should the agent choose new actions to learn more about the environment, or should it choose actions that it already knows to be good?</p>
<div id="exm-advertising" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 (Online advertising)</strong></span> Let’s suppose you, the agent, are an advertising company. You have <span class="math inline">\(K\)</span> different ads that you can show to users; For concreteness, let’s suppose there’s just a single user. You receive <span class="math inline">\(1\)</span> reward if the user clicks the ad, and <span class="math inline">\(0\)</span> otherwise. Thus, the unknown <em>reward distribution</em> associated to each ad is a Bernoulli distribution defined by the probability that the user clicks on the ad. Your goal is to maximize the total number of clicks by the user.</p>
</div>
<div id="exm-clinical-trials" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 (Clinical trials)</strong></span> Suppose you’re a pharmaceutical company, and you’re testing a new drug. You have <span class="math inline">\(K\)</span> different dosages of the drug that you can administer to patients. You receive <span class="math inline">\(1\)</span> reward if the patient recovers, and <span class="math inline">\(0\)</span> otherwise. Thus, the unknown <em>reward distribution</em> associated to each dosage is a Bernoulli distribution defined by the probability that the patient recovers. Your goal is to maximize the total number of patients that recover.</p>
</div>
<p>In this chapter, we will introduce the multi-armed bandits setting, and discuss some of the challenges that arise when trying to solve problems in this setting. We will also introduce some of the key concepts that we will use throughout the book, such as regret and exploration-exploitation tradeoffs.</p>
<div id="3db793b4" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> jaxtyping <span class="im">import</span> Float, Array</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> latexify</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> Callable, Union</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> solutions.bandits <span class="im">as</span> solutions</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>np.random.seed(<span class="dv">184</span>)</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> random_argmax(ary: Array) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Take an argmax and randomize between ties."""</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    max_idx <span class="op">=</span> np.flatnonzero(ary <span class="op">==</span> ary.<span class="bu">max</span>())</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.random.choice(max_idx).item()</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># used as decorator</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>latex <span class="op">=</span> latexify.algorithmic(</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    trim_prefixes<span class="op">=</span>{<span class="st">"mab"</span>},</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    id_to_latex<span class="op">=</span>{<span class="st">"arm"</span>: <span class="st">"a_t"</span>, <span class="st">"reward"</span>: <span class="st">"r"</span>, <span class="st">"means"</span>: <span class="st">"\mu"</span>},</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    use_math_symbols<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="rem-multi-armed" class="proof remark">
<p><span class="proof-title"><em>Remark 3.1</em> (Namesake). </span>The name “multi-armed bandits” comes from slot machines in casinos, which are often called “one-armed bandits” since they have one arm (the lever) and rob money from the player.</p>
</div>
<p>Let <span class="math inline">\(K\)</span> denote the number of arms. We’ll label them <span class="math inline">\(0, \dots, K-1\)</span> and use <em>superscripts</em> to indicate the arm index; since we seldom need to raise a number to a power, this won’t cause much confusion. In this chapter, we’ll consider the <strong>Bernoulli bandit</strong> setting from the examples above, where arm <span class="math inline">\(k\)</span> either returns reward <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\mu^k\)</span> or <span class="math inline">\(0\)</span> otherwise. The agent gets to pull an arm <span class="math inline">\(T\)</span> times in total. We can formalize the Bernoulli bandit in the following Python code:</p>
<div id="26537493" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> MAB:</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The Bernoulli multi-armed bandit environment.</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    :param means: the means (success probabilities) of the reward distributions for each arm</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    :param T: the time horizon</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, means: Float[Array, <span class="st">" K"</span>], T: <span class="bu">int</span>):</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">assert</span> <span class="bu">all</span>(<span class="dv">0</span> <span class="op">&lt;=</span> p <span class="op">&lt;=</span> <span class="dv">1</span> <span class="cf">for</span> p <span class="kw">in</span> means)</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.means <span class="op">=</span> means</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> <span class="va">self</span>.means.size</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_arm <span class="op">=</span> random_argmax(<span class="va">self</span>.means)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> pull(<span class="va">self</span>, k: <span class="bu">int</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Pull the `k`-th arm and sample from its (Bernoulli) reward distribution."""</span></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> np.random.rand() <span class="op">&lt;</span> <span class="va">self</span>.means[k].item()</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="op">+</span>reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="c1c8f499" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>mab <span class="op">=</span> MAB(means<span class="op">=</span>np.array([<span class="fl">0.1</span>, <span class="fl">0.8</span>, <span class="fl">0.4</span>]), T<span class="op">=</span><span class="dv">100</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In pseudocode, the agent’s interaction with the MAB environment can be described by the following process:</p>
<div id="b9b916e7" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="at">@latex</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mab_loop(mab: MAB, agent: <span class="st">"Agent"</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(mab.T):</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>        arm <span class="op">=</span> agent.choose_arm()  <span class="co"># in 0, ..., K-1</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        reward <span class="op">=</span> mab.pull(arm)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        agent.update_history(arm, reward)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>mab_loop</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display cell-output-markdown" data-execution_count="4">
$
<span class="math display">\[\begin{array}{l} \mathbf{function} \ \mathrm{mab\_loop}(\mathrm{mab}: \mathrm{MAB}, \mathrm{agent}: \textrm{"Agent"}) \\ \hspace{1em} \mathbf{for} \ t \in \mathrm{range} \mathopen{}\left( T \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} a_t \gets \mathrm{agent}.\mathrm{choose\_arm} \mathopen{}\left( \mathclose{}\right) \\ \hspace{2em} r \gets \mathrm{pull} \mathopen{}\left( a_t \mathclose{}\right) \\ \hspace{2em} \mathrm{agent}.\mathrm{update\_history} \mathopen{}\left( a_t, r \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \mathbf{end \ function} \end{array}\]</span>
<p>$</p>
</div>
</div>
<p>The <code>Agent</code> class stores the pull history and uses it to decide which arm to pull next. Since we are working with Bernoulli bandits, we can summarize the pull history concisely in a <span class="math inline">\(\mathbb{N}^{K \times 2}\)</span> array.</p>
<div id="e9c57a9d" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Agent:</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>):</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The MAB agent that decides how to choose an arm given the past history."""</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.K <span class="op">=</span> K</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.T <span class="op">=</span> T</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards <span class="op">=</span> []  <span class="co"># for plotting</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.choices <span class="op">=</span> []</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.history <span class="op">=</span> np.zeros((K, <span class="dv">2</span>), dtype<span class="op">=</span><span class="bu">int</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose an arm of the MAB. Algorithm-specific."""</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> count(<span class="va">self</span>) <span class="op">-&gt;</span> <span class="bu">int</span>:</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""The number of pulls made. Also the current step index."""</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.rewards)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards.append(reward)</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.choices.append(arm)</span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.history[arm, reward] <span class="op">+=</span> <span class="dv">1</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>What’s the <em>optimal</em> strategy for the agent, i.e.&nbsp;the one that achieves the highest expected reward? Convince yourself that the agent should try to always pull the arm with the highest expected reward:</p>
<p><span class="math display">\[
\mu^\star := \max_{k \in [K]} \mu^k.
\]</span></p>
<p>The goal, then, can be rephrased as to minimize the <strong>regret</strong>, defined below:</p>
<div id="def-regret" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 (Regret)</strong></span> The agent’s <strong>regret</strong> after <span class="math inline">\(T\)</span> timesteps is defined as</p>
<p><span class="math display">\[
\text{Regret}_T := \sum_{t=0}^{T-1} \mu^\star - \mu^{a_t}.
\]</span></p>
</div>
<div id="5b0cb7ed" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> regret_per_step(mab: MAB, agent: Agent):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Get the difference from the average reward of the optimal arm. The sum of these is the regret."""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> [mab.means[mab.best_arm] <span class="op">-</span> mab.means[arm] <span class="cf">for</span> arm <span class="kw">in</span> agent.choices]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Note that this depends on the <em>true means</em> of the pulled arms, <em>not</em> the actual observed rewards. We typically think of this as a random variable where the randomness comes from the agent’s strategy (i.e.&nbsp;the sequence of actions <span class="math inline">\(a_0, \dots, a_{T-1}\)</span>).</p>
<p>Throughout the chapter, we will try to upper bound the regret of various algorithms in two different senses:</p>
<ol type="1">
<li><p>Upper bound the <em>expected regret,</em> i.e.&nbsp;show <span class="math inline">\(\mathbb{E}[\text{Regret}_T] \le M_T\)</span>.</p></li>
<li><p>Find a <em>high-probability</em> upper bound on the regret, i.e.&nbsp;show <span class="math inline">\(\mathbb{P}(\text{Regret}_T \le M_{T, \delta}) \ge 1-\delta\)</span>.</p></li>
</ol>
<p>Note that these two different approaches say very different things about the regret. The first approach says that the <em>average</em> regret is at most <span class="math inline">\(M_T\)</span>. However, the agent might still achieve higher regret on many runs. The second approach says that, <em>with high probability</em>, the agent will achieve regret at most <span class="math inline">\(M_{T, \delta}\)</span>. However, it doesn’t say anything about the regret in the remaining <span class="math inline">\(\delta\)</span> fraction of runs, which might be arbitrarily high.</p>
<p>We’d like to achieve <strong>sublinear regret</strong> in expectation, i.e.&nbsp;<span class="math inline">\(\mathbb{E}[\text{Regret}_T] = o(T)\)</span>. That is, as we learn more about the environment, we’d like to be able to exploit that knowledge to take the optimal arm as often as possible.</p>
<p>The rest of the chapter comprises a series of increasingly sophisticated MAB algorithms.</p>
<div id="e3467f3a" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_strategy(mab: MAB, agent: Agent):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">6</span>))</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># plot reward and cumulative regret</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(mab.T), np.cumsum(agent.rewards), label<span class="op">=</span><span class="st">"reward"</span>)</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    cum_regret <span class="op">=</span> np.cumsum(regret_per_step(mab, agent))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    plt.plot(np.arange(mab.T), cum_regret, label<span class="op">=</span><span class="st">"cumulative regret"</span>)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># draw colored circles for arm choices</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    colors <span class="op">=</span> [<span class="st">"red"</span>, <span class="st">"green"</span>, <span class="st">"blue"</span>]</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    color_array <span class="op">=</span> [colors[k] <span class="cf">for</span> k <span class="kw">in</span> agent.choices]</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    plt.scatter(np.arange(mab.T), np.zeros(mab.T), c<span class="op">=</span>color_array, label<span class="op">=</span><span class="st">"arm"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># labels and title</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">"timestep"</span>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="ss">f"</span><span class="sc">{</span>agent<span class="sc">.</span>__class__<span class="sc">.</span><span class="va">__name__</span><span class="sc">}</span><span class="ss"> reward and regret"</span>)</span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="sec-pure-exploration" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-pure-exploration"><span class="header-section-number">3.2</span> Pure exploration</h2>
<p>A trivial strategy is to always choose arms at random (i.e.&nbsp;“pure exploration”).</p>
<div id="1125f48c" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PureExploration(Agent):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose an arm uniformly at random."""</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.pure_exploration_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Note that</p>
<p><span class="math display">\[
\mathbb{E}_{a_t \sim \text{Unif}([K])}[\mu^{a_t}] = \bar \mu = \frac{1}{K} \sum_{k=1}^K \mu^k
\]</span></p>
<p>so the expected regret is simply</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[\text{Regret}_T] &amp;= \sum_{t=0}^{T-1} \mathbb{E}[\mu^\star - \mu^{a_t}] \\
    &amp;= T (\mu^\star - \bar \mu) &gt; 0.
\end{aligned}
\]</span></p>
<p>This scales as <span class="math inline">\(\Theta(T)\)</span>, i.e.&nbsp;<em>linear</em> in the number of timesteps <span class="math inline">\(T\)</span>. There’s no learning here: the agent doesn’t use any information about the environment to improve its strategy. You can see that the distribution over its arm choices always appears “(uniformly) random”.</p>
<div id="327e8486" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> PureExploration(mab.K, mab.T)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-10-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="sec-pure-greedy" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-pure-greedy"><span class="header-section-number">3.3</span> Pure greedy</h2>
<p>How might we improve on pure exploration? Instead, we could try each arm once, and then commit to the one with the highest observed reward. We’ll call this the <strong>pure greedy</strong> strategy.</p>
<div id="f05cbc35" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> PureGreedy(Agent):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Choose the arm with the highest observed reward on its first pull."""</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.pure_greedy_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Note we’ve used superscripts <span class="math inline">\(r^k\)</span> during the exploration phase to indicate that we observe exactly one reward for each arm. Then we use subscripts <span class="math inline">\(r_t\)</span> during the exploitation phase to indicate that we observe a sequence of rewards from the chosen greedy arm <span class="math inline">\(\hat k\)</span>.</p>
<p>How does the expected regret of this strategy compare to that of pure exploration? We’ll do a more general analysis in the following section. Now, for intuition, suppose there’s just <span class="math inline">\(K=2\)</span> arms, with Bernoulli reward distributions with means <span class="math inline">\(\mu^0 &gt; \mu^1\)</span>.</p>
<p>Let’s let <span class="math inline">\(r^0\)</span> be the random reward from the first arm and <span class="math inline">\(r^1\)</span> be the random reward from the second. If <span class="math inline">\(r^0 &gt; r^1\)</span>, then we achieve zero regret. Otherwise, we achieve regret <span class="math inline">\(T(\mu^0 - \mu^1)\)</span>. Thus, the expected regret is simply:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}[\text{Regret}_T] &amp;= \mathbb{P}(r^0 &lt; r^1) \cdot T(\mu^0 - \mu^1) + c \\
    &amp;= (1 - \mu^0) \mu^1 \cdot T(\mu^0 - \mu^1) + c
\end{aligned}
\]</span></p>
<p>Which is still <span class="math inline">\(\Theta(T)\)</span>, the same as pure exploration!</p>
<div id="03238d8c" class="cell" data-execution_count="11">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> PureGreedy(mab.K, mab.T)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-12-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The cumulative regret is a straight line because the regret only depends on the arms chosen and not the actual reward observed. In fact, if the greedy algorithm happens to get lucky on the first set of pulls, it may act entirely optimally for that episode! But its <em>average</em> regret is what measures its effectiveness.</p>
</section>
<section id="sec-etc" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="sec-etc"><span class="header-section-number">3.4</span> Explore-then-commit</h2>
<p>We can improve the pure greedy algorithm as follows: let’s reduce the variance of the reward estimates by pulling each arm <span class="math inline">\(N_{\text{explore}}&gt; 1\)</span> times before committing. This is called the <strong>explore-then-commit</strong> strategy. Note that the “pure greedy” strategy above is just the special case where <span class="math inline">\(N_{\text{explore}}= 1\)</span>.</p>
<div id="7d41dad7" class="cell" data-execution_count="12">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ExploreThenCommit(Agent):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, N_explore: <span class="bu">int</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.N_explore <span class="op">=</span> N_explore</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.etc_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="0f9aa383" class="cell" data-execution_count="13">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ExploreThenCommit(mab.K, mab.T, mab.T <span class="op">//</span> <span class="dv">15</span>)</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-14-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Notice that now, the graphs are much more consistent, and the algorithm finds the true optimal arm and sticks with it much more frequently. We would expect ETC to then have a better (i.e.&nbsp;lower) average regret. Can we prove this?</p>
<section id="sec-etc-regret-analysis" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="sec-etc-regret-analysis"><span class="header-section-number">3.4.1</span> ETC regret analysis</h3>
<p>Let’s analyze the expected regret of the explore-then-commit strategy by splitting it up into the exploration and exploitation phases.</p>
<section id="exploration-phase." class="level4" data-number="3.4.1.1">
<h4 data-number="3.4.1.1" class="anchored" data-anchor-id="exploration-phase."><span class="header-section-number">3.4.1.1</span> Exploration phase.</h4>
<p>This phase takes <span class="math inline">\(N_{\text{explore}}K\)</span> timesteps. Since at each step we incur at most <span class="math inline">\(1\)</span> regret, the total regret is at most <span class="math inline">\(N_{\text{explore}}K\)</span>.</p>
</section>
<section id="exploitation-phase." class="level4" data-number="3.4.1.2">
<h4 data-number="3.4.1.2" class="anchored" data-anchor-id="exploitation-phase."><span class="header-section-number">3.4.1.2</span> Exploitation phase.</h4>
<p>This will take a bit more effort. We’ll prove that for any total time <span class="math inline">\(T\)</span>, we can choose <span class="math inline">\(N_{\text{explore}}\)</span> such that with arbitrarily high probability, the regret is sublinear.</p>
<p>Let <span class="math inline">\(\hat k\)</span> denote the arm chosen after the exploration phase. We know the regret from the exploitation phase is</p>
<p><span class="math display">\[
T_{\text{exploit}} (\mu^\star - \mu^{\hat k}) \qquad \text{where} \qquad T_{\text{exploit}} := T - N_{\text{explore}}K.
\]</span></p>
<p>So we’d like to bound <span class="math inline">\(\mu^\star - \mu^{\hat k} = o(1)\)</span> (as a function of <span class="math inline">\(T\)</span>) in order to achieve sublinear regret. How can we do this?</p>
<p>Let’s define <span class="math inline">\(\Delta^k = \hat \mu^k - \mu^k\)</span> to denote how far the mean estimate for arm <span class="math inline">\(k\)</span> is from the true mean. How can we bound this quantity? We’ll use the following useful inequality for i.i.d. bounded random variables:</p>
<div id="thm-hoeffding" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.1 (Hoeffding’s inequality)</strong></span> Let <span class="math inline">\(X_0, \dots, X_{n-1}\)</span> be i.i.d. random variables with <span class="math inline">\(X_i \in [0, 1]\)</span> almost surely for each <span class="math inline">\(i \in [n]\)</span>. Then for any <span class="math inline">\(\delta &gt; 0\)</span>,</p>
<p><span class="math display">\[
\mathbb{P}\left( \left| \frac{1}{n} \sum_{i=1}^n (X_i - \mathbb{E}[X_i]) \right| &gt; \sqrt{\frac{\ln(2/\delta)}{2n}} \right) \le \delta.
\]</span></p>
</div>
<p>The proof of this inequality is beyond the scope of this book. See <span class="citation" data-cites="vershynin_high-dimensional_2018">Vershynin (<a href="references.html#ref-vershynin_high-dimensional_2018" role="doc-biblioref">2018</a>)</span> Chapter 2.2.</p>
<p>We can apply this directly to the rewards for a given arm <span class="math inline">\(k\)</span>, since the rewards from that arm are i.i.d.:</p>
<p><span id="eq-hoeffding-etc"><span class="math display">\[
\mathbb{P}\left(|\Delta^k | &gt; \sqrt{\frac{\ln(2/\delta)}{2N_{\text{explore}}}} \right) \le \delta.
\tag{3.1}\]</span></span></p>
<p>But note that we can’t apply this to arm <span class="math inline">\(\hat k\)</span> directly since <span class="math inline">\(\hat k\)</span> is itself a random variable. Instead, we need to “uniform-ize” this bound across <em>all</em> the arms, i.e.&nbsp;bound the error across all the arms simultaneously, so that the resulting bound will apply <em>no matter what</em> <span class="math inline">\(\hat k\)</span> “crystallizes” to.</p>
<p>The <strong>union bound</strong> provides a simple way to do this:</p>
<div id="thm-union-bound" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.2 (Union bound)</strong></span> Consider a set of events <span class="math inline">\(A_0, \dots, A_{n-1}\)</span>. Then</p>
<p><span class="math display">\[
\mathbb{P}(\exists i \in [n]. A_i) \le \sum_{i=0}^{n-1} \mathbb{P}(A_i).
\]</span></p>
<p>In particular, if <span class="math inline">\(\mathbb{P}(A_i) \ge 1 - \delta\)</span> for each <span class="math inline">\(i \in [n]\)</span>, we have</p>
<p><span class="math display">\[
\mathbb{P}(\forall i \in [n]. A_i) \ge 1 - n \delta.
\]</span></p>
</div>
<p><strong>Exercise:</strong> Prove the second statement above.</p>
<p>Applying the union bound across the arms for the l.h.s. event of <a href="#eq-hoeffding-etc" class="quarto-xref">Equation&nbsp;<span>3.1</span></a>, we have</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left( \forall k \in [K], |\Delta^k | \le \sqrt{\frac{\ln(2/\delta)}{2N_{\text{explore}}}} \right) &amp;\ge 1-K\delta
\end{aligned}
\]</span></p>
<p>Then to apply this bound to <span class="math inline">\(\hat k\)</span> in particular, we can apply the useful trick of “adding zero”:</p>
<p><span class="math display">\[
\begin{aligned}
    \mu^{k^\star} - \mu^{\hat k} &amp;= \mu^{k^\star} - \mu^{\hat k} + (\hat \mu^{k^\star} - \hat \mu^{k^\star}) + (\hat \mu^{\hat k} - \hat \mu^{\hat k}) \\
    &amp;= \Delta^{\hat k} - \Delta^{k^*} + \underbrace{(\hat \mu^{k^\star} - \hat \mu^{\hat k})}_{\le 0 \text{ by definition of } \hat k} \\
    &amp;\le 2 \sqrt{\frac{\ln(2K/\delta')}{2N_{\text{explore}}}} \text{ with probability at least } 1-\delta'
\end{aligned}
\]</span></p>
<p>where we’ve set <span class="math inline">\(\delta' = K\delta\)</span>. Putting this all together, we’ve shown that, with probability <span class="math inline">\(1 - \delta'\)</span>,</p>
<p><span class="math display">\[
\text{Regret}_T \le N_{\text{explore}}K + T_{\text{exploit}} \cdot \sqrt{\frac{2\ln(2K/\delta')}{N_{\text{explore}}}}.
\]</span></p>
<p>Note that it suffices for <span class="math inline">\(N_{\text{explore}}\)</span> to be on the order of <span class="math inline">\(\sqrt{T}\)</span> to achieve sublinear regret. In particular, we can find the optimal <span class="math inline">\(N_{\text{explore}}\)</span> by setting the derivative of the r.h.s. to zero:</p>
<p><span class="math display">\[
\begin{aligned}
    0 &amp;= K - T_{\text{exploit}} \cdot \frac{1}{2} \sqrt{\frac{2\ln(2K/\delta')}{N_{\text{explore}}^3}} \\
    N_{\text{explore}}&amp;= \left( T_{\text{exploit}} \cdot \frac{\sqrt{\ln(2K/\delta')/2}}{K} \right)^{2/3}
\end{aligned}
\]</span></p>
<p>Plugging this into the expression for the regret, we have (still with probability <span class="math inline">\(1-\delta'\)</span>)</p>
<p><span class="math display">\[
\begin{aligned}
    \text{Regret}_T &amp;\le 3 T^{2/3} \sqrt[3]{K \ln(2K/\delta') / 2} \\
    &amp;= \tilde{O}(T^{2/3} K^{1/3}).
\end{aligned}
\]</span></p>
<p>The ETC algorithm is rather “abrupt” in that it switches from exploration to exploitation after a fixed number of timesteps. In practice, it’s often better to use a more gradual transition, which brings us to the <em>epsilon-greedy</em> algorithm.</p>
</section>
</section>
</section>
<section id="epsilon-greedy" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="epsilon-greedy"><span class="header-section-number">3.5</span> Epsilon-greedy</h2>
<p>Instead of doing all of the exploration and then all of the exploitation separately – which additionally requires knowing the time horizon beforehand – we can instead interleave exploration and exploitation by, at each timestep, choosing a random action with some probability. We call this the <strong>epsilon-greedy</strong> algorithm.</p>
<div id="ab9faaba" class="cell" data-execution_count="14">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EpsilonGreedy(Agent):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        K: <span class="bu">int</span>,</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        T: <span class="bu">int</span>,</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        ε_array: Float[Array, <span class="st">" T"</span>],</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.ε_array <span class="op">=</span> ε_array</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.epsilon_greedy_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="7be4c5b6" class="cell" data-execution_count="15">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> EpsilonGreedy(mab.K, mab.T, np.full(mab.T, <span class="fl">0.1</span>))</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-16-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Note that we let <span class="math inline">\(\epsilon\)</span> vary over time. In particular, we might want to gradually <em>decrease</em> <span class="math inline">\(\epsilon\)</span> as we learn more about the reward distributions and no longer need to spend time exploring.</p>
<div class="{attention}">
<p>What is the expected regret of the algorithm if we set <span class="math inline">\(\epsilon\)</span> to be a constant?</p>
</div>
<p>It turns out that setting <span class="math inline">\(\epsilon_t = \sqrt[3]{K \ln(t)/t}\)</span> also achieves a regret of <span class="math inline">\(\tilde O(t^{2/3} K^{1/3})\)</span> (ignoring the logarithmic factors). (We will not prove this here; a proof can be found in <span class="citation" data-cites="agarwal_reinforcement_2022">(<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">Agarwal et al. 2022</a>)</span>.)</p>
<p>In ETC, we had to set <span class="math inline">\(N_{\text{explore}}\)</span> based on the total number of timesteps <span class="math inline">\(T\)</span>. But the epsilon-greedy algorithm actually handles the exploration <em>automatically</em>: the regret rate holds for <em>any</em> <span class="math inline">\(t\)</span>, and doesn’t depend on the final horizon <span class="math inline">\(T\)</span>.</p>
<p>But the way these algorithms explore is rather naive: we’ve been exploring <em>uniformly</em> across all the arms. But what if we could be smarter about it, and explore <em>more</em> for arms that we’re less certain about?</p>
</section>
<section id="sec-ucb" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-ucb"><span class="header-section-number">3.6</span> Upper Confidence Bound (UCB)</h2>
<p>To quantify how <em>certain</em> we are about the mean of each arm, we’ll compute <em>confidence intervals</em> for our estimators, and then choose the arm with the highest <em>upper confidence bound</em>. This operates on the principle of <strong>the benefit of the doubt (i.e.&nbsp;optimism in the face of uncertainty)</strong>: we’ll choose the arm that we’re most optimistic about.</p>
<p>In particular, for each arm <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>, we’d like to compute some upper confidence bound <span class="math inline">\(M^k_t\)</span> such that <span class="math inline">\(\hat \mu^k_t \le M^k_t\)</span> with high probability, and then choose <span class="math inline">\(a_t := \arg \max_{k \in [K]} M^k_t\)</span>. But how should we compute <span class="math inline">\(M^k_t\)</span>?</p>
<p>In <a href="#sec-etc-regret-analysis" class="quarto-xref"><span>Section 3.4.1</span></a>, we were able to compute this bound using Hoeffding’s inequality, which assumes that the number of samples is <em>fixed</em>. This was the case in ETC (where we pull each arm <span class="math inline">\(N_{\text{explore}}\)</span> times), but in UCB, the number of times we pull each arm depends on the agent’s actions, which in turn depend on the random rewards and are therefore stochastic. So we <em>can’t</em> use Hoeffding’s inequality directly.</p>
<p>Instead, we’ll apply the same trick we used in the ETC analysis: we’ll use the <strong>union bound</strong> to compute a <em>looser</em> bound that holds <em>uniformly</em> across all timesteps and arms. Let’s introduce some notation to discuss this.</p>
<p>Let <span class="math inline">\(N^k_t\)</span> denote the (random) number of times arm <span class="math inline">\(k\)</span> has been pulled within the first <span class="math inline">\(t\)</span> timesteps, and <span class="math inline">\(\hat \mu^k_t\)</span> denote the sample average of those pulls. That is,</p>
<p><span class="math display">\[
\begin{aligned}
    N^k_t &amp;:= \sum_{\tau=0}^{t-1} \mathbf{1} \{ a_\tau = k \} \\
    \hat \mu^k_t &amp;:= \frac{1}{N^k_t} \sum_{\tau=0}^{t-1} \mathbf{1} \{ a_\tau = k \} r_\tau.
\end{aligned}
\]</span></p>
<p>To achieve the “fixed sample size” assumption, we’ll need to shift our index from <em>time</em> to <em>number of samples from each arm</em>. In particular, we’ll define <span class="math inline">\(\tilde r^k_n\)</span> to be the <span class="math inline">\(n\)</span>th sample from arm <span class="math inline">\(k\)</span>, and <span class="math inline">\(\tilde \mu^k_n\)</span> to be the sample average of the first <span class="math inline">\(n\)</span> samples from arm <span class="math inline">\(k\)</span>. Then, for a fixed <span class="math inline">\(n\)</span>, this satisfies the “fixed sample size” assumption, and we can apply Hoeffding’s inequality to get a bound on <span class="math inline">\(\tilde \mu^k_n\)</span>.</p>
<p>So how can we extend our bound on <span class="math inline">\(\tilde\mu^k_n\)</span> to <span class="math inline">\(\hat \mu^k_t\)</span>? Well, we know <span class="math inline">\(N^k_t \le t\)</span> (where equality would be the case if and only if we had pulled arm <span class="math inline">\(k\)</span> every time). So we can apply the same trick as last time, where we uniform-ize across all possible values of <span class="math inline">\(N^k_t\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left( \forall n \le t, |\tilde \mu^k_n - \mu^k | \le \sqrt{\frac{\ln(2/\delta)}{2n}} \right) &amp;\ge 1-t\delta.
\end{aligned}
\]</span></p>
<p>In particular, since <span class="math inline">\(N^k_t \le t\)</span>, and <span class="math inline">\(\tilde \mu^k_{N^k_t} = \hat \mu^k_t\)</span> by definition, we have</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left( |\hat \mu^k_t - \mu^k | \le \sqrt{\frac{\ln(2t/\delta')}{2N^k_t}} \right) &amp;\ge 1-\delta' \text{ where } \delta' := t \delta.
\end{aligned}
\]</span></p>
<p>This bound would then suffice for applying the UCB algorithm! That is, the upper confidence bound for arm <span class="math inline">\(k\)</span> would be</p>
<p><span class="math display">\[
M^k_t := \hat \mu^k_t + \sqrt{\frac{\ln(2t/\delta')}{2N^k_t}},
\]</span></p>
<p>where we can choose <span class="math inline">\(\delta'\)</span> depending on how tight we want the interval to be.</p>
<ul>
<li>A smaller <span class="math inline">\(\delta'\)</span> would give us a larger and higher-confidence interval, emphasizing the exploration term.</li>
<li>A larger <span class="math inline">\(\delta'\)</span> would give a tighter and lower-confidence interval, prioritizing the current sample averages.</li>
</ul>
<p>We can now use this to define the UCB algorithm.</p>
<div id="ccf50c01" class="cell" data-execution_count="16">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UCB(Agent):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, delta: <span class="bu">float</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.delta <span class="op">=</span> delta</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> solutions.ucb_choose_arm(<span class="va">self</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Intuitively, UCB prioritizes arms where:</p>
<ol type="1">
<li><p><span class="math inline">\(\hat \mu^k_t\)</span> is large, i.e.&nbsp;the arm has a high sample average, and we’d choose it for <em>exploitation</em>, and</p></li>
<li><p><span class="math inline">\(\sqrt{\frac{\ln(2t/\delta')}{2N^k_t}}\)</span> is large, i.e.&nbsp;we’re still uncertain about the arm, and we’d choose it for <em>exploration</em>.</p></li>
</ol>
<p>As desired, this explores in a smarter, <em>adaptive</em> way compared to the previous algorithms. Does it achieve lower regret?</p>
<div id="a0c49934" class="cell" data-execution_count="17">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> UCB(mab.K, mab.T, <span class="fl">0.9</span>)</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-18-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<section id="ucb-regret-analysis" class="level3" data-number="3.6.1">
<h3 data-number="3.6.1" class="anchored" data-anchor-id="ucb-regret-analysis"><span class="header-section-number">3.6.1</span> UCB regret analysis</h3>
<p>First we’ll bound the regret incurred at each timestep. Then we’ll bound the <em>total</em> regret across timesteps.</p>
<p>For the sake of analysis, we’ll use a slightly looser bound that applies across the whole time horizon and across all arms. We’ll omit the derivation since it’s very similar to the above (walk through it yourself for practice).</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}\left(\forall k \le K, t &lt; T. |\hat \mu^k_t - \mu^k | \le B^k_t \right) &amp;\ge 1-\delta'' \\
    \text{where} \quad B^k_t &amp;:= \sqrt{\frac{\ln(2TK/\delta'')}{2N^k_t}}.
\end{aligned}
\]</span></p>
<p>Intuitively, <span class="math inline">\(B^k_t\)</span> denotes the <em>width</em> of the CI for arm <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>. Then, assuming the above uniform bound holds (which occurs with probability <span class="math inline">\(1-\delta''\)</span>), we can bound the regret at each timestep as follows:</p>
<p><span class="math display">\[
\begin{aligned}
    \mu^\star - \mu^{a_t} &amp;\le \hat \mu^{k^*}_t + B_t^{k^*} - \mu^{a_t} &amp;&amp; \text{applying UCB to arm } k^\star \\
    &amp;\le \hat \mu^{a_t}_t + B^{a_t}_t - \mu^{a_t} &amp;&amp; \text{since UCB chooses } a_t = \arg \max_{k \in [K]} \hat \mu^k_t + B_t^{k} \\
    &amp;\le 2 B^{a_t}_t &amp;&amp; \text{since } \hat \mu^{a_t}_t - \mu^{a_t} \le B^{a_t}_t \text{ by definition of } B^{a_t}_t \\
\end{aligned}
\]</span></p>
<p>Summing this across timesteps gives</p>
<p><span class="math display">\[
\begin{aligned}
    \text{Regret}_T &amp;\le \sum_{t=0}^{T-1} 2 B^{a_t}_t \\
    &amp;= \sqrt{2\ln(2TK/\delta'')} \sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} \\
    \sum_{t=0}^{T-1} (N^{a_t}_t)^{-1/2} &amp;= \sum_{t=0}^{T-1} \sum_{k=1}^K \mathbf{1}\{ a_t = k \} (N^k_t)^{-1/2} \\
    &amp;= \sum_{k=1}^K \sum_{n=1}^{N_T^k} n^{-1/2} \\
    &amp;\le K \sum_{n=1}^T n^{-1/2} \\
    \sum_{n=1}^T n^{-1/2} &amp;\le 1 + \int_1^T x^{-1/2} \ \mathrm{d}x \\
    &amp;= 1 + (2 \sqrt{x})_1^T \\
    &amp;= 2 \sqrt{T} - 1 \\
    &amp;\le 2 \sqrt{T} \\
\end{aligned}
\]</span></p>
<p>Putting everything together gives</p>
<p><span class="math display">\[
\begin{aligned}
    \text{Regret}_T &amp;\le 2 K \sqrt{2T \ln(2TK/\delta'')} &amp;&amp; \text{with probability } 1-\delta'' \\
    &amp;= \tilde O(K\sqrt{T})
\end{aligned}
\]</span></p>
<p>In fact, we can do a more sophisticated analysis to trim off a factor of <span class="math inline">\(\sqrt{K}\)</span> and show <span class="math inline">\(\text{Regret}_T = \tilde O(\sqrt{TK})\)</span>.</p>
</section>
<section id="lower-bound-on-regret-intuition" class="level3" data-number="3.6.2">
<h3 data-number="3.6.2" class="anchored" data-anchor-id="lower-bound-on-regret-intuition"><span class="header-section-number">3.6.2</span> Lower bound on regret (intuition)</h3>
<p>Is it possible to do better than <span class="math inline">\(\Omega(\sqrt{T})\)</span> in general? In fact, no! We can show that any algorithm must incur <span class="math inline">\(\Omega(\sqrt{T})\)</span> regret in the worst case. We won’t rigorously prove this here, but the intuition is as follows.</p>
<p>The Central Limit Theorem tells us that with <span class="math inline">\(T\)</span> i.i.d. samples from some distribution, we can only learn the mean of the distribution to within <span class="math inline">\(\Omega(1/\sqrt{T})\)</span> (the standard deviation). Then, since we get <span class="math inline">\(T\)</span> samples spread out across the arms, we can only learn each arm’s mean to an even looser degree.</p>
<p>That is, if two arms have means that are within about <span class="math inline">\(1/\sqrt{T}\)</span>, we won’t be able to confidently tell them apart, and will sample them about equally. But then we’ll incur regret <span class="math display">\[
\Omega((T/2) \cdot (1/\sqrt{T})) = \Omega(\sqrt{T}).
\]</span></p>
</section>
</section>
<section id="sec-thompson-sampling" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="sec-thompson-sampling"><span class="header-section-number">3.7</span> Thompson sampling and Bayesian bandits</h2>
<p>So far, we’ve treated the parameters <span class="math inline">\(\mu^0, \dots, \mu^{K-1}\)</span> of the reward distributions as <em>fixed</em>. Instead, we can take a <strong>Bayesian</strong> approach where we treat them as random variables from some <strong>prior distribution</strong>. Then, upon pulling an arm and observing a reward, we can simply <em>condition</em> on this observation to exactly describe the <strong>posterior distribution</strong> over the parameters. This fully describes the information we gain about the parameters from observing the reward.</p>
<p>From this Bayesian perspective, the <strong>Thompson sampling</strong> algorithm follows naturally: just sample from the distribution of the optimal arm, given the observations!</p>
<div id="fc4e36e4" class="cell" data-execution_count="18">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Distribution:</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" K"</span>]:</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Sample a vector of means for the K arms."""</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        ...</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">float</span>):</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">"""Condition on obtaining `reward` from the given arm."""</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        ...</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="e7c1e39c" class="cell" data-execution_count="19">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ThompsonSampling(Agent):</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, prior: Distribution):</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distribution <span class="op">=</span> prior</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>):</span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        means <span class="op">=</span> <span class="va">self</span>.distribution.sample()</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random_argmax(means)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().update_history(arm, reward)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.distribution.update(arm, reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In other words, we sample each arm proportionally to how likely we think it is to be optimal, given the observations so far. This strikes a good exploration-exploitation tradeoff: we explore more for arms that we’re less certain about, and exploit more for arms that we’re more certain about. Thompson sampling is a simple yet powerful algorithm that achieves state-of-the-art performance in many settings.</p>
<div id="exm-bayesian-bernoulli" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 (Bayesian Bernoulli bandit)</strong></span> We’ve been working in the Bernoulli bandit setting, where arm <span class="math inline">\(k\)</span> yields a reward of <span class="math inline">\(1\)</span> with probability <span class="math inline">\(\mu^k\)</span> and no reward otherwise. The vector of success probabilities <span class="math inline">\(\boldsymbol{\mu} = (\mu^1, \dots, \mu^K)\)</span> thus describes the entire MAB.</p>
<p>Under the Bayesian perspective, we think of <span class="math inline">\(\boldsymbol{\mu}\)</span> as a <em>random</em> vector drawn from some prior distribution <span class="math inline">\(\pi(\boldsymbol{\mu})\)</span>. For example, we might have <span class="math inline">\(\pi\)</span> be the Uniform distribution over the unit hypercube <span class="math inline">\([0, 1]^K\)</span>, that is,</p>
<p><span class="math display">\[
\pi(\boldsymbol{\mu}) = \begin{cases}
    1 &amp; \text{if } \boldsymbol{\mu}\in [0, 1]^K \\
    0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>In this case, upon viewing some reward, we can exactly calculate the <strong>posterior</strong> distribution of <span class="math inline">\(\boldsymbol{\mu}\)</span> using Bayes’s rule (i.e.&nbsp;the definition of conditional probability):</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{P}(\boldsymbol{\mu} \mid a_0, r_0) &amp;\propto \mathbb{P}(r_0 \mid a_0, \boldsymbol{\mu}) \mathbb{P}(a_0 \mid \boldsymbol{\mu}) \mathbb{P}(\boldsymbol{\mu}) \\
    &amp;\propto (\mu^{a_0})^{r_0} (1 - \mu^{a_0})^{1-r_0}.
\end{aligned}
\]</span></p>
<p>This is the PDF of the <span class="math inline">\(\text{Beta}(1 + r_0, 1 + (1 - r_0))\)</span> distribution, which is a conjugate prior for the Bernoulli distribution. That is, if we start with a Beta prior on <span class="math inline">\(\mu^k\)</span> (note that <span class="math inline">\(\text{Unif}([0, 1]) = \text{Beta}(1, 1)\)</span>), then the posterior, after conditioning on samples from <span class="math inline">\(\text{Bern}(\mu^k)\)</span>, will also be Beta. This is a very convenient property, since it means we can simply update the parameters of the Beta distribution upon observing a reward, rather than having to recompute the entire posterior distribution from scratch.</p>
</div>
<div id="5a0d5a6e" class="cell" data-execution_count="20">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Beta(Distribution):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, K: <span class="bu">int</span>, alpha: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>, beta: <span class="bu">int</span> <span class="op">=</span> <span class="dv">1</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> np.full(K, alpha)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> np.full(K, beta)</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> sample(<span class="va">self</span>):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.random.beta(<span class="va">self</span>.alphas, <span class="va">self</span>.betas)</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas[arm] <span class="op">+=</span> reward</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas[arm] <span class="op">+=</span> <span class="dv">1</span> <span class="op">-</span> reward</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="d354f3b6" class="cell" data-execution_count="21">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>beta_distribution <span class="op">=</span> Beta(mab.K)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>agent <span class="op">=</span> ThompsonSampling(mab.K, mab.T, beta_distribution)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>mab_loop(mab, agent)</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>plot_strategy(mab, agent)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="bandits_files/figure-html/cell-22-output-1.png" width="789" height="523" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It turns out that asymptotically, Thompson sampling is optimal in the following sense. <span class="citation" data-cites="lai_asymptotically_1985">Lai and Robbins (<a href="references.html#ref-lai_asymptotically_1985" role="doc-biblioref">1985</a>)</span> prove an <em>instance-dependent</em> lower bound that says for <em>any</em> bandit algorithm,</p>
<p><span class="math display">\[
\liminf_{T \to \infty} \frac{\mathbb{E}[N_T^k]}{\ln(T)} \ge \frac{1}{\text{KL}(\mu^k \parallel \mu^\star)}
\]</span></p>
<p>where</p>
<p><span class="math display">\[
\text{KL}(\mu^k \parallel \mu^\star) = \mu^k \ln \frac{\mu^k}{\mu^\star} + (1 - \mu^k) \ln \frac{1 - \mu^k}{1 - \mu^\star}
\]</span></p>
<p>measures the <strong>Kullback-Leibler divergence</strong> from the Bernoulli distribution with mean <span class="math inline">\(\mu^k\)</span> to the Bernoulli distribution with mean <span class="math inline">\(\mu^\star\)</span>. It turns out that Thompson sampling achieves this lower bound with equality! That is, not only is the error <em>rate</em> optimal, but the <em>constant factor</em> is optimal as well.</p>
</section>
<section id="contextual-bandits" class="level2" data-number="3.8">
<h2 data-number="3.8" class="anchored" data-anchor-id="contextual-bandits"><span class="header-section-number">3.8</span> Contextual bandits</h2>
<p>In the above MAB environment, the reward distributions of the arms remain constant. However, in many real-world settings, we might receive additional information that affects these distributions. For example, in the online advertising case where each arm corresponds to an ad we could show the user, we might receive information about the user’s preferences that changes how likely they are to click on a given ad. We can model such environments using <strong>contextual bandits</strong>.</p>
<div id="def-contextual-bandit" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 (Contextual bandit)</strong></span> At each timestep <span class="math inline">\(t\)</span>, a new <em>context</em> <span class="math inline">\(x_t\)</span> is drawn from some distribution <span class="math inline">\(\nu_{\text{x}}\)</span>. The learner gets to observe the context, and choose an action <span class="math inline">\(a_t\)</span> according to some context-dependent policy <span class="math inline">\(\pi_t(x_t)\)</span>. Then, the learner observes the reward from the chosen arm <span class="math inline">\(r_t \sim \nu^{a_t}(x_t)\)</span>. The reward distribution also depends on the context.</p>
</div>
<p>Assuming our context is <em>discrete</em>, we can just perform the same algorithms, treating each context-arm pair as its own arm. This gives us an enlarged MAB of <span class="math inline">\(K |\mathcal{X}|\)</span> arms.</p>
<div class="{attention}">
<p>Write down the UCB algorithm for this enlarged MAB. That is, write an expression for <span class="math inline">\(\pi_t(x_t) = \arg\max_a \dots\)</span>.</p>
</div>
<p>Recall that running UCB for <span class="math inline">\(T\)</span> timesteps on an MAB with <span class="math inline">\(K\)</span> arms achieves a regret bound of <span class="math inline">\(\tilde{O}(\sqrt{TK})\)</span>. So in this problem, we would achieve regret <span class="math inline">\(\tilde{O}(\sqrt{TK|\mathcal{X}|})\)</span> in the contextual MAB, which has a polynomial dependence on <span class="math inline">\(|\mathcal{X}|\)</span>. But in a situation where we have large, or even infinitely many contexts, e.g.&nbsp;in the case where our context is a continuous value, this becomes intractable.</p>
<p>Note that this “enlarged MAB” treats the different contexts as entirely unrelated to each other, while in practice, often contexts are <em>related</em> to each other in some way: for example, we might want to advertise similar products to users with similar preferences. How can we incorporate this structure into our solution?</p>
<section id="sec-lin-ucb" class="level3" data-number="3.8.1">
<h3 data-number="3.8.1" class="anchored" data-anchor-id="sec-lin-ucb"><span class="header-section-number">3.8.1</span> Linear contextual bandits</h3>
<p>We want to model the <em>mean reward</em> of arm <span class="math inline">\(k\)</span> as a function of the context, i.e.&nbsp;<span class="math inline">\(\mu^k(x)\)</span>. One simple model is the <em>linear</em> one: <span class="math inline">\(\mu^k(x) = x^\top \theta^k\)</span>, where <span class="math inline">\(x \in \mathcal{X} = \mathbb{R}^d\)</span> and <span class="math inline">\(\theta^k \in \mathbb{R}^d\)</span> describes a <em>feature direction</em> for arm <span class="math inline">\(k\)</span>. Recall that <strong>supervised learning</strong> gives us a way to estimate a conditional expectation from samples: We learn a <em>least squares</em> estimator from the timesteps where arm <span class="math inline">\(k\)</span> was selected: <span class="math display">\[
\hat \theta_t^k = \arg\min_{\theta \in \mathbb{R}^d} \sum_{\{ i \in [t] : a_i = k \}} (r_i - x_i^\top \theta)^2.
\]</span> This has the closed-form solution known as the <em>ordinary least squares</em> (OLS) estimator:</p>
<p><span id="eq-ols-bandit"><span class="math display">\[
\begin{aligned}
    \hat \theta_t^k          &amp; = (A_t^k)^{-1} \sum_{\{ i \in [t] : a_i = k \}} x_i r_i \\
    \text{where} \quad A_t^k &amp; = \sum_{\{ i \in [t] : a_i = k \}} x_i x_i^\top.
\end{aligned}
\tag{3.2}\]</span></span></p>
<p>We can now apply the UCB algorithm in this environment in order to balance <em>exploration</em> of new arms and <em>exploitation</em> of arms that we believe to have high reward. But how should we construct the upper confidence bound? Previously, we treated the pulls of an arm as i.i.d. samples and used Hoeffding’s inequality to bound the distance of the sample mean, our estimator, from the true mean. However, now our estimator is not a sample mean, but rather the OLS estimator above <a href="#eq-ols-bandit" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>. Instead, we’ll use <strong>Chebyshev’s inequality</strong> to construct an upper confidence bound.</p>
<div id="thm-chebyshev" class="theorem">
<p><span class="theorem-title"><strong>Theorem 3.3 (Chebyshev’s inequality)</strong></span> For a random variable <span class="math inline">\(Y\)</span> such that <span class="math inline">\(\mathbb{E}Y = 0\)</span> and <span class="math inline">\(\mathbb{E}Y^2 = \sigma^2\)</span>, <span class="math display">\[
|Y| \le \beta \sigma \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\]</span></p>
</div>
<p>Since the OLS estimator is known to be unbiased (try proving this yourself), we can apply Chebyshev’s inequality to <span class="math inline">\(x_t^\top (\hat \theta_t^k - \theta^k)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    x_t^\top \theta^k \le x_t^\top \hat \theta_t^k + \beta \sqrt{x_t^\top (A_t^k)^{-1} x_t} \quad \text{with probability} \ge 1 - \frac{1}{\beta^2}
\end{aligned}
\]</span></p>
<div class="{attention}">
<p>We haven’t explained why <span class="math inline">\(x_t^\top (A_t^k)^{-1} x_t\)</span> is the correct expression for the variance of <span class="math inline">\(x_t^\top \hat \theta_t^k\)</span>. This result follows from some algebra on the definition of the OLS estimator <a href="#eq-ols-bandit" class="quarto-xref">Equation&nbsp;<span>3.2</span></a>.</p>
</div>
<p>The first term is exactly our predicted reward <span class="math inline">\(\hat \mu^k_t(x_t)\)</span>. To interpret the second term, note that <span class="math display">\[
x_t^\top (A_t^k)^{-1} x_t = \frac{1}{N_t^k} x_t^\top (\Sigma_t^k)^{-1} x_t,
\]</span> where <span class="math display">\[
\Sigma_t^k = \frac{1}{N_t^k} \sum_{\{ i \in [t] : a_i = k \}} x_i x_i^\top
\]</span> is the empirical covariance matrix of the contexts (assuming that the context has mean zero). That is, the learner is encouraged to choose arms when <span class="math inline">\(x_t\)</span> is <em>not aligned</em> with the data seen so far, or if arm <span class="math inline">\(k\)</span> has not been explored much and so <span class="math inline">\(N_t^k\)</span> is small.</p>
<p>We can now substitute these quantities into UCB to get the <strong>LinUCB</strong> algorithm:</p>
<div id="bd5d6bc2" class="cell" data-execution_count="22">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> LinUCBPseudocode(Agent):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>, K: <span class="bu">int</span>, T: <span class="bu">int</span>, D: <span class="bu">int</span>, lam: <span class="bu">float</span>, get_c: Callable[[<span class="bu">int</span>], <span class="bu">float</span>]</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>    ):</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(K, T)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.lam <span class="op">=</span> lam</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.get_c <span class="op">=</span> get_c</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.contexts <span class="op">=</span> [<span class="va">None</span> <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(K)]</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A <span class="op">=</span> np.repeat(lam <span class="op">*</span> np.eye(D)[...], K)  <span class="co"># regularization</span></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets <span class="op">=</span> np.zeros(K, D)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w <span class="op">=</span> np.zeros(K, D)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> choose_arm(<span class="va">self</span>, context: Float[Array, <span class="st">" D"</span>]):</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>        c <span class="op">=</span> <span class="va">self</span>.get_c(<span class="va">self</span>.count)</span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>        scores <span class="op">=</span> <span class="va">self</span>.w <span class="op">@</span> context <span class="op">+</span> c <span class="op">*</span> np.sqrt(</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>            context.T <span class="op">@</span> np.linalg.solve(<span class="va">self</span>.A, context)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> random_argmax(scores)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_history(<span class="va">self</span>, context: Float[Array, <span class="st">" D"</span>], arm: <span class="bu">int</span>, reward: <span class="bu">int</span>):</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.A[arm] <span class="op">+=</span> np.outer(context, context)</span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.targets[arm] <span class="op">+=</span> context <span class="op">*</span> reward</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.w[arm] <span class="op">=</span> np.linalg.solve(<span class="va">self</span>.A[arm], <span class="va">self</span>.targets[arm])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="{attention}">
<p>Note that the matrix <span class="math inline">\(A_t^k\)</span> above might not be invertible. When does this occur? One way to address this is to include a <span class="math inline">\(\lambda I\)</span> regularization term to ensure that <span class="math inline">\(A_t^k\)</span> is invertible. This is equivalent to solving a <em>ridge regression</em> problem instead of the unregularized least squares problem. Implement this solution.</p>
</div>
<p><span class="math inline">\(c_t\)</span> is similar to the <span class="math inline">\(\log (2t/\delta')\)</span> term of UCB: It controls the width of the confidence interval. Here, we treat it as a tunable parameter, though in a theoretical analysis, it would depend on <span class="math inline">\(A_t^k\)</span> and the probability <span class="math inline">\(\delta\)</span> with which the bound holds.</p>
<p>Using similar tools for UCB, we can also prove an <span class="math inline">\(\tilde{O}(\sqrt{T})\)</span> regret bound. The full details of the analysis can be found in Section 3 of <span class="citation" data-cites="agarwal_reinforcement_2022">(<a href="references.html#ref-agarwal_reinforcement_2022" role="doc-biblioref">Agarwal et al. 2022</a>)</span>.</p>
</section>
</section>
<section id="summary" class="level2" data-number="3.9">
<h2 data-number="3.9" class="anchored" data-anchor-id="summary"><span class="header-section-number">3.9</span> Summary</h2>
<p>In this chapter, we explored the <strong>multi-armed bandit</strong> setting for analyzing sequential decision-making in an unknown environment. An MAB consists of multiple arms, each with an unknown reward distribution. The agent’s task is to learn about these through interaction, eventually minimizing the <em>regret</em>, which measures how suboptimal the chosen arms were.</p>
<p>We saw algorithms such as <strong>upper confidence bound</strong> and <strong>Thompson sampling</strong> that handle the tradeoff between <em>exploration</em> and <em>exploitation</em>, that is, the tradeoff between choosing arms that the agent is <em>uncertain</em> about and arms the agent already supposes are be good.</p>
<p>We finally discussed <strong>contextual bandits</strong>, in which the agent gets to observe some <em>context</em> that affects the reward distributions. We can approach these problems through <strong>supervised learning</strong> approaches.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-agarwal_reinforcement_2022" class="csl-entry" role="listitem">
Agarwal, Alekh, Nan Jiang, Sham M Kakade, and Wen Sun. 2022. <em>Reinforcement <span>Learning</span>: <span>Theory</span> and <span>Algorithms</span></em>. <a href="https://rltheorybook.github.io/rltheorybook_AJKS.pdf">https://rltheorybook.github.io/rltheorybook_AJKS.pdf</a>.
</div>
<div id="ref-lai_asymptotically_1985" class="csl-entry" role="listitem">
Lai, T. L, and Herbert Robbins. 1985. <span>“Asymptotically Efficient Adaptive Allocation Rules.”</span> <em>Advances in Applied Mathematics</em> 6 (1): 4–22. <a href="https://doi.org/10.1016/0196-8858(85)90002-8">https://doi.org/10.1016/0196-8858(85)90002-8</a>.
</div>
<div id="ref-vershynin_high-dimensional_2018" class="csl-entry" role="listitem">
Vershynin, Roman. 2018. <em>High-<span>Dimensional Probability</span>: <span>An Introduction</span> with <span>Applications</span> in <span>Data Science</span></em>. Cambridge University Press. <a href="https://books.google.com?id=NDdqDwAAQBAJ">https://books.google.com?id=NDdqDwAAQBAJ</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./control.html" class="pagination-link" aria-label="Linear Quadratic Regulators">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./supervised_learning.html" class="pagination-link" aria-label="Supervised learning">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>