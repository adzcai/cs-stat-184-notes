@article{lai_asymptotically_1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  year = {1985},
  journal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  url = {https://www.sciencedirect.com/science/article/pii/0196885885900028},
  urldate = {2023-10-23},
}

@book{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {Determination Press},
  url = {http://neuralnetworksanddeeplearning.com/},
  urldate = {2024-03-10}
}


@InProceedings{pmlr-v70-azar17a,
  title = 	 {Minimax Regret Bounds for Reinforcement Learning},
  author =       {Mohammad Gheshlaghi Azar and Ian Osband and R{\'e}mi Munos},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {263--272},
  year = 	 {2017},
  editor = 	 {Precup, Doina and Teh, Yee Whye},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {06--11 Aug},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/azar17a/azar17a.pdf},
  url = 	 {https://proceedings.mlr.press/v70/azar17a.html},
  abstract = 	 {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of $\tilde {O}( \sqrt{HSAT} + H^2S^2A+H\sqrt{T})$ where $H$ is the time horizon, $S$ the number of states, $A$ the number of actions and $T$ the number of time-steps. This result improves over the best previous known bound $\tilde {O}(HS \sqrt{AT})$ achieved by the UCRL2 algorithm. The key significance of our new results is that when $T\geq H^3S^3A$ and $SA\geq H$, it leads to a regret of $\tilde{O}(\sqrt{HSAT})$ that matches the established lower bound of $\Omega(\sqrt{HSAT})$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in $S$), and we define Bernstein-based “exploration bonuses” that use the empirical variance of the estimated values at the next states (to improve scaling in $H$).}
}

@article{agarwal_reinforcement_2022,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {{{AJKS}}},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  date = {2022-01-31},
  pages = {205},
  url = {https://rltheorybook.github.io/rltheorybook_AJKS.pdf},
  langid = {english},
}

