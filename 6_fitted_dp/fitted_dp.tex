(fitted\_dp)= \# Fitted dynamic programming algorithms

\texttt{\{contents\}\ :local:}

We borrow these definitions from the \{ref\}\texttt{mdps} chapter:

```\{code-cell\} ipython3 :tags: {[}hide-input{]}

from typing import NamedTuple, Callable, Optional from jaxtyping import
Float, Int, Array import jax.numpy as np from jax import grad, vmap,
tree\_map import jax.random as rand from functools import partial from
tqdm import tqdm import gymnasium as gym

key = rand.PRNGKey(184)

class Transition(NamedTuple): s: int a: int r: float

Trajectory = list{[}Transition{]}

def get\_num\_actions(trajectories: list{[}Trajectory{]})
-\textgreater{} int: ``\,````Get the number of actions in the dataset.
Assumes actions range from 0 to A-1.''``\,'' return max(max(t.a for t in
τ) for τ in trajectories) + 1

State = Float{[}Array, ``\ldots{}''{]} \# arbitrary shape

\section{\texorpdfstring{assume finite \texttt{A} actions and f outputs
an array of
Q-values}{assume finite A actions and f outputs an array of Q-values}}\label{assume-finite-a-actions-and-f-outputs-an-array-of-q-values}

\section{i.e.~Q(s, a, h) is implemented as f(s,
h){[}a{]}}\label{i.e.-qs-a-h-is-implemented-as-fs-ha}

QFunction = Callable{[}{[}State, int{]}, Float{[}Array, ``A''{]}{]}

def Q\_zero(A: int) -\textgreater{} QFunction: ``\,````A Q-function that
always returns zero.''``\,'' return lambda s, a: np.zeros(A)

\section{a deterministic time-dependent
policy}\label{a-deterministic-time-dependent-policy}

Policy = Callable{[}{[}State, int{]}, int{]}

def q\_to\_greedy(Q: QFunction) -\textgreater{} Policy: ``\,````Get the
greedy policy for the given state-action value function.''``\,'' return
lambda s, h: np.argmax(Q(s, h))

\begin{verbatim}

## Introduction

The {ref}`mdps` chapter discussed the case of **finite** MDPs, where the state and action spaces $\mathcal{S}$ and $\mathcal{A}$ were finite.
This gave us a closed-form expression for computing the r.h.s. of {prf:ref}`the Bellman one-step consistency equation <bellman_consistency>`.
In this chapter, we consider the case of **large** or **continuous** state spaces, where the state space is too large to be enumerated.
In this case, we need to *approximate* the value function and Q-function using methods from **supervised learning**.

We will first take a quick detour to introduce the _empirical risk minimization_ framework for function approximation.
We will then see its application to _fitted_ RL algorithms,
which attempt to learn the optimal value function (and the optimal policy) from a dataset of trajectories.

(erm)=
## Empirical risk minimization

The **supervised learning** task is as follows: We seek to learn the relationship
between some "input variables" $x$ and some output variable $y$.
Precisely, we want to find a function $\hat f : x \mapsto y$ that minimizes the
_squared error_ of the prediction:

$$
\hat f = \arg\min_{f} \E[(y - f(x))^2]
$$

An equivalent framing is that we seek to approximate the *conditional expectation* of $y$ given $x$:

:::{prf:theorem} Conditional expectation minimizes mean squared error
:label: conditional_expectation_minimizes_mse

$$
\arg\min_{f} \E[(y - f(x))^2] = (x \mapsto \E[y \mid x])
$$
:::

::::{prf:proof}
We can decompose the mean squared error as

$$
\begin{aligned}
\E[(y - f(x))^2] &= \E[ (y - \E[y \mid x] + \E[y \mid x] - f(x))^2 ] \\
&= \E[ (y - \E[y \mid x])^2 ] + \E[ (\E[y \mid x] - f(x))^2 ] + 2 \E[ (y - \E[y \mid x])(\E[y \mid x] - f(x)) ] \\
\end{aligned}
$$

:::{attention}
Use the law of iterated expectations to show that the last term is zero.
:::

The first term is the irreducible error, and the second term is the error due to the approximation,
which is minimized at $0$ when $f(x) = \E[y \mid x]$.
::::

In most applications, the joint distribution of $x, y$ is unknown or extremely complex, and so we can't
analytically evaluate $\E [y \mid x]$.
Instead, our strategy is to draw $N$ samples $(x_i, y_i)$ from the joint distribution of $x$ and $y$,
and then use the _sample average_ $\sum_{i=1}^N (y_i - f(x_i))^2 / N$ to approximate the mean squared error.
Then we use a _fitting method_ to find a function $\hat f$ that minimizes this objective
and thus approximates the conditional expectation.
This approach is called **empirical risk minimization**.

:::{prf:definition} Empirical risk minimization
:label: empirical_risk_minimization

Given a dataset of samples $(x_1, y_1), \dots, (x_N, y_N)$, empirical risk minimization seeks to find a function $f$ (from some class of functions $\mathcal{F}$) that minimizes the empirical risk:

$$
\hat f = \arg\min_{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2
$$

We will cover the details of the minimization process in {ref}`the next section <supervised_learning>`.
:::

:::{attention}
Why is it important that we constrain our search to a class of functions $\mathcal{F}$?

Hint: Consider the function $f(x) = \sum_{i=1}^N y_i \mathbb{1}_{\{ x = x_i \}}$. What is the empirical risk of this function? Would you consider it a good approximation of the conditional expectation?
:::

## Fitted value iteration

Let us apply ERM to the RL problem of computing the optimal policy / value function.

How did we compute the optimal value function in MDPs with _finite_ state and action spaces?

- In a {ref}`finite-horizon MDP <finite_horizon_mdps>`, we can use {prf:ref}`dynamic programming <pi_star_dp>`, working backwards from the end of the time horizon, to compute the optimal value function exactly.

- In an {ref}`infinite-horizon MDP <infinite_horizon_mdps>`, we can use {ref}`value iteration <value_iteration>`, which iterates the Bellman optimality operator {eq}`bellman_optimality_operator` to approximately compute the optimal value function.

Our existing approaches represent the value function, and the MDP itself,
in matrix notation.
But what happens if the state space is extremely large, or even infinite (e.g. real-valued)?
Then computing a weighted sum over all possible next states, which is required to compute the Bellman operator,
becomes intractable.

Instead, we will need to use *function approximation* methods from supervised learning to solve for the value function in an alternative way.

In particular, suppose we have a dataset of $N$ trajectories $\tau_1, \dots, \tau_N \sim \rho_{\pi}$ from some policy $\pi$ (called the **data collection policy**) acting in the MDP of interest.
Let us indicate the trajectory index in the superscript, so that

$$
\tau_i = \{ s_0^i, a_0^i, r_0^i, s_1^i, a_1^i, r_1^i, \dots, s_{\hor-1}^i, a_{\hor-1}^i, r_{\hor-1}^i \}.
$$

```{code-cell} ipython3
def collect_data(
    env: gym.Env, N: int, H: int, key: rand.PRNGKey, π: Optional[Policy] = None
) -> list[Trajectory]:
    """Collect a dataset of trajectories from the given policy (or a random one)."""
    trajectories = []
    keys = rand.split(key, N)
    for i in tqdm(range(N)):
        τ = []
        s, _ = env.reset(seed=rand.bits(keys[i]).item())
        for h in range(H):
            # sample from a random policy
            a = π(s, h) if π else env.action_space.sample()
            s_next, r, terminated, truncated, _ = env.step(a)
            τ.append(Transition(s, a, r))
            if terminated or truncated:
                break
            s = s_next
        trajectories.append(τ)
    return trajectories
\end{verbatim}

\texttt{\{code-cell\}\ ipython3\ env\ =\ gym.make("LunarLander-v2")\ trajectories\ =\ collect\_data(env,\ 100,\ 300,\ key)\ trajectories{[}0{]}{[}:5{]}\ \ \#\ show\ first\ five\ transitions\ from\ first\ trajectory}

We want to \emph{learn} the optimal value function (or Q-function) from
this dataset. Recall that we can characterize the optimal Q-function
using the
\{prf:ref\}\texttt{Bellman\ optimality\ equations\ \textless{}bellman\_consistency\_optimal\textgreater{}},
which don't depend on an actual policy:

\[
Q_\hi^\star(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [\max_{a'} Q_{\hi+1}^\star(s', a')]
\]

Can we view the dataset of trajectories as a ``labelled dataset'' in
order to apply supervised learning to approximate the optimal
Q-function? Yes! We can think of the arguments to the Q-function --
i.e.~the current state, action, and timestep \(\hi\) -- as the inputs
\(x\), and the r.h.s. of the above equation as the label \(f(x)\). Note
that the r.h.s. can also be expressed as a \textbf{conditional
expectation}:

\[
f(x) = \E [y \mid x] \quad \text{where} \quad y = r(s_\hi, a_\hi) + \max_{a'} Q^\star_{\hi + 1}(s', a').
\]

Approximating the conditional expectation is precisely the task that
\href{erm}{empirical risk minimization} is suited for!

Our above dataset would give us \(N \cdot \hor\) samples in the dataset:

\[
x_{i \hi} = (s_\hi^i, a_\hi^i, \hi) \qquad y_{i \hi} = r(s_\hi^i, a_\hi^i) + \max_{a'} Q^\star_{\hi + 1}(s_{\hi + 1}^i, a')
\]

```\{code-cell\} ipython3 def get\_X(trajectories:
list{[}Trajectory{]}): ``\,``\,'' We pass the state and timestep as
input to the Q-function and return an array of Q-values. ``\,``\,'' rows
= {[}(τ{[}h{]}.s, τ{[}h{]}.a, h) for τ in trajectories for h in
range(len(τ)){]} return {[}np.stack(ary) for ary in zip(*rows){]}

def get\_y( trajectories: list{[}Trajectory{]}, f:
Optional{[}QFunction{]} = None, π: Optional{[}Policy{]} = None, ):
``\,``\,'' Transform the dataset of trajectories into a dataset for
supervised learning. If \texttt{π} is None, instead estimates the
optimal Q function. Otherwise, estimates the Q function of π. ``\,``\,''
f = f or Q\_zero(get\_num\_actions(trajectories)) y = {[}{]} for τ in
trajectories: for h in range(len(τ) - 1): s, a, r = τ{[}h{]} Q\_values =
f(s, h + 1) y.append(r + (Q\_values{[}π(s, h + 1){]} if π else
Q\_values.max())) y.append(τ{[}-1{]}.r) return np.array(y)

\begin{verbatim}

```{code-cell} ipython3
s, a, h = get_X(trajectories[:1])
print("states:", s[:5])
print("actions:", a[:5])
print("timesteps:", h[:5])
\end{verbatim}

\texttt{\{code-cell\}\ ipython3\ get\_y(trajectories{[}:1{]}){[}:5{]}}

Then we can use empirical risk minimization to find a function
\(\hat f\) that approximates the optimal Q-function.

\texttt{\{code-cell\}\ ipython3\ \#\ We\ will\ see\ some\ examples\ of\ fitting\ methods\ in\ the\ next\ section\ FittingMethod\ =\ Callable{[}{[}Float{[}Array,\ "N\ D"{]},\ Float{[}Array,\ "N"{]}{]},\ QFunction{]}}

But notice that the definition of \(y_{i \hi}\) depends on the
Q-function itself! How can we resolve this circular dependency? Recall
that we faced the same issue \href{iterative_pe}{when evaluating a
policy in an infinite-horizon MDP}. There, we iterated the
\{prf:ref\}\texttt{Bellman\ operator\ \textless{}bellman\_operator\textgreater{}}
since we knew that the policy's value function was a fixed point of the
policy's Bellman operator. We can apply the same strategy here, using
the \(\hat f\) from the previous iteration to compute the labels
\(y_{i \hi}\), and then using this new dataset to fit the next iterate.

:::\{prf:algorithm\} Fitted Q-function iteration :label:
fitted\_q\_iteration

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize some function \(\hat f(s, a, h) \in \mathbb{R}\).
\item
  Iterate the following:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Generate a supervised learning dataset \(X, y\) from the
    trajectories and the current estimate \(f\), where the labels come
    from the r.h.s. of the Bellman optimality operator
    \{eq\}\texttt{bellman\_optimality\_operator}
  \item
    Set \(\hat f\) to the function that minimizes the empirical risk:

    \[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]
    :::
  \end{enumerate}
\end{enumerate}

\texttt{\{code-cell\}\ ipython3\ def\ fitted\_q\_iteration(\ \ \ \ \ trajectories:\ list{[}Trajectory{]},\ \ \ \ \ fit:\ FittingMethod,\ \ \ \ \ epochs:\ int,\ \ \ \ \ Q\_init:\ Optional{[}QFunction{]}\ =\ None,\ )\ -\textgreater{}\ QFunction:\ \ \ \ \ """\ \ \ \ \ Run\ fitted\ Q-function\ iteration\ using\ the\ given\ dataset.\ \ \ \ \ Returns\ an\ estimate\ of\ the\ optimal\ Q-function.\ \ \ \ \ """\ \ \ \ \ Q\_hat\ =\ Q\_init\ or\ Q\_zero(get\_num\_actions(trajectories))\ \ \ \ \ X\ =\ get\_X(trajectories)\ \ \ \ \ for\ \_\ in\ range(epochs):\ \ \ \ \ \ \ \ \ y\ =\ get\_y(trajectories,\ Q\_hat)\ \ \ \ \ \ \ \ \ Q\_hat\ =\ fit(X,\ y)\ \ \ \ \ return\ Q\_hat}

We can also use this fixed-point interation to \emph{evaluate} a policy
using the dataset (not necessarily the one used to generate the
trajectories):

:::\{prf:algorithm\} Fitted policy evaluation :label: fitted\_evaluation

\textbf{Input:} Policy
\(\pi : \mathcal{S} \times [H] \to \Delta(\mathcal{A})\) to be
evaluated.

\textbf{Output:} An approximation of the value function \(Q^\pi\) of the
policy.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize some function \(\hat f(s, a, h) \in \mathbb{R}\).
\item
  Iterate the following:

  \begin{enumerate}
  \def\labelenumii{\arabic{enumii}.}
  \item
    Generate a supervised learning dataset \(X, y\) from the
    trajectories and the current estimate \(f\), where the labels come
    from the r.h.s. of the
    \{prf:ref\}\texttt{Bellman\ consistency\ equation\ \textless{}bellman\_consistency\textgreater{}}
    for the given policy.
  \item
    Set \(\hat f\) to the function that minimizes the empirical risk:

    \[\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.\]
    :::
  \end{enumerate}
\end{enumerate}

\texttt{\{code-cell\}\ ipython3\ def\ fitted\_evaluation(\ \ \ \ \ trajectories:\ list{[}Trajectory{]},\ \ \ \ \ fit:\ FittingMethod,\ \ \ \ \ π:\ Policy,\ \ \ \ \ epochs:\ int,\ \ \ \ \ Q\_init:\ Optional{[}QFunction{]}\ =\ None,\ )\ -\textgreater{}\ QFunction:\ \ \ \ \ """\ \ \ \ \ Run\ fitted\ policy\ evaluation\ using\ the\ given\ dataset.\ \ \ \ \ Returns\ an\ estimate\ of\ the\ Q-function\ of\ the\ given\ policy.\ \ \ \ \ """\ \ \ \ \ Q\_hat\ =\ Q\_init\ or\ Q\_zero(get\_num\_actions(trajectories))\ \ \ \ \ X\ =\ get\_X(trajectories)\ \ \ \ \ for\ \_\ in\ tqdm(range(epochs)):\ \ \ \ \ \ \ \ \ y\ =\ get\_y(trajectories,\ Q\_hat,\ π)\ \ \ \ \ \ \ \ \ Q\_hat\ =\ fit(X,\ y)\ \ \ \ \ return\ Q\_hat}

Spot the difference between \texttt{fitted\_evaluation} and
\texttt{fitted\_q\_iteration}. (See the definition of \texttt{get\_y}.)
How would you modify this algorithm to evaluate the data collection
policy?

We can use this policy evaluation algorithm to adapt the
\{ref\}\texttt{policy\ iteration\ algorithm\ \textless{}policy\_iteration\textgreater{}}
to this new setting. The algorithm remains exactly the same --
repeatedly make the policy greedy w.r.t. its own value function --
except now we must evaluate the policy (i.e.~compute its value function)
using the iterative \texttt{fitted\_evaluation} algorithm.

\texttt{\{code-cell\}\ ipython3\ def\ fitted\_policy\_iteration(\ \ \ \ \ trajectories:\ list{[}Trajectory{]},\ \ \ \ \ fit:\ FittingMethod,\ \ \ \ \ epochs:\ int,\ \ \ \ \ evaluation\_epochs:\ int,\ \ \ \ \ π\_init:\ Optional{[}Policy{]}\ =\ lambda\ s,\ h:\ 0,\ \ \#\ constant\ zero\ policy\ ):\ \ \ \ \ """Run\ fitted\ policy\ iteration\ using\ the\ given\ dataset."""\ \ \ \ \ π\ =\ π\_init\ \ \ \ \ for\ \_\ in\ range(epochs):\ \ \ \ \ \ \ \ \ Q\_hat\ =\ fitted\_evaluation(trajectories,\ fit,\ π,\ evaluation\_epochs)\ \ \ \ \ \ \ \ \ π\ =\ q\_to\_greedy(Q\_hat)\ \ \ \ \ return\ π}

(supervised\_learning)= \#\# Supervised learning

This section will cover the details of implementing the \texttt{fit}
function above: That is, how to use a dataset of labelled samples
\((x_1, y_1), \dots, (x_N, y_N)\) to find a function \(f\) that
minimizes the empirical risk. This requires two ingredients:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A \textbf{function class} \(\mathcal{F}\) to search over
\item
  A \textbf{fitting method} for minimizing the empirical risk over this
  class
\end{enumerate}

The two main function classes we will cover are \textbf{linear models}
and \textbf{neural networks}. Both of these function classes are
\emph{parameterized} by some parameters \(\theta\), and the fitting
method will search over these parameters to minimize the empirical risk:

:::\{prf:definition\} Parameterized empirical risk minimization :label:
parameterized\_empirical\_risk\_minimization

Given a dataset of samples \((x_1, y_1), \dots, (x_N, y_N)\) and a class
of functions \(\mathcal{F}\) parameterized by \(\theta\), we to find a
parameter (vector) \(\hat \theta\) that minimizes the empirical risk:

\[
\hat \theta = \arg\min_{\theta} \frac{1}{N} \sum_{i=1}^N (y_i - f_\theta(x_i))^2
\] :::

The most common fitting method for parameterized models is
\textbf{gradient descent}.

:::\{prf:algorithm\} Gradient descent Letting
\(L(\theta) \in \mathbb{R}\) denote the empirical risk in terms of the
parameters, the gradient descent algorithm updates the parameters
according to the rule

\[
\theta^{t+1} = \theta^t - \eta \nabla_\theta L(\theta^t)
\]

where \(\eta > 0\) is the \textbf{learning rate}. :::

```\{code-cell\} ipython3 Params = Float{[}Array, ``D''{]}

def gradient\_descent( loss: Callable{[}{[}Params{]}, float{]}, θ\_init:
Params, η: float, epochs: int, ): ``\,``\,'' Run gradient descent to
minimize the given loss function (expressed in terms of the parameters).
``\,``\,'' θ = θ\_init for \_ in range(epochs): θ = θ - η *
grad(loss)(θ) return θ

\begin{verbatim}

### Linear regression

In linear regression, we assume that the function $f$ is linear in the parameters:

$$
\mathcal{F} = \{ x \mapsto \theta^\top x \mid \theta \in \mathbb{R}^D \}
$$

This function class is extremely simple and only contains linear functions.
To expand its expressivity, we can _transform_ the input $x$ using some feature function $\phi$,
i.e. $\widetilde x = \phi(x)$, and then fit a linear model in the transformed space instead.

```{code-cell} ipython3
def fit_linear(X: Float[Array, "N D"], y: Float[Array, "N"], φ=lambda x: x):
    """Fit a linear model to the given dataset using ordinary least squares."""
    X = vmap(φ)(X)
    θ = np.linalg.lstsq(X, y, rcond=None)[0]
    return lambda x: np.dot(φ(x), θ)
\end{verbatim}

\subsubsection{Neural networks}\label{neural-networks}

In neural networks, we assume that the function \(f\) is a composition
of linear functions (represented by matrices \(W_i\)) and non-linear
activation functions (denoted by \(\sigma\)):

\[
\mathcal{F} = \{ x \mapsto \sigma(W_L \sigma(W_{L-1} \dots \sigma(W_1 x + b_1) \dots + b_{L-1}) + b_L) \}
\]

where \(W_i \in \mathbb{R}^{D_{i+1} \times D_i}\) and
\(b_i \in \mathbb{R}^{D_{i+1}}\) are the parameters of the \(i\)-th
layer, and \(\sigma\) is the activation function.

This function class is much more expressive and contains many more
parameters. This makes it more susceptible to overfitting on smaller
datasets, but also allows it to represent more complex functions. In
practice, however, neural networks exhibit interesting phenomena during
training, and are often able to generalize well even with many
parameters.

Another reason for their popularity is the efficient
\textbf{backpropagation} algorithm for computing the gradient of the
empirical risk with respect to the parameters. Essentially, the
hierarchical structure of the neural network, i.e.~computing the output
of the network as a composition of functions, allows us to use the chain
rule to compute the gradient of the output with respect to the
parameters of each layer.

\{cite\}\texttt{nielsen\_neural\_2015} provides a comprehensive
introduction to neural networks and backpropagation.
