
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>3. Finite Markov Decision Processes &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '3_mdps/mdps';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="4. Fitted dynamic programming algorithms" href="../4_fitted_dp/fitted_dp.html" />
    <link rel="prev" title="2. Bandits" href="../2_bandits/bandits.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="../_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../1_intro/intro.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_bandits/bandits.html">2. Bandits</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_fitted_dp/fitted_dp.html">4. Fitted dynamic programming algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_control/control.html">5. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">6. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/3_mdps/mdps.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Finite Markov Decision Processes</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-horizon-mdps">3.1. Finite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policies">3.1.1. Policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectories">3.1.2. Trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">3.1.3. Value functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-policies">3.1.3.1. Greedy policies</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-one-step-bellman-consistency-equation">3.1.4. The one-step (Bellman) consistency equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-one-step-bellman-operator">3.1.5. The one-step Bellman operator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-finite-horizon-mdps">3.2. Solving finite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-in-finite-horizon-mdps">3.2.1. Policy evaluation in finite-horizon MDPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-policies-in-finite-horizon-mdps">3.2.2. Optimal policies in finite-horizon MDPs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">3.3. Infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discounted-rewards">3.3.1. Discounted rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-policies">3.3.2. Stationary policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions-and-bellman-consistency">3.3.3. Value functions and Bellman consistency</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-infinite-horizon-mdps">3.4. Solving infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bellman-operator-is-a-contraction-mapping">3.4.1. The Bellman operator is a contraction mapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-in-infinite-horizon-mdps">3.4.2. Policy evaluation in infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-inversion-for-deterministic-policies">3.4.2.1. Matrix inversion for deterministic policies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-policy-evaluation">3.4.2.2. Iterative policy evaluation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-policies-in-infinite-horizon-mdps">3.4.3. Optimal policies in infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">3.4.3.1. Value iteration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">3.4.3.2. Policy iteration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.5. Summary</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="finite-markov-decision-processes">
<span id="mdps"></span><h1><span class="section-number">3. </span>Finite Markov Decision Processes<a class="headerlink" href="#finite-markov-decision-processes" title="Link to this heading">#</a></h1>
<nav class="contents local" id="contents">
<ul class="simple">
<li><p><a class="reference internal" href="#finite-horizon-mdps" id="id5">Finite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#policies" id="id6">Policies</a></p></li>
<li><p><a class="reference internal" href="#trajectories" id="id7">Trajectories</a></p></li>
<li><p><a class="reference internal" href="#value-functions" id="id8">Value functions</a></p>
<ul>
<li><p><a class="reference internal" href="#greedy-policies" id="id9">Greedy policies</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-one-step-bellman-consistency-equation" id="id10">The one-step (Bellman) consistency equation</a></p></li>
<li><p><a class="reference internal" href="#the-one-step-bellman-operator" id="id11">The one-step Bellman operator</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#solving-finite-horizon-mdps" id="id12">Solving finite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#policy-evaluation-in-finite-horizon-mdps" id="id13">Policy evaluation in finite-horizon MDPs</a></p></li>
<li><p><a class="reference internal" href="#optimal-policies-in-finite-horizon-mdps" id="id14">Optimal policies in finite-horizon MDPs</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#infinite-horizon-mdps" id="id15">Infinite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#discounted-rewards" id="id16">Discounted rewards</a></p></li>
<li><p><a class="reference internal" href="#stationary-policies" id="id17">Stationary policies</a></p></li>
<li><p><a class="reference internal" href="#value-functions-and-bellman-consistency" id="id18">Value functions and Bellman consistency</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#solving-infinite-horizon-mdps" id="id19">Solving infinite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#the-bellman-operator-is-a-contraction-mapping" id="id20">The Bellman operator is a contraction mapping</a></p></li>
<li><p><a class="reference internal" href="#policy-evaluation-in-infinite-horizon-mdps" id="id21">Policy evaluation in infinite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#matrix-inversion-for-deterministic-policies" id="id22">Matrix inversion for deterministic policies</a></p></li>
<li><p><a class="reference internal" href="#iterative-policy-evaluation" id="id23">Iterative policy evaluation</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#optimal-policies-in-infinite-horizon-mdps" id="id24">Optimal policies in infinite-horizon MDPs</a></p>
<ul>
<li><p><a class="reference internal" href="#value-iteration" id="id25">Value iteration</a></p></li>
<li><p><a class="reference internal" href="#policy-iteration" id="id26">Policy iteration</a></p></li>
</ul>
</li>
</ul>
</li>
<li><p><a class="reference internal" href="#summary" id="id27">Summary</a></p></li>
</ul>
</nav>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">NamedTuple</span><span class="p">,</span> <span class="n">Optional</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Float</span><span class="p">,</span> <span class="n">Int</span><span class="p">,</span> <span class="n">Array</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">partial</span>
</pre></div>
</div>
</div>
</details>
</div>
<p>The field of RL studies how an agent can learn to make sequential
decisions in an interactive environment. This is a very general problem!
How can we <em>formalize</em> this task in a way that is both <em>sufficiently
general</em> yet also tractable enough for <em>fruitful analysis</em>?</p>
<p>Let’s consider some examples of sequential decision problems to identify
the key common properties we’d like to capture:</p>
<ul class="simple">
<li><p><strong>Board games</strong> like chess or Go, where the player takes turns with
the opponent to make moves on the board.</p></li>
<li><p><strong>Video games</strong> like Super Mario Bros or Breakout, where the player
controls a character to reach the goal.</p></li>
<li><p><strong>Robotic control</strong>, where the robot can move and interact with the
real-world environment to complete some task.</p></li>
</ul>
<p>All of these fit into the RL framework. Furthermore, these are
environments where the <strong>state transitions</strong>, the “rules” of the
environment, only depend on the <em>most recent</em> state and action. This is
called the <strong>Markov property</strong>.</p>
<div class="proof definition admonition" id="markov">
<p class="admonition-title"><span class="caption-number">Definition 3.1 </span> (Markov property)</p>
<section class="definition-content" id="proof-content">
<p>An interactive environment satisfies the <strong>Markov property</strong> if the
probability of transitioning to a new state only depends on the current
state and action:</p>
<div class="math notranslate nohighlight">
\[\P(s_{\hi+1} \mid s_0, a_0, \dots, s_\hi, a_\hi) = P(s_{\hi+1} \mid s_\hi, a_\hi)\]</div>
<p>where <span class="math notranslate nohighlight">\(P : \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})\)</span> describes the state transitions.
(We’ll elaborate on this notation later in the chapter.)</p>
</section>
</div><p>We’ll see that this simple assumption leads to a rich set of problems
and algorithms. Environments with the Markov property are called
<strong>Markov decision processes</strong> (MDPs) and will be the focus of this
chapter.</p>
<p><strong>Exercise:</strong> What information might be encoded in the state for each of
the above examples? What might the valid set of actions be? Describe the
state transitions heuristically and verify that they satisfy the Markov
property.</p>
<p>MDPs are usually classified as <strong>finite-horizon</strong>, where the
interactions end after some finite number of time steps, or
<strong>infinite-horizon</strong>, where the interactions can continue indefinitely.
We’ll begin with the finite-horizon case and discuss the
infinite-horizon case in the second half of the chapter.</p>
<p>In each setting, we’ll describe how to evaluate different <strong>policies</strong>
(strategies for choosing actions) and how to compute (or approximate)
the <strong>optimal policy</strong> for a given MDP. We’ll introduce the <strong>Bellman
consistency condition</strong>, which allows us to analyze the whole series of
interactions in terms of individual timesteps.</p>
<section id="finite-horizon-mdps">
<h2><a class="toc-backref" href="#id5" role="doc-backlink"><span class="section-number">3.1. </span>Finite-horizon MDPs</a><a class="headerlink" href="#finite-horizon-mdps" title="Link to this heading">#</a></h2>
<div class="proof definition admonition" id="finite_mdp">
<p class="admonition-title"><span class="caption-number">Definition 3.2 </span> (Finite-horizon Markov decision process)</p>
<section class="definition-content" id="proof-content">
<p>The components of a finite-horizon Markov decision process are:</p>
<ol class="arabic simple">
<li><p>The <strong>state</strong> that the agent interacts with. We use <span class="math notranslate nohighlight">\(\mathcal{S}\)</span> to denote
the set of possible states, called the <strong>state space</strong>.</p></li>
<li><p>The <strong>actions</strong> that the agent can take. We use <span class="math notranslate nohighlight">\(\mathcal{A}\)</span> to denote the
set of possible actions, called the <strong>action space</strong>.</p></li>
<li><p>Some <strong>initial state distribution</strong> <span class="math notranslate nohighlight">\(\mu \in \Delta(\mathcal{S})\)</span>.</p></li>
<li><p>The <strong>state transitions</strong> (a.k.a. <strong>dynamics</strong>)
<span class="math notranslate nohighlight">\(P : \mathcal{S} \times \mathcal{A} \to \Delta(\mathcal{S})\)</span> that describe what state the agent
transitions to after taking an action.</p></li>
<li><p>The <strong>reward</strong> signal. In this course we’ll take it to be a
deterministic function on state-action pairs,
<span class="math notranslate nohighlight">\(r : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>, but in general many results will
extend to a <em>stochastic</em> reward signal.</p></li>
<li><p>A time horizon <span class="math notranslate nohighlight">\(H \in \mathbb{N}\)</span> that specifies the number of
interactions in an <strong>episode</strong>.</p></li>
</ol>
<p>Combined together, these objects specify a finite-horizon Markov
decision process:</p>
<div class="math notranslate nohighlight">
\[M = (\mathcal{S}, \mathcal{A}, \mu, P, r, H).\]</div>
<p>When there are <strong>finitely</strong> many states and actions, i.e.
<span class="math notranslate nohighlight">\(|\mathcal{S}|, |\mathcal{A}| &lt; \infty\)</span>, we can express
the relevant quantities as vectors and matrices (i.e. <em>tables</em> of
values):</p>
<div class="math notranslate nohighlight">
\[
\begin{aligned}
    r &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|} &amp;
    P &amp;\in [0, 1]^{(|\mathcal{S} \times \mathcal{A}|) \times |\mathcal{S}|} &amp;
    \mu &amp;\in [0, 1]^{|\mathcal{S}|}
\end{aligned}
\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Verify that these types make sense!</p>
</div>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MDP</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">S</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># number of states</span>
    <span class="n">A</span><span class="p">:</span> <span class="nb">int</span>  <span class="c1"># number of actions</span>
    <span class="n">μ</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]</span>
    <span class="n">P</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A S&quot;</span><span class="p">]</span>
    <span class="n">r</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]</span>
    <span class="n">H</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">γ</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span>  <span class="c1"># discount factor (used later)</span>
</pre></div>
</div>
</div>
</div>
<div class="proof example admonition" id="tidy_mdp">
<p class="admonition-title"><span class="caption-number">Example 3.1 </span> (Tidying MDP)</p>
<section class="example-content" id="proof-content">
<p>Let’s consider an extremely simple decision problem throughout this
chapter: the task of keeping your room tidy!</p>
<p>Your room has the possible states
<span class="math notranslate nohighlight">\(\mathcal{S} = \{ \text{orderly}, \text{messy} \}\)</span>. You can take either
of the actions <span class="math notranslate nohighlight">\(\mathcal{A} = \{ \text{ignore}, \text{tidy} \}\)</span>. The room starts
off orderly.</p>
<p>The <strong>state transitions</strong> are as follows: if you tidy the room, it becomes
(or remains) orderly; if you ignore the room, it <em>might</em> become messy (see table
below).</p>
<p>The <strong>rewards</strong> are as follows: You get penalized for tidying an orderly
room (a waste of time) or ignoring a messy room, but you get rewarded
for ignoring an orderly room (since you can enjoy). Tidying a messy room
is a chore that gives no reward.</p>
<p>These are summarized in the following table:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{array}{ccccc}
    s &amp; a &amp; P(\text{orderly} \mid s, a) &amp; P(\text{messy} \mid s, a) &amp; r(s, a) \\
    \text{orderly} &amp; \text{ignore} &amp; 0.7 &amp; 0.3 &amp; 1 \\
    \text{orderly} &amp; \text{tidy} &amp; 1 &amp; 0 &amp; -1 \\
    \text{messy} &amp; \text{ignore} &amp; 0 &amp; 1 &amp; -1 \\
    \text{messy} &amp; \text{tidy} &amp; 1 &amp; 0 &amp; 0 \\
\end{array}\end{split}\]</div>
<p>Consider a time horizon of <span class="math notranslate nohighlight">\(H = 7\)</span> days (one interaction per day). Let
<span class="math notranslate nohighlight">\(t = 0\)</span> correspond to Monday and <span class="math notranslate nohighlight">\(t = 6\)</span> correspond to Sunday.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tidy_mdp</span> <span class="o">=</span> <span class="n">MDP</span><span class="p">(</span>
    <span class="n">S</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># 0 = orderly, 1 = messy</span>
    <span class="n">A</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>  <span class="c1"># 0 = ignore, 1 = tidy</span>
    <span class="n">μ</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]),</span>  <span class="c1"># start in orderly state</span>
    <span class="n">P</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span>
            <span class="p">[</span><span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span>  <span class="c1"># orderly, ignore</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>  <span class="c1"># orderly, tidy</span>
        <span class="p">],</span>
        <span class="p">[</span>
            <span class="p">[</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">],</span>  <span class="c1"># messy, ignore</span>
            <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">],</span>  <span class="c1"># messy, tidy</span>
        <span class="p">],</span>
    <span class="p">]),</span>
    <span class="n">r</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="p">[</span>
            <span class="mf">1.0</span><span class="p">,</span>   <span class="c1"># orderly, ignore</span>
            <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># orderly, tidy</span>
        <span class="p">],</span>
        <span class="p">[</span>
            <span class="o">-</span><span class="mf">1.0</span><span class="p">,</span>  <span class="c1"># messy, ignore</span>
            <span class="mf">0.0</span><span class="p">,</span>   <span class="c1"># messy, tidy</span>
        <span class="p">],</span>
    <span class="p">]),</span>
    <span class="n">H</span><span class="o">=</span><span class="mi">7</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="policies">
<h3><a class="toc-backref" href="#id6" role="doc-backlink"><span class="section-number">3.1.1. </span>Policies</a><a class="headerlink" href="#policies" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="policy">
<p class="admonition-title"><span class="caption-number">Definition 3.3 </span> (Policies)</p>
<section class="definition-content" id="proof-content">
<p>A <strong>policy</strong> <span class="math notranslate nohighlight">\(\pi\)</span> describes the agent’s strategy: which actions it
takes in a given situation. A key goal of RL is to find the <strong>optimal
policy</strong> that maximizes the total reward on average.</p>
<p>There are three axes along which policies can vary: their outputs,
inputs, and time-dependence. We’ll discuss each of these in turn.</p>
<ol class="arabic simple">
<li><p><strong>Deterministic or stochastic.</strong> A deterministic policy outputs
actions while a stochastic policy outputs <em>distributions</em> over
actions.</p></li>
<li><p><strong>State-dependent or history-dependent.</strong> A state-dependent (a.k.a.
“Markovian”) policy only depends on the current state, while a
history-dependent policy depends on the sequence of past states,
actions, and rewards. We’ll only consider state-dependent policies
in this course.</p></li>
<li><p><strong>Stationary or time-dependent.</strong> A stationary policy remains the
same function at all time steps, while a time-dependent policy
<span class="math notranslate nohighlight">\(\pi = \{ \pi_0, \dots, \pi_{H-1} \}\)</span> specifies a different function
<span class="math notranslate nohighlight">\(\pi_\hi\)</span> at each time step <span class="math notranslate nohighlight">\(\hi\)</span>.</p></li>
</ol>
</section>
</div><p>Note that for finite state and action spaces,
we can represent a randomized mapping <span class="math notranslate nohighlight">\(\mathcal{S} \to \Delta(\mathcal{A})\)</span>
as a matrix <span class="math notranslate nohighlight">\(\pi \in [0, 1]^{\mathcal{S}, \mathcal{A}}\)</span> where each row describes
the policy’s distribution over actions for the corresponding state.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># In code, we use the `Policy` type to represent a randomized mapping from states to actions.</span>
<span class="c1"># In the finite-horizon case, an array of `H` of these, one for at each time step,</span>
<span class="c1"># would constitute a time-dependent policy.</span>
<span class="n">Policy</span> <span class="o">=</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>A fascinating result is that every finite-horizon MDP has an optimal
deterministic time-dependent policy! Intuitively, the Markov property
implies that the current state contains all the information we need to
make the optimal decision. We’ll prove this result constructively later
in the chapter.</p>
<div class="proof example admonition" id="tidy_policy">
<p class="admonition-title"><span class="caption-number">Example 3.2 </span> (Tidying policies)</p>
<section class="example-content" id="proof-content">
<p>Here are some possible policies for the tidying MDP <a class="reference internal" href="#tidy_mdp">Example 3.1</a>:</p>
<ul class="simple">
<li><p>Always tidy: <span class="math notranslate nohighlight">\(\pi(s) = \text{tidy}\)</span>.</p></li>
<li><p>Only tidy on weekends: <span class="math notranslate nohighlight">\(\pi_\hi(s) = \text{tidy}\)</span> if
<span class="math notranslate nohighlight">\(\hi \in \{ 5, 6 \}\)</span> and <span class="math notranslate nohighlight">\(\pi_\hi(s) = \text{ignore}\)</span> otherwise.</p></li>
<li><p>Only tidy if the room is messy: <span class="math notranslate nohighlight">\(\pi_\hi(\text{messy}) = \text{tidy}\)</span>
and <span class="math notranslate nohighlight">\(\pi_\hi(\text{orderly}) = \text{ignore}\)</span> for all <span class="math notranslate nohighlight">\(\hi\)</span>.</p></li>
</ul>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># arrays of shape (H, S, A) represent time-dependent policies</span>
<span class="n">tidy_policy_always_tidy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">tidy_policy_weekends</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">5</span><span class="p">:</span><span class="mi">7</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">tidy_policy_messy_only</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">7</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mf">1.0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="trajectories">
<h3><a class="toc-backref" href="#id7" role="doc-backlink"><span class="section-number">3.1.2. </span>Trajectories</a><a class="headerlink" href="#trajectories" title="Link to this heading">#</a></h3>
<div class="proof definition admonition" id="trajectory">
<p class="admonition-title"><span class="caption-number">Definition 3.4 </span> (Trajectories)</p>
<section class="definition-content" id="proof-content">
<p>A sequence of states, actions, and rewards is called a <strong>trajectory</strong>:</p>
<div class="math notranslate nohighlight">
\[\tau = (s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1})\]</div>
<p>where
<span class="math notranslate nohighlight">\(r_\hi = r(s_\hi, a_\hi)\)</span>. (Note that sources differ as to whether to include
the reward at the final time step. This is a minor detail.)</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Transition</span><span class="p">(</span><span class="n">NamedTuple</span><span class="p">):</span>
    <span class="n">s</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">a</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">r</span><span class="p">:</span> <span class="nb">float</span>

<span class="n">Trajectory</span> <span class="o">=</span> <span class="nb">list</span><span class="p">[</span><span class="n">Transition</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Once we’ve chosen a policy, we can sample trajectories by repeatedly
choosing actions according to the policy, transitioning according to the
state transitions, and observing the rewards. That is, a policy induces
a distribution <span class="math notranslate nohighlight">\(\rho^{\pi}\)</span> over trajectories. (We assume that <span class="math notranslate nohighlight">\(\mu\)</span> and
<span class="math notranslate nohighlight">\(P\)</span> are clear from context.)</p>
<div class="proof example admonition" id="tidy_traj">
<p class="admonition-title"><span class="caption-number">Example 3.3 </span> (Trajectories in the tidying environment)</p>
<section class="example-content" id="proof-content">
<p>Here is a possible trajectory for the tidying example:</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p><span class="math notranslate nohighlight">\(t\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(0\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(1\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(2\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(3\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(4\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(5\)</span></p></th>
<th class="head text-center"><p><span class="math notranslate nohighlight">\(6\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(s\)</span></p></td>
<td class="text-center"><p>orderly</p></td>
<td class="text-center"><p>orderly</p></td>
<td class="text-center"><p>orderly</p></td>
<td class="text-center"><p>messy</p></td>
<td class="text-center"><p>messy</p></td>
<td class="text-center"><p>orderly</p></td>
<td class="text-center"><p>orderly</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(a\)</span></p></td>
<td class="text-center"><p>tidy</p></td>
<td class="text-center"><p>ignore</p></td>
<td class="text-center"><p>ignore</p></td>
<td class="text-center"><p>ignore</p></td>
<td class="text-center"><p>tidy</p></td>
<td class="text-center"><p>ignore</p></td>
<td class="text-center"><p>ignore</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(r\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(-1\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(-1\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(0\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(1\)</span></p></td>
</tr>
</tbody>
</table>
<p>Could any of the policies in <a class="reference internal" href="#tidy_policy">Example 3.2</a> have generated this trajectory?</p>
</section>
</div><p>Note that for a state-dependent policy, using the Markov property <a class="reference internal" href="#markov">Definition 3.1</a>, we can specify this probability distribution in
an <strong>autoregressive</strong> way (i.e. one timestep at a time):</p>
<div class="proof definition admonition" id="autoregressive_trajectories">
<p class="admonition-title"><span class="caption-number">Definition 3.5 </span> (Autoregressive trajectory distribution)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\rho^{\pi}(\tau) := \mu(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots P(s_{H-1} \mid s_{H-2}, a_{H-2}) \pi_{H-1}(a_{H-1} \mid s_{H-1})\]</div>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">trajectory_log_likelihood</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">τ</span><span class="p">:</span> <span class="n">Trajectory</span><span class="p">,</span> <span class="n">π</span><span class="p">:</span> <span class="n">Policy</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the log likelihood of a trajectory under a given MDP and policy.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">μ</span><span class="p">[</span><span class="n">τ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">])</span>
    <span class="n">total</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">π</span><span class="p">[</span><span class="n">τ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">mdp</span><span class="o">.</span><span class="n">H</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">τ</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">])</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">π</span><span class="p">[</span><span class="n">τ</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">s</span><span class="p">,</span> <span class="n">τ</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">a</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">total</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>How would you modify this to include stochastic rewards?</p>
</div>
<p>For a deterministic policy <span class="math notranslate nohighlight">\(\pi\)</span>, we have that
<span class="math notranslate nohighlight">\(\pi_\hi(a \mid s) = \mathbb{I}[a = \pi_\hi(s)]\)</span>; that is, the probability
of taking an action is <span class="math notranslate nohighlight">\(1\)</span> if it’s the unique action prescribed by the
policy for that state and <span class="math notranslate nohighlight">\(0\)</span> otherwise. In this case, the only
randomness in sampling trajectories comes from the initial state
distribution <span class="math notranslate nohighlight">\(\mu\)</span> and the state transitions <span class="math notranslate nohighlight">\(P\)</span>.</p>
</section>
<section id="value-functions">
<h3><a class="toc-backref" href="#id8" role="doc-backlink"><span class="section-number">3.1.3. </span>Value functions</a><a class="headerlink" href="#value-functions" title="Link to this heading">#</a></h3>
<p>The main goal of RL is to find a policy that maximizes the average total
reward <span class="math notranslate nohighlight">\(r_0 + \cdots + r_{H-1}\)</span>. (Note that this is a random variable
that depends on the policy.) Let’s introduce some notation for analyzing
this quantity.</p>
<p>A policy’s <strong>value function</strong> at time <span class="math notranslate nohighlight">\(h\)</span> is its expected remaining reward <em>from a given state</em>:</p>
<div class="proof definition admonition" id="value">
<p class="admonition-title"><span class="caption-number">Definition 3.6 </span> (Value function)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight">
\[V_\hi^\pi(s) := \E_{\tau \sim \rho^\pi} [r_\hi + \cdots + r_{H-1} \mid s_\hi = s]\]</div>
</section>
</div><p>Similarly, we can define the <strong>action-value function</strong> (aka the
<strong>Q-function</strong>) at time <span class="math notranslate nohighlight">\(h\)</span> as the expected remaining reward <em>from a given state and taking a given action</em>:</p>
<div class="proof definition admonition" id="action_value">
<p class="admonition-title"><span class="caption-number">Definition 3.7 </span> (Action-value function)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight">
\[Q_\hi^\pi(s, a) := \E_{\tau \sim \rho^\pi} [r_\hi + \cdots + r_{H-1} \mid s_\hi = s, a_\hi = a]\]</div>
</section>
</div><p>Note that the value function is just the expected action-value over
actions drawn from the policy:</p>
<div class="math notranslate nohighlight">
\[V_\hi^\pi(s) = \E_{a \sim \pi_\hi(s)} [Q_\hi^\pi(s, a)]\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_to_v</span><span class="p">(</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">],</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the value function for a given policy in a known finite MDP</span>
<span class="sd">    at a single timestep from its action-value function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">policy</span> <span class="o">*</span> <span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>and the
action-value can be expressed in terms of the value of the following
state:</p>
<div class="math notranslate nohighlight">
\[Q_\hi^\pi(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [V_{\hi+1}^\pi(s')]\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">v_to_q</span><span class="p">(</span>
    <span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the action-value function in a known finite MDP</span>
<span class="sd">    at a single timestep from the corresponding value function.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># the discount factor is relevant later</span>
    <span class="k">return</span> <span class="n">mdp</span><span class="o">.</span><span class="n">r</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span> <span class="o">@</span> <span class="n">v</span>

<span class="n">v_ary_to_q_ary</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">v_to_q</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="greedy-policies">
<h4><a class="toc-backref" href="#id9" role="doc-backlink"><span class="section-number">3.1.3.1. </span>Greedy policies</a><a class="headerlink" href="#greedy-policies" title="Link to this heading">#</a></h4>
<p>For any given <span class="math notranslate nohighlight">\(q \in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}\)</span>, we can define the <strong>greedy policy</strong> <span class="math notranslate nohighlight">\(\hat \pi_q\)</span> as the policy that selects the action with the highest <span class="math notranslate nohighlight">\(q\)</span>-value at each state:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">q_to_greedy</span><span class="p">(</span><span class="n">q</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get the (deterministic) greedy policy w.r.t. an action-value function.</span>
<span class="sd">    Return the policy as a matrix of shape (S, A) where each row is a one-hot vector.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="k">def</span> <span class="nf">v_to_greedy</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Get the (deterministic) greedy policy w.r.t. a value function.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">q_to_greedy</span><span class="p">(</span><span class="n">v_to_q</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="the-one-step-bellman-consistency-equation">
<span id="bellman-consistency"></span><h3><a class="toc-backref" href="#id10" role="doc-backlink"><span class="section-number">3.1.4. </span>The one-step (Bellman) consistency equation</a><a class="headerlink" href="#the-one-step-bellman-consistency-equation" title="Link to this heading">#</a></h3>
<p>Note that by simply considering the cumulative reward as the sum of the
<em>current</em> reward and the <em>future</em> cumulative reward, we can describe the
value function recursively (in terms of itself). This is named the
<strong>Bellman consistency equation</strong> after <strong>Richard Bellman</strong> (1920–1984),
who is credited with introducing dynamic programming in 1953.</p>
<div class="proof theorem admonition" id="bellman_consistency">
<p class="admonition-title"><span class="caption-number">Theorem 3.1 </span> (Bellman consistency equation for the value function)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\begin{split}V_\hi^\pi(s) = \E_{\substack{a \sim \pi_\hi(s) \\ s' \sim P(s, a)}} [r(s, a) + V_{\hi+1}^\pi(s')]\end{split}\]</div>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check_bellman_consistency_v</span><span class="p">(</span>
    <span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;H S A&quot;</span><span class="p">],</span>
    <span class="n">v_ary</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;H S&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Check that the given (time-dependent) &quot;value function&quot;</span>
<span class="sd">    satisfies the Bellman consistency equation.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">all</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span>
            <span class="c1"># lhs</span>
            <span class="n">v_ary</span><span class="p">[</span><span class="n">h</span><span class="p">],</span>
            <span class="c1"># rhs</span>
            <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">policy</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">r</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span> <span class="o">@</span> <span class="n">v_ary</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span>
        <span class="p">)</span>
        <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">H</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Verify that this equation holds by expanding <span class="math notranslate nohighlight">\(V_\hi^\pi(s)\)</span>
and <span class="math notranslate nohighlight">\(V_{\hi+1}^\pi(s')\)</span>.</p>
</div>
<p>One can analogously derive the Bellman consistency equation for the
action-value function:</p>
<div class="proof theorem admonition" id="bellman_consistency_action">
<p class="admonition-title"><span class="caption-number">Theorem 3.2 </span> (Bellman consistency equation for action-values)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\begin{split}Q_\hi^\pi(s, a) = r(s, a) + \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi_{\hi+1}(s')}} [Q_{\hi+1}^\pi(s', a')]\end{split}\]</div>
</section>
</div><div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Write a <code class="docutils literal notranslate"><span class="pre">check_bellman_consistency_q</span></code> function for the action-value function.</p>
</div>
<div class="proof remark admonition" id="bellman_det">
<p class="admonition-title"><span class="caption-number">Remark 3.1 </span> (The Bellman consistency equation for deterministic policies)</p>
<section class="remark-content" id="proof-content">
<p>Note that for deterministic policies, the Bellman consistency equation
simplifies to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V_\hi^\pi(s) &amp;= r(s, \pi_\hi(s)) + \E_{s' \sim P(s, \pi_\hi(s))} [V_{\hi+1}^\pi(s')] \\
    Q_\hi^\pi(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [Q_{\hi+1}^\pi(s', \pi_{\hi+1}(s'))]
\end{aligned}
\end{split}\]</div>
</section>
</div></section>
<section id="the-one-step-bellman-operator">
<span id="bellman-operator"></span><h3><a class="toc-backref" href="#id11" role="doc-backlink"><span class="section-number">3.1.5. </span>The one-step Bellman operator</a><a class="headerlink" href="#the-one-step-bellman-operator" title="Link to this heading">#</a></h3>
<p>Fix a policy <span class="math notranslate nohighlight">\(\pi\)</span>. Consider the higher-order operator that takes in a
“value function” <span class="math notranslate nohighlight">\(v : \mathcal{S} \to \mathbb{R}\)</span> and returns the r.h.s. of the Bellman
equation for that “value function”:</p>
<div class="proof definition admonition" id="bellman_operator">
<p class="admonition-title"><span class="caption-number">Definition 3.8 </span> (Bellman operator)</p>
<section class="definition-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\begin{split}[\mathcal{J}^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v(s')].\end{split}\]</div>
</section>
</div><div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bellman_operator</span><span class="p">(</span>
    <span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">],</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Looping definition of the Bellman operator.</span>
<span class="sd">    Concise version is below</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">v_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">A</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">s_next</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">):</span>
                <span class="n">v_new</span><span class="p">[</span><span class="n">s</span><span class="p">]</span> <span class="o">+=</span> <span class="n">policy</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">s_next</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">s</span><span class="p">,</span> <span class="n">a</span><span class="p">]</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">v</span><span class="p">[</span><span class="n">s_next</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">v_new</span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bellman_operator</span><span class="p">(</span>
    <span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span>
    <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">],</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">],</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;For a known finite MDP, the Bellman operator can be exactly evaluated.&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">policy</span> <span class="o">*</span> <span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">r</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span> <span class="o">@</span> <span class="n">v</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">q_to_v</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">v_to_q</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">v</span><span class="p">))</span>  <span class="c1"># equivalent</span>
</pre></div>
</div>
</div>
</div>
<p>We’ll call <span class="math notranslate nohighlight">\(\mathcal{J}^\pi : (\mathcal{S} \to \mathbb{R}) \to (\mathcal{S} \to \mathbb{R})\)</span> the <strong>Bellman
operator</strong> of <span class="math notranslate nohighlight">\(\pi\)</span>. Note that it’s defined on any “value function”
mapping states to real numbers; <span class="math notranslate nohighlight">\(v\)</span> doesn’t have to be a well-defined
value function for some policy (hence the lowercase notation). The
Bellman operator also gives us a concise way to express the Bellman
consistency equation <a class="reference internal" href="#bellman_consistency">Theorem 3.1</a> for the value function:</p>
<div class="math notranslate nohighlight">
\[V_\hi^\pi = \mathcal{J}^{\pi}(V_{\hi+1}^\pi)\]</div>
<p>Intuitively, the output of the Bellman operator, a new “value function”,
evaluates states as follows: from a given state, take one action
according to <span class="math notranslate nohighlight">\(\pi\)</span>, observe the reward, and then evaluate the next state
using the input “value function”.</p>
<p>When we discuss infinite-horizon MDPs, the Bellman operator will turn
out to be more than just a notational convenience: We’ll use it to
construct algorithms for computing the optimal policy.</p>
</section>
</section>
<section id="solving-finite-horizon-mdps">
<span id="id1"></span><h2><a class="toc-backref" href="#id12" role="doc-backlink"><span class="section-number">3.2. </span>Solving finite-horizon MDPs</a><a class="headerlink" href="#solving-finite-horizon-mdps" title="Link to this heading">#</a></h2>
<section id="policy-evaluation-in-finite-horizon-mdps">
<span id="eval-dp"></span><h3><a class="toc-backref" href="#id13" role="doc-backlink"><span class="section-number">3.2.1. </span>Policy evaluation in finite-horizon MDPs</a><a class="headerlink" href="#policy-evaluation-in-finite-horizon-mdps" title="Link to this heading">#</a></h3>
<p>How can we actually compute the value function of a given policy? This
is the task of <strong>policy evaluation</strong>.</p>
<div class="proof algorithm admonition" id="algorithm-14">
<p class="admonition-title"><span class="caption-number">Algorithm 3.1 </span> (DP algorithm to evaluate a policy in a finite-horizon MDP)</p>
<section class="algorithm-content" id="proof-content">
<p>The Bellman consistency equation
<a class="reference internal" href="#bellman_consistency">Theorem 3.1</a>
gives us a convenient algorithm for
evaluating stationary policies: it expresses the value function at
timestep <span class="math notranslate nohighlight">\(\hi\)</span> as a function of the value function at timestep <span class="math notranslate nohighlight">\(\hi+1\)</span>. This
means we can start at the end of the time horizon, where the value is
known, and work backwards in time, using the Bellman consistency
equation to compute the value function at each time step.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dp_eval_finite</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;H S&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Evaluate a policy using dynamic programming.&quot;&quot;&quot;</span>
    <span class="n">V_ary</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">H</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">)]</span>  <span class="c1"># initialize to 0 at end of time horizon</span>
    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">H</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">V_ary</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="n">bellman_operator</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">policy</span><span class="p">[</span><span class="n">h</span><span class="p">],</span> <span class="n">V_ary</span><span class="p">[</span><span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V_ary</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>This runs in time <span class="math notranslate nohighlight">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span> by counting the
loops.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Do you see where we compute <span class="math notranslate nohighlight">\(Q^\pi_\hi\)</span> along the way? Make
this step explicit.</p>
</div>
<div class="proof example admonition" id="tidy_eval_finite">
<p class="admonition-title"><span class="caption-number">Example 3.4 </span> (Tidying policy evaluation)</p>
<section class="example-content" id="proof-content">
<p>Let’s evaluate the policy from
<a class="reference internal" href="#tidy_policy">Example 3.2</a> in the tidying MDP
that tidies if and only if the room is
messy. We’ll use the Bellman consistency equation to compute the value
function at each time step.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
V_{H-1}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) \\
&amp;= 1 \\
V_{H-1}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) \\
&amp;= 0 \\
V_{H-2}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-1}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1 + 0.3 \cdot 0 \\
&amp;= 1.7 \\
V_{H-2}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-1}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0 \cdot V_{H-1}^{\pi}(\text{messy}) \\
&amp;= 1 \\
V_{H-3}^\pi(\text{orderly}) &amp;= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-2}^\pi(s')] \\
&amp;= 1 + 0.7 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1 + 0.7 \cdot 1.7 + 0.3 \cdot 1 \\
&amp;= 2.49 \\
V_{H-3}^\pi(\text{messy}) &amp;= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-2}^\pi(s')] \\
&amp;= 0 + 1 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0 \cdot V_{H-2}^{\pi}(\text{messy}) \\
&amp;= 1.7
\end{aligned}
\end{split}\]</div>
<p>etc. You may wish to repeat this computation for the
other policies to get a better sense of this algorithm.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">V_messy</span> <span class="o">=</span> <span class="n">dp_eval_finite</span><span class="p">(</span><span class="n">tidy_mdp</span><span class="p">,</span> <span class="n">tidy_policy_messy_only</span><span class="p">)</span>
<span class="n">V_messy</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[5.5621696, 4.7927704],
       [4.7927704, 4.0241003],
       [4.0241003, 3.253    ],
       [3.253    , 2.49     ],
       [2.49     , 1.7      ],
       [1.7      , 1.       ],
       [1.       , 0.       ]], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="optimal-policies-in-finite-horizon-mdps">
<span id="opt-dynamic-programming"></span><h3><a class="toc-backref" href="#id14" role="doc-backlink"><span class="section-number">3.2.2. </span>Optimal policies in finite-horizon MDPs</a><a class="headerlink" href="#optimal-policies-in-finite-horizon-mdps" title="Link to this heading">#</a></h3>
<p>We’ve just seen how to <em>evaluate</em> a given policy. But how can we find
the <strong>optimal policy</strong> for a given environment?</p>
<div class="proof definition admonition" id="optimal_policy_finite">
<p class="admonition-title"><span class="caption-number">Definition 3.9 </span> (Optimal policies)</p>
<section class="definition-content" id="proof-content">
<p>We call a policy optimal, and denote it by <span class="math notranslate nohighlight">\(\pi^\star\)</span>, if it does at
least as well as <em>any</em> other policy <span class="math notranslate nohighlight">\(\pi\)</span> (including stochastic and
history-dependent ones) in all situations:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V_\hi^{\pi^\star}(s) &amp;= \E_{\tau \sim \rho^{\pi^{\star}}}[r_\hi + \cdots + r_{H-1} \mid s_\hi = s] \\
    &amp;\ge \E_{\tau \sim \rho^{\pi}}[r_\hi + \cdots + r_{H-1} \mid \tau_\hi] \quad \forall \pi, \tau_\hi, \hi \in [H]
\end{aligned}
\end{split}\]</div>
<p>where we condition on the
trajectory up to time <span class="math notranslate nohighlight">\(\hi\)</span>, denoted
<span class="math notranslate nohighlight">\(\tau_\hi = (s_0, a_0, r_0, \dots, s_\hi)\)</span>, where <span class="math notranslate nohighlight">\(s_\hi = s\)</span>.</p>
</section>
</div><p>Convince yourself that all optimal policies must have the same value
function. We call this the <strong>optimal value function</strong> and denote it by
<span class="math notranslate nohighlight">\(V_\hi^\star(s)\)</span>. The same goes for the action-value function
<span class="math notranslate nohighlight">\(Q_\hi^\star(s, a)\)</span>.</p>
<p>It is a stunning fact that <strong>every finite-horizon MDP has an optimal
policy that is time-dependent and deterministic.</strong> In particular, we can
construct such a policy by acting <em>greedily</em> with respect to the optimal
action-value function:</p>
<div class="proof theorem admonition" id="optimal_greedy">
<p class="admonition-title"><span class="caption-number">Theorem 3.3 </span> (It is optimal to be greedy w.r.t. the optimal value function)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\pi_\hi^\star(s) = \arg\max_a Q_\hi^\star(s, a).\]</div>
</section>
</div><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Proof<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">Let <span class="math notranslate nohighlight">\(V^{\star}\)</span> and <span class="math notranslate nohighlight">\(Q^{\star}\)</span> denote the optimal value and
action-value functions. Consider the greedy policy</p>
<div class="math notranslate nohighlight">
\[\hat \pi_\hi(s) := \arg\max_a Q_\hi^{\star}(s, a).\]</div>
<p class="sd-card-text">We aim to show that
<span class="math notranslate nohighlight">\(\hat \pi\)</span> is optimal; that is, <span class="math notranslate nohighlight">\(V^{\hat \pi} = V^{\star}\)</span>.</p>
<p class="sd-card-text">Fix an arbitrary state <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span> and time <span class="math notranslate nohighlight">\(\hi \in [H]\)</span>.</p>
<p class="sd-card-text">Firstly, by the definition of <span class="math notranslate nohighlight">\(V^{\star}\)</span>, we already know
<span class="math notranslate nohighlight">\(V_\hi^{\star}(s) \ge V_\hi^{\hat \pi}(s)\)</span>. So for equality to hold we just
need to show that <span class="math notranslate nohighlight">\(V_\hi^{\star}(s) \le V_\hi^{\hat \pi}(s)\)</span>. We’ll first
show that the Bellman operator <span class="math notranslate nohighlight">\(\mathcal{J}^{\hat \pi}\)</span> never decreases
<span class="math notranslate nohighlight">\(V_\hi^{\star}\)</span>. Then we’ll apply this result recursively to show that
<span class="math notranslate nohighlight">\(V^{\star} = V^{\hat \pi}\)</span>.</p>
<div class="proof lemma admonition" id="lemma-18">
<p class="admonition-title"><span class="caption-number">Lemma 3.1 </span> (The Bellman operator never decreases the optimal value function)</p>
<section class="lemma-content" id="proof-content">
<p class="sd-card-text"><span class="math notranslate nohighlight">\(\mathcal{J}^{\hat \pi}\)</span> never decreases <span class="math notranslate nohighlight">\(V_\hi^{\star}\)</span>
(elementwise):</p>
<div class="math notranslate nohighlight">
\[[\mathcal{J}^{\hat \pi} (V_{\hi+1}^{\star})](s) \ge V_\hi^{\star}(s).\]</div>
<p class="sd-card-text"><strong>Proof:</strong></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V_\hi^{\star}(s) &amp;= \max_{\pi \in \Pi} V_\hi^{\pi}(s) \\
    &amp;= \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^\pi(s') \right] &amp;&amp; \text{Bellman consistency} \\
    &amp;\le \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^{\star}(s') \right] &amp;&amp; \text{definition of } V^\star \\
    &amp;= \max_{a} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{\hi+1}^{\star}(s') \right] &amp;&amp; \text{only depends on } \pi \text{ via } a \\
    &amp;= [\mathcal{J}^{\hat \pi}(V_{\hi+1}^{\star})](s).    
\end{aligned}
\end{split}\]</div>
<p class="sd-card-text">Note that the chosen action <span class="math notranslate nohighlight">\(a \sim \pi(\dots)\)</span> above
might depend on the past history; this isn’t shown in the notation and
doesn’t affect our result (make sure you see why).</p>
</section>
</div><p class="sd-card-text">We can now apply this result recursively to get</p>
<div class="math notranslate nohighlight">
\[V^{\star}_t(s) \le V^{\hat \pi}_t(s)\]</div>
<p class="sd-card-text">as follows. (Note that even
though <span class="math notranslate nohighlight">\(\hat \pi\)</span> is deterministic, we’ll use the <span class="math notranslate nohighlight">\(a \sim \hat \pi(s)\)</span>
notation to make it explicit that we’re sampling a trajectory from it.)</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V_{t}^{\star}(s) &amp;\le [\mathcal{J}^{\hat \pi}(V_{\hi+1}^{\star})](s) \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue} V_{\hi+1}^{\star}(s')} \right] \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue}[ \mathcal{J}^{\hat \pi} (V_{t+2}^{\star})] (s')} \right] \right] &amp;&amp; \text{above lemma} \\
    &amp;= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)}{\color{blue} \left[ \mathop{\mathbb{E}}_{a' \sim \hat \pi}  r(s', a') + \mathop{\mathbb{E}}_{s''} V_{t+2}^{\star}(s'') \right]} \right] &amp;&amp; \text{definition of } \mathcal{J}^{\hat \pi} \\
    &amp;\le \cdots &amp;&amp; \text{apply at all timesteps} \\
    &amp;= \mathop{\mathbb{E}}_{\tau \sim \rho^{\hat \pi}} [G_{t} \mid s_\hi = s] &amp;&amp; \text{rewrite expectation} \\
    &amp;= V_{t}^{\hat \pi}(s) &amp;&amp; \text{definition}
\end{aligned}
\end{split}\]</div>
<p class="sd-card-text">And so we have <span class="math notranslate nohighlight">\(V^{\star} = V^{\hat \pi}\)</span>, making <span class="math notranslate nohighlight">\(\hat \pi\)</span> optimal.</p>
</div>
</details><p>Note that this also gives simplified forms of the <a class="reference internal" href="#bellman-consistency"><span class="std std-ref">Bellman consistency</span></a> equations for the optimal policy:</p>
<div class="proof corollary admonition" id="bellman_consistency_optimal">
<p class="admonition-title"><span class="caption-number">Corollary 3.1 </span> (Bellman consistency equations for the optimal policy)</p>
<section class="corollary-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V_\hi^\star(s) &amp;= \max_a Q_\hi^\star(s, a) \\
    Q_\hi^\star(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [V_{\hi+1}^\star(s')]
\end{aligned}
\end{split}\]</div>
</section>
</div><p>Now that we’ve shown this particular greedy policy is optimal, all we
need to do is compute the optimal value function and optimal policy. We
can do this by working backwards in time using <strong>dynamic programming</strong>
(DP).</p>
<div class="proof algorithm admonition" id="pi_star_dp">
<p class="admonition-title"><span class="caption-number">Algorithm 3.2 </span> (DP algorithm to compute an optimal policy in a finite-horizon MDP)</p>
<section class="algorithm-content" id="proof-content">
<p><strong>Base case.</strong> At the end of the episode (time step <span class="math notranslate nohighlight">\(H-1\)</span>), we can’t
take any more actions, so the <span class="math notranslate nohighlight">\(Q\)</span>-function is simply the reward that
we obtain:</p>
<div class="math notranslate nohighlight">
\[Q^\star_{H-1}(s, a) = r(s, a)\]</div>
<p>so the best thing to do
is just act greedily and get as much reward as we can!</p>
<div class="math notranslate nohighlight">
\[\pi^\star_{H-1}(s) = \arg\max_a Q^\star_{H-1}(s, a)\]</div>
<p>Then
<span class="math notranslate nohighlight">\(V^\star_{H-1}(s)\)</span>, the optimal value of state <span class="math notranslate nohighlight">\(s\)</span> at the end of the
trajectory, is simply whatever action gives the most reward.</p>
<div class="math notranslate nohighlight">
\[V^\star_{H-1} = \max_a Q^\star_{H-1}(s, a)\]</div>
<p><strong>Recursion.</strong> Then, we can work backwards in time, starting from the
end, using our consistency equations! i.e. for each
<span class="math notranslate nohighlight">\(t = H-2, \dots, 0\)</span>, we set</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    Q^\star_{t}(s, a) &amp;= r(s, a) + \E_{s' \sim P(s, a)} [V^\star_{\hi+1}(s')] \\
    \pi^\star_{t}(s) &amp;= \arg\max_a Q^\star_{t}(s, a) \\
    V^\star_{t}(s) &amp;= \max_a Q^\star_{t}(s, a)
\end{aligned}
\end{split}\]</div>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_optimal_policy</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">H</span>
    <span class="n">π</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">H</span>
    <span class="n">V</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">H</span> <span class="o">+</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">)]</span>  <span class="c1"># initialize to 0 at end of time horizon</span>

    <span class="k">for</span> <span class="n">h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">H</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
        <span class="n">Q</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">r</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span> <span class="o">@</span> <span class="n">V</span><span class="p">[</span><span class="n">h</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">π</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">)[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">h</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>  <span class="c1"># one-hot</span>
        <span class="n">V</span><span class="p">[</span><span class="n">h</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">h</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">Q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">π</span><span class="p">)</span>
    <span class="n">V</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">V</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">π</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">Q</span>
</pre></div>
</div>
</div>
</div>
<p>At each of the <span class="math notranslate nohighlight">\(H\)</span> timesteps, we must compute <span class="math notranslate nohighlight">\(Q^{\star}\)</span> for each of
the <span class="math notranslate nohighlight">\(|\mathcal{S}| |\mathcal{A}|\)</span> state-action pairs. Each computation takes <span class="math notranslate nohighlight">\(|\mathcal{S}|\)</span>
operations to evaluate the average value over <span class="math notranslate nohighlight">\(s'\)</span>. This gives a total
computation time of <span class="math notranslate nohighlight">\(O(H \cdot |\mathcal{S}|^2 \cdot |\mathcal{A}|)\)</span>.</p>
<p>Note that this algorithm is identical to the policy evaluation algorithm
<a class="reference internal" href="#eval-dp"><span class="std std-ref"><code class="docutils literal notranslate"><span class="pre">dp_eval_finite</span></code></span></a>, but instead of <em>averaging</em> over the
actions chosen by a policy, we instead simply take a <em>maximum</em> over the
action-values. We’ll see this relationship between <strong>policy evaluation</strong>
and <strong>optimal policy computation</strong> show up again in the infinite-horizon
setting.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">π_opt</span><span class="p">,</span> <span class="n">V_opt</span><span class="p">,</span> <span class="n">Q_opt</span> <span class="o">=</span> <span class="n">find_optimal_policy</span><span class="p">(</span><span class="n">tidy_mdp</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">π_opt</span><span class="p">,</span> <span class="n">tidy_policy_messy_only</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">V_opt</span><span class="p">,</span> <span class="n">V_messy</span><span class="p">)</span>
<span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">Q_opt</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">v_ary_to_q_ary</span><span class="p">(</span><span class="n">tidy_mdp</span><span class="p">,</span> <span class="n">V_messy</span><span class="p">)[</span><span class="mi">1</span><span class="p">:])</span>
<span class="s2">&quot;Assertions passed (the &#39;tidy when messy&#39; policy is optimal)&quot;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&quot;Assertions passed (the &#39;tidy when messy&#39; policy is optimal)&quot;
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="infinite-horizon-mdps">
<span id="id2"></span><h2><a class="toc-backref" href="#id15" role="doc-backlink"><span class="section-number">3.3. </span>Infinite-horizon MDPs</a><a class="headerlink" href="#infinite-horizon-mdps" title="Link to this heading">#</a></h2>
<p>What happens if a trajectory is allowed to continue forever (i.e.
<span class="math notranslate nohighlight">\(H = \infty\)</span>)? This is the setting of <strong>infinite horizon</strong> MDPs.</p>
<p>In this chapter, we’ll describe the necessary adjustments from the
finite-horizon case to make the problem tractable. We’ll show that the
<a class="reference internal" href="#bellman-operator"><span class="std std-ref">Bellman operator</span></a> in the discounted reward setting is a
<strong>contraction mapping</strong> for any policy. We’ll discuss how to evaluate
policies (i.e. compute their corresponding value functions). Finally,
we’ll present and analyze two iterative algorithms, based on the Bellman
operator, for computing the optimal policy: <strong>value iteration</strong> and
<strong>policy iteration</strong>.</p>
<section id="discounted-rewards">
<h3><a class="toc-backref" href="#id16" role="doc-backlink"><span class="section-number">3.3.1. </span>Discounted rewards</a><a class="headerlink" href="#discounted-rewards" title="Link to this heading">#</a></h3>
<p>First of all, note that maximizing the cumulative reward
<span class="math notranslate nohighlight">\(r_\hi + r_{\hi+1} + r_{\hi+2} + \cdots\)</span> is no longer a good idea since it
might blow up to infinity. Instead of a time horizon <span class="math notranslate nohighlight">\(H\)</span>, we now need a
<strong>discount factor</strong> <span class="math notranslate nohighlight">\(\gamma \in [0, 1)\)</span> such that rewards become less
valuable the further into the future they are:</p>
<div class="math notranslate nohighlight">
\[r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots = \sum_{k=0}^\infty \gamma^k r_{\hi+k}.\]</div>
<p>We can think of <span class="math notranslate nohighlight">\(\gamma\)</span> as measuring how much we care about the future:
if it’s close to <span class="math notranslate nohighlight">\(0\)</span>, we only care about the near-term rewards; it’s
close to <span class="math notranslate nohighlight">\(1\)</span>, we put more weight into future rewards.</p>
<p>You can also analyze <span class="math notranslate nohighlight">\(\gamma\)</span> as the probability of <em>continuing</em> the
trajectory at each time step. (This is equivalent to <span class="math notranslate nohighlight">\(H\)</span> being
distributed by a First Success distribution with success probability
<span class="math notranslate nohighlight">\(\gamma\)</span>.) This accords with the above interpretation: if <span class="math notranslate nohighlight">\(\gamma\)</span> is
close to <span class="math notranslate nohighlight">\(0\)</span>, the trajectory will likely be very short, while if
<span class="math notranslate nohighlight">\(\gamma\)</span> is close to <span class="math notranslate nohighlight">\(1\)</span>, the trajectory will likely continue for a long
time.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Assuming that <span class="math notranslate nohighlight">\(r_\hi \in [0, 1]\)</span> for all <span class="math notranslate nohighlight">\(\hi \in \mathbb{N}\)</span>,
what is the maximum <strong>discounted</strong> cumulative reward? You may find it
useful to review geometric series.</p>
</div>
<p>The other components of the MDP remain the same:</p>
<div class="math notranslate nohighlight">
\[M = (\mathcal{S}, \mathcal{A}, \mu, P, r, \gamma).\]</div>
<p>Code-wise, we can reuse the <code class="docutils literal notranslate"><span class="pre">MDP</span></code> class from before <a class="reference internal" href="#finite_mdp">Definition 3.2</a> and set <code class="docutils literal notranslate"><span class="pre">mdp.H</span> <span class="pre">=</span> <span class="pre">float('inf')</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tidy_mdp_inf</span> <span class="o">=</span> <span class="n">tidy_mdp</span><span class="o">.</span><span class="n">_replace</span><span class="p">(</span><span class="n">H</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s1">&#39;inf&#39;</span><span class="p">),</span> <span class="n">γ</span><span class="o">=</span><span class="mf">0.95</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="stationary-policies">
<h3><a class="toc-backref" href="#id17" role="doc-backlink"><span class="section-number">3.3.2. </span>Stationary policies</a><a class="headerlink" href="#stationary-policies" title="Link to this heading">#</a></h3>
<p>The time-dependent policies from the finite-horizon case become
difficult to handle in the infinite-horizon case. In particular, many of
the DP approaches we saw required us to start at the end of the
trajectory, which is no longer possible. We’ll shift to <strong>stationary</strong>
policies <span class="math notranslate nohighlight">\(\pi : \mathcal{S} \to \mathcal{A}\)</span> (deterministic) or <span class="math notranslate nohighlight">\(\Delta(\mathcal{A})\)</span> (stochastic).</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Which of the policies in <a class="reference internal" href="#tidy_policy">Example 3.2</a> are stationary?</p>
</div>
</section>
<section id="value-functions-and-bellman-consistency">
<h3><a class="toc-backref" href="#id18" role="doc-backlink"><span class="section-number">3.3.3. </span>Value functions and Bellman consistency</a><a class="headerlink" href="#value-functions-and-bellman-consistency" title="Link to this heading">#</a></h3>
<p>We also consider stationary value functions <span class="math notranslate nohighlight">\(V^\pi : \mathcal{S} \to \mathbb{R}\)</span> and
<span class="math notranslate nohighlight">\(Q^\pi : \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>. We need to insert a factor of <span class="math notranslate nohighlight">\(\gamma\)</span>
into the Bellman consistency equation <a class="reference internal" href="#bellman_consistency">Theorem 3.1</a> to account for the discounting:</p>
<div class="math notranslate nohighlight" id="equation-bellman-consistency-infinite">
<span class="eqno">(3.1)<a class="headerlink" href="#equation-bellman-consistency-infinite" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    V^\pi(s) &amp;= \E_{\tau \sim \rho^\pi} [r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} \cdots \mid s_\hi = s] &amp;&amp; \text{for any } \hi \in \mathbb{N} \\
    &amp;= \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma V^\pi(s')]\\
    Q^\pi(s, a) &amp;= \E_{\tau \sim \rho^\pi} [r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots \mid s_\hi = s, a_\hi = a] &amp;&amp; \text{for any } \hi \in \mathbb{N} \\
    &amp;= r(s, a) + \gamma \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi(s')}} [Q^\pi(s', a')]
\end{aligned}\end{split}\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Heuristically speaking, why does it no longer matter which
time step we condition on when defining the value function?</p>
</div>
</section>
</section>
<section id="solving-infinite-horizon-mdps">
<h2><a class="toc-backref" href="#id19" role="doc-backlink"><span class="section-number">3.4. </span>Solving infinite-horizon MDPs</a><a class="headerlink" href="#solving-infinite-horizon-mdps" title="Link to this heading">#</a></h2>
<section id="the-bellman-operator-is-a-contraction-mapping">
<h3><a class="toc-backref" href="#id20" role="doc-backlink"><span class="section-number">3.4.1. </span>The Bellman operator is a contraction mapping</a><a class="headerlink" href="#the-bellman-operator-is-a-contraction-mapping" title="Link to this heading">#</a></h3>
<p>Recall from <a class="reference internal" href="#bellman-operator"><span class="std std-ref">The one-step Bellman operator</span></a> that the Bellman operator <span class="math notranslate nohighlight">\(\mathcal{J}^{\pi}\)</span>
for a policy <span class="math notranslate nohighlight">\(\pi\)</span> takes in a “value function” <span class="math notranslate nohighlight">\(v : \mathcal{S} \to \mathbb{R}\)</span> and
returns the r.h.s. of the Bellman equation for that “value function”. In
the infinite-horizon setting, this is</p>
<div class="math notranslate nohighlight">
\[\begin{split}[\mathcal{J}^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma v(s')].\end{split}\]</div>
<p>The crucial property of the Bellman operator is that it is a
<strong>contraction mapping</strong> for any policy. Intuitively, if we start with
two “value functions” <span class="math notranslate nohighlight">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>, if we repeatedly apply the
Bellman operator to each of them, they will get closer and closer
together at an exponential rate.</p>
<div class="proof definition admonition" id="contraction">
<p class="admonition-title"><span class="caption-number">Definition 3.10 </span> (Contraction mapping)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be some space with a norm <span class="math notranslate nohighlight">\(\|\cdot\|\)</span>. We call an operator
<span class="math notranslate nohighlight">\(f: X \to X\)</span> a <strong>contraction mapping</strong> if for any <span class="math notranslate nohighlight">\(x, y \in X\)</span>,</p>
<div class="math notranslate nohighlight">
\[\|f(x) - f(y)\| \le \gamma \|x - y\|\]</div>
<p>for some fixed <span class="math notranslate nohighlight">\(\gamma \in (0, 1)\)</span>.</p>
</section>
</div><div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Show that for a contraction mapping <span class="math notranslate nohighlight">\(f\)</span> with coefficient
<span class="math notranslate nohighlight">\(\gamma\)</span>, for all <span class="math notranslate nohighlight">\(t \in \mathbb{N}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\|f^{(t)}(x) - f^{(t)}(y)\| \le \gamma^t \|x - y\|,\]</div>
<p>i.e. that any
two points will be pushed closer by at least a factor of <span class="math notranslate nohighlight">\(\gamma\)</span> at
each iteration.</p>
</div>
<p>It is a powerful fact (known as the <strong>Banach fixed-point theorem</strong>) that
every contraction mapping has a unique <strong>fixed point</strong> <span class="math notranslate nohighlight">\(x^\star\)</span> such
that <span class="math notranslate nohighlight">\(f(x^\star) = x^\star\)</span>. This means that if we repeatedly apply <span class="math notranslate nohighlight">\(f\)</span>
to any starting point, we will eventually converge to <span class="math notranslate nohighlight">\(x^\star\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-contraction-convergence">
<span class="eqno">(3.2)<a class="headerlink" href="#equation-contraction-convergence" title="Link to this equation">#</a></span>\[\|f^{(t)}(x) - x^\star\| \le \gamma^t \|x - x^\star\|.\]</div>
<p>Let’s return to the RL setting and apply this result to the Bellman
operator. How can we measure the distance between two “value functions”
<span class="math notranslate nohighlight">\(v, u : \mathcal{S} \to \mathbb{R}\)</span>? We’ll take the <strong>supremum norm</strong> as our distance
metric:</p>
<div class="math notranslate nohighlight">
\[\| v - u \|_{\infty} := \sup_{s \in \mathcal{S}} |v(s) - u(s)|,\]</div>
<p>i.e.
we compare the “value functions” on the state that causes the biggest
gap between them. Then <a class="reference internal" href="#equation-contraction-convergence">(3.2)</a> implies that if we repeatedly
apply <span class="math notranslate nohighlight">\(\mathcal{J}^\pi\)</span> to any starting “value function”, we will eventually
converge to <span class="math notranslate nohighlight">\(V^\pi\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-bellman-convergence">
<span class="eqno">(3.3)<a class="headerlink" href="#equation-bellman-convergence" title="Link to this equation">#</a></span>\[\|(\mathcal{J}^\pi)^{(t)}(v) - V^\pi \|_{\infty} \le \gamma^{t} \| v - V^\pi\|_{\infty}.\]</div>
<p>We’ll use this useful fact to prove the convergence of several
algorithms later on.</p>
<div class="proof theorem admonition" id="bellman_contraction">
<p class="admonition-title"><span class="caption-number">Theorem 3.4 </span> (The Bellman operator is a contraction mapping)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[
\|\mathcal{J}^{\pi} (v) - \mathcal{J}^{\pi} (u) \|_{\infty} \le \gamma \|v - u \|_{\infty}.
\]</div>
</section>
</div><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Proof of <a class="reference internal" href="#bellman_contraction">Theorem 3.4</a><div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">For all states <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
|[\mathcal{J}^{\pi} (v)](s) - [\mathcal{J}^{\pi} (u)](s)|&amp;= \Big| \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right] \\
&amp;\qquad - \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} u(s') \right] \Big| \\
&amp;= \gamma \left|\mathop{\mathbb{E}}_{s' \sim P(s, a)} [v(s') - u(s')] \right| \\
&amp;\le \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)}|v(s') - u(s')| \qquad \text{(Jensen's inequality)} \\
&amp;\le \gamma \max_{s'} |v(s') - u(s')| \\
&amp;= \gamma \|v - u \|_{\infty}.
\end{aligned}
\end{split}\]</div>
</div>
</details></section>
<section id="policy-evaluation-in-infinite-horizon-mdps">
<h3><a class="toc-backref" href="#id21" role="doc-backlink"><span class="section-number">3.4.2. </span>Policy evaluation in infinite-horizon MDPs</a><a class="headerlink" href="#policy-evaluation-in-infinite-horizon-mdps" title="Link to this heading">#</a></h3>
<p>The backwards DP technique we used in <a class="reference internal" href="#eval-dp"><span class="std std-ref">the finite-horizon case</span></a> no
longer works since there is no “final timestep” to start from. We’ll
need another approach to policy evaluation.</p>
<p>The Bellman consistency conditions yield a system of equations we can
solve to evaluate a deterministic policy <em>exactly</em>. For a faster approximate solution,
we can iterate the policy’s Bellman operator, since we know that it has
a unique fixed point at the true value function.</p>
<section id="matrix-inversion-for-deterministic-policies">
<h4><a class="toc-backref" href="#id22" role="doc-backlink"><span class="section-number">3.4.2.1. </span>Matrix inversion for deterministic policies</a><a class="headerlink" href="#matrix-inversion-for-deterministic-policies" title="Link to this heading">#</a></h4>
<p>Note that when the policy <span class="math notranslate nohighlight">\(\pi\)</span> is deterministic, the actions can be
determined from the states, and so we can chop off the action dimension
for the rewards and state transitions:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    r^{\pi} &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; P^{\pi} &amp;\in [0, 1]^{|\mathcal{S}| \times |\mathcal{S}|} &amp; \mu &amp;\in [0, 1]^{|\mathcal{S}|} \\
    \pi &amp;\in \mathcal{A}^{|\mathcal{S}|} &amp; V^\pi &amp;\in \mathbb{R}^{|\mathcal{S}|} &amp; Q^\pi &amp;\in \mathbb{R}^{|\mathcal{S}| \times |\mathcal{A}|}.
\end{aligned}
\end{split}\]</div>
<p>For <span class="math notranslate nohighlight">\(P^\pi\)</span>, we’ll treat the rows as the states and the
columns as the next states. Then <span class="math notranslate nohighlight">\(P^\pi_{s, s'}\)</span> is the probability of
transitioning from state <span class="math notranslate nohighlight">\(s\)</span> to state <span class="math notranslate nohighlight">\(s'\)</span> under policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p>
<div class="proof example admonition" id="tidy_tabular">
<p class="admonition-title"><span class="caption-number">Example 3.5 </span> (Tidying MDP)</p>
<section class="example-content" id="proof-content">
<p>The tabular MDP from before has <span class="math notranslate nohighlight">\(|\mathcal{S}| = 2\)</span> and <span class="math notranslate nohighlight">\(|\mathcal{A}| = 2\)</span>. Let’s write
down the quantities for the policy <span class="math notranslate nohighlight">\(\pi\)</span> that tidies if and only if the
room is messy:</p>
<div class="math notranslate nohighlight">
\[\begin{split}r^{\pi} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
        P^{\pi} = \begin{bmatrix} 0.7 &amp; 0.3 \\ 1 &amp; 0 \end{bmatrix}, \quad
        \mu = \begin{bmatrix} 1 \\ 0 \end{bmatrix}\end{split}\]</div>
<p>We’ll see how to
evaluate this policy in the next section.</p>
</section>
</div><p>The Bellman consistency equation for a deterministic policy can be
written in tabular notation as</p>
<div class="math notranslate nohighlight">
\[V^\pi = r^\pi + \gamma P^\pi V^\pi.\]</div>
<p>(Unfortunately, this notation doesn’t simplify the expression for
<span class="math notranslate nohighlight">\(Q^\pi\)</span>.) This system of equations can be solved with a matrix
inversion:</p>
<div class="math notranslate nohighlight" id="equation-matrix-inversion-pe">
<span class="eqno">(3.4)<a class="headerlink" href="#equation-matrix-inversion-pe" title="Link to this equation">#</a></span>\[V^\pi = (I - \gamma P^\pi)^{-1} r^\pi.\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Note we’ve assumed that <span class="math notranslate nohighlight">\(I - \gamma P^\pi\)</span> is invertible. Can you see
why this is the case?</p>
<p>(Recall that a linear operator, i.e. a square matrix, is invertible if
and only if its null space is trivial; that is, it doesn’t map any
nonzero vector to zero. In this case, we can see that <span class="math notranslate nohighlight">\(I - \gamma P^\pi\)</span>
is invertible because it maps any nonzero vector to a vector with at
least one nonzero element.)</p>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">eval_deterministic_infinite</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">policy</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
    <span class="n">π</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">policy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># un-one-hot</span>
    <span class="n">P_π</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">),</span> <span class="n">π</span><span class="p">]</span>
    <span class="n">r_π</span> <span class="o">=</span> <span class="n">mdp</span><span class="o">.</span><span class="n">r</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">),</span> <span class="n">π</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">)</span> <span class="o">-</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">P_π</span><span class="p">,</span> <span class="n">r_π</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="proof example admonition" id="tidy_eval_infinite">
<p class="admonition-title"><span class="caption-number">Example 3.6 </span> (Tidying policy evaluation)</p>
<section class="example-content" id="proof-content">
<p>Let’s use the same policy <span class="math notranslate nohighlight">\(\pi\)</span> that tidies if and only if the room is
messy. Setting <span class="math notranslate nohighlight">\(\gamma = 0.95\)</span>, we must invert</p>
<div class="math notranslate nohighlight">
\[\begin{split}I - \gamma P^{\pi} = \begin{bmatrix} 1 - 0.95 \times 0.7 &amp; - 0.95 \times 0.3 \\ - 0.95 \times 1 &amp; 1 - 0.95 \times 0 \end{bmatrix} = \begin{bmatrix} 0.335 &amp; -0.285 \\ -0.95 &amp; 1 \end{bmatrix}.\end{split}\]</div>
<p>The inverse to two decimal points is</p>
<div class="math notranslate nohighlight">
\[\begin{split}(I - \gamma P^{\pi})^{-1} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix}.\end{split}\]</div>
<p>Thus the value function is</p>
<div class="math notranslate nohighlight">
\[\begin{split}V^{\pi} = (I - \gamma P^{\pi})^{-1} r^{\pi} = \begin{bmatrix} 15.56 &amp; 4.44 \\ 14.79 &amp; 5.21 \end{bmatrix} \begin{bmatrix} 1 \\ 0 \end{bmatrix} = \begin{bmatrix} 15.56 \\ 14.79 \end{bmatrix}.\end{split}\]</div>
<p>Let’s sanity-check this result. Since rewards are at most <span class="math notranslate nohighlight">\(1\)</span>, the
maximum cumulative return of a trajectory is at most
<span class="math notranslate nohighlight">\(1/(1-\gamma) = 20\)</span>. We see that the value function is indeed slightly
lower than this.</p>
</section>
</div><div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_deterministic_infinite</span><span class="p">(</span><span class="n">tidy_mdp_inf</span><span class="p">,</span> <span class="n">tidy_policy_messy_only</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([15.56419, 14.78598], dtype=float32)
</pre></div>
</div>
</div>
</div>
</section>
<section id="iterative-policy-evaluation">
<span id="iterative-pe"></span><h4><a class="toc-backref" href="#id23" role="doc-backlink"><span class="section-number">3.4.2.2. </span>Iterative policy evaluation</a><a class="headerlink" href="#iterative-policy-evaluation" title="Link to this heading">#</a></h4>
<p>The matrix inversion above takes roughly <span class="math notranslate nohighlight">\(O(|\mathcal{S}|^3)\)</span> time.
It also only works for deterministic policies.
Can we trade off the requirement of finding the <em>exact</em> value function for a faster
<em>approximate</em> algorithm that will also extend to stochastic policies?</p>
<p>Let’s use the Bellman operator to define an iterative algorithm for
computing the value function. We’ll start with an initial guess
<span class="math notranslate nohighlight">\(v^{(0)}\)</span> with elements in <span class="math notranslate nohighlight">\([0, 1/(1-\gamma)]\)</span> and then iterate the
Bellman operator:</p>
<div class="math notranslate nohighlight">
\[v^{(t+1)} = \mathcal{J}^{\pi}(v^{(t)}),\]</div>
<p>i.e. <span class="math notranslate nohighlight">\(v^{(t)} = (\mathcal{J}^{\pi})^{(t)} (v^{(0)})\)</span>. Note that each iteration
takes <span class="math notranslate nohighlight">\(O(|\mathcal{S}|^2)\)</span> time for the matrix-vector multiplication.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">supremum_norm</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>  <span class="c1"># same as np.linalg.norm(v, np.inf)</span>

<span class="k">def</span> <span class="nf">loop_until_convergence</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Repeatedly apply op to v until convergence (in supremum norm).&quot;&quot;&quot;</span>
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="n">v_new</span> <span class="o">=</span> <span class="n">op</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">supremum_norm</span><span class="p">(</span><span class="n">v_new</span> <span class="o">-</span> <span class="n">v</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">ε</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">v_new</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v_new</span>

<span class="k">def</span> <span class="nf">iterative_evaluation</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">π</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">],</span> <span class="n">ε</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">bellman_operator</span><span class="p">,</span> <span class="n">mdp</span><span class="p">,</span> <span class="n">π</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loop_until_convergence</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">),</span> <span class="n">ε</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Then, as we showed in <a class="reference internal" href="#equation-bellman-convergence">(3.3)</a>, by the Banach fixed-point theorem:</p>
<div class="math notranslate nohighlight">
\[\|v^{(t)} - V^\pi \|_{\infty} \le \gamma^{t} \| v^{(0)} - V^\pi\|_{\infty}.\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">iterative_evaluation</span><span class="p">(</span><span class="n">tidy_mdp_inf</span><span class="p">,</span> <span class="n">tidy_policy_messy_only</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([15.564166, 14.785956], dtype=float32)
</pre></div>
</div>
</div>
</div>
<div class="proof remark admonition" id="iterations_vi">
<p class="admonition-title"><span class="caption-number">Remark 3.2 </span> (Convergence of iterative policy evaluation)</p>
<section class="remark-content" id="proof-content">
<p>How many iterations do we need for an <span class="math notranslate nohighlight">\(\epsilon\)</span>-accurate estimate? We
can work backwards to solve for <span class="math notranslate nohighlight">\(t\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \gamma^t \|v^{(0)} - V^\pi\|_{\infty} &amp;\le \epsilon \\
    t &amp;\ge \frac{\log (\epsilon / \|v^{(0)} - V^\pi\|_{\infty})}{\log \gamma} \\
    &amp;= \frac{\log (\|v^{(0)} - V^\pi\|_{\infty} / \epsilon)}{\log (1 / \gamma)},
\end{aligned}
\end{split}\]</div>
<p>and so the number of iterations required for an
<span class="math notranslate nohighlight">\(\epsilon\)</span>-accurate estimate is</p>
<div class="math notranslate nohighlight">
\[
T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).
\]</div>
<p>Note that we’ve applied the inequalities
<span class="math notranslate nohighlight">\(\|v^{(0)} - V^\pi\|_{\infty} \le 1/(1-\gamma)\)</span> and
<span class="math notranslate nohighlight">\(\log (1/x) \ge 1-x\)</span>.</p>
</section>
</div></section>
</section>
<section id="optimal-policies-in-infinite-horizon-mdps">
<span id="optimal-policy-finite"></span><h3><a class="toc-backref" href="#id24" role="doc-backlink"><span class="section-number">3.4.3. </span>Optimal policies in infinite-horizon MDPs</a><a class="headerlink" href="#optimal-policies-in-infinite-horizon-mdps" title="Link to this heading">#</a></h3>
<p>Now let’s move on to solving for an optimal policy in the
infinite-horizon case. As in <a class="reference internal" href="#optimal_policy_finite">the finite-horizon case</a>, an <strong>optimal policy</strong> <span class="math notranslate nohighlight">\(\pi^\star\)</span>
is one that does at least as well as any other policy in all situations.
That is, for all policies <span class="math notranslate nohighlight">\(\pi\)</span>, states <span class="math notranslate nohighlight">\(s \in \mathcal{S}\)</span>, times
<span class="math notranslate nohighlight">\(\hi \in \mathbb{N}\)</span>, and initial trajectories
<span class="math notranslate nohighlight">\(\tau_\hi = (s_0, a_0, r_0, \dots, s_\hi)\)</span> where <span class="math notranslate nohighlight">\(s_\hi = s\)</span>,</p>
<div class="math notranslate nohighlight" id="equation-optimal-policy-infinite">
<span class="eqno">(3.5)<a class="headerlink" href="#equation-optimal-policy-infinite" title="Link to this equation">#</a></span>\[\begin{split}\begin{aligned}
    V^{\pi^\star}(s) &amp;= \E_{\tau \sim \rho^{\pi^{\star}}}[r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2}  + \cdots \mid s_\hi = s] \\
    &amp;\ge \E_{\tau \sim \rho^{\pi}}[r_\hi + \gamma r_{\hi+1} + \gamma^2 r_{\hi+2} + \cdots \mid \tau_\hi]
\end{aligned}\end{split}\]</div>
<p>Once again, all optimal policies share the same <strong>optimal value function</strong> <span class="math notranslate nohighlight">\(V^\star\)</span>, and the greedy policy w.r.t. this value function
is optimal.</p>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Verify this by modifying the proof <a class="reference internal" href="#optimal_greedy">Theorem 3.3</a> from the finite-horizon case.</p>
</div>
<p>So how can we compute such an optimal policy? We can’t use the backwards
DP approach from the finite-horizon case <a class="reference internal" href="#pi_star_dp">Algorithm 3.2</a> since there’s no “final timestep” to start
from. Instead, we’ll exploit the fact that the Bellman consistency
equation <a class="reference internal" href="#equation-bellman-consistency-infinite">(3.1)</a> for the optimal value
function doesn’t depend on any policy:</p>
<div class="math notranslate nohighlight" id="equation-bellman-optimality">
<span class="eqno">(3.6)<a class="headerlink" href="#equation-bellman-optimality" title="Link to this equation">#</a></span>\[V^\star(s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^\star(s'). \right]\]</div>
<div class="admonition attention">
<p class="admonition-title">Attention</p>
<p>Verify this by substituting the greedy policy into the
Bellman consistency equation.</p>
</div>
<p>As before, thinking of the r.h.s. of <a class="reference internal" href="#equation-bellman-optimality">(3.6)</a> as an operator on value functions
gives the <strong>Bellman optimality operator</strong></p>
<div class="math notranslate nohighlight" id="equation-bellman-optimality-operator">
<span class="eqno">(3.7)<a class="headerlink" href="#equation-bellman-optimality-operator" title="Link to this equation">#</a></span>\[[\mathcal{J}^{\star}(v)](s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v(s') \right]\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">bellman_optimality_operator</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">r</span> <span class="o">+</span> <span class="n">mdp</span><span class="o">.</span><span class="n">γ</span> <span class="o">*</span> <span class="n">mdp</span><span class="o">.</span><span class="n">P</span> <span class="o">@</span> <span class="n">v</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">check_optimal</span><span class="p">(</span><span class="n">v</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">],</span> <span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">bellman_optimality_operator</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">mdp</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<section id="value-iteration">
<span id="id3"></span><h4><a class="toc-backref" href="#id25" role="doc-backlink"><span class="section-number">3.4.3.1. </span>Value iteration</a><a class="headerlink" href="#value-iteration" title="Link to this heading">#</a></h4>
<p>Since the optimal policy is still a policy, our result that the Bellman
operator is a contracting map still holds, and so we can repeatedly
apply this operator to converge to the optimal value function! This
algorithm is known as <strong>value iteration</strong>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">value_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">ε</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iterate the Bellman optimality operator until convergence.&quot;&quot;&quot;</span>
    <span class="n">op</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">bellman_optimality_operator</span><span class="p">,</span> <span class="n">mdp</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loop_until_convergence</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">),</span> <span class="n">ε</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">value_iteration</span><span class="p">(</span><span class="n">tidy_mdp_inf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([15.564166, 14.785956], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Note that the runtime analysis for an <span class="math notranslate nohighlight">\(\epsilon\)</span>-optimal value function
is exactly the same as <a class="reference internal" href="#iterative-pe"><span class="std std-ref">iterative policy evaluation</span></a>! This is because value iteration is simply
the special case of applying iterative policy evaluation to the
<em>optimal</em> value function.</p>
<p>As the final step of the algorithm, to return an actual policy
<span class="math notranslate nohighlight">\(\hat \pi\)</span>, we can simply act greedily w.r.t. the final iteration
<span class="math notranslate nohighlight">\(v^{(T)}\)</span> of our above algorithm:</p>
<div class="math notranslate nohighlight">
\[\hat \pi(s) = \arg\max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v^{(T)}(s') \right].\]</div>
<p>We must be careful, though: the value function of this greedy policy,
<span class="math notranslate nohighlight">\(V^{\hat \pi}\)</span>, is <em>not</em> the same as <span class="math notranslate nohighlight">\(v^{(T)}\)</span>, which need not even be a
well-defined value function for some policy!</p>
<p>The bound on the policy’s quality is actually quite loose: if
<span class="math notranslate nohighlight">\(\|v^{(T)} - V^\star\|_{\infty} \le \epsilon\)</span>, then the greedy policy
<span class="math notranslate nohighlight">\(\hat \pi\)</span> satisfies
<span class="math notranslate nohighlight">\(\|V^{\hat \pi} - V^\star\|_{\infty} \le \frac{2\gamma}{1-\gamma} \epsilon\)</span>,
which might potentially be very large.</p>
<div class="proof theorem admonition" id="greedy_worsen">
<p class="admonition-title"><span class="caption-number">Theorem 3.5 </span> (Greedy policy value worsening)</p>
<section class="theorem-content" id="proof-content">
<div class="math notranslate nohighlight">
\[\|V^{\hat \pi} - V^\star \|_{\infty} \le \frac{2 \gamma}{1-\gamma} \|v - V^\star\|_{\infty}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat \pi(s) = \arg\max_a q(s, a)\)</span> is the greedy policy w.r.t.</p>
<div class="math notranslate nohighlight">
\[q(s, a) = r(s, a) + \E_{s' \sim P(s, a)} v(s').\]</div>
</section>
</div><details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
Proof<div class="sd-summary-down docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-down" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M5.22 8.72a.75.75 0 000 1.06l6.25 6.25a.75.75 0 001.06 0l6.25-6.25a.75.75 0 00-1.06-1.06L12 14.44 6.28 8.72a.75.75 0 00-1.06 0z"></path></svg></div>
<div class="sd-summary-up docutils">
<svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-up" viewBox="0 0 24 24" aria-hidden="true"><path fill-rule="evenodd" d="M18.78 15.28a.75.75 0 000-1.06l-6.25-6.25a.75.75 0 00-1.06 0l-6.25 6.25a.75.75 0 101.06 1.06L12 9.56l5.72 5.72a.75.75 0 001.06 0z"></path></svg></div>
</summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">We first have</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        V^{\star}(s) - V^{\hat \pi}(s) &amp;= Q^{\star}(s,\pi^\star(s)) - Q^{\hat \pi}(s, \hat \pi(s))\\
        &amp;= [Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s))] + [Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))].
    
\end{aligned}
\end{split}\]</div>
<p class="sd-card-text">Let’s bound these two quantities separately.</p>
<p class="sd-card-text">For the first quantity, note that by the definition of <span class="math notranslate nohighlight">\(\hat \pi\)</span>, we
have</p>
<div class="math notranslate nohighlight">
\[q(s, \hat \pi(s)) \ge q(s,\pi^\star(s)).\]</div>
<p class="sd-card-text">Let’s add
<span class="math notranslate nohighlight">\(q(s, \hat \pi(s)) - q(s,\pi^\star(s)) \ge 0\)</span> to the first term to get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s)) &amp;\le [Q^{\star}(s,\pi^\star(s))- q(s,\pi^\star(s))] + [q(s, \hat \pi(s)) - Q^{\star}(s, \hat \pi(s))] \\
        &amp;= \gamma \E_{s' \sim P(s, \pi^{\star}(s))} [ V^{\star}(s') - v(s') ] + \gamma \E_{s' \sim P(s, \hat \pi(s))} [ v(s') - V^{\star}(s') ] \\
        &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty}.
    
\end{aligned}
\end{split}\]</div>
<p class="sd-card-text">The second quantity is bounded by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))
        &amp;=
        \gamma \E_{s'\sim P(s, \hat \pi(s))}\left[ V^\star(s') - V^{\hat \pi}(s') \right] \\
        &amp; \leq 
        \gamma \|V^{\star} - V^{\hat \pi}\|_\infty
    
\end{aligned}
\end{split}\]</div>
<p class="sd-card-text">and thus</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le 2 \gamma \|v - V^{\star}\|_{\infty} + \gamma \|V^{\star} - V^{\hat \pi}\|_\infty \\
        \|V^\star - V^{\hat \pi}\|_\infty &amp;\le \frac{2 \gamma \|v - V^{\star}\|_{\infty}}{1-\gamma}.
    
\end{aligned}
\end{split}\]</div>
</div>
</details><p>So in order to compensate and achieve
<span class="math notranslate nohighlight">\(\|V^{\hat \pi} - V^{\star}\| \le \epsilon\)</span>, we must have</p>
<div class="math notranslate nohighlight">
\[\|v^{(T)} - V^\star\|_{\infty} \le \frac{1-\gamma}{2 \gamma} \epsilon.\]</div>
<p>This means, using <a class="reference internal" href="#iterations_vi">Remark 3.2</a>, we need to run value iteration for</p>
<div class="math notranslate nohighlight">
\[T = O\left( \frac{1}{1-\gamma} \log\left(\frac{\gamma}{\epsilon (1-\gamma)^2}\right) \right)\]</div>
<p>iterations to achieve an <span class="math notranslate nohighlight">\(\epsilon\)</span>-accurate estimate of the optimal
value function.</p>
</section>
<section id="policy-iteration">
<span id="id4"></span><h4><a class="toc-backref" href="#id26" role="doc-backlink"><span class="section-number">3.4.3.2. </span>Policy iteration</a><a class="headerlink" href="#policy-iteration" title="Link to this heading">#</a></h4>
<p>Can we mitigate this “greedy worsening”? What if instead of
approximating the optimal value function and then acting greedily by it
at the very end, we iteratively improve the policy and value function
<em>together</em>? This is the idea behind <strong>policy iteration</strong>. In each step,
we simply set the policy to act greedily with respect to its own value
function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">policy_iteration</span><span class="p">(</span><span class="n">mdp</span><span class="p">:</span> <span class="n">MDP</span><span class="p">,</span> <span class="n">ε</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;S A&quot;</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Iteratively improve the policy and value function.&quot;&quot;&quot;</span>
    <span class="n">op</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">π</span><span class="p">:</span> <span class="n">v_to_greedy</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">eval_deterministic_infinite</span><span class="p">(</span><span class="n">mdp</span><span class="p">,</span> <span class="n">π</span><span class="p">))</span>
    <span class="n">π_init</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">mdp</span><span class="o">.</span><span class="n">S</span><span class="p">,</span> <span class="n">mdp</span><span class="o">.</span><span class="n">A</span><span class="p">))</span> <span class="o">/</span> <span class="n">mdp</span><span class="o">.</span><span class="n">A</span>  <span class="c1"># uniform random policy</span>
    <span class="k">return</span> <span class="n">loop_until_convergence</span><span class="p">(</span><span class="n">op</span><span class="p">,</span> <span class="n">π_init</span><span class="p">,</span> <span class="n">ε</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">policy_iteration</span><span class="p">(</span><span class="n">tidy_mdp_inf</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[1., 0.],
       [0., 1.]], dtype=float32)
</pre></div>
</div>
</div>
</div>
<p>Although PI appears more complex than VI, we’ll use the same contraction
property
<a class="reference internal" href="#bellman_contraction">Theorem 3.4</a> to show convergence. This will give
us the same runtime bound as value iteration and iterative policy
evaluation for an <span class="math notranslate nohighlight">\(\epsilon\)</span>-optimal value function
<a class="reference internal" href="#iterations_vi">Remark 3.2</a>, although in practice, PI often converges
much faster.</p>
<div class="proof theorem admonition" id="pi_iter_analysis">
<p class="admonition-title"><span class="caption-number">Theorem 3.6 </span> (Policy Iteration runtime and convergence)</p>
<section class="theorem-content" id="proof-content">
<p>We aim to show that the number of iterations required for an
<span class="math notranslate nohighlight">\(\epsilon\)</span>-accurate estimate of the optimal value function is</p>
<div class="math notranslate nohighlight">
\[T = O\left( \frac{1}{1-\gamma} \log\left(\frac{1}{\epsilon (1-\gamma)}\right) \right).\]</div>
<p>This bound follows from the contraction property <a class="reference internal" href="#equation-bellman-convergence">(3.3)</a>:</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.\]</div>
<p>We’ll prove that the iterates of PI respect the contraction property by
showing that the policies improve monotonically:</p>
<div class="math notranslate nohighlight">
\[V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s).\]</div>
<p>Then we’ll use this to show
<span class="math notranslate nohighlight">\(V^{\pi^{t+1}}(s) \ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)\)</span>. Note that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
(s) &amp;= \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^{\pi^{t}}(s') \right] \\
    &amp;= r(s, \pi^{t+1}(s)) + \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} V^{\pi^{t}}(s')
\end{aligned}
\end{split}\]</div>
<p>Since
<span class="math notranslate nohighlight">\([\mathcal{J}^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s)\)</span>, we then have</p>
<div class="math notranslate nohighlight" id="equation-pi-iter-proof">
<span class="eqno">(3.8)<a class="headerlink" href="#equation-pi-iter-proof" title="Link to this equation">#</a></span>\[\begin{split}$$
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) \\
    &amp;= \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right].
\end{aligned}
$$\end{split}\]</div>
<p>But note that the
expression being averaged is the same as the expression on the l.h.s.
with <span class="math notranslate nohighlight">\(s\)</span> replaced by <span class="math notranslate nohighlight">\(s'\)</span>. So we can apply the same inequality
recursively to get</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &amp;\ge  \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge \gamma^2 \E_{\substack{s' \sim P(s, \pi^{t+1}(s)) \\ s'' \sim P(s', \pi^{t+1}(s'))}} \left[V^{\pi^{t+1}}(s'') -  V^{\pi^{t}}(s'') \right]\\
    &amp;\ge \cdots
\end{aligned}
\end{split}\]</div>
<p>which implies that <span class="math notranslate nohighlight">\(V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)\)</span>
for all <span class="math notranslate nohighlight">\(s\)</span> (since the r.h.s. converges to zero). We can then plug this
back into
<a class="reference internal" href="#equation-pi-iter-proof">(3.8)</a>
to get the desired result:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    V^{\pi^{t+1}}(s) - \mathcal{J}^{\star} (V^{\pi^{t}})(s) &amp;= \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &amp;\ge 0 \\
    V^{\pi^{t+1}}(s) &amp;\ge [\mathcal{J}^{\star}(V^{\pi^{t}})](s)
\end{aligned}
\end{split}\]</div>
<p>This means we can now apply the Bellman convergence result <a class="reference internal" href="#equation-bellman-convergence">(3.3)</a> to get</p>
<div class="math notranslate nohighlight">
\[\|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \|\mathcal{J}^{\star} (V^{\pi^{t}}) - V^{\star}\|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.\]</div>
</section>
</div></section>
</section>
</section>
<section id="summary">
<h2><a class="toc-backref" href="#id27" role="doc-backlink"><span class="section-number">3.5. </span>Summary</a><a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Markov decision processes (MDPs) are a framework for sequential
decision making under uncertainty. They consist of a state space
<span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, an action space <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>, an initial state distribution
<span class="math notranslate nohighlight">\(\mu \in \Delta(\mathcal{S})\)</span>, a transition function <span class="math notranslate nohighlight">\(P(s' \mid s, a)\)</span>, and a
reward function <span class="math notranslate nohighlight">\(r(s, a)\)</span>. They can be finite-horizon (ends after
<span class="math notranslate nohighlight">\(H\)</span> timesteps) or infinite-horizon (where rewards scale by
<span class="math notranslate nohighlight">\(\gamma \in (0, 1)\)</span> at each timestep).</p></li>
<li><p>Our goal is to find a policy <span class="math notranslate nohighlight">\(\pi\)</span> that maximizes expected total
reward. Policies can be <strong>deterministic</strong> or <strong>stochastic</strong>,
<strong>state-dependent</strong> or <strong>history-dependent</strong>, <strong>stationary</strong> or
<strong>time-dependent</strong>.</p></li>
<li><p>A policy induces a distribution over <strong>trajectories</strong>.</p></li>
<li><p>We can evaluate a policy by computing its <strong>value function</strong>
<span class="math notranslate nohighlight">\(V^\pi(s)\)</span>, which is the expected total reward starting from state
<span class="math notranslate nohighlight">\(s\)</span> and following policy <span class="math notranslate nohighlight">\(\pi\)</span>. We can also compute the
<strong>state-action value function</strong> <span class="math notranslate nohighlight">\(Q^\pi(s, a)\)</span>, which is the expected
total reward starting from state <span class="math notranslate nohighlight">\(s\)</span>, taking action <span class="math notranslate nohighlight">\(a\)</span>, and then
following policy <span class="math notranslate nohighlight">\(\pi\)</span>. In the finite-horizon setting, these also
depend on the timestep <span class="math notranslate nohighlight">\(\hi\)</span>.</p></li>
<li><p>The <strong>Bellman consistency equation</strong> is an equation that the value
function must satisfy. It can be used to solve for the value
functions exactly. Thinking of the r.h.s. of this equation as an
operator on value functions gives the <strong>Bellman operator</strong>.</p></li>
<li><p>In the finite-horizon setting, we can compute the optimal policy
using <strong>dynamic programming</strong>.</p></li>
<li><p>In the infinite-horizon setting, we can compute the optimal policy
using <strong>value iteration</strong> or <strong>policy iteration</strong>.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./3_mdps"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../2_bandits/bandits.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">2. </span>Bandits</p>
      </div>
    </a>
    <a class="right-next"
       href="../4_fitted_dp/fitted_dp.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Fitted dynamic programming algorithms</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finite-horizon-mdps">3.1. Finite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policies">3.1.1. Policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trajectories">3.1.2. Trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions">3.1.3. Value functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-policies">3.1.3.1. Greedy policies</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-one-step-bellman-consistency-equation">3.1.4. The one-step (Bellman) consistency equation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-one-step-bellman-operator">3.1.5. The one-step Bellman operator</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-finite-horizon-mdps">3.2. Solving finite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-in-finite-horizon-mdps">3.2.1. Policy evaluation in finite-horizon MDPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-policies-in-finite-horizon-mdps">3.2.2. Optimal policies in finite-horizon MDPs</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#infinite-horizon-mdps">3.3. Infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discounted-rewards">3.3.1. Discounted rewards</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stationary-policies">3.3.2. Stationary policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#value-functions-and-bellman-consistency">3.3.3. Value functions and Bellman consistency</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#solving-infinite-horizon-mdps">3.4. Solving infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-bellman-operator-is-a-contraction-mapping">3.4.1. The Bellman operator is a contraction mapping</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-evaluation-in-infinite-horizon-mdps">3.4.2. Policy evaluation in infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-inversion-for-deterministic-policies">3.4.2.1. Matrix inversion for deterministic policies</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterative-policy-evaluation">3.4.2.2. Iterative policy evaluation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimal-policies-in-infinite-horizon-mdps">3.4.3. Optimal policies in infinite-horizon MDPs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#value-iteration">3.4.3.1. Value iteration</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#policy-iteration">3.4.3.2. Policy iteration</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">3.5. Summary</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>