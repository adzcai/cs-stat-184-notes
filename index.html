
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Introduction &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'index';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="1. Markov Decision Processes" href="mdps.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="#">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="mdps.html">1. Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">2. Linear Quadratic Regulators</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">3. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="supervised_learning.html">4. Supervised learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="fitted_dp.html">5. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="pg.html">6. Policy Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="imitation_learning.html">7. Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="planning.html">8. Planning</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">9. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="contextual_bandits.html">10. Contextual bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">11. Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="background.html">12. Appendix: Background</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/index.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-in-a-nutshell">Reinforcement learning in a nutshell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-tasks-in-reinforcement-learning">Core tasks in reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#programming">Programming</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h1>
<p>Welcome to the study of reinforcement learning!
This textbook accompanies the undergraduate course <a class="reference external" href="http://lucasjanson.fas.harvard.edu/CS_Stat_184_0.html">CS 1840/STAT 184</a> taught at Harvard.
It is intended to be a friendly yet rigorous introduction to this active subfield of machine learning.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>This book assumes the same prerequisites as the course: You should be familiar with multivariable calculus, linear algebra, and probability.
For Harvard undergraduates, this is fulfilled by Math 21a, Math 21b, and Stat 110, or their equivalents.
Stat 111 is strongly recommended but not required.
Specifically, we will assume that you know the following topics. The <em>italicized terms</em> have brief re-introductions in the text or in the <a class="reference internal" href="background.html#background"><span class="std std-ref">Appendix: Background</span></a>:</p>
<ul class="simple">
<li><p><strong>Linear Algebra:</strong> Vectors and matrices, matrix multiplication, matrix
inversion, eigenvalues and eigenvectors.</p></li>
<li><p><strong>Multivariable Calculus:</strong> Partial derivatives, the chain rule, Taylor series, <em>gradients, directional derivatives, Lagrange multipliers.</em></p></li>
<li><p><strong>Probability:</strong> Random variables, probability distributions,
expectation and variance, the law of iterated expectations (Adam’s rule), covariance, conditional probability, Bayes’s rule, and the law of total probability.</p></li>
</ul>
<p>You should also be comfortable with programming in Python.
See <a class="reference internal" href="#programming"><span class="std std-ref">Programming</span></a> for more about this textbook’s philosophy regarding programming.</p>
</section>
<section id="reinforcement-learning-in-a-nutshell">
<h2>Reinforcement learning in a nutshell<a class="headerlink" href="#reinforcement-learning-in-a-nutshell" title="Link to this heading">#</a></h2>
<p>Broadly speaking,
RL studies <strong>sequential decision-making</strong> in <strong>dynamic environments.</strong>
An RL algorithm finds a <strong>policy,</strong> or strategy, that maximizes the <strong>reward</strong> it obtains from the environment.</p>
<p>RL provides a powerful framework for attacking a wide variety of problems,
including robotic control, video games and board games, resource management, language modelling, and more.
It also provides an interdisciplinary paradigm for studying animal and human behavior.
Many of the most stunning results in machine learning, ranging from AlphaGo to ChatGPT, are built on top of RL.</p>
<p>How does RL compare to <strong>supervised learning</strong> and <strong>unsupervised learning,</strong>
the other two core machine learning paradigms?</p>
<ul>
<li><p><strong>Supervised learning</strong> (SL) concerns itself with learning a mapping from inputs to outputs.
Typically the data takes the form of <em>statistically independent</em> input-output pairs.
In RL, however, the data is generated by the agent interacting with the environment,
meaning the sequential observations of the state are <strong>not independent</strong> from each other.</p>
<p>Conversely, SL is a well-studied field that provides many useful tools for RL.
For example, it may be useful to use supervised learning to predict how valuable a given state is, or to predict the probability of transitioning to a given state.
We will see concrete examples in <a class="reference internal" href="fitted_dp.html#fitted-dp"><span class="std std-ref">Fitted Dynamic Programming Algorithms</span></a>.</p>
</li>
<li><p><strong>Unsupervised learning</strong> concerns itself with learning the <em>structure</em> of data without the use of “labels”.
In RL, though, the agent receives a <strong>reward signal</strong> from the environment,
which can be thought of as a type of “label”.</p>
<p>Unsupervised learning (especially <strong>self-supervised</strong> learning) is crucial in many applications of RL for purposes such as dimensionality reduction or representation learning.</p>
</li>
</ul>
</section>
<section id="core-tasks-in-reinforcement-learning">
<h2>Core tasks in reinforcement learning<a class="headerlink" href="#core-tasks-in-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>What tasks, exactly, does RL comprise?
RL algorithms typically must solve two main subtasks:</p>
<ul class="simple">
<li><p><strong>Policy evaluation (prediction):</strong>
How ‘good’ is a specific state, or state-action pair (for a specific agent)?
That is, how much reward does it lead to in the long run?</p></li>
<li><p><strong>Policy optimization (control):</strong>
Suppose we have a complete, accurate model of how the environment behaves.
What is the best action to take in every scenario?</p></li>
</ul>
<p>This decomposition also provides a useful framework for analyzing the algorithms we will encounter.</p>
<!-- **Recursion (bootstrapping):** How can we "reuse" our current predictions to generate new information? -->
<!-- **Exploration-exploitation tradeoff:** Should we try new actions, or capitalize on actions that we currently believe to be good? -->
</section>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading">#</a></h2>
<p>The course will progress through the following units:</p>
<p><a class="reference internal" href="mdps.html#mdps"><span class="std std-ref">Markov Decision Processes</span></a> introduces (finite) <strong>Markov Decision Processes,</strong>
the core mathematical framework for describing interactive environments,
as well as the core vocabulary we will use throughout the course.</p>
<p><a class="reference internal" href="control.html#lqr"><span class="std std-ref">The Linear Quadratic Regulator</span></a> is a standalone chapter on the <strong>linear quadratic regulator</strong>,
an important tool for <em>continuous control</em>,
in which the state and action spaces are no longer finite but rather continuous.
This has widespread applications in robotics.</p>
<p><a class="reference internal" href="bandits.html#bandits"><span class="std std-ref">Multi-Armed Bandits</span></a> introduces the <strong>multi-armed bandit</strong> model for <strong>stateless</strong> sequential decision-making tasks.
In exploring a number of algorithms,
we will see how each of them strikes a different balance between <em>exploring</em> new options and <em>exploiting</em> known options.
This <strong>exploration-exploitation tradeoff</strong> is a core consideration in RL algorithm design.</p>
<p><a class="reference internal" href="supervised_learning.html#supervised-learning"><span class="std std-ref">Supervised learning</span></a> is a standalone crash course on some tools from supervised learning that we will use in later chapters.</p>
<p><a class="reference internal" href="fitted_dp.html#fitted-dp"><span class="std std-ref">Fitted Dynamic Programming Algorithms</span></a> introduces <strong>fitted dynamic programming</strong> algorithms for solving MDPs.
These algorithms use supervised learning to approximately evaluate policies when they cannot be evaluated exactly.</p>
<p><a class="reference internal" href="pg.html#pg"><span class="std std-ref">Policy Optimization</span></a> walks through an important class of algorithms based on iteratively improving the policy.</p>
<p><a class="reference internal" href="imitation_learning.html#imitation-learning"><span class="std std-ref">Imitation Learning</span></a> attempts to learn a good policy from expert demonstrations.
At its most basic, this is an application of supervised learning to RL tasks.</p>
<p><a class="reference internal" href="planning.html#planning"><span class="std std-ref">Planning</span></a> looks at ways to <em>explicitly</em> plan ahead when the dynamics of the environment are known.
We will study the <em>Monte Carlo Tree Search</em> heuristic,
which has been used to great success in the famous AlphaGo algorithm and its successors.</p>
<p><a class="reference internal" href="exploration.html#exploration"><span class="std std-ref">Exploration in MDPs</span></a> continues to investigate the exploration-exploitation tradeoff.
We will extend ideas from multi-armed bandits to the MDP setting.</p>
<p><a class="reference internal" href="contextual_bandits.html#contextual-bandits"><span class="std std-ref">Contextual bandits</span></a> extends the multi-armed bandit setting with some observed state.</p>
<p><a class="reference internal" href="background.html#background"><span class="std std-ref">Appendix: Background</span></a> contains an overview of selected background mathematical content and programming content.</p>
<!-- 
| Chapter | States | Actions | Rewards (or costs) |
|:-------:|:------:|:-------:|:-------:|
| {ref}`bandits` | N/A | Finite | Stochastic |
| {ref}`mdps` | Finite | Finite | Deterministic |
| {ref}`fitted_dp` | Large or continuous | Finite | Deterministic |
| {ref}`lqr` | Continuous | Continuous | Deterministic |
-->
</section>
<section id="notation">
<h2>Notation<a class="headerlink" href="#notation" title="Link to this heading">#</a></h2>
<p>We will use the following notation throughout the book.
This notation is inspired by <span id="id1">[<a class="reference internal" href="bibliography.html#id4" title="Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning Series. The MIT Press, Cambridge, Massachusetts, second edition edition, 2018. ISBN 978-0-262-03924-6.">Sutton and Barto, 2018</a>]</span> and <span id="id2">[<a class="reference internal" href="bibliography.html#id5">Agarwal <em>et al.</em>, 2022</a>]</span>.
We use <span class="math notranslate nohighlight">\([N]\)</span> as shorthand for the set <span class="math notranslate nohighlight">\(\{ 0, 1, \dots, N-1 \}\)</span>.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-center"><p>Element</p></th>
<th class="head text-center"><p>Space</p></th>
<th class="head text-left"><p>Definition (of element)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(s\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{S}\)</span></p></td>
<td class="text-left"><p>A state.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(a\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{A}\)</span></p></td>
<td class="text-left"><p>An action.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(r\)</span></p></td>
<td class="text-center"><p></p></td>
<td class="text-left"><p>A reward.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(\gamma\)</span></p></td>
<td class="text-center"><p></p></td>
<td class="text-left"><p>A discount factor.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(\tau\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{T}\)</span></p></td>
<td class="text-left"><p>A trajectory.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(\pi\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\Pi\)</span></p></td>
<td class="text-left"><p>A policy.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(V^\pi\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{S} \to \mathbb{R}\)</span></p></td>
<td class="text-left"><p>The value function of policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p><span class="math notranslate nohighlight">\(Q^\pi\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span></p></td>
<td class="text-left"><p>The action-value function (a.k.a. Q-function) of policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></td>
</tr>
<tr class="row-even"><td class="text-center"><p><span class="math notranslate nohighlight">\(A^\pi\)</span></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span></p></td>
<td class="text-left"><p>The advantage function of policy <span class="math notranslate nohighlight">\(\pi\)</span>.</p></td>
</tr>
<tr class="row-odd"><td class="text-center"><p></p></td>
<td class="text-center"><p><span class="math notranslate nohighlight">\(\triangle(\mathcal{X})\)</span></p></td>
<td class="text-left"><p>A distribution supported on <span class="math notranslate nohighlight">\(\mathcal{X}\)</span>.</p></td>
</tr>
</tbody>
</table>
<!-- |   $\mu$      | $\triangle(\mathcal{S})$ | A distribution over states.        | -->
<p>|    <span class="math notranslate nohighlight">\(\hi\)</span>     |   <span class="math notranslate nohighlight">\([\hor]\)</span>               | Time horizon index of an MDP.    |
|    <span class="math notranslate nohighlight">\(k\)</span>       |   <span class="math notranslate nohighlight">\([K]\)</span>                  | Arm index of a multi-armed bandit. |
|    <span class="math notranslate nohighlight">\(t\)</span>       |   <span class="math notranslate nohighlight">\([T]\)</span>                  | Iteration index of an algorithm.  |
|    <span class="math notranslate nohighlight">\(\theta\)</span>  | <span class="math notranslate nohighlight">\(\Theta\)</span>                 | A set of parameters. |</p>
<p>Note that throughout the text, certain symbols will stand for either random variables or fixed values.
We aim to clarify in ambiguous settings.</p>
</section>
<section id="programming">
<span id="id3"></span><h2>Programming<a class="headerlink" href="#programming" title="Link to this heading">#</a></h2>
<p>Why include code in a textbook?
We believe that implementing an algorithm is a strong test of your understanding of it;
mathematical notation can often abstract away details,
while a computer must be given every single instruction.
We have sought to write readable Python code that is self-contained within each file.
This approach is inspired by <span id="id4">[<a class="reference internal" href="bibliography.html#id9" title="Gerald Jay Sussman, Jack Wisdom, and Will Farr. Functional Differential Geometry. The MIT Press, Cambridge, MA, 2013. ISBN 978-0-262-01934-7.">Sussman <em>et al.</em>, 2013</a>]</span>.
There are some ways in which the code style differs from typical software projects:</p>
<ul class="simple">
<li><p>We keep use of language features to a minimum,
even if it leads to code that could otherwise be more concisely or idiomatically expressed.</p></li>
<li><p>The variable names used in the code match those used in the main text.
For example, the variable <code class="docutils literal notranslate"><span class="pre">s</span></code> will be used instead of the more explicit <code class="docutils literal notranslate"><span class="pre">state</span></code>.</p></li>
</ul>
<p>We also make extensive use of Python <em>type annotations</em> to explicitly specify variable types, including shapes of vectors and matrices using the <a class="reference external" href="https://github.com/patrick-kidger/jaxtyping">jaxtyping</a> library.</p>
<p>This is an interactive book built with <a class="reference external" href="https://jupyterbook.org/en/stable/intro.html">Jupyter Book</a>.
It uses <a class="reference external" href="https://docs.python.org/3.11/contents.html">Python 3.11</a>.
It uses the <a class="reference external" href="https://jax.readthedocs.io/en/latest/index.html">JAX</a> library for numerical computing.
JAX was chosen for the clarity of its functional style and due to its mature RL ecosystem,
sustained in large part by the Google DeepMind research group and a large body of open-source contributors.
We use the standard <a class="reference external" href="https://gymnasium.farama.org/">Gymnasium</a> library for interfacing with RL environments.</p>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="right-next"
       href="mdps.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Markov Decision Processes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforcement-learning-in-a-nutshell">Reinforcement learning in a nutshell</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#core-tasks-in-reinforcement-learning">Core tasks in reinforcement learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overview">Overview</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#notation">Notation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#programming">Programming</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>