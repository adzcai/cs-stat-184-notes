\documentclass[../main/main]{subfiles}

\setcounter{chapter}{0}

\begin{document}


\chapter{Markov Decision Processes}

\tableofcontents

\section{Introduction}

The field of RL studies how an agent can learn to make sequential decisions in an environment. This is a very general problem! How can we \emph{formalize} this task in a way that is both \emph{sufficiently general} yet also tractable enough for \emph{fruitful analysis}?

Let's consider some examples of sequential decision problems to identify the key common properties we'd like to capture:

\begin{itemize}
    \item \textbf{Board games} like chess or Go, where the player takes turns with the opponent to make moves on the board.
    \item \textbf{Video games} like Super Mario Bros or Breakout, where the player controls a character to reach the goal.
    \item \textbf{Robotic control}, where the robot can move and interact with the real-world environment to complete some task.
\end{itemize}

% https://unsplash.com/photos/JrZvYuBYzCU

% \todo{Insert pictures}

All of these fit into the RL framework! Consider what the agent, state, and possible reward signals are in each example.

These are environments where the \textbf{state transitions}, the ``rules'' of the environment, only depend on the \emph{most recent} state and action. We can formalize such environments using \textbf{Markov decision processes} (MDPs). Formally, we say that the state transitions satisfy the \textbf{Markov property}:

\[
    \P(s_{t+1} \mid s_0, a_0, \dots, s_t, a_t) = P(s_{t+1} \mid s_t, a_t).
\]

Where $P : \S \times \A \to \Delta(\S)$ is the state transition function. We'll see that this simple assumption leads to a rich set of problems and algorithms.

MDPs are usually classified as \textbf{finite-horizon} (aka \textbf{episodic}), where the interactions end after some finite number of time steps, or \textbf{infinite-horizon}, where the interactions can continue indefinitely. We'll begin with the finite-horizon case and then discuss the infinite-horizon case.

In each setting, we'll describe how to evaluate different \textbf{policies} (strategies for choosing actions) and how to compute or approximate the \textbf{optimal policy}. We'll introduce the \textbf{Bellman consistency condition}, which allows us to analyze the whole series of interactions in terms of individual timesteps.

\section{Finite horizon (episodic) MDPs}

The components of a finite-horizon (aka \textbf{episodic}) Markov decision process are:

\begin{enumerate}
    \item The \textbf{state} that the agent interacts with. We use $\S$ to denote the set of possible states, called the \textbf{state space}.
    \item The \textbf{actions} that the agent can take. We use $\A$ to denote the set of possible actions, called the \textbf{action space}.
    \item Some \textbf{initial state distribution} $\mu \in \Delta(\S)$.
    \item The \textbf{state transitions} (a.k.a. \textbf{dynamics}) $P : \S \times \A \to \Delta(\S)$ that describe what state the agent transitions to after taking an action.
    \item The \textbf{reward} signal. In this course we'll take it to be a deterministic function on state-action pairs, $r : \S \times \A \to \mathbb{R}$, but in general many results will extend to a \emph{stochastic} reward signal.
    \item A time horizon $H \in \mathbb{N}$ that specifies the number of interactions in an \textbf{episode}.
\end{enumerate}

Combined together, these objects specify a finite-horizon Markov decision process:

\[
    M = (\S, \A, \mu, P, r, H).
\]

\begin{example}{Tidying}{tidy}
    Let's consider an extremely simple decision problem throughout this chapter: the task of keeping your room tidy!

    Your room has the possible states $\mathcal{S} = \{ \text{orderly}, \text{messy} \}$. You can take either of the actions $\A = \{ \text{tidy}, \text{ignore} \}$. The room starts off orderly.
    
    The state transitions are as follows: if you tidy the room, it becomes (or remains) orderly; if you ignore the room, it might become messy.
    
    The rewards are as follows: You get penalized for tidying an orderly room (a waste of time) or ignoring a messy room, but you get rewarded for ignoring an orderly room (since you can enjoy). Tidying a messy room is a chore that gives no reward.
    
    These are summarized in the following table:

    \[
    \begin{array}{ccccc}
        s & a & P(\text{orderly} \mid s, a) & P(\text{messy} \mid s, a) & r(s, a) \\
        \text{orderly} & \text{tidy} & 1 & 0 & -1 \\
        \text{orderly} & \text{ignore} & 0.7 & 0.3 & 1 \\
        \text{messy} & \text{tidy} & 1 & 0 & 0 \\
        \text{messy} & \text{ignore} & 0 & 1 & -1
    \end{array}
    \]

    Consider a time horizon of $H = 7$ days (one interaction per day). Let $t = 0$ correspond to Monday and $t = 6$ corresponds to Sunday.
\end{example}

\subsection{Policies}

A \textbf{policy} $\pi$ describes the agent's strategy: which actions it takes in a given situation. A key goal of RL is to find the \textbf{optimal policy} that maximizes the total reward on average.

There are two axes along which policies can vary:

\begin{itemize}
    \item Policies can be \textbf{deterministic} or \textbf{stochastic}. A deterministic policy maps ``situations'' to actions while a stochastic policy maps ``situations'' to \emph{probability distributions} over actions. (The use of ``situation'' is clarified below.)
    \item Policies can be \textbf{stationary} or \textbf{history-dependent}. A stationary policy only depends on the current state, while a history-dependent policy can depend on the entire history of states, actions, and rewards. In the episodic setting, we'll also consider \textbf{time-dependent} policies that depend on the time step $t$, i.e. $\pi = \{ \pi_0, \dots, \pi_{H-1} \}$ where $\pi_t : \mathcal{S} \to \A$ or $\Delta(\A)$ for each $t \in [H]$.
\end{itemize}

A fascinating result is that every finite-horizon MDP has an optimal policy that is time-dependent and deterministic! Intuitively, the Markov property implies that the current state contains all the information we need to make the optimal decision. We'll prove this result constructively later in the chapter.

% \todo{Insert timeline diagram of policy}

\begin{example}{Tidying policy}{tidy_policy}
    Here are some possible policies for the tidying example:

    \begin{itemize}
        \item Always tidy: $\pi_t(s) = \text{tidy}$ for all $t$.
        \item Only tidy on weekends: $\pi_t(s) = \text{tidy}$ if $t \in \{ 5, 6 \}$ and $\pi_t(s) = \text{ignore}$ otherwise.
        \item Only tidy if the room is messy: $\pi_t(\text{messy}) = \text{tidy}$ and $\pi_t(\text{orderly}) = \text{ignore}$ for all $t$.
    \end{itemize}
\end{example}

\subsection{Trajectories}

A sequence of states, actions, and rewards is called a \textbf{trajectory}:

\[
    \tau = (s_0, a_0, r_0, \dots, s_{H-1}, a_{H-1}, r_{H-1})
\]

(Note that sources differ as to whether to include the reward at the final time step. This is a minor detail.)

Once we've chosen a policy, we can sample trajectories by choosing actions according to the policy and observing the state transitions and rewards. That is, a policy induces a distribution $\rho^{\pi}$ over trajectories. (We assume that $\mu$ and $P$ are clear from context.)

\begin{example}{Trajectories in the tidying environment}{tidy_traj}
    Here is a possible trajectory for the tidying example:

    \begin{center}
    \begin{tabular}{cccccccc}
        $t$ & $0$ & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ \\
        \hline
        $s$ & orderly & orderly & orderly & messy & messy & orderly & orderly \\
        $a$ & tidy & ignore & ignore & ignore & tidy & ignore & ignore \\
        $r$ & $-1$ & $1$ & $1$ & $-1$ & $0$ & $1$ & $1$
    \end{tabular}
    \end{center}

    Could any of the above policies have generated this trajectory?
\end{example}

Note that for a stationary policy, using the Markov property, we can specify this probability distribution in an \textbf{autoregressive} way (i.e. one timestep at a time):

\[
    \rho^{\pi}(\tau) := \mu(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots P(s_{H-1} \mid s_{H-2}, a_{H-2}) \pi_{H-1}(a_{H-1} \mid s_{H-1})
\]

\textbf{Exercise:} How would you modify this to include stochastic rewards?

For a deterministic policy $\pi$, we have that $\pi_t(a \mid s) = \mathbb{I}[a = \pi_t(s)]$; that is, the probability of taking an action is $1$ if it's the unique action prescribed by the policy for that state and $0$ otherwise. In this case, the only randomness in sampling trajectories comes from the initial state distribution $\mu$ and the state transitions $P$.

\subsection{Value functions}

We'll use $G_t$ to denote the cumulative reward from a given time step onwards (called the \textbf{return-to-go}):

\[
    G_t(\tau) := r_t + \cdots + r_{H-1}
\]

(The dependence on $\tau$ will be omitted for brevity.) The main goal of RL is to find an \textbf{optimal policy} $\pi^\star$ that maximizes the expected return:

\[
    \pi^{\star} = \argmax_{\pi} \E_{\tau \sim \rho^\pi} [G_0].
\]

We'll need a concise way to refer to the expected return conditional on \emph{starting in a given state at a given time}. We call this the \textbf{value function} of $\pi$ at time $t$ and denote it by

\[
    V_t^\pi(s) := \E_{\tau \sim \rho^\pi} [G_t \mid s_t = s]
\]


Similarly, we can define the \textbf{action-value function} (aka the \textbf{Q-function}) as the expected return when starting in a given state and taking a given action:

\[
    Q_t^\pi(s, a) := \E_{\tau \sim \rho^\pi} [G_t \mid s_t = s, a_t = a]
\]

\begin{remark}{Connection between value and action-value functions}{value_connection}
    Note that the value function is just the average action-value over actions drawn from the policy:

    \[
        V_t^\pi(s) = \E_{a \sim \pi_t(s)} [Q_t^\pi(s, a)]
    \]

    and the action-value can be expressed in terms of the value of the following state:

    \[
        Q_t^\pi(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [V_{t+1}^\pi(s')]
    \]
\end{remark}

\subsubsection{The one-step (Bellman) consistency equation}

Note that by simply considering the return as the sum of the \emph{current} reward and the \emph{future} return, we can describe the value function recursively (in terms of itself):

\begin{equation}
    V_t^\pi(s) = \E_{\substack{a \sim \pi_t(s) \\ s' \sim P(s, a)}} [r(s, a) + V_{t+1}^\pi(s')]
\end{equation}

This is named the \textbf{Bellman consistency equation} after \textbf{Richard Bellman} (1920--1984), who is credited with introducing dynamic programming in 1953.

\begin{remark}{The Bellman consistency equation for deterministic policies}{bellman_det}
    Note that for deterministic policies, the Bellman consistency equation simplifies to

    \[
        V_t^\pi(s) = r(s, \pi_t(s)) + \E_{s' \sim P(s, \pi_t(s))} [V_{t+1}^\pi(s')]
    \]
\end{remark}

\begin{remark}{Bellman consistency for the action-value function}{bellman_q}
    One can analogously derive the Bellman consistency equation for the action-value function:

    \[
        Q_t^\pi(s, a) = r(s, a) + \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi_{t+1}(s')}} [Q_{t+1}^\pi(s', a')]
    \]

    which can also be simplified for deterministic policies:

    \[
        Q_t^\pi(s, a) = r(s, a) + \E_{s' \sim P(s, a)} [Q_{t+1}^\pi(s', \pi_{t+1}(s'))]
    \]
\end{remark}

\subsubsection{The Bellman operator}
\label{sec:bellman_operator}

Fix a policy $\pi$. Consider the higher-order operator that takes in a ``value function'' $v : \S \to \R$ and returns the r.h.s. of the Bellman equation for that ``value function'':

\[
    [\bop^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + v(s')]
\]

We'll call $\bop^\pi$ the \textbf{Bellman operator} for $\pi$. Note that it's defined on any ``value function'' mapping states to real numbers; $v$ doesn't necessarily have to be a well-defined value function for some policy. It also gives us a concise way to express the Bellman consistency equation:

\[
    V_t^\pi = \bop^{\pi}(V_{t+1}^\pi)
\]

Intuitively, the output of the Bellman operator, a new ``value function'', evaluates states as follows: from a given state, take one action according to $\pi$, observe the reward, and then evaluate the next state using the input ``value function''.


\subsection{Policy evaluation}

How can we actually compute the value function of a given policy? This is the task of \textbf{policy evaluation}.

\subsubsection{Dynamic programming}

The Bellman consistency equation gives us a convenient algorithm for evaluating stationary policies: it expresses the value function at time $t$ as a function of the value function at time $t+1$. This means we can start at the end of the time horizon, where the value is known, and work backwards in time, using the Bellman consistency equation to compute the value function at each time step.

\begin{definition}{Dynamic programming for policy evaluation in finite-horizon MDPs}{dp_finite}
    \begin{algorithmic}
\State $V_t(s) \gets 0$ for all $t \in \{ 0, \dots, H \}, s \in S$
\For{$t = H-1, \dots, 0$}
    \For{$s \in \S, a \in \A, s' \in \S$}
        \State $V_t(s) \gets V_t(s) + \pi_t(a \mid s) P(s' \mid s, a) [r(s, a) + V_{t+1}(s')]$
    \EndFor
\EndFor
    \end{algorithmic}
\end{definition}


\begin{example}{Tidying policy evaluation}{tidy_eval}
    Let's evaluate the policy from \ref{eg:tidy_policy} that tidies iff the room is messy. We'll use the Bellman consistency equation to compute the value function at each time step.

    \begin{align*}
        V_{H-1}^\pi(\text{orderly}) &= r(\text{orderly}, \text{ignore}) \\
        &= 1 \\
        V_{H-1}^\pi(\text{messy}) &= r(\text{messy}, \text{tidy}) \\
        &= 0 \\
        V_{H-2}^\pi(\text{orderly}) &= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-1}^\pi(s')] \\
        &= 1 + 0.7 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-1}^{\pi}(\text{messy}) \\
        &= 1 + 0.7 \cdot 1 + 0.3 \cdot 0 \\
        &= 1.7 \\
        V_{H-2}^\pi(\text{messy}) &= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-1}^\pi(s')] \\
        &= 0 + 1 \cdot V_{H-1}^{\pi}(\text{orderly}) + 0 \cdot V_{H-1}^{\pi}(\text{messy}) \\
        &= 1 \\
        V_{H-3}^\pi(\text{orderly}) &= r(\text{orderly}, \text{ignore}) + \E_{s' \sim P(\text{orderly}, \text{ignore})} [V_{H-2}^\pi(s')] \\
        &= 1 + 0.7 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0.3 \cdot V_{H-2}^{\pi}(\text{messy}) \\
        &= 1 + 0.7 \cdot 1.7 + 0.3 \cdot 1 \\
        &= 2.49 \\
        V_{H-3}^\pi(\text{messy}) &= r(\text{messy}, \text{tidy}) + \E_{s' \sim P(\text{messy}, \text{tidy})} [V_{H-2}^\pi(s')] \\
        &= 0 + 1 \cdot V_{H-2}^{\pi}(\text{orderly}) + 0 \cdot V_{H-2}^{\pi}(\text{messy}) \\
        &= 1.7
    \end{align*}

    You may wish to repeat this computation for the other policies to get a better sense of this algorithm.
\end{example}

\subsection{Optimal policies}

We've just seen how to \emph{evaluate} a given policy. But how can we find the \textbf{optimal} policy for a given environment? Formally speaking, we call a policy $\pi^\star$ optimal if it does at least as well as \emph{any} other policy $\pi$ (including stochastic and history-dependent ones) in all situations:

\[
    V_t^{\pi^\star}(s) \geq V_t^\pi(s) \quad \forall s \in \S, t \in [H]
\]

Convince yourself that all optimal policies must have the same value function. We call this the \textbf{optimal value function} and denote it by $V_t^\star(s)$. The same goes for the action-value function $Q_t^\star(s, a)$.

We mentioned earlier that every MDP has an optimal policy that is stationary and deterministic. In particular, we can construct such a policy by acting \emph{greedily} with respect to the optimal action-value function:

\[
    \pi_t^\star(s) = \argmax_a Q_t^\star(s, a)
\]

\begin{theorem}{It is optimal to be greedy w.r.t. the optimal value function}{optimal}
    Let $V^{\star}$ and $Q^{\star}$ denote the optimal value and action-value functions. Consider the greedy policy

    \[
        \hat \pi_t(s) := \argmax_a Q_t^{\star}(s, a).
    \]

    We aim to show that $\hat \pi$ is optimal; that is, $V^{\hat \pi} = V^{\star}$.

    Fix an arbitrary state $s \in \S$ and time $t \in [H]$.

    Firstly, by the definition of $V^{\star}$, we already know $V_t^{\star}(s) \ge V_t^{\hat \pi}(s)$. So for equality to hold we just need to show that $V_t^{\star}(s) \le V_t^{\hat \pi}(s)$. We'll first show that the Bellman operator $\bop^{\hat \pi}$ never decreases $V_t^{\star}$. Then we'll apply this result recursively to show that $V^{\star} = V^{\hat \pi}$.
    
    \textbf{Lemma:} $\bop^{\hat \pi}$ never decreases $V_t^{\star}$ (elementwise):
    
    \[
        [\bop^{\hat \pi} (V_{t+1}^{\star})](s) \ge V_t^{\star}(s).
    \]
    
    \textbf{Proof:}
    
    \begin{align*}
    V_t^{\star}(s) &= \max_{\pi \in \Pi} V_t^{\pi}(s) \\
    &= \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{t+1}^\pi(s') \right] && \text{Bellman consistency} \\
    &\le \max_{\pi \in \Pi} \mathop{\mathbb{E}}_{a \sim \pi(\dots)}\left[r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{t+1}^{\star}(s') \right] && \text{definition of } V^\star \\
    &= \max_{a} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} V_{t+1}^{\star}(s') \right] && \text{only depends on } \pi \text{ via } a \\
    &= [\bop^{\hat \pi}(V_{t+1}^{\star})](s).
    \end{align*}
    
    Note that the chosen action $a \sim \pi(\dots)$ above might depend on the past history; this isn't shown in the notation and doesn't affect our result (make sure you see why). We can now apply this result recursively to get
    
    $$
    V^{\star}_t(s) \le V^{\hat \pi}_t(s)
    $$

    as follows. (Note that even though $\hat \pi$ is deterministic, we'll use the $a \sim \hat \pi(s)$ notation to make it explicit that we're sampling a trajectory from it.)
    
    \begin{align*}
    V_{t}^{\star}(s) &\le [\bop^{\hat \pi}(V_{t+1}^{\star})](s) \\
    &= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue} V_{t+1}^{\star}(s')} \right] \right] && \text{definition of } \bop^{\hat \pi} \\
    &\le \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)} \left[ {\color{blue}[ \bop^{\hat \pi} (V_{t+2}^{\star})] (s')} \right] \right] && \text{above lemma} \\
    &= \mathop{\mathbb{E}}_{a \sim \hat \pi(s)} \left[ r(s, a) + \mathop{\mathbb{E}}_{s' \sim P(s, a)}{\color{blue} \left[ \mathop{\mathbb{E}}_{a' \sim \hat \pi}  r(s', a') + \mathop{\mathbb{E}}_{s''} V_{t+2}^{\star}(s'') \right]} \right] && \text{definition of } \bop^{\hat \pi} \\
    &\le \cdots && \text{apply at all timesteps} \\
    &= \mathop{\mathbb{E}}_{\tau \sim \rho^{\hat \pi}} [G_{t} \mid s_t = s] && \text{rewrite expectation} \\
    &= V_{t}^{\hat \pi}(s) && \text{definition}
    \end{align*}

    And so we have $V^{\star} = V^{\hat \pi}$, making $\hat \pi$ optimal.
\end{theorem}


\subsubsection{Dynamic programming}

Now that we've shown this particular greedy policy is optimal, all we need to do is compute the optimal value function and optimal policy. We can do this by working backwards in time and using \textbf{dynamic programming} (DP).


\begin{definition}{DP for optimal policy}{pi_star_dp}
    We can solve for the optimal policy in an episodic MDP using \textbf{dynamic programming}.
    
    \begin{itemize}
    \item \emph{Base case.} At the end of the episode (time step $H-1$),
        we can't take any more actions, so the $Q$-function is simply the reward
        that we obtain:
    
        \[
            Q^\star_{H-1}(s, a) = r(s, a)
        \]
    
        so the best thing to do is just act greedily
        and get as much reward as we can!
    
        \[
            \pi^\star_{H-1}(s) = \argmax_a Q^\star_{H-1}(s, a)
        \]
    
        Then $V^\star_{H-1}(s)$, the optimal value of state $s$ at the end of the
        trajectory, is simply whatever action gives the most reward.
    
        \[
            V^\star_{H-1} = \max_a Q^\star_{H-1}(s, a)
        \]
    
    \item \emph{Recursion.} Then, we can work backwards in time, starting from the
        end, using our consistency equations! i.e. for each $t = H-2, \dots, 0$, we set

        \begin{align*}
            Q^\star_{t}(s, a) &= r(s, a) + \E_{s' \sim P(s, a)} [V^\star_{t+1}(s')] \\
            \pi^\star_{t}(s) &= \argmax_a Q^\star_{t}(s, a) \\
            V^\star_{t}(s) &= \max_a Q^\star_{t}(s, a)
        \end{align*}
    \end{itemize}
    
    % Note that this is exactly just value iteration and policy iteration combined, since our policy is nonstationary, so we can exactly specify its decisions at each time step!
    
    \tcbsubtitle[colback=white]{Analysis}
    
    
    At each of the $H$ timesteps, we must compute $Q^{\star}$ for each of the $|\S| |\A|$ state-action pairs. Each computation takes $|\S|$ operations to evaluate the average value over $s'$. This gives a total computation time of $O(H |\S|^2 |\A|)$.
\end{definition}


\begin{example}{Optimal policy for the tidying MDP}{tidy_dp}
    Left as an exercise.
\end{example}

\newpage

\section{Infinite horizon MDPs}

What happens if a trajectory is allowed to continue forever? This is the setting of \textbf{infinite horizon} MDPs. We'll need to make a few adjustments to make the problem tractable.

First of all, note that using the total cumulative reward is no longer a good idea, since it might blow up to infinity. Instead of a time horizon $H$, we now need a \textbf{discount factor} $\gamma \in (0, 1)$ such that rewards become less valuable the further into the future they are:

\[
    G_t = r_t + \gamma r_{t+1} + \gamma^2 r_{t+2} + \cdots = \sum_{k=0}^\infty \gamma^k r_{t+k}
\]

This also has a nice real-world interpretation: it's better to get a reward now than later, since you can invest it to get more reward in the future.

The other components of the MDP remain the same:

\[ M = (\S, \A, \mu, P, r, \gamma). \]

It also becomes impossible to store any quantities that scale with $H$, e.g. the time-dependent policies and value functions from the episodic case. We'll shift to time-independent policies $\pi : \S \to \A$ (deterministic) or $\Delta(\A)$ (stochastic).

\textbf{Exercise:} Which of the policies in \ref{eg:tidy_policy} are time-independent?

We also consider time-independent value functions $V^\pi : \S \to \R$ and $Q^\pi : \S \times \A \to \R$. We need to adjust the Bellman consistency functions accordingly to account for the discounting:

\begin{align*}
    V^\pi(s) &= \E_{\tau \sim \rho^\pi} [G_t \mid s_t = s] && \text{for any } t \in \mathbb{N} \\
    &= \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma V^\pi(s')]\\
    Q^\pi(s, a) &= \E_{\tau \sim \rho^\pi} [G_t \mid s_t = s, a_t = a] && \text{for any } t \in \mathbb{N} \\
    &= r(s, a) + \gamma \E_{\substack{s' \sim P(s, a) \\ a' \sim \pi(s')}} [Q^\pi(s', a')]
\end{align*}

\textbf{Exercise:} Heuristically speaking, why doesn't it matter which time step we condition on when defining the value functions?

In this chapter, we'll show that the Bellman operator \ref{sec:bellman_operator} is a contraction mapping for any policy. We'll discuss how to evaluate policies (i.e. compute their corresponding value functions). Finally, we'll present and analyze two iterative algorithms, based on the Bellman operator, for computing the optimal policy: \textbf{value iteration} and \textbf{policy iteration}.


\subsection{The Bellman operator is a contraction mapping}

Recall from \ref{sec:bellman_operator} that the Bellman operator $\bop^{\pi}$ for a policy $\pi$ takes in a ``value function'' $v : \S \to \R$ and returns the r.h.s. of the Bellman equation for that ``value function''. In the infinite-horizon setting, this is

\[
    [\bop^{\pi}(v)](s) := \E_{\substack{a \sim \pi(s) \\ s' \sim P(s, a)}} [r(s, a) + \gamma v(s')].
\]

The crucial property of the Bellman operator is that it is a \textbf{contraction mapping} for any policy. Intuitively, if we start with two ``value functions'' $v, u : \S \to \R$, if we repeatedly apply the Bellman operator to each of them, they will get closer and closer together at an exponential rate. A useful fact known as the \textbf{Banach fixed-point theorem} tells us that this procedure converges to the true value function! (We won't prove this here, though it only takes some algebraic manipulation.)

Let's make this more rigorous. How can we measure the distance between two value functions? We'll take the \textbf{supremum norm} as our distance metric:

\[
    \| v - u \|_{\infty} := \sup_{s \in \S} |v(s) - u(s)|
\]

We aim to show that

\[\|\bop^{\pi} (v) - \bop^{\pi} (u) \|_{\infty} \le \gamma \|v - u \|_{\infty}.\]

\textbf{Proof:}

\begin{align*}
|[\bop^{\pi} (v)](s) - [\bop^{\pi} (u)](s)|&= \left| \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[ r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} v(s') \right] - \mathop{\mathbb{E}}_{a \sim \pi(s)} \left[r(s, a) + \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)} u(s') \right] \right| \\
&= \gamma \left|\mathop{\mathbb{E}}_{s' \sim P(s, a)} [v(s') - u(s')] \right| \\
&\le \gamma \mathop{\mathbb{E}}_{s' \sim P(s, a)}|v(s') - u(s')| \qquad \text{(Jensen's)} \\
&\le \gamma \max_{s'} |v(s') - u(s')| \\
&= \gamma \|v - u \|_{\infty}.
\end{align*}

Then the Banach fixed-point theorem tells us that repeatedly applying the Bellman operator will converge to the true value function exponentially:

\begin{equation}
    \|(\bop^\pi)^{(t)}(v) - V^\pi \|_{\infty} \le \gamma^{t} \| v - V^\pi\|_{\infty} \label{eq:bellman_convergence}
\end{equation}

where $(J^\pi)^{(t)}(v)$ denotes applying $J^\pi$ to $v$ $t$ times. We'll use this useful fact to prove the convergence of several algorithms later on.

\subsection{Tabular case (linear algebraic notation)}

When there's not many states and actions, we call the MDP \textbf{tabular} since we can express the relevant quantities as low-dimensional vectors and matrices:

\begin{align*}
    r &\in \R^{|\S| \times |\A|} &
    P &\in [0, 1]^{(|\S \times \A|) \times |\S|} &
    \mu &\in [0, 1]^{|\S|} \\
    \pi &\in [0, 1]^{|\A| \times |\S|} &
    V^\pi &\in \R^{|\S|} &
    Q^\pi &\in \R^{|\S| \times |\A|}.
\end{align*}

(Verify that these types make sense!)

Note that when the policy $\pi$ is deterministic, the actions can be determined from the states, and so we can chop off the action dimension for the rewards and state transitions:

\begin{align*}
    r^{\pi} &\in \R^{|\S|} & P^{\pi} &\in [0, 1]^{|\S| \times |\S|} & \mu &\in [0, 1]^{|\S|} \\
    \pi &\in \A^{|\S|} & V^\pi &\in \R^{|\S|} & Q^\pi &\in \R^{|\S| \times |\A|}.
\end{align*}

For $P^\pi$, we'll treat the rows as the states and the columns as the next states. Then $P^\pi_{s, s'}$ is the probability of transitioning from state $s$ to state $s'$ under policy $\pi$.

\begin{example}{Tidying MDP}{tidy_tabular}
    The tabular MDP from before has $|\S| = 2$ and $|\A| = 2$. Let's write down the quantities for the policy $\pi$ that tidies iff the room is messy:

    \[
        r^{\pi} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}, \quad
        P^{\pi} = \begin{bmatrix} 0.7 & 0.3 \\ 1 & 0 \end{bmatrix}, \quad
        \mu = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
    \]

    We'll see how to evaluate this policy in the next section.
\end{example}

\subsection{Policy evaluation}

The backwards DP technique we used in the episodic case no longer works since there is no ``final timestep'' to start from. We'll need another approach to policy evaluation.

The Bellman consistency conditions yield a system of equations we can solve to evaluate a policy \emph{exactly}. For a faster approximate solution, we can iterate the policy's Bellman operator, since we know that it has a unique fixed point at the true value function.


\subsubsection{Tabular case for deterministic policies}

The Bellman consistency equation for a deterministic policy can be written in tabular notation as

\[
    V^\pi = r^\pi + \gamma P^\pi V^\pi.
\]

(Unfortunately, this notation doesn't simplify the expression for $Q^\pi$.) This system of equations can be solved with a matrix inversion:

\begin{equation}
    V^\pi = (I - \gamma P^\pi)^{-1} r^\pi. \label{eq:matrix_inversion_pe}
\end{equation}

Note we've assumed that $I - \gamma P^\pi$ is invertible. Can you see why this is the case?

(Recall that a linear operator, i.e. a square matrix, is invertible if and only if its null space is trivial; that is, it doesn't map any nonzero vector to zero. In this case, we can see that $I - \gamma P^\pi$ is invertible because it maps any nonzero vector to a vector with at least one nonzero element.)

\begin{example}{Tidying policy evaluation}{tidy_eval}
    Left as an exercise.
\end{example}


\subsubsection{Iterative policy evaluation}

The matrix inversion above takes roughly $O(|\S|^3)$ time. Can we trade off the requirement of finding the \emph{exact} value function for a faster \emph{approximate} algorithm?

Let's use the Bellman operator to define an iterative algorithm for computing the value function. We'll start with an initial guess $v^{(0)}$ with elements in $[0, 1/(1-\gamma)]$ and then iterate the Bellman operator:

\[
    v^{(t+1)} = \bop^{\pi}(v^{(t)}) = r^{\pi} + \gamma P^{\pi} v^{(t)}.
\]

i.e. $v^{(t)} = (\bop^{\pi})^{(t)} (v^{(0)})$. Note that each iteration takes $O(|\S|^2)$ time for the matrix-vector multiplication.

Then, as we showed in \ref{eq:bellman_convergence}, by the Banach fixed-point theorem:


\[ \|v^{(t)} - V^\pi \|_{\infty} \le \gamma^{t} \| v^{(0)} - V^\pi\|_{\infty} \]


How many iterations do we need for an $\epsilon$-accurate estimate? We can work backwards to solve for $t$:

\begin{align*}
    \gamma^t \|v^{(0)} - V^\pi\|_{\infty} &\le \epsilon \\
    t &\ge \frac{\log (\epsilon / \|v^{(0)} - V^\pi\|_{\infty})}{\log \gamma} \\
    &= \frac{\log (\|v^{(0)} - V^\pi\|_{\infty} / \epsilon)}{\log (1 / \gamma)} \\
\end{align*}

And so the number of iterations required for an $\epsilon$-accurate estimate is

\begin{equation}
    T = O\left( \frac{\log(1/(\epsilon (1-\gamma)))}{1-\gamma} \right). \label{eq:iterations_vi}
\end{equation}

Note that we've applied the inequalities $\|v^{(0)} - V^\pi\|_{\infty} \le 1/(1-\gamma)$ and $\log (1/x) \ge 1-x$.

\subsection{Optimal policies}

Now let's move on to solving for the optimal policy. Once again, we can't use the backwards DP approach from the episodic case since there's no ``final timestep'' to start from. Instead, we'll exploit the fact that the Bellman consistency equation for the optimal value function doesn't depend on any policy:

\[
    V^\star(s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^\star(s') \right]
\]

To see this, recall we showed that the greedy policy w.r.t. the optimal value function is optimal; substituting this policy into the standard Bellman consistency equation gives the above expression.

\textbf{Exercise:} Verify this.

As before, thinking of the r.h.s. as an operator on value functions gives the \textbf{Bellman optimality operator}

\[
    [\bop^{\star}(v)](s) = \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v(s') \right].
\]

\subsubsection{Value iteration}

Since the optimal policy is still a policy, our result that the Bellman operator is a contracting map still holds, and so we can repeatedly apply this operator to converge to the optimal value function! This algorithm is known as \textbf{value iteration}.

\begin{definition}{Value iteration pseudocode}{value_pseudocode}
    \begin{algorithmic}
        \State $v^{(0)} \gets 0$
        \For{$t = 0, 1, 2, \dots, T-1$}
            \State $v^{(t+1)} \gets \bop^{\star}(v^{(t)})$
        \EndFor
        \State \Return $v^{(T)}$
    \end{algorithmic}
\end{definition}


As the final step of the algorithm, to return an actual policy $\hat \pi$, we can simply act greedily w.r.t. the final iteration $v^{(T)}$ of our above algorithm:

\[
    \hat \pi(s) = \argmax_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} v^{(T)}(s') \right].
\]

We must be careful, though: the value function of this greedy policy, $V^{\hat \pi}$, is \emph{not} the same as $v^{(T)}$, which need not even be a well-defined value function for some policy!

The bound on the policy's quality is actually quite loose: if $\|v^{(T)} - V^\star\|_{\infty} \le \epsilon$, then the greedy policy $\hat \pi$ satisfies $\|V^{\hat \pi} - V^\star\|_{\infty} \le \frac{2\gamma}{1-\gamma} \epsilon$, which might potentially be very large.

\begin{theorem}{Greedy policy value degradation}{greedy_worsen}
    We aim to show that

    \[
        \|V^{\hat \pi} - V^\star \|_{\infty} \le \frac{2 \gamma}{1-\gamma} \|v - V^\star\|_{\infty}
    \]

    where $\hat \pi(s) = \argmax_a q(s, a)$ is the greedy policy w.r.t. \[ q(s, a) = r(s, a) + \E_{s' \sim P(s, a)} v(s'). \]

    \textbf{Proof:} We first have

    \begin{align*}
        V^{\star}(s) - V^{\hat \pi}(s) &= Q^{\star}(s,\pi^\star(s)) - Q^{\hat \pi}(s, \hat \pi(s))\\
        &= [Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s))] + [Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))]
    \end{align*}

    Let's bound these two quantities separately.

    For the first quantity, note that by the definition of $\hat \pi$, we have
    
    \[ q(s, \hat \pi(s)) \ge q(s,\pi^\star(s)). \]
    
    Let's add $q(s, \hat \pi(s)) - q(s,\pi^\star(s)) \ge 0$ to the first term to get

    \begin{align*}
        Q^{\star}(s,\pi^\star(s)) - Q^{\star}(s, \hat \pi(s)) &\le [Q^{\star}(s,\pi^\star(s))- q(s,\pi^\star(s))] + [q(s, \hat \pi(s)) - Q^{\star}(s, \hat \pi(s))] \\
        &= \gamma \E_{s' \sim P(s, \pi^{\star}(s))} [ V^{\star}(s') - v(s') ] + \gamma \E_{s' \sim P(s, \hat \pi(s))} [ v(s') - V^{\star}(s') ] \\
        &\le 2 \gamma \|v - V^{\star}\|_{\infty}
    \end{align*}

    The second one is bounded by

    \begin{align*}
        Q^{\star}(s, \hat \pi(s)) - Q^{\hat \pi}(s, \hat \pi(s))
        &=
        \gamma \E_{s'\sim P(s, \hat \pi(s))}\left[ V^\star(s') - V^{\hat \pi}(s') \right] \\
        & \leq 
        \gamma \|V^{\star} - V^{\hat \pi}\|_\infty
    \end{align*}

    and thus

    \begin{align*}
        \|V^\star - V^{\hat \pi}\|_\infty &\le 2 \gamma \|v - V^{\star}\|_{\infty} + \gamma \|V^{\star} - V^{\hat \pi}\|_\infty \\
        \|V^\star - V^{\hat \pi}\|_\infty &\le {2 \gamma \|v - V^{\star}\|_{\infty} \over 1-\gamma}.
    \end{align*}
\end{theorem}

So in order to compensate and achieve $\|V^{\hat \pi} - V^{\star}\| \le \epsilon$, we must have

\[
    \|v^{(T)} - V^\star\|_{\infty} \le \frac{1-\gamma}{2 \gamma} \epsilon.
\]

This means, using \ref{eq:iterations_vi}, we need to run the algorithm for

\[
    T = O\left( \frac{\log(\gamma/(\epsilon (1-\gamma)^2))}{1-\gamma} \right)
\]

iterations to achieve an $\epsilon$-accurate estimate of the optimal value function.

\subsubsection{Policy iteration}

Can we mitigate this ``greedy worsening''? What if instead of approximating the optimal value function and then acting greedily w.r.t. it, we iteratively improve the policy and value function together? This is the idea behind \textbf{policy iteration}.

\begin{theorem}{Policy Iteration}{pi_iter}

Remember, for now we're only considering policies that are \emph{stationary and deterministic}. There's $|\S|^{|\A|}$ of these, so let's start off by choosing one at random. Let's call this initial policy $\pi^0$, using the superscript to indicate the time step.

Now for $t = 0, 1, \dots$, we perform the following:

\begin{enumerate}
    
\item \emph{Policy Evaluation}: First use the exact policy evaluation algorithm \ref{eq:matrix_inversion_pe} to calculate $V^{\pi^t}(s)$ for all states $s$. Then use this to calculate the state-action values:

    \[
        Q^{\pi^t}(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^{\pi^t} (s')
    \]

\item \emph{Policy Improvement}: Update the policy so that, at each state,
    it chooses the action with the highest action-value (according to the current iterate):

    \[
        \pi^{t+1}(s) = \argmax_a Q^{\pi^t} (s, a)
    \]

    In other words, we're setting it to act greedily with respect to the current Q-function.
\end{enumerate}

\tcbsubtitle[colback=white]{Analysis}

As in \ref{eq:bellman_convergence}, we'd like to show

\[
    \|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]

We'll prove convergence by showing that the policies improve monotonically:

\[
    V^{\pi^{t+1}}(s) \ge [\bop^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s).
\]

The right-hand inequality follows from the fact that $\bop^{\star}$ is a contraction mapping with a fixed point at $V^{\star}$.

To show the left-hand inequality, we'll first show that iterates improve monotonically, that is, $V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)$ for all $s$. Then we'll use this to show $V^{\pi^{t+1}}(s) \ge [\bop^{\star}(V^{\pi^{t}})](s)$. Note that

\begin{align*}
    [\bop^{\star} (V^{\pi^{t}})](s) &= \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V^{\pi^{t}}(s') \right] \\
    &= r(s, \pi^{t+1}(s)) + \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} V^{\pi^{t}}(s')
\end{align*}

Since $[\bop^{\star}(V^{\pi^{t}})](s) \ge V^{\pi^{t}}(s)$, we then have

\begin{equation}
    \begin{aligned}
        V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &\ge V^{\pi^{t+1}}(s) - \bop^{\star} (V^{\pi^{t}})(s) \nonumber \\
        &= \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right].
    \end{aligned} \label{eq:pi_iter_proof}
\end{equation}

But note that the expression being averaged is the same as the expression on the l.h.s. with $s$ replaced by $s'$. So we can apply the same inequality recursively to get

\begin{align*}
    V^{\pi^{t+1}}(s) - V^{\pi^{t}}(s) &\ge  \gamma \E_{s' \sim P(s, \pi^{t+1}(s))} \left[V^{\pi^{t+1}}(s') -  V^{\pi^{t}}(s') \right] \\
    &\ge \gamma^2 \E_{\substack{s' \sim P(s, \pi^{t+1}(s)) \\ s'' \sim P(s', \pi^{t+1}(s'))}} \left[V^{\pi^{t+1}}(s'') -  V^{\pi^{t}}(s'') \right]\\
    &\ge \cdots
\end{align*}

which implies that $V^{\pi^{t+1}}(s) \ge V^{\pi^{t}}(s)$ for all $s$. We can then plug this back into \ref{eq:pi_iter_proof} to get the desired result.

This means we can now apply the Bellman convergence result \ref{eq:bellman_convergence} to get

\[
    \|V^{\pi^{t+1}} - V^\star \|_{\infty} \le \|\bop^{\star} (V^{\pi^{t}}) - V^{\star}\|_{\infty} \le \gamma \|V^{\pi^{t}} - V^\star \|_{\infty}.
\]

\end{theorem}


\end{document}
