\documentclass[../main/main]{subfiles}

\setcounter{chapter}{1}

\begin{document}

    
\chapter{Markov Decision Processes}

\tableofcontents

\section{Introduction}

RL studies how an agent can learn to make sequential decisions in an environment. This is a very general problem! How can we \emph{formalize} this task in a way that is both \emph{sufficiently general} yet also tractable enough for \emph{fruitful analysis}?

Let's consider some examples of sequential decision problems to identify the key common properties we'd like to capture:

\begin{itemize}
    \item \textbf{Board games} like chess or Go, where the player takes turns with the opponent to make moves.
    \item \textbf{Video games} like Super Mario Bros, where the player can move around and interact with the environment.
    \item \textbf{Robotic control}, where the robot can move and interact with the environment.
\end{itemize}

All of these fit into the RL framework (consider what the agent, state, and possible reward signals are in each example). In particular, the \emph{rules} of the environment stay the same over time. We can formalize such environments using \textbf{Markov decision processes} (MDPs).

% there is some agent that takes actions within its environment. These actions change the state of the environment, which in turn affects the agent's future observations. The agent's goal is to maximize its ``reward'' (e.g. by winning the game, earning points, or arriving at a desired location).

% For motivation, let's begin by considering a simple example of a sequential decision problem: a \textbf{grid-world} in which the agent can move up, down, left, or right in a grid.

% \begin{figure}[h]
%     \centering
%     \begin{tikzpicture}
%         \draw[step=1cm,gray,very thin] (0, 0) grid (4, 3);
%         \fill[black] (0, 0) rectangle (1, 1);
%         \fill[black] (3, 2) rectangle (4, 3);
%         \fill[red!20] (1, 2) rectangle (2, 3);
%         \fill[red!20] (2, 0) rectangle (3, 1);
%         % draw a small user icon in the first square
%         \node[anchor=center] at (0.5, 0.5) {\includegraphics[width=0.5cm]{../assets/user.png}};
%     \end{tikzpicture}
%     \caption{A grid-world.}
% \end{figure}

\subsection{Definition}

MDPs are environments where the state transitions only depend on the most recent state and action. Formally, we say that the state transitions satisfy the \textbf{Markov property}, that is,

\[
    \P(s_{t+1} \mid (s_\tau, a_\tau)_{\tau=0}^t) = P(s_{t+1} \mid s_t, a_t).
\]

We'll see that this simple assumption leads to a rich set of problems and algorithms.

MDPs are usually classified as \textbf{finite-horizon}, where the interactions end after some finite number of time steps, or \textbf{infinite-horizon}, where the interactions can continue indefinitely. We'll begin with the finite-horizon case for ease of analysis, but the ideas naturally extend to the infinite-horizon case.

% We'll also assume that the interactions end after some finite number of time steps, called the \textbf{time horizon}. (It's straightforward to extend the framework to infinite-horizon problems, but we'll leave that for another time.)

\begin{definition}{(Finite-horizon) Markov Decision Process}{mdp}

The key components of a (finite-horizon) Markov decision process are:

\begin{enumerate}
    \item The \textbf{state} that the agent interacts with. We use $\S$ to denote the set of possible states, called the \textbf{state space}.
    \item The \textbf{actions} that the agent can take. We use $\A$ to denote the set of possible actions, called the \textbf{action space}.
    \item Some \textbf{initial state distribution} $\mu \in \Delta(\S)$.
    \item The \textbf{state transitions} (a.k.a. \textbf{dynamics}) that describe what state we transition to after taking an action. We'll denote this by $P : \S \times \A \to \Delta(\S)$. % (as opposed to $\P$ which denotes the underlying probability measure.)
    \item The \textbf{reward} signal. In this course we'll take it to be a deterministic function on state-action pairs, i.e. $r : \S \times \A \to \mathbb{R}$. % In general, though, the reward function can also be stochastic, and it can also accept the \emph{resulting} state as an argument; that is, $r : \S \times \A \times \S \to \Delta(\R)$.
    % \item A \emph{discount factor} $\gamma \in [0, 1)$. We'll see later that this ensures that the \emph{return}, or total reward, is well-defined in infinite-horizon problems.
    \item A time horizon $H \in \mathbb{N}$.
\end{enumerate}

Combined together, these objects specify a finite-horizon Markov decision process:

\[
    M = (\S, \A, \mu, P, r, H).
\]

\end{definition}

% \begin{example}{Examples of MDPs}{mdp_examples}
%     \textbf{Board games and video games} are often MDPs. For example, in chess or Go, the state of the game only depends on the pieces on the board and not on the previous history. Several possible reward functions could be possible, e.g. $+1$ upon winning the game and $0$ otherwise, or to receive reward upon taking the opponent's pieces. The state transitions are based on the opponent's moves.

%     \textbf{Robotic control} can be framed as an MDP task. In this setting, physics provides the state transitions. A possible action might be activating a motor to move forwards. The reward function could be designed based on the task; for example, one could reward the robot for arriving at a desired location.
% \end{example}

% \subsection{Additional concepts}

We call the total reward the \textbf{return}:

\[
    G := R_0 + \cdots + R_{H-1},
\]

where $R_t := r(S_t, A_t)$. The key \emph{goal} in a reinforcement learning task is to \emph{maximize expected return} $\E[G]$.

We'll also consider the \textbf{return-to-go}, the total reward from a given time step onwards:

\[
    G_t := R_t + \cdots + R_{H-1}.
\]



% However, for infinite-horizon problems (i.e. $T = \infty$), in order for this to be well-defined, we need to \emph{discount} future rewards:

% \[
%     G_t := R_t + \gamma R_{t+1} + \cdots + \gamma^{\tau-t} R_{\tau} + \cdots = \sum_{\tau = t}^\infty \gamma^{\tau-t} R_\tau.
% \]

% Can you see why this ensures that $G_t$ is finite?

% Note that we recover the finite-horizon definition by letting $\gamma = 1$ and $T$ be finite.


Why can't we just maximize the current reward at each timestep, i.e. use a greedy strategy? Well, in RL as in real life, making greedy decisions (e.g. procrastinating) will often leave you worse off than if you make some short-term sacrifices for long-term gains.

We call the ``video recording'' of states, actions, and rewards a \textbf{trajectory}:

\[
    \tau = (s_t, a_t, r_t)_{t=0}^{H-1}
\]


\section{Policies and value functions}

A \textbf{policy} describes the agent's strategy: which actions it takes in a given situation. One key goal of RL is to find the \textbf{optimal policy} that maximizes the expected return.

Policies can either be \textbf{deterministic} (in the same situation, the agent will always take the same action) or \textbf{stochastic} (in the same situation, the agent will sample an action from a distribution).

What do we mean by ``situation''? In the most general setting, this could include all of the states, actions, and rewards in the trajectory so far. However, due to the Markov assumption, the state transitions only depend on the current state. Thus a \textbf{stationary} policy $\pi : \S \to \Delta(\A)$ --- one that only depends on the current state --- can do just as well.

In the finite-horizon case, we'll consider \textbf{time-dependent} policies that depend on the time step $t$ as well, i.e. $\pi = \{ \pi_0, \dots, \pi_{H-1} \}$.

Once we've chosen a policy, we can sample trajectories by choosing actions according to the policy and observing the state transitions and rewards. We call this the \textbf{trajectory distribution}:

\begin{align*}
    \P(\tau ; \pi) &= \mu(s_0) \pi_0(a_0 \mid s_0) P(s_1 \mid s_0, a_0) \cdots \pi_{H-1}(a_{H-1} \mid s_{H-1}) \\
    \text{where} \quad \tau &= (s_t, a_t, r_t)_{t=0}^{H-1}.
\end{align*}

We'll abuse notation and use $\tau \sim \pi$ to denote that $\tau$ is sampled from this distribution.

We'd like a concise way to refer to the expected return when \emph{starting in a given state at a given time} and acting according to $\pi$. We call this the \textbf{value function} of $\pi$ and denote it by

\begin{align*}
    V_t^\pi(s) &:= \E_{\tau \sim \pi} [G_t \mid S_t = s] % \\
    % &= \E_\pi \left[ \sum_{t=0}^\infty \gamma^t R_t \mid S_0 = s \right].
\end{align*}

% We start at time $0$ without loss of generality; can you see why we could have chosen to start at any time? % see AJKS for proof

Similarly, we can define the \textbf{action-value function} of $\pi$ (aka the \textbf{Q-function}) as the expected return when starting in a given state and taking a given action:

\begin{align*}
    Q_t^\pi(s, a) &:= \E_{\tau \sim \pi} [G_t \mid S_t = s, A_t = a] % \\
    % &= \E_\pi [\sum_{t=0}^\infty \gamma^t R_t \mid S_0 = s, A_0 = a].
\end{align*}

\subsection{Bellman self-consistency equations}

Note that we can break down the return as

\[
    G_t = R_t + \gamma G_{t+1},
\]

the reward from the \emph{current time-step} plus the total reward from from \emph{future time-steps}. It turns out that this simple observation, along with linearity of expectation, gives us a set of equations to solve for the value function analytically!

Let's expand out the definition of the value function to see what I mean. Let's first consider the simple case where $\pi : \S \to \A$ is deterministic:

\begin{align*}
    V^\pi(s) &:= \E_\pi[G_0 \mid S_0 = s] \\
    &= r(s, \pi(s)) + \gamma \E_{s' \sim P(\cdot \mid s, \pi(s))} {\color{blue} \E_\pi [G_1 \mid S_1 = s']} \\
    &= r(s, \pi(s)) + \gamma \sum_{s' \in \S} P(s' \mid s, \pi(s)) {\color{blue} V^\pi(s')}.
\end{align*}

This is a set of $|\mathcal{S}|$ equations (one per state) in $|\mathcal{S}|$ unknowns (the value of each state), which we can solve for $V^\pi$.

For stochastic policies, we simply average out over the relevant quantities:

\begin{align*}
    V^\pi(s) &:= \E_\pi [G_0 \mid S_0 = s] \\
    % &= \E_\pi \left[ \sum_{t=0}^\infty \gamma^t R_t \mid S_0 = s \right] \\
    &= \E_\pi \left[ R_0 + \gamma G_1 \mid S_0 = s \right] \\
    &= \E_{a \sim \pi(\cdot \mid s)} \left[ r(s, a) + \gamma \E_{s' \sim P(\cdot \mid s, a)}  {\color{blue} \E_\pi [G_1 \mid S_1 = s']} \right] \\
    &= \sum_a \pi(a \mid s) \left[r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) {\color{blue} V^\pi(s')} \right].
\end{align*}

These are called the \textbf{Bellman self-consistency equations}. They encapsulate that the value function's prediction of the current state must be consistent with its prediction of other states.

Can you write the Bellman self-consistency equations for the action-value function?


\section{Tabular MDPs}

When the state and action space are finite and small, we can think of the value function and $Q$-function as \emph{lookup tables} with each cell corresponding to the value of a state (or state-action pair). We can neatly express quantities as vectors and matrices:

\[
    r \in \R^{|\S| \times |\A|}, \quad P \in [0, 1]^{(|\S| \times |\A|) \times |\S|}, \quad \rho \in [0, 1]^{|\S|}, \quad \pi \in [0, 1]^{|\A| \times |\S|}, \quad V^\pi \in \R^{|\S|}, \quad Q^\pi \in \R^{|\S| \times |\A|}.
\]

Make sure that these dimensions make sense!

Note that when the policy is deterministic, by definition, the actions can be determined from the state, and so we can chop off the action dimension in most cases:

\begin{align*}
    r^{\pi} \in \R^{|\S|}, \quad P^{\pi} \in [0, 1]^{|\S| \times |\S|}, \quad \rho \in [0, 1]^{|\S|}, \quad \pi \in \A^{|\S|}, \quad V^\pi \in \R^{|\S|}.
\end{align*}

Then, rewriting the system of Bellman equations in this notation gives

\[
    V^\pi = r^\pi + \gamma P^\pi V^\pi \implies V^\pi = (I - \gamma P^\pi)^{-1} r^\pi.
\]

Note that we've assumed that $I - \gamma P^\pi$ is invertible. Can you see why this is the case?

Recall that a linear operator, i.e. a square matrix, is invertible if and only if its null space is trivial; that is, it doesn't map any nonzero vector to zero. In this case, we can see that $I - \gamma P^\pi$ is invertible because it maps any nonzero vector to a vector with at least one nonzero element.

% \subsection{Exercises}

% Show that without discounting, the reward


% For now, we'll assume that the world is known. This involves the state transitions and the reward.

% Unknown systems are similar to complex systems. In both, once we don't access the world everywhere, we need to actually \emph{learn} about the world around us.



\section{Optimality}


\begin{theorem}{Value Iteration}{val_iter}

Initialize:

\[
    V^0 \sim \|V^0\|_\infty \in [0, 1/1-\gamma]
\]

Iterate until convergence:

\[
    V^{t+1} \gets \mathcal{J}(V^t)
\]

\tcbsubtitle[colback=white]{Analysis}

This algorithm runs in $O(|\S|^3)$ time since we need to perform a matrix
inversion.

\end{theorem}





\begin{theorem}{Exact Policy Evaluation}{exact_pi_eval}

Represent the reward from each state-action pair as a vector

\[ R^\pi \in \R^{|\S|} \qquad R^\pi_s = r(s, \pi(s)) \]

Also represent the state transitions

\[ P^\pi \in \R^{|\S \times \S} \qquad P^\pi_{s, s'} = P(s' | s, \pi(s)) \]

That is, row $i$ of $P^\pi$ is a distribution over the \emph{next state}
given that the current state is $s_i$
and we choose an action using policy $\pi$.

Using this notation, we can express the Bellman consistency equation as

\begin{align*}
    \begin{pmatrix}
        \vdots \\ V^\pi(s) \\ \vdots
    \end{pmatrix}
    &=
    \begin{pmatrix}
        \vdots \\ r(s, \pi(s)) \\ \vdots
    \end{pmatrix}
    +
    \gamma
    \begin{pmatrix}
        & \vdots & \\
        \quad & P(s' \mid s, \pi(s)) & \quad \\
        & \vdots &
    \end{pmatrix}
    \begin{pmatrix}
        \vdots \\ V^\pi(s') \\ \vdots
    \end{pmatrix} \\
    V^\pi &= R^\pi + \gamma P^\pi V^\pi \\
    (I - \gamma P^\pi) V^\pi &= R^\pi \\
    V^\pi &= (I - \gamma P^\pi) R^\pi
\end{align*}

if $I - \gamma P^\pi$ is invertible, which we can prove is the case.


\end{theorem}



\begin{theorem}{Iterative Policy Evaluation}{iter_pi_eval}

How can we calculate the value function $V^\pi$ of a policy $\pi$?

Above, we saw an exact function that runs in $O(|\S|^2)$.
But say we really need a fast algorithm, and we're okay with having an
approximate answer. Can we do better? Yes!

Using the same notation as above,
let's initialize $V^0$ such that the elements are drawn uniformly
from $[0, 1/(1-\gamma)]$.

Then we can iterate the fixed-point equation we found above:

\[ V^{t+1} \gets R + \gamma P V^t \]


\end{theorem}

% One case in which we might want a fast, approximate algorithm like the one we
% just found is when we want to use it as part of another algorithm.

How can we use this fast approximate algorithm?

\begin{theorem}{Policy Iteration}{pi_iter}

Remember, for now we're only considering policies that are
\emph{stationary and deterministic}. There's $|\S|^{|\A}$ of these, so let's
start off by choosing one at random. Let's call this initial policy $\pi^0$,
using the superscript to indicate the time step.

Now for $t = 0, 1, \dots$, we perform the following:

\begin{enumerate}
    
\item \emph{Policy Evaluation}: First use the algorithm from earlier to
    calculate $V^{\pi^t}(s)$ for all states $s$. Then use this to calculate the
    state-action values:

    \[
        Q^{\pi^t}(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) V^{\pi^t} (s')
    \]

\item \emph{Policy Improvement}: Update the policy so that, at each state,
    it chooses the action with the highest action-value:

    \[
        \pi^{t+1}(s) = \argmax_a Q^{\pi^t} (s, a)
    \]

    In other words, we're setting it to act greedily with respect to the new Q-function.

\end{enumerate}

What's the computational complexity of this?

% TODO

\end{theorem}




\section{Finite Horizon MDPs}


% Now, instead of discounting, all we care about is the (average) total reward
% that we get over this time.

% \[ \E[ \sum_{t=0}^{H-1} r(s_t, a_t) ] \]

% To be more precise, we'll consider policies that depend on the time.
% We'll denote the policy at timestep $h$ as $\pi_h : \S \to \A$. In other
% words, we're dropping the constraint that policies must be stationary.

This is also called an \emph{episodic model}.

% How to solve these problems? \emph{Dynamic programming}

% A bit more annoying notationally to keep track of time

Note that since our policy is nonstationary, we also need to adjust our value
function (and Q-function) to account for this.
Instead of considering the total infinite-horizon discounted reward like we did
earlier,
we'll instead consider the \emph{remaining} reward from a given timestep
onwards:

\begin{align*}
    V^\pi_h(s) &= \E \left[ \sum_\tau^{H-1} r(s_\tau, a_\tau) \mid s_h = s, a_\tau = \pi_h(s_h) \right] \\
    Q^\pi_h(s, a) &= \E \left[ \sum_\tau^{H-1} r(s_\tau, a_\tau) \mid (s_h, a_h) = (s, a) \right]
\end{align*}


We can also define our Bellman consistency equations, by splitting up the total
reward into the immediate reward (at this time step) and the future reward,
represented by our state value function from that next time step:

\[
    Q^\pi_h(s, a) = r(s, a) + \E_{s' \sim P(s, a)}[V^\pi_{h+1}(s')]
\]

\begin{theorem}{Computing the optimal policy}{pi_star_dp}

We can solve for the optimal policy using dynamic programming.

\begin{itemize}
\item \emph{Base case.} At the end of the episode (time step $H-1$),
    we can't take any more actions, so the $Q$-function is simply the reward
    that we obtain:

    \[
        Q^\star_{H-1}(s, a) = r(s, a)
    \]

    so the best thing to do is just act greedily
    and get as much reward as we can!

    \[
        \pi^\star_{H-1}(s) = \argmax_a Q^\star_{H-1}(s, a)
    \]

    Then $V^\star_{H-1}(s)$, the optimal value of state $s$ at the end of the
    trajectory, is simply whatever action gives the most reward.

    \[
        V^\star_{H-1} = \max_a Q^\star_{H-1}(s, a)
    \]

\item \emph{Recursion.} Then, we can work backwards in time, starting from the
    end, using our consistency equations!
\end{itemize}

Note that this is exactly just value iteration and policy iteration combined,
since our policy is nonstationary, so we can exactly specify its decisions at
each time step!

\tcbsubtitle[colback=white]{Analysis}


Total computation time $O(H |\S|^2 |\A|)$



\end{theorem}

\end{document}
