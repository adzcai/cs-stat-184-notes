
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>6. Policy Gradient Algorithms &#8212; CS/STAT 184: Introduction to Reinforcement Learning</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=87e54e7c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script kind="utterances">

    var commentsRunWhenDOMLoaded = cb => {
    if (document.readyState != 'loading') {
        cb()
    } else if (document.addEventListener) {
        document.addEventListener('DOMContentLoaded', cb)
    } else {
        document.attachEvent('onreadystatechange', function() {
        if (document.readyState == 'complete') cb()
        })
    }
}

var addUtterances = () => {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src = "https://utteranc.es/client.js";
    script.async = "async";

    script.setAttribute("repo", "adzcai/cs-stat-184-notes");
    script.setAttribute("issue-term", "pathname");
    script.setAttribute("theme", "github-light");
    script.setAttribute("label", "💬 comment");
    script.setAttribute("crossorigin", "anonymous");

    sections = document.querySelectorAll("div.section");
    if (sections !== null) {
        section = sections[sections.length-1];
        section.appendChild(script);
    }
}
commentsRunWhenDOMLoaded(addUtterances);
</script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"tex": {"macros": {"E": "\\mathop{\\mathbb{E}}", "P": "\\mathop{\\mathbb{P}}", "kl": ["\\mathrm{KL}\\left(#1\\parallel#2\\right)", 2], "ind": ["\\mathbf{1}\\left\\{#1\\right\\}", 1], "hi": "h", "hor": "H", "st": "s", "act": "a"}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pg';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Exploration in MDPs" href="exploration.html" />
    <link rel="prev" title="5. Linear Quadratic Regulators" href="control.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/184.png" class="logo__image only-light" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>
    <script>document.write(`<img src="_static/184.png" class="logo__image only-dark" alt="CS/STAT 184: Introduction to Reinforcement Learning - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">2. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="mdps.html">3. Finite Markov Decision Processes</a></li>
<li class="toctree-l1"><a class="reference internal" href="fitted_dp.html">4. Fitted Dynamic Programming Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="control.html">5. Linear Quadratic Regulators</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">6. Policy Gradient Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration.html">7. Exploration in MDPs</a></li>
<li class="toctree-l1"><a class="reference internal" href="imitation_learning.html">8. Imitation Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">9. Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/pg.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Policy Gradient Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">6.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-policy-gradient-ascent">6.2. (Stochastic) Policy Gradient Ascent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforce-and-importance-sampling">6.3. REINFORCE and Importance Sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines-and-advantages">6.4. Baselines and advantages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-policy-parameterizations">6.5. Example policy parameterizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-in-features">6.5.1. Linear in features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-policies">6.5.2. Neural policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-action-spaces">6.5.3. Continuous action spaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#local-policy-optimization">6.6. Local policy optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-policy-gradient">6.6.1. Motivation for policy gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trust-region-policy-optimization">6.6.2. Trust region policy optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-policy-gradient">6.6.3. Natural policy gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proximal-policy-optimization">6.6.4. Proximal policy optimization</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="policy-gradient-algorithms">
<h1><span class="section-number">6. </span>Policy Gradient Algorithms<a class="headerlink" href="#policy-gradient-algorithms" title="Link to this heading">#</a></h1>
<section id="motivation">
<h2><span class="section-number">6.1. </span>Motivation<a class="headerlink" href="#motivation" title="Link to this heading">#</a></h2>
<p>The scope of our problem has been gradually expanding:</p>
<ol class="arabic simple">
<li><p>In the first chapter, we considered the <em>multi-armed bandit setting</em> with a finite number of arms, where the only stochasticity involved was their rewards.</p></li>
<li><p>In the second chapter, we considered <em>MDPs</em> more generally, involving a finite number of states and actions, where the state transitions are Markovian.</p></li>
<li><p>In the third chapter, we considered <em>continuous</em> state and action spaces and developed the <em>Linear Quadratic Regulator.</em> We then showed how to use it to find <em>locally optimal solutions</em> to problems with nonlinear dynamics and non-quadratic cost functions.</p></li>
</ol>
<p>Now, we’ll continue to investigate the case of finding optimal policies in large MDPs using the self-explanatory approach of <em>policy optimization.</em> This is a general term encompassing many specific algorithms we’ve already seen:</p>
<ul class="simple">
<li><p><em>Policy iteration</em> for finite MDPs,</p></li>
<li><p><em>Iterative LQR</em> for locally optimal policies in continuous control.</p></li>
</ul>
<p>Here we’ll see some algorithms that allow us to optimize policies for <em>general</em> kinds of problems. These algorithms have been used in many groundbreaking applications, including AlphaGo, OpenAI Five, and ChatGPT. (TODO Come up with better examples) These methods also bring us into the domain where we can use <em>deep learning</em> to approximate complex, nonlinear functions.</p>
</section>
<section id="stochastic-policy-gradient-ascent">
<h2><span class="section-number">6.2. </span>(Stochastic) Policy Gradient Ascent<a class="headerlink" href="#stochastic-policy-gradient-ascent" title="Link to this heading">#</a></h2>
<p>Let’s suppose our policy can be <em>parameterized</em> by some parameters <span class="math notranslate nohighlight">\(\theta\)</span>. For example, in a finite MDP with <span class="math notranslate nohighlight">\(|\mathcal{S}|\)</span> states and <span class="math notranslate nohighlight">\(|\mathcal{A}|\)</span> actions, we might assign one scalar value <span class="math notranslate nohighlight">\(\theta_{s, a}\)</span> to each state-action pair, and compute the policy as <span class="math notranslate nohighlight">\(\pi(s) = \argmax_a \theta_{s, a}\)</span>. In a high-dimensional case, the weights and biases of a deep neural network. We’ll talk more about possible parameterizations in <a class="reference internal" href="#parameterizations"><span class="std std-ref">Example policy parameterizations</span></a>.</p>
<p>Remember that in reinforcement learning, the goal is to <em>maximize reward.</em> Specifically, we seek the parameters that maximize the expected total reward, which we can express concisely using the value function we defined earlier:</p>
<div class="math notranslate nohighlight" id="equation-objective-fn">
<span class="eqno">(6.1)<a class="headerlink" href="#equation-objective-fn" title="Link to this equation">#</a></span>\[\begin{split}\begin{split}
    J(\theta) := \E_{s_0 \sim \mu_0} V^{\pi_\theta} (s_0) = &amp; \E \sum_{t=0}^{T-1} r_t \\
    \text{where} \quad &amp; s_0 \sim \mu_0 \\
    &amp; s_{t+1} \sim P(s_t, a_t), \\
    &amp; a_\hi = \pi_\theta(s_\hi) \\
    &amp; r_\hi = r(s_\hi, a_\hi).
\end{split}\end{split}\]</div>
<p>We call a sequence of states, actions, and rewards a <strong>trajectory</strong> <span class="math notranslate nohighlight">\(\tau = (s_i, a_i, r_i)_{i=0}^{T-1}\)</span>. The total time-discounted reward is also often called the <strong>return</strong> <span class="math notranslate nohighlight">\(R(\tau)\)</span> of a trajectory. Note that the above is the <em>undiscounted, finite-horizon case,</em> which we’ll continue to use throughout the chapter, but analogous results hold for the <em>discounted, infinite-horizon case.</em></p>
<p>Note that when the state transitions are Markov (i.e. <span class="math notranslate nohighlight">\(s_{t}\)</span> only depends on <span class="math notranslate nohighlight">\(s_{t-1}, a_{t-1}\)</span>) and the policy is stationary (i.e. <span class="math notranslate nohighlight">\(a_t \sim \pi_\theta (s_t)\)</span>), we can write out the <em>likelihood of a trajectory</em> under the policy <span class="math notranslate nohighlight">\(\pi_\theta\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-trajectory-likelihood">
<span class="eqno">(6.2)<a class="headerlink" href="#equation-trajectory-likelihood" title="Link to this equation">#</a></span>\[\begin{split}\begin{split}
        \rho_\theta(\tau) &amp;= \mu(s_0) \pi_\theta(a_0 | s_0) \\
        &amp;\qquad \times P(s_1 | s_0, a_0) \pi_\theta(a_1 | s_1) \\
        &amp;\qquad \times \cdots \\
        &amp;\qquad \times P(s_{H-1} | s_{H-2}, a_{H-2}) \pi_\theta(a_{H-1} | s_{H-1}).
\end{split}\end{split}\]</div>
<p>This lets us rewrite <span class="math notranslate nohighlight">\(J(\theta) = \E_{\tau \sim \rho_\theta} R(\tau).\)</span></p>
<p>Now how do we optimize for this function (the expected total reward)? One very general optimization technique is <em>gradient ascent.</em> Namely, the <strong>gradient</strong> of a function at a given point answers: At this point, which direction should we move to increase the function the most? By repeatedly moving in this direction, we can keep moving up on the graph of this function. Expressing this iteratively, we have: $<span class="math notranslate nohighlight">\(\theta_{t+1} = \theta_t + \eta \nabla_\theta J(\pi_\theta) \Big|_{\theta = \theta_t},\)</span>$</p>
<p>Where <span class="math notranslate nohighlight">\(\eta\)</span> is a <em>hyperparameter</em> that says how big of a step to take each time.</p>
<p>In order to apply this technique, we need to be able to evaluate the gradient <span class="math notranslate nohighlight">\(\nabla_\theta J(\pi_\theta).\)</span> How can we do this?</p>
<p>In practice, it’s often impractical to evaluate the gradient directly. For example, in supervised learning, <span class="math notranslate nohighlight">\(J(\theta)\)</span> might be the sum of squared prediction errors across an entire <strong>training dataset.</strong> However, if our dataset is very large, we might not be able to fit it into our computer’s memory!</p>
<p>Instead, we can <em>estimate</em> a gradient step using some estimator <span class="math notranslate nohighlight">\(\tilde \nabla J(\theta).\)</span> This is called <strong><em>stochastic</em> gradient descent</strong> (SGD). Ideally, we want this estimator to be <strong>unbiased,</strong> that is, on average, it matches a single true gradient step: $<span class="math notranslate nohighlight">\(\E [\tilde \nabla J(\theta)] = \nabla J(\theta).\)</span><span class="math notranslate nohighlight">\( If \)</span>J$ is defined in terms of some training dataset, we might randomly choose a <em>minibatch</em> of samples and use them to estimate the prediction error across the <em>whole</em> dataset. (This approach is known as <strong><em>minibatch</em> SGD</strong>.)</p>
<p>Notice that our parameters will stop changing once <span class="math notranslate nohighlight">\(\nabla J(\theta) = 0.\)</span> This implies that our current parameters are ‘locally optimal’ in some sense; it’s impossible to increase the function by moving in any direction. If <span class="math notranslate nohighlight">\(J\)</span> is convex, then the only point where this happens is at the <em>global optimum.</em> Otherwise, if <span class="math notranslate nohighlight">\(J\)</span> is nonconvex, the best we can hope for is a <em>local optimum.</em></p>
<p>We can actually show that in a finite number of steps, SGD will find a <span class="math notranslate nohighlight">\(\theta\)</span> that is “close” to a local optimum. More formally, suppose we run SGD for <span class="math notranslate nohighlight">\(T\)</span> steps, using an unbiased gradient estimator. Let the step size <span class="math notranslate nohighlight">\(\eta_t\)</span> scale as <span class="math notranslate nohighlight">\(O(1/ \sqrt{t}).\)</span> Then if <span class="math notranslate nohighlight">\(J\)</span> is bounded and <span class="math notranslate nohighlight">\(\beta\)</span>-smooth, and the norm of the gradient estimator has a finite variance, then after <span class="math notranslate nohighlight">\(T\)</span> steps: $<span class="math notranslate nohighlight">\(\|\nabla_\theta J(\theta)\|^2 \le O \left( M \beta \sigma^2 / T\right).\)</span><span class="math notranslate nohighlight">\( In another perspective, the local &quot;landscape&quot; of \)</span>J<span class="math notranslate nohighlight">\( around \)</span>\theta$ becomes flatter and flatter the longer we run SGD.</p>
</section>
<section id="reinforce-and-importance-sampling">
<h2><span class="section-number">6.3. </span>REINFORCE and Importance Sampling<a class="headerlink" href="#reinforce-and-importance-sampling" title="Link to this heading">#</a></h2>
<p>Note that the objective function above, <span class="math notranslate nohighlight">\(J(\theta) = \E_{\tau \sim \rho_\theta}R(\tau),\)</span> is very difficult, or even intractable, to compute exactly! This is because it involves taking an expectation over all possible trajectories <span class="math notranslate nohighlight">\(\tau.\)</span> Can we rewrite this in a form that’s more convenient to implement?</p>
<p>Specifically, suppose there is some distribution over trajectories <span class="math notranslate nohighlight">\(\rho(\tau)\)</span> that’s easy to sample from (e.g. a database of existing trajectories). We can then rewrite the gradient of objective function, a.k.a. the <em>policy gradient</em>, as follows (all gradients are being taken w.r.t. <span class="math notranslate nohighlight">\(\theta\)</span>): $<span class="math notranslate nohighlight">\(\begin{aligned}
    \nabla J(\theta) &amp; = \nabla \E_{\tau \sim \rho_\theta} [ R(\tau) ]                                                                                         \\
                     &amp; = \nabla \E_{\tau \sim \rho} \left[ \frac{\rho_\theta(\tau)}{\rho(\tau)} R(\tau) \right] &amp;  &amp; \text{likelihood ratio trick}             \\
                     &amp; = \E_{\tau \sim \rho} \left[ \frac{\nabla \rho_\theta(\tau)}{\rho(\tau)} R(\tau) \right] &amp;  &amp; \text{switching gradient and expectation}
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Note that setting \)</span>\rho = \rho_\theta<span class="math notranslate nohighlight">\( allows us to express \)</span>\nabla J<span class="math notranslate nohighlight">\( as an expectation. (Notice the swapped order of \)</span>\nabla<span class="math notranslate nohighlight">\( and \)</span>\E<span class="math notranslate nohighlight">\(!) \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} [ \nabla \log \rho_\theta(\tau) \cdot R(\tau)].
\end{aligned}\)</span><span class="math notranslate nohighlight">\( Consider expanding out \)</span>\rho_\theta.<span class="math notranslate nohighlight">\( Note that taking its \)</span>\log<span class="math notranslate nohighlight">\( turns it into a sum of \)</span>\log<span class="math notranslate nohighlight">\( terms, of which only the \)</span>\pi_\theta(a_t | s_t)<span class="math notranslate nohighlight">\( terms depend on \)</span>\theta,<span class="math notranslate nohighlight">\( so we can simplify even further to obtain the following expression for the policy gradient, known as the &quot;REINFORCE&quot; policy gradient: \)</span><span class="math notranslate nohighlight">\(\begin{aligned}
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) R(\tau) \right]
\end{aligned}\)</span><span class="math notranslate nohighlight">\( This expression allows us to estimate the gradient by sampling a few sample trajectories from \)</span>\pi_\theta,$ calculating the likelihoods of the chosen actions, and substituting these into the expression above.</p>
<p>In fact, we can perform one more simplification. Intuitively, the action taken at step <span class="math notranslate nohighlight">\(t\)</span> does not affect the reward from previous timesteps, since they’re already in the past! You can also show rigorously that this is the case, and that we only need to consider the present and future rewards to calculate the policy gradient:</p>
<div class="math notranslate nohighlight" id="equation-pg-with-q">
<span class="eqno">(6.3)<a class="headerlink" href="#equation-pg-with-q" title="Link to this equation">#</a></span>\[\begin{split}\begin{split}
        \nabla J(\theta) &amp;= \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) \sum_{t' = t}^{T-1} r(s_{t'}, a_{t'}) \right] \\
        &amp;= \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) Q^{\pi_\theta}(s_{t}, a_{t}) \right]
    \end{split}\end{split}\]</div>
<p><strong>Exercise:</strong> Prove that this is equivalent to the previous definitions. What modification to the expression must be made for the discounted, infinite-horizon setting?</p>
<p>For some intuition into how this method works, recall that we update our parameters according to</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
    \theta_{t+1} &amp;= \theta_t + \nabla J(\theta_t) \\
    &amp;= \theta_t + \E_{\tau \sim \rho_{\theta_t}} \nabla \log \rho_{\theta_t}(\tau) \cdot R(\tau).
\end{split}\end{split}\]</div>
<p>Consider the “good” trajectories where <span class="math notranslate nohighlight">\(R(\tau)\)</span> is large. Then <span class="math notranslate nohighlight">\(\theta\)</span> gets updated so that these trajectories become more likely. To see why, recall that <span class="math notranslate nohighlight">\(\rho_{\theta}(\tau)\)</span> is the likelihood of the trajectory <span class="math notranslate nohighlight">\(\tau\)</span> under the policy <span class="math notranslate nohighlight">\(\pi_\theta,\)</span> so evaluating the gradient points in the direction that makes <span class="math notranslate nohighlight">\(\tau\)</span> more likely.</p>
<p>This is an example of <strong>importance sampling:</strong> updating a distribution to put more density on “more important” samples (in this case trajectories).</p>
</section>
<section id="baselines-and-advantages">
<h2><span class="section-number">6.4. </span>Baselines and advantages<a class="headerlink" href="#baselines-and-advantages" title="Link to this heading">#</a></h2>
<p>A central idea from supervised learning is the bias-variance tradeoff. So far, our method is <em>unbiased,</em> meaning that its average is the true policy gradient. Can we find ways to reduce the variance of our estimator as well?</p>
<p>We can instead subtract a <strong>baseline function</strong> <span class="math notranslate nohighlight">\(b_t : \mathcal{S} \to \mathbb{R}\)</span> at each timestep <span class="math notranslate nohighlight">\(t.\)</span> This modifies the policy gradient as follows: $<span class="math notranslate nohighlight">\(\nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{H-1} \nabla \log \pi_\theta (a_t | s_t) \left(
        \left(
        \sum_{t' = t}^{H-1} r_t
        \right)
        - b_t(s_t)
        \right)
        \right].
    \label{eq:pg_baseline}\)</span>$</p>
<p>For example, we might want <span class="math notranslate nohighlight">\(b_t\)</span> to estimate the average reward-to-go at a given timestep: <span class="math notranslate nohighlight">\(b_t^\theta = \E_{\tau \sim \rho_\theta} R_t(\tau).\)</span> This way, the random variable <span class="math notranslate nohighlight">\(R_t(\tau) - b_t^\theta\)</span> is centered around zero, making certain algorithms more stable.</p>
<p>As a better baseline, we could instead choose the <em>value function.</em> Note that the random variable <span class="math notranslate nohighlight">\(Q^\pi_t(s, a) - V^\pi_t(s),\)</span> where the randomness is taken over the actions, is also centered around zero. (Recall <span class="math notranslate nohighlight">\(V^\pi_t(s) = \E_{a \sim \pi} Q^\pi_t(s, a).\)</span>) In fact, this quantity has a particular name: the <strong>advantage function.</strong> This measures how much better this action does than the average for that policy. (Note that for an optimal policy <span class="math notranslate nohighlight">\(\pi^\star,\)</span> the advantage of a given state-action pair is always nonpositive.)</p>
<p>We can now express the policy gradient as follows. Note that the advantage function effectively replaces the <span class="math notranslate nohighlight">\(Q\)</span>-function from <a class="reference internal" href="#equation-pg-with-q">(6.3)</a>:</p>
<div class="math notranslate nohighlight" id="equation-pg-advantage">
<span class="eqno">(6.4)<a class="headerlink" href="#equation-pg-advantage" title="Link to this equation">#</a></span>\[\nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_t | s_t) A^{\pi_\theta}_t (s_t, a_t)
        \right].\]</div>
<p>Note that to avoid correlations between the gradient estimator and the value estimator (i.e. baseline), we must estimate them with independently sampled trajectories:</p>
<div class="proof definition admonition" id="definition-0">
<p class="admonition-title"><span class="caption-number">Definition 6.1 </span></p>
<section class="definition-content" id="proof-content">
<p>Policy gradient with a learned baselinepg_baseline</p>
<!-- :::{prf:algorithmic}
Learning rate $\eta_0, \dots, \eta_{K-1}$ Initialization $\theta^0$ Sample $N$ trajectories from $\pi_{\theta^k}$ to estimate a baseline $\tilde b$ such that $\tilde b_\hi(s) \approx V_\hi^{\theta^k}(s)$ Sample $M$ trajectories $\tau_0, \dots, \tau_{M-1} \sim \rho_{\theta^k}$ Compute the policy gradient estimate $$\tilde{\nabla}_\theta J(\theta^k) = \frac{1}{M} \sum_{m=0}^{M-1} \sum_{h=0}^{H-1} \nabla \log \pi_{\theta^k} (a_\hi \mid s_\hi) (R_\hi(\tau_m) - \tilde b_\hi(s_\hi))$$ Gradient ascent update $\theta^{k+1} \gets \theta^k + \tilde \nabla_\theta J(\theta^k)$
::: -->
<p>The baseline estimation step can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.</p>
</section>
</div></section>
<section id="example-policy-parameterizations">
<span id="parameterizations"></span><h2><span class="section-number">6.5. </span>Example policy parameterizations<a class="headerlink" href="#example-policy-parameterizations" title="Link to this heading">#</a></h2>
<p>What are some different ways we could parameterize our policy?</p>
<p>If both the state and action spaces are finite, perhaps we could simply learn a preference value <span class="math notranslate nohighlight">\(\theta_{s,a}\)</span> for each state-action pair. Then to turn this into a valid distribution, we perform a “softmax” operation: we exponentiate each of them, and divide by the total: $<span class="math notranslate nohighlight">\(\pi^\text{softmax}_\theta(a | s) = \frac{\exp(\theta_{s,a})}{\sum_{s,a'} \exp (\theta_{s,a'})}.\)</span>$ However, this doesn’t make use of any structure in the states or actions, so while this is flexible, it is also prone to overfitting.</p>
<section id="linear-in-features">
<h3><span class="section-number">6.5.1. </span>Linear in features<a class="headerlink" href="#linear-in-features" title="Link to this heading">#</a></h3>
<p>Instead, what if we map each state-action pair into some <strong>feature space</strong> <span class="math notranslate nohighlight">\(\phi(s, a) \in \mathbb{R}^p\)</span>? Then, to map a feature vector to a probability, we take a linear combination <span class="math notranslate nohighlight">\(\theta \in \mathbb{R}^p\)</span> of the features and take a softmax: $<span class="math notranslate nohighlight">\(\pi^\text{linear in features}_{\theta}(a|s) = \frac{\exp(\theta^\top \phi(s, a))}{\sum_{a'} \exp(\theta^\top \phi(s, a'))}.\)</span><span class="math notranslate nohighlight">\( Another interpretation is that \)</span>\theta<span class="math notranslate nohighlight">\( represents the feature vector of the &quot;ideal&quot; state-action pair, as state-action pairs whose features align closely with \)</span>\theta$ are given higher probability.</p>
<p>The score function for this parameterization is also quite elegant:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{split}
        \nabla \log \pi_\theta(a|s) &amp;= \nabla \left( \theta^\top \phi(s, a) - \log \left( \sum_{a'} \exp(\theta^\top \phi(s, a')) \right) \right) \\
        &amp;= \phi(s, a) - \E_{a' \sim \pi_\theta(s)} \phi(s, a')
    \end{split}\end{split}\]</div>
<p>Plugging this into our policy gradient expression, we get</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \nabla J(\theta) &amp; = \E_{\tau \sim \rho_\theta} \left[
    \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_t | s_t) A_t^{\pi_\theta}
    \right]                                                                                                                    \\
                     &amp; = \E_{\tau \sim \rho_\theta} \left[
    \sum_{t=0}^{T-1} \left( \phi(s_t, a_t) - \E_{a' \sim \pi(s_t)} \phi(s_t, a') \right) A_t^{\pi_\theta}(s_t, a_t)
    \right]                                                                                                                    \\
                     &amp; = \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \phi(s_t, a_t) A_t^{\pi_\theta} (s_t, a_t) \right]
\end{aligned}\end{split}\]</div>
<p>Why can we drop the <span class="math notranslate nohighlight">\(\E \phi(s_t, a')\)</span> term? By linearity of expectation, consider the dropped term at a single timestep: <span class="math notranslate nohighlight">\(\E_{\tau \sim \rho_\theta} \left[ \left( \E_{a' \sim \pi(s_t)} \phi(s, a') \right) A_t^{\pi_\theta}(s_t, a_t) \right].\)</span> By Adam’s Law, we can wrap the advantage term in a conditional expectation on the state <span class="math notranslate nohighlight">\(s_t.\)</span> Then we already know that <span class="math notranslate nohighlight">\(\E_{a \sim \pi(s)} A_t^{\pi}(s, a) = 0,\)</span> and so this entire term vanishes.</p>
</section>
<section id="neural-policies">
<h3><span class="section-number">6.5.2. </span>Neural policies<a class="headerlink" href="#neural-policies" title="Link to this heading">#</a></h3>
<p>More generally, we could map states and actions to unnormalized scores via some parameterized function <span class="math notranslate nohighlight">\(f_\theta : \mathcal{S} \times \mathcal{A} \to \mathbb{R},\)</span> such as a neural network, and choose actions according to a softmax: $<span class="math notranslate nohighlight">\(\pi^\text{general}_\theta(a|s) = \frac{\exp(f_{\theta}(s,a))}{\sum_{a'} \exp(f_{\theta}(s,a'))}.\)</span>$</p>
<p>The score can then be written as $<span class="math notranslate nohighlight">\(\nabla \log \pi_\theta(a|s) = \nabla f_\theta(s, a) - \E_{a \sim \pi_\theta(s)} \nabla f_\theta (s, a')\)</span>$</p>
</section>
<section id="continuous-action-spaces">
<h3><span class="section-number">6.5.3. </span>Continuous action spaces<a class="headerlink" href="#continuous-action-spaces" title="Link to this heading">#</a></h3>
<p>Consider a continuous <span class="math notranslate nohighlight">\(n\)</span>-dimensional action space <span class="math notranslate nohighlight">\(\mathcal{A} = \mathbb{R}^n\)</span>. Then for a stochastic policy, we could use a function to predict the <em>mean</em> action and then add some random noise about it. For example, we could use a neural network to predict the mean action <span class="math notranslate nohighlight">\(\mu_\theta(s)\)</span> and then add some noise <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> to it:</p>
<div class="math notranslate nohighlight">
\[\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma^2 I).\]</div>
<p><strong>Exercise:</strong> Can you extend the “linear in features” policy to continuous action spaces in a similar way?</p>
</section>
</section>
<section id="local-policy-optimization">
<h2><span class="section-number">6.6. </span>Local policy optimization<a class="headerlink" href="#local-policy-optimization" title="Link to this heading">#</a></h2>
<section id="motivation-for-policy-gradient">
<h3><span class="section-number">6.6.1. </span>Motivation for policy gradient<a class="headerlink" href="#motivation-for-policy-gradient" title="Link to this heading">#</a></h3>
<p>Recall the policy iteration algorithm discussed in the MDP section: We alternate between these two steps:</p>
<ul class="simple">
<li><p>Estimating the <span class="math notranslate nohighlight">\(Q\)</span>-function of the current policy</p></li>
<li><p>Updating the policy to be greedy w.r.t. this approximate <span class="math notranslate nohighlight">\(Q\)</span>-function.</p></li>
</ul>
<p>(Note that we could equivalently estimate the advantage function.)</p>
<p>What advantages does the policy gradient algorithm have over policy iteration? Both policy gradient and policy iteration are iterative algorithms.</p>
<p>To analyze the difference between them, we’ll make use of the <strong>performance difference lemma</strong>.</p>
<div class="proof theorem admonition" id="pdl">
<p class="admonition-title"><span class="caption-number">Theorem 6.1 </span> (Performance difference lemma)</p>
<section class="theorem-content" id="proof-content">
<p>Suppose Beatrice and Joan are playing a game and want to compare their average rewards starting in state <span class="math notranslate nohighlight">\(s\)</span>.
However, only Beatrice is allowed to take actions, while Joan can evaluate those actions from her own perspective. That is, she knows how good Beatrice’s action is compared to her typical strategy in that state. (This is her <em>advantage function</em> <span class="math notranslate nohighlight">\(A_\hi^{\text{Joan}}(s_\hi, a_\hi)\)</span>).</p>
<p>The performance difference lemma says that this is all they need to compare themselves! That is,</p>
<div class="math notranslate nohighlight" id="equation-pdl-eq">
<span class="eqno">(6.5)<a class="headerlink" href="#equation-pdl-eq" title="Link to this equation">#</a></span>\[ \begin{align}\begin{aligned}V_0^{\text{Beatrice}}(s) - V_0^{\text{Joan}}(s) = \E_{\tau \sim \rho_{\text{Beatrice}, s}} \left[ \sum_{h=0}^{H-1} A_\hi^{\text{Joan}} (s_\hi, a_\hi) \right]
:::{math}\\where $\rho_{\text{Beatrice}, s}$ denotes the distribution over trajectories starting in state $s$ when Beatrice is playing.\\To see why, consider just a single step $\hi$ of the trajectory. At this step we compute how much better actions from Joan are than the actions from Beatrice, on average. But this is exactly the average Joan-evaluated-advantage across actions from Beatrice, as described in the PDL!\\Formally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that\\\begin{split}$$
\begin{align*}
A^\pi_\hi(s_\hi, a_\hi) &amp;= Q^\pi_\hi(s_\hi, a_\hi) - V^\pi_\hi(s_\hi) \\
&amp;= r_\hi(s_\hi, a_\hi) + \E_{s_{\hi+1} \sim P(s_\hi, a_\hi)} [V^\pi_{\hi+1}(s_{\hi+1})] - V^\pi_\hi(s_\hi)
\end{align*}
$$\end{split}\\so expanding out the r.h.s. expression of {eq}`pdl_eq` and grouping terms together gives\\\begin{split}$$
\begin{align*}
\E_{\tau \sim \rho_{\text{Beatrice}, s}} \left[ \sum_{\hi=0}^{\hor-1} A_\hi^{\text{Joan}} (s_\hi, a_\hi) \right] &amp;= \E_{\tau \sim \rho_{\text{Beatrice}, s}} \left[ \left( \sum_{\hi=0}^{\hor-1} r_\hi(s_\hi, a_\hi) \right) + \left( V^{\text{Joan}}_1(s_1) + \cdots + V^{\text{Joan}}_\hor(s_\hor) \right) - \left( V^{\text{Joan}_0}(s_0) + \cdots + V^{\text{Joan}}_{\hor-1}(s_{\hor-1}) \right) \right] \\
&amp;= V^{\text{Beatrice}}_0(s) - V^{\text{Joan}}_0(s)
\end{align*}
$$\end{split}\\as desired. (Note that the &quot;inner&quot; expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid.)\end{aligned}\end{align} \]</div>
</section>
</div><p>Let’s analyze why fitted approaches such as PI don’t work as well in the RL setting. To start, let’s ask, where <em>do</em> fitted approaches work well? They are commonly seen in <em>supervised learning</em>, where a prediction rule is fit using some labelled training set, and then assessed on a test set from the same distribution. Does this assumption still hold when doing PI?</p>
<p>Let’s consider a single iteration of PI. Suppose the new policy <span class="math notranslate nohighlight">\(\tilde \pi\)</span> chooses some action with a negative advantage w.r.t. <span class="math notranslate nohighlight">\(\pi\)</span>. Define <span class="math notranslate nohighlight">\(\Delta_\infty = \min_{s \in \mathcal{S}} A^{\pi}_\hi(s, \tilde \pi(s))\)</span>. If this is negative, then the PDL shows that there may exist some state <span class="math notranslate nohighlight">\(s\)</span> and time <span class="math notranslate nohighlight">\(h\)</span> such that</p>
<div class="math notranslate nohighlight">
\[V_\hi^{\tilde \pi}(s) \ge V_\hi^{\pi}(s) - H \cdot |\Delta_\infty|.\]</div>
<p>In general, PI cannot avoid particularly bad situations where the new policy <span class="math notranslate nohighlight">\(\tilde \pi\)</span> often visits these bad states, causing an actual degradation. It does not enforce that the trajectory distributions <span class="math notranslate nohighlight">\(\rho_\pi\)</span> and <span class="math notranslate nohighlight">\(\rho_{\tilde \pi}\)</span> be close to each other. In other words, the “training distribution” that our prediction rule is fitted on, <span class="math notranslate nohighlight">\(\rho_\pi\)</span>, may differ significantly from the “evaluation distribution” <span class="math notranslate nohighlight">\(\rho_{\tilde \pi}\)</span> — we must address this issue of <em>distributional shift</em>.</p>
<p>How can we enforce that the <em>trajectory distributions</em> do not change much at each step? In fact, policy gradient already does this to a small extent: Supposing that the mapping from parameters to trajectory distributions is relatively smooth, then, by adjusting the parameters a small distance from the current iterate, we end up at a new policy with a similar trajectory distribution. But this is not very rigorous, and in practice the parameter-to-distribution mapping may not be smooth. Can we constrain the distance between the resulting distributions more explicitly? This brings us to the next two methods: <strong>trust region policy optimization</strong> (TRPO) and the <strong>natural policy gradient</strong> (NPG).</p>
</section>
<section id="trust-region-policy-optimization">
<h3><span class="section-number">6.6.2. </span>Trust region policy optimization<a class="headerlink" href="#trust-region-policy-optimization" title="Link to this heading">#</a></h3>
<p>TRPO is another iterative algorithm for policy optimization. It is similar to policy iteration, except we constrain the updated policy to be “close to” the current policy in terms of the trajectory distributions they induce.</p>
<p>To formalize “close to”, we typically use the <strong>Kullback-Leibler divergence (KLD)</strong>:</p>
<div class="proof definition admonition" id="definition-2">
<p class="admonition-title"><span class="caption-number">Definition 6.2 </span></p>
<section class="definition-content" id="proof-content">
<p>Kullback-Leibler divergencekld For two PDFs <span class="math notranslate nohighlight">\(p, q\)</span>, $<span class="math notranslate nohighlight">\(\kl{p}{q} := \E_{x \sim p} \left[ \log \frac{p(x)}{q(x)} \right]\)</span><span class="math notranslate nohighlight">\( This can be interpreted in many different ways, many stemming from information theory. Note that \)</span>\kl{p}{q} = 0<span class="math notranslate nohighlight">\( if and only if \)</span>p = q$. Also note that it is generally not symmetric.</p>
</section>
</div><p>Additionally, rather than estimating the <span class="math notranslate nohighlight">\(Q\)</span>-function of the current policy, we can use the RHS of the Performance Difference Lemma <a class="reference internal" href="#pdl">Theorem 6.1</a> as our optimization target.</p>
<div class="proof definition admonition" id="trpo">
<p class="admonition-title"><span class="caption-number">Definition 6.3 </span> (Trust region policy optimization (exact))</p>
<section class="definition-content" id="proof-content">
<!-- :::{prf:algorithmic}
Trust region radius $\delta$ Initialize $\theta^0$ $\theta^{k+1} \gets \argmax_{\theta} \E_{s_0, \dots, s_{H-1} \sim \pi^k} \left[ \sum_\hi \E_{a_\hi \sim \pi_\theta(s_\hi)} A^{\pi^k}(s_\hi, a_\hi) \right]$ See below where $\kl{\rho_{\pi^k}}{\rho_{\pi_{\theta}}} \le \delta$ $\pi^K$
::: -->
<p>Note that the objective function is not identical to the r.h.s. of the Performance Difference Lemma. Here, we still use the <em>states</em> sampled from the old policy, and only use the <em>actions</em> from the new policy. This is because it would be computationally infeasible to sample entire trajectories from <span class="math notranslate nohighlight">\(\pi_\theta\)</span> as we are optimizing over <span class="math notranslate nohighlight">\(\theta\)</span>. This approximation is also reasonable in the sense that it matches the r.h.s. of the Performance Difference Lemma to first order in <span class="math notranslate nohighlight">\(\theta\)</span>. (We will elaborate more on this later.)</p>
</section>
</div><p>Both the objective function and the KLD constraint involve a weighted average over the space of all trajectories. This is intractable in general, so we need to estimate the expectation. As before, we can do this by taking an empirical average over samples from the trajectory distribution. However, the inner expectation over <span class="math notranslate nohighlight">\(a_\hi \sim \pi_{\theta}\)</span> involves the optimizing variable <span class="math notranslate nohighlight">\(\theta\)</span>, and we’d like an expression that has a closed form in terms of <span class="math notranslate nohighlight">\(\theta\)</span> to make optimization tractable. Otherwise, we’d need to resample many times each time we made an update to <span class="math notranslate nohighlight">\(\theta\)</span>. To address this, we’ll use a common technique known as <strong>importance sampling</strong>.</p>
<div class="proof definition admonition" id="importance_sampling">
<p class="admonition-title"><span class="caption-number">Definition 6.4 </span> (Importance sampling)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we want to estimate <span class="math notranslate nohighlight">\(\E_{x \sim \tilde p}[f(x)]\)</span>. However, <span class="math notranslate nohighlight">\(\tilde p\)</span> is difficult to sample from, so we can’t take an empirical average directly. Instead, there is some other distribution <span class="math notranslate nohighlight">\(p\)</span> that is easier to sample from, e.g. we could draw samples from an existing dataset, as in the case of <strong>offline RL</strong>.</p>
<p>Then note that $<span class="math notranslate nohighlight">\(\E_{x \sim \tilde p} [f(x)] = \E_{x \sim p}\left[ \frac{\tilde p(x)}{p(x)} f(x) \right]\)</span><span class="math notranslate nohighlight">\( so, given i.i.d. samples \)</span>x_0, \dots, x_{N-1} \sim p<span class="math notranslate nohighlight">\(, we can construct an unbiased estimate of \)</span>\E_{x \sim \tilde p} [f(x)]<span class="math notranslate nohighlight">\( by *reweighting* these samples according to the likelihood ratio \)</span>\tilde p(x)/p(x)<span class="math notranslate nohighlight">\(: \)</span><span class="math notranslate nohighlight">\(\frac{1}{N} \sum_{n=0}^{N-1} \frac{\tilde p(x_n)}{p(x_n)} f(x_n)\)</span>$</p>
<p>Doesn’t this seem too good to be true? If there were no drawbacks, we could use this to estimate <em>any</em> expectation of any function on any arbitrary distribution! The drawback is that the variance may be very large due to the likelihood ratio term. If the sampling distribution <span class="math notranslate nohighlight">\(p\)</span> assigns low probability to any region where <span class="math notranslate nohighlight">\(\tilde p\)</span> assigns high probability, then the likelihood ratio will be very large and cause the variance to blow up.</p>
</section>
</div><p>Applying importance sampling allows us to estimate the TRPO objective as follows:</p>
<div class="proof definition admonition" id="trpo_implement">
<p class="admonition-title"><span class="caption-number">Definition 6.5 </span> (Trust region policy optimization (implementation))</p>
<section class="definition-content" id="proof-content">
<!-- :::{prf:algorithmic} TODO
Initialize $\theta^0$ Sample $N$ trajectories from $\rho^k$ to learn a value estimator $\tilde b_\hi(s) \approx V^{\pi^k}_\hi(s)$ Sample $M$ trajectories $\tau_0, \dots, \tau_{M-1} \sim \rho^k$ $$\begin{gathered}
            \theta^{k+1} \gets \argmax_{\theta} \frac{1}{M} \sum_{m=0}^{M-1} \sum_{h=0}^{H-1} \frac{\pi_\theta(a_\hi \mid s_\hi)}{\pi^k(a_\hi \mid s_\hi)} [ R_\hi(\tau_m) - \tilde b_\hi(s_\hi) ] \\
            \text{where } \sum_{m=0}^{M-1} \sum_{h=0}^{H-1} \log \frac{\pi_k(a_\hi^m \mid s_\hi^m)}{\pi_\theta(a_\hi^m \mid s_\hi^m)} \le \delta
        
\end{gathered}$$
::: -->
</section>
</div></section>
<section id="natural-policy-gradient">
<h3><span class="section-number">6.6.3. </span>Natural policy gradient<a class="headerlink" href="#natural-policy-gradient" title="Link to this heading">#</a></h3>
<p>Instead, we can solve an approximation to the TRPO optimization problem. This will link us back to the policy gradient from before. We take a first-order approximation to the objective function and a second-order approximation to the KLD constraint. This results in the optimization problem $<span class="math notranslate nohighlight">\(\begin{gathered}
        \max_\theta \nabla_\theta J(\pi_{\theta^k})^\top (\theta - \theta^k) \\
        \text{where } \frac{1}{2} (\theta - \theta^k)^\top F_{\theta^k} (\theta - \theta^k) \le \delta
    \end{gathered}
    \label{npg_optimization}\)</span><span class="math notranslate nohighlight">\( where \)</span>F_{\theta^k}$ is the <strong>Fisher information matrix</strong> defined below.</p>
<div class="proof definition admonition" id="definition-6">
<p class="admonition-title"><span class="caption-number">Definition 6.6 </span></p>
<section class="definition-content" id="proof-content">
<p>Fisher information matrixfisher_matrix Let <span class="math notranslate nohighlight">\(p_\theta\)</span> denote a parameterized distribution. Its Fisher information matrix <span class="math notranslate nohighlight">\(F_\theta\)</span> can be defined equivalently as: $$\begin{aligned}
F_{\theta} &amp; = \E_{x \sim p_\theta} \left[ (\nabla_\theta \log p_\theta(x)) (\nabla_\theta \log p_\theta(x))^\top \right] &amp; \text{covariance matrix of the Fisher score}          \
&amp; = \E_{x \sim p_{\theta}} [- \nabla_\theta^2 \log p_\theta(x)]                                                &amp; \text{average Hessian of the negative log-likelihood}</p>
<p>\end{aligned}$<span class="math notranslate nohighlight">\( Recall that the Hessian of a function describes its curvature: That is, for a vector \)</span>\delta \in \Theta<span class="math notranslate nohighlight">\(, the quantity \)</span>\delta^\top F_\theta \delta<span class="math notranslate nohighlight">\( describes how rapidly the negative log-likelihood changes if we move by \)</span>\delta$.</p>
<p>In particular, when <span class="math notranslate nohighlight">\(p_\theta = \rho_{\theta}\)</span> denotes a trajectory distribution, we can further simplify the expression: $<span class="math notranslate nohighlight">\(F_{\theta} = \E_{\tau \sim \rho_\theta} \left[ \sum_{h=0}^{H-1} (\nabla \log \pi_\theta (a_\hi \mid s_\hi)) (\nabla \log \pi_\theta(a_\hi \mid s_\hi))^\top \right]
        \label{eq:fisher_trajectory}\)</span>$ Note that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.</p>
</section>
</div><p>This is a convex optimization problem, and so we can find the global optima by setting the gradient of the Lagrangian to zero:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{aligned}
    \mathcal{L}(\theta, \eta)                     &amp; = \nabla_\theta J(\pi_{\theta^k})^\top (\theta - \theta^k) - \eta \left[ \frac{1}{2} (\theta - \theta^k)^\top F_{\theta^k} (\theta - \theta^k) - \delta \right] \\
    \nabla_\theta \mathcal{L}(\theta^{k+1}, \eta) &amp; = 0                                                                                                                                                             \\
    \nabla_\theta J(\pi_{\theta^k})        &amp; = \eta F_{\theta^k} (\theta^{k+1} - \theta^k)                                                                                                                   \\
    \theta^{k+1}                           &amp; = \theta^k + \eta F_{\theta^k}^{-1} \nabla_\theta J(\pi_{\theta^k})                                                                                             \\
    \text{where } \eta                     &amp; = \sqrt{\frac{\delta}{\nabla_\theta J(\pi_{\theta^k})^\top F_{\theta^k} \nabla_\theta J(\pi_{\theta^k})}}
\end{aligned}\end{split}\]</div>
<div class="proof definition admonition" id="definition-7">
<p class="admonition-title"><span class="caption-number">Definition 6.7 </span></p>
<section class="definition-content" id="proof-content">
<p>Natural policy gradientnpg</p>
<!-- :::{prf:algorithmic}
Learning rate $\eta > 0$ Initialize $\theta^0$ Estimate the policy gradient $\hat g \approx \nabla_\theta J(\pi_{\theta^k})$ See [\[eq:pg_advantage\]](#eq:pg_advantage){reference-type="eqref" reference="eq:pg_advantage"} Estimate the Fisher information matrix $\hat F \approx F_{\theta^k}$ See [\[eq:fisher_trajectory\]](#eq:fisher_trajectory){reference-type="eqref" reference="eq:fisher_trajectory"} $\theta^{k+1} \gets \theta^k + \eta \hat F^{-1} \hat g$ Natural gradient update
::: -->
<p>How many trajectory samples do we need to accurately estimate the Fisher information matrix? As a rule of thumb, the sample complexity should scale with the dimension of the parameter space. This makes this approach intractable in the deep learning setting where we might have a very large number of parameters.</p>
</section>
</div><p>For some intuition: The typical gradient descent algorithm treats the parameter space as “flat”, treating the objective function as some black box value. However, in the case here where the parameters map to a <em>distribution</em>, using the natural gradient update is equivalent to optimizing over distribution space rather than distribution space.</p>
<div class="proof example admonition" id="example-8">
<p class="admonition-title"><span class="caption-number">Example 6.1 </span></p>
<section class="example-content" id="proof-content">
<p>Natural gradient on a simple problemnatural_simple Let’s step away from reinforcement learning specifically and consider the following optimization problem over Bernoulli distributions <span class="math notranslate nohighlight">\(\pi \in \Delta(\{ 0, 1 \})\)</span>: $$\begin{aligned}
J(\pi) &amp; = 100 \cdot \pi(1) + 1 \cdot \pi(0)</p>
<p>\end{aligned}$<span class="math notranslate nohighlight">\( Clearly the optimal distribution is the constant one \)</span>\pi(1) = 1<span class="math notranslate nohighlight">\(. Suppose we optimize over the parameterized family \)</span>\pi_\theta(1) = \frac{\exp(\theta)}{1+\exp(\theta)}<span class="math notranslate nohighlight">\(. Then our optimization algorithm should set \)</span>\theta<span class="math notranslate nohighlight">\( to be unboundedly large. Then the vanilla gradient is \)</span><span class="math notranslate nohighlight">\(\nabla_\theta J(\pi_\theta) = \frac{99 \exp(\theta)}{(1 + \exp(\theta))^2}.\)</span><span class="math notranslate nohighlight">\( Note that as \)</span>\theta \to \infty<span class="math notranslate nohighlight">\( that the increments get closer and closer to \)</span>0<span class="math notranslate nohighlight">\(. However, if we compute the Fisher information scalar \)</span>$\begin{aligned}
F_\theta &amp; = \E_{x \sim \pi_\theta} [ (\nabla_\theta \log \pi_\theta(x))^2 ] \
&amp; = \frac{\exp(\theta)}{(1 + \exp(\theta))^2}</p>
<p>\end{aligned}$<span class="math notranslate nohighlight">\( resulting in the natural gradient update \)</span>$\begin{aligned}
\theta^{k+1} &amp; = \theta^k + \eta F_{\theta^k}^{-1} \nabla_ \theta J(\theta^k) \
&amp; = \theta^k + 99 \eta</p>
<p>\end{aligned}$$ which increases at a constant rate, i.e. improves the objective more quickly than vanilla gradient ascent.</p>
</section>
</div></section>
<section id="proximal-policy-optimization">
<h3><span class="section-number">6.6.4. </span>Proximal policy optimization<a class="headerlink" href="#proximal-policy-optimization" title="Link to this heading">#</a></h3>
<p>Can we improve on the computational efficiency of the above methods?</p>
<p>We can relax the TRPO objective in a different way: Rather than imposing a hard constraint on the KL distance, we can instead impose a <em>soft</em> constraint by incorporating it into the objective:</p>
<div class="proof definition admonition" id="definition-9">
<p class="admonition-title"><span class="caption-number">Definition 6.8 </span></p>
<section class="definition-content" id="proof-content">
<p>Proximal policy optimization (exact)ppo</p>
<!-- :::{prf:algorithmic}
Regularization parameter $\lambda$ Initialize $\theta^0$ $\theta^{k+1} \gets \argmax_{\theta} \E_{s_0, \dots, s_{H-1} \sim \pi^k} \left[ \sum_\hi \E_{a_\hi \sim \pi_\theta(s_\hi)} A^{\pi^k}(s_\hi, a_\hi) \right] - \lambda \kl{\rho_{\pi^k}}{\rho_{\pi_{\theta}}}$ $\theta^K$
::: -->
<p>Note that like the original TRPO algorithm <a class="reference internal" href="#trpo">Definition 6.3</a>, PPO is not gradient-based; rather, at each step, we try to maximize local advantage relative to the current policy.</p>
</section>
</div><p>Let us now turn this into an implementable algorithm, assuming we can sample trajectories from <span class="math notranslate nohighlight">\(\pi_{\theta^k}\)</span>.</p>
<p>Let us simplify the <span class="math notranslate nohighlight">\(\kl{\rho_{\pi^k}}{\rho_{\pi_{\theta}}}\)</span> term first. Expanding gives $<span class="math notranslate nohighlight">\(\begin{aligned}
    \kl{\rho_{\pi^k}}{\rho_{\pi_{\theta}}} &amp; = \E_{\tau \sim \rho_{\pi^k}} \left[\log \frac{\rho_{\pi^k}(\tau)}{\rho_{\pi_{\theta}}(\tau)}\right]                                                       \\
                                           &amp; = \E_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \log \frac{\pi^k(a_\hi \mid s_\hi)}{\pi_{\theta}(a_\hi \mid s_\hi)}\right] &amp; \text{state transitions cancel} \\
                                           &amp; = \E_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \log \frac{1}{\pi_{\theta}(a_\hi \mid s_\hi)}\right] + c
\end{aligned}\)</span><span class="math notranslate nohighlight">\( where \)</span>c<span class="math notranslate nohighlight">\( is some constant relative to \)</span>\theta$.</p>
<p>As we did for TRPO <a class="reference internal" href="#trpo">Definition 6.3</a>, we can use importance sampling <a class="reference internal" href="#importance_sampling">Definition 6.4</a> to rewrite the inner expectation. Combining the expectations together, this gives the (exact) objective $<span class="math notranslate nohighlight">\(\max_{\theta} \E_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \left( \frac{\pi_\theta(a_\hi \mid s_\hi)}{\pi^k(a_\hi \mid s_\hi)} A^{\pi^k}(s_\hi, a_\hi) - \lambda \log \frac{1}{\pi_\theta(a_\hi \mid s_\hi)} \right) \right]\)</span>$</p>
<p>Now we can use gradient ascent on the parameters <span class="math notranslate nohighlight">\(\theta\)</span> until convergence to maximize this function, completing a single iteration of PPO (i.e. <span class="math notranslate nohighlight">\(\theta^{k+1} \gets \theta\)</span>).</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="control.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">5. </span>Linear Quadratic Regulators</p>
      </div>
    </a>
    <a class="right-next"
       href="exploration.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Exploration in MDPs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation">6.1. Motivation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-policy-gradient-ascent">6.2. (Stochastic) Policy Gradient Ascent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reinforce-and-importance-sampling">6.3. REINFORCE and Importance Sampling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#baselines-and-advantages">6.4. Baselines and advantages</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-policy-parameterizations">6.5. Example policy parameterizations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-in-features">6.5.1. Linear in features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-policies">6.5.2. Neural policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#continuous-action-spaces">6.5.3. Continuous action spaces</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#local-policy-optimization">6.6. Local policy optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#motivation-for-policy-gradient">6.6.1. Motivation for policy gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trust-region-policy-optimization">6.6.2. Trust region policy optimization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#natural-policy-gradient">6.6.3. Natural policy gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#proximal-policy-optimization">6.6.4. Proximal policy optimization</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Alexander D. Cai
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>