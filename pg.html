<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Policy Gradient Methods – CS 1840: Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./imitation_learning.html" rel="next">
<link href="./fitted_dp.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./pg.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CS 1840: Introduction to Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">6.1</span> Introduction</a></li>
  <li><a href="#gradient-ascent" id="toc-gradient-ascent" class="nav-link" data-scroll-target="#gradient-ascent"><span class="header-section-number">6.2</span> Gradient Ascent</a>
  <ul class="collapse">
  <li><a href="#sec-computing-derivatives" id="toc-sec-computing-derivatives" class="nav-link" data-scroll-target="#sec-computing-derivatives"><span class="header-section-number">6.2.1</span> Computing derivatives</a></li>
  <li><a href="#stochastic-gradient-ascent" id="toc-stochastic-gradient-ascent" class="nav-link" data-scroll-target="#stochastic-gradient-ascent"><span class="header-section-number">6.2.2</span> Stochastic gradient ascent</a></li>
  </ul></li>
  <li><a href="#policy-stochastic-gradient-ascent" id="toc-policy-stochastic-gradient-ascent" class="nav-link" data-scroll-target="#policy-stochastic-gradient-ascent"><span class="header-section-number">6.3</span> Policy (stochastic) gradient ascent</a>
  <ul class="collapse">
  <li><a href="#sec-parameterizations" id="toc-sec-parameterizations" class="nav-link" data-scroll-target="#sec-parameterizations"><span class="header-section-number">6.3.1</span> Example policy parameterizations</a></li>
  <li><a href="#sec-importance-sampling" id="toc-sec-importance-sampling" class="nav-link" data-scroll-target="#sec-importance-sampling"><span class="header-section-number">6.3.2</span> Importance Sampling</a></li>
  </ul></li>
  <li><a href="#the-reinforce-policy-gradient" id="toc-the-reinforce-policy-gradient" class="nav-link" data-scroll-target="#the-reinforce-policy-gradient"><span class="header-section-number">6.4</span> The REINFORCE policy gradient</a></li>
  <li><a href="#baselines-and-advantages" id="toc-baselines-and-advantages" class="nav-link" data-scroll-target="#baselines-and-advantages"><span class="header-section-number">6.5</span> Baselines and advantages</a></li>
  <li><a href="#comparing-policy-gradient-algorithms-to-policy-iteration" id="toc-comparing-policy-gradient-algorithms-to-policy-iteration" class="nav-link" data-scroll-target="#comparing-policy-gradient-algorithms-to-policy-iteration"><span class="header-section-number">6.6</span> Comparing policy gradient algorithms to policy iteration</a></li>
  <li><a href="#sec-trpo" id="toc-sec-trpo" class="nav-link" data-scroll-target="#sec-trpo"><span class="header-section-number">6.7</span> Trust region policy optimization</a></li>
  <li><a href="#natural-policy-gradient" id="toc-natural-policy-gradient" class="nav-link" data-scroll-target="#natural-policy-gradient"><span class="header-section-number">6.8</span> Natural policy gradient</a></li>
  <li><a href="#sec-ppo" id="toc-sec-ppo" class="nav-link" data-scroll-target="#sec-ppo"><span class="header-section-number">6.9</span> Proximal policy optimization</a></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">6.10</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-pg" class="quarto-section-identifier"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">6.1</span> Introduction</h2>
<p>The core task of RL is finding the <strong>optimal policy</strong> in a given environment. This is essentially an <em>optimization problem:</em> out of some space of policies, we want to find the one that achieves the maximum total reward (in expectation).</p>
<p>It’s typically intractable to compute the optimal policy exactly in some finite number of steps. Instead, <strong>policy optimization algorithms</strong> start from some randomly initialized policy, and then <em>improve</em> it step by step. We’ve already seen some examples of these, namely <a href="mdps.html#sec-policy-iteration" class="quarto-xref"><span>Section 1.3.7.2</span></a> for finite MDPs and <a href="control.html#sec-iterative-lqr" class="quarto-xref"><span>Section 2.6.4</span></a> in continuous control.</p>
<p>In particular, we often use policies that can be described by some finite set of <strong>parameters.</strong> We will see some examples in <a href="#sec-parameterizations" class="quarto-xref"><span>Section 6.3.1</span></a>. For such parameterized policies, we can approximate the <strong>policy gradient:</strong> the gradient of the expected total reward with respect to the parameters. This tells us the direction the parameters should be updated to achieve a higher expected total reward. Policy gradient methods are responsible for groundbreaking applications including AlphaGo, OpenAI Five, and large language models, many of which use policies parameterized as deep neural networks.</p>
<ol type="1">
<li>We begin the chapter with a short review of gradient ascent, a general <strong>optimization method.</strong></li>
<li>We’ll then see how to estimate the <strong>policy gradient,</strong> enabling us to apply (stochastic) gradient ascent in the RL setting.</li>
<li>Then we’ll explore some <em>proximal optimization</em> techniques that ensure the steps taken are “not too large”. This is helpful to stabilize training and widely used in practice.</li>
</ol>
<div id="c5a99257" class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>load_ext autoreload</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>autoreload <span class="dv">2</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="57d69d17" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> utils <span class="im">import</span> plt, Array, Float, Callable, jax, jnp, latex, gym</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="gradient-ascent" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="gradient-ascent"><span class="header-section-number">6.2</span> Gradient Ascent</h2>
<div class="{note}">
<p>You may have previously heard of <em>gradient descent</em> for minimizing functions. Optimization problems are usually posed as <em>minimization</em> problems by convention. However, in RL, we usually talk about <em>maximizing</em> the expected total reward, and so we perform gradient <em>ascent</em> instead.</p>
</div>
<p><strong>Gradient ascent</strong> is a general optimization algorithm for any differentiable function. A suitable analogy for this algorithm is hiking up a mountain, where you keep taking steps in the steepest direction upwards. Here, your vertical position <span class="math inline">\(y\)</span> is the function being optimized, and your horizontal position <span class="math inline">\((x, z)\)</span> is the input to the function. The <em>slope</em> of the mountain at your current position is given by the <em>gradient</em>, written <span class="math inline">\(\nabla y(x, z) \in \mathbb{R}^2\)</span>.</p>
<div id="489fe14f" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> f(x, y):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Himmelblau's function"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (x<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> y <span class="op">-</span> <span class="dv">11</span>)<span class="op">**</span><span class="dv">2</span> <span class="op">+</span> (x <span class="op">+</span> y<span class="op">**</span><span class="dv">2</span> <span class="op">-</span> <span class="dv">7</span>)<span class="op">**</span><span class="dv">2</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create a grid of points</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">400</span>)</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> jnp.linspace(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">400</span>)</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> jnp.meshgrid(x, y)</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> f(X, Y)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">6</span>))</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>img <span class="op">=</span> ax.imshow(Z, extent<span class="op">=</span>[<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>], origin<span class="op">=</span><span class="st">'lower'</span>)</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>fig.colorbar(img, ax<span class="op">=</span>ax)</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>tx, ty <span class="op">=</span> <span class="fl">1.0</span>, <span class="fl">1.0</span></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>gx, gy <span class="op">=</span> jax.grad(f, argnums<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>))(tx, ty)</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>ax.scatter(tx, ty, color<span class="op">=</span><span class="st">'red'</span>, s<span class="op">=</span><span class="dv">100</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>ax.arrow(tx, ty, gx <span class="op">*</span> <span class="fl">0.01</span>, gy <span class="op">*</span> <span class="fl">0.01</span>, head_width<span class="op">=</span><span class="fl">0.3</span>, head_length<span class="op">=</span><span class="fl">0.3</span>, fc<span class="op">=</span><span class="st">'blue'</span>, ec<span class="op">=</span><span class="st">'blue'</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>ax.set_title(<span class="st">"Gradient ascent example"</span>)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="pg_files/figure-html/cell-4-output-1.png" width="541" height="486" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>For differentiable functions, this can be thought of as the vector of partial derivatives,</p>
<p><span class="math display">\[
\nabla y(x, z) = \begin{pmatrix}
\frac{\partial y}{\partial x} \\
\frac{\partial y}{\partial z}
\end{pmatrix}.
\]</span></p>
<p>To calculate the <em>slope</em> (aka “directional derivative”) of the mountain in a given direction <span class="math inline">\((\Delta x, \Delta z)\)</span>, you take the dot product of the difference vector with the gradient. This means that the direction with the highest slope is exactly the gradient itself, so we can describe the gradient ascent algorithm as follows:</p>
<div id="def-gradient-ascent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 (Gradient ascent)</strong></span> <span class="math display">\[
\begin{pmatrix}
x^{k+1} \\ z^{k+1}
\end{pmatrix}
=
\begin{pmatrix}
x^{k} \\ z^{k}
\end{pmatrix}
+
\eta \nabla y(x^{k}, z^{k})
\]</span></p>
</div>
<p>where <span class="math inline">\(k\)</span> denotes the iteration of the algorithm and <span class="math inline">\(\eta &gt; 0\)</span> is a “step size” hyperparameter that controls the size of the steps we take. (Note that we could also vary the step size across iterations, that is, <span class="math inline">\(\eta^0, \dots, \eta^K\)</span>.)</p>
<p>The case of a two-dimensional input is easy to visualize. But this idea can be straightforwardly extended to higher-dimensional inputs.</p>
<p>From now on, we’ll use <span class="math inline">\(J\)</span> to denote the function we’re trying to maximize, and <span class="math inline">\(\theta\)</span> to denote the parameters being optimized over. (In the above example, <span class="math inline">\(\theta = \begin{pmatrix} x &amp; z \end{pmatrix}^\top\)</span>).</p>
<p>Notice that our parameters will stop changing once <span class="math inline">\(\nabla J(\theta) = 0.\)</span> Once we reach this <strong>stationary point,</strong> our current parameters are ‘locally optimal’ in some sense; it’s impossible to increase the function by moving in any direction. If <span class="math inline">\(J\)</span> is <em>convex</em>, then the only point where this happens is at the <em>global optimum.</em> Otherwise, if <span class="math inline">\(J\)</span> is nonconvex, the best we can hope for is a <em>local optimum.</em></p>
<section id="sec-computing-derivatives" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="sec-computing-derivatives"><span class="header-section-number">6.2.1</span> Computing derivatives</h3>
<p>How does a computer compute the gradient of a function?</p>
<p>One way is <em>symbolic differentiation,</em> which is similar to the way you might compute it by hand: the computer applies a list of rules to transform the <em>symbols</em> involved. Python’s <code>sympy</code> package supports symbolic differentiation. However, functions implemented in code may not always have a straightforward symbolic representation.</p>
<p>Another way is <em>numerical differentiation,</em> which is based on the limit definition of a (directional) derivative:</p>
<p><span class="math display">\[
\nabla_{\boldsymbol{u}} J(\boldsymbol{x}) = \lim_{\varepsilon \to 0}
\frac{J(\boldsymbol{x} + \varepsilon \boldsymbol{u}) - J(\boldsymbol{x})}{\varepsilon}
\]</span></p>
<p>Then, we can substitute a small value of <span class="math inline">\(\varepsilon\)</span> on the r.h.s. to approximate the directional derivative. How small, though? If we need an accurate estimate, we may need such a small value of <span class="math inline">\(\varepsilon\)</span> that typical computers will run into rounding errors. Also, to compute the full gradient, we would need to compute the r.h.s. once for each input dimension. This is an issue if computing <span class="math inline">\(J\)</span> is expensive.</p>
<p><strong>Automatic differentiation</strong> achieves the best of both worlds. Like symbolic differentiation, we manually implement the derivative rules for a few basic operations. However, instead of executing these on the <em>symbols</em>, we execute them on the <em>values</em> when the function gets called, like in numerical differentiation. This allows us to differentiate through programming constructs such as branches or loops, and doesn’t involve any arbitrarily small values. <span class="citation" data-cites="baydin_automatic_2018">Baydin et al. (<a href="references.html#ref-baydin_automatic_2018" role="doc-biblioref">2018</a>)</span> provides an accessible survey of automatic differentiation.</p>
</section>
<section id="stochastic-gradient-ascent" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="stochastic-gradient-ascent"><span class="header-section-number">6.2.2</span> Stochastic gradient ascent</h3>
<p>In real applications, computing the gradient of the target function is not so simple. As an example from supervised learning, <span class="math inline">\(J(\theta)\)</span> might be the sum of squared prediction errors across an entire training dataset. However, if our dataset is very large, it might not fit into our computer’s memory! Typically in these cases, we compute some <em>estimate</em> of the gradient at each step, and walk in that direction instead. This is called <strong>stochastic</strong> gradient ascent. In the SL example above, we might randomly choose a <em>minibatch</em> of samples and use them to estimate the true prediction error. (This approach is known as <strong><em>minibatch</em></strong> <strong>SGD</strong>.)</p>
<div id="fad2aa01" class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> sgd(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    theta_init: Float[Array, <span class="st">" D"</span>],</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    estimate_gradient: Callable[[Float[Array, <span class="st">" D"</span>]], Float[Array, <span class="st">" D"</span>]],</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    eta: <span class="bu">float</span>,</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    n_steps: <span class="bu">int</span>,</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Perform `n_steps` steps of SGD.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="co"># `estimate_gradient` eats the current parameters and returns an estimate of the objective function's gradient at those parameters.</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> step <span class="kw">in</span> <span class="bu">range</span>(n_steps):</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">+=</span> eta <span class="op">*</span> estimate_gradient(theta)</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>latex(sgd)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="4">
\begin{array}{l} \mathbf{function} \ \mathrm{sgd}(\theta_{\mathrm{init}}: \mathbb{R}^{D}, \mathrm{estimate\_gradient}: \mathrm{Callable}_{\mathopen{}\left[ \mathbb{R}^{D} \mathclose{}\right], \mathbb{R}^{D}}, \eta: \mathrm{float}, n_{\mathrm{steps}}: \mathrm{int}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ \mathrm{step} \in \mathrm{range} \mathopen{}\left( n_{\mathrm{steps}} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \theta \gets \theta + \eta \cdot \mathrm{estimate\_gradient} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}
</div>
</div>
<p>What makes one gradient estimator better than another? Ideally, we want this estimator to be <strong>unbiased;</strong> that is, on average, it matches a single true gradient step:</p>
<p><span class="math display">\[
\mathbb{E}[\tilde \nabla J(\theta)] = \nabla J(\theta).
\]</span></p>
<p>We also want the <em>variance</em> of the estimator to be low so that its performance doesn’t change drastically at each step.</p>
<p>We can actually show that, for many “nice” functions, in a finite number of steps, SGD will find a <span class="math inline">\(\theta\)</span> that is “close” to a stationary point. In another perspective, for such functions, the local “landscape” of <span class="math inline">\(J\)</span> around <span class="math inline">\(\theta\)</span> becomes flatter and flatter the longer we run SGD.</p>
<section id="sgd-convergence" class="level4 {rem-sgd-convergence}" data-number="6.2.2.1">
<h4 data-number="6.2.2.1" class="anchored" data-anchor-id="sgd-convergence"><span class="header-section-number">6.2.2.1</span> SGD convergence</h4>
<p>More formally, suppose we run SGD for <span class="math inline">\(K\)</span> steps, using an unbiased gradient estimator. Let the step size <span class="math inline">\(\eta^k\)</span> scale as <span class="math inline">\(O(1/\sqrt{k}).\)</span> Then if <span class="math inline">\(J\)</span> is bounded and <span class="math inline">\(\beta\)</span>-smooth (see below), and the <em>norm</em> of the gradient estimator has a bounded second moment <span class="math inline">\(\sigma^2,\)</span></p>
<p><span class="math display">\[
\|\nabla J(\theta^K)\|^2 \le O \left( M \beta \sigma^2 / K\right).
\]</span></p>
<p>We call a function <span class="math inline">\(\beta\)</span>-smooth if its gradient is Lipschitz continuous with constant <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\|\nabla J(\theta) - \nabla J(\theta')\| \le \beta \|\theta - \theta'\|.
\]</span></p>
</section>
<p>We’ll now see a concrete application of gradient ascent in the context of policy optimization.</p>
</section>
</section>
<section id="policy-stochastic-gradient-ascent" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="policy-stochastic-gradient-ascent"><span class="header-section-number">6.3</span> Policy (stochastic) gradient ascent</h2>
<p>Remember that in RL, the primary goal is to find the <em>optimal policy</em> that achieves the maximimum total reward, which we can express using <a href="mdps.html#def-value" class="quarto-xref">Definition&nbsp;<span>1.5</span></a>:</p>
<p><span id="eq-objective-fn"><span class="math display">\[
\begin{aligned}
    J(\pi) := \mathbb{E}_{s_0 \sim \mu_0} V^{\pi} (s_0) = &amp; \mathbb{E}_{\tau \sim \rho^\pi} \sum_{h=0}^{H-1} r(s_h, a_h)
\end{aligned}
\tag{6.1}\]</span></span></p>
<p>where <span class="math inline">\(\rho^\pi\)</span> is the distribution over trajectories induced by <span class="math inline">\(\pi\)</span> (see <a href="mdps.html#eq-autoregressive-trajectories" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>).</p>
<p>(Note that we’ll continue to work in the <em>undiscounted, finite-horizon case.</em> Analogous results hold for the <em>discounted, infinite-horizon setup.</em>)</p>
<p>As shown by the notation, this is exactly the function <span class="math inline">\(J\)</span> that we want to maximize using gradient ascent. What variables are we optimizing over in this problem? Well, the objective function <span class="math inline">\(J\)</span> is a function of the policy <span class="math inline">\(\pi\)</span>, but in general, <span class="math inline">\(\pi\)</span> is a function, and optimizing over the entire space of arbitrary input-output mappings would be intractable. Instead, we need to describe <span class="math inline">\(\pi\)</span> in terms of some finite set of <em>parameters</em> <span class="math inline">\(\theta\)</span>.</p>
<section id="sec-parameterizations" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="sec-parameterizations"><span class="header-section-number">6.3.1</span> Example policy parameterizations</h3>
<p>What are some ways we could parameterize our policy?</p>
<div id="exm-tabular-repr" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 (Tabular representation)</strong></span> If both the state and action spaces are finite, perhaps we could simply learn a preference value <span class="math inline">\(\theta_{s,a}\)</span> for each state-action pair. Then to turn this into a valid distribution, we perform a <strong>softmax</strong> operation: we exponentiate each of them, and then normalize to form a valid distribution.</p>
<p><span class="math display">\[
\pi^\text{softmax}_\theta(a | s) = \frac{\exp(\theta_{s,a})}{\sum_{s,a'} \exp (\theta_{s,a'})}.
\]</span></p>
<p>However, this doesn’t make use of any structure in the states or actions, so while this is flexible, it is also prone to overfitting.</p>
</div>
<div id="exm-linear-in-features" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 (Linear in features)</strong></span> Another approach is to map each state-action pair into some <strong>feature space</strong> <span class="math inline">\(\phi(s, a) \in \mathbb{R}^p\)</span>. Then, to map a feature vector to a probability, we take a linear combination of the features and take a softmax:</p>
<p><span class="math display">\[
\pi^\text{linear in features}_{\theta}(a|s) = \frac{\exp(\theta^\top \phi(s, a))}{\sum_{a'} \exp(\theta^\top \phi(s, a'))}.
\]</span></p>
<p>Another interpretation is that <span class="math inline">\(\theta\)</span> represents the feature vector of the “desired” state-action pair, as state-action pairs whose features align closely with <span class="math inline">\(\theta\)</span> are given higher probability.</p>
</div>
<div id="exm-neural-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 (Neural policies)</strong></span> More generally, we could map states and actions to unnormalized scores via some parameterized function <span class="math inline">\(f_\theta : \mathcal{S} \times \mathcal{A} \to \mathbb{R},\)</span> such as a neural network, and choose actions according to a softmax:</p>
<p><span class="math display">\[
\pi^\text{general}_\theta(a|s) = \frac{\exp(f_{\theta}(s,a))}{\sum_{a'} \exp(f_{\theta}(s,a'))}.
\]</span></p>
</div>
<div id="exm-gaussian-policy" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 (Diagonal Gaussian policies for continuous action spaces)</strong></span> Consider a continuous <span class="math inline">\(n\)</span>-dimensional action space <span class="math inline">\(\mathcal{A} = \mathbb{R}^n\)</span>. Then for a stochastic policy, we could use a function to predict the <em>mean</em> action and then add some random noise about it. For example, we could use a neural network to predict the mean action <span class="math inline">\(\mu_\theta(s)\)</span> and then add some noise <span class="math inline">\(\epsilon \sim \mathcal{N}(0, \sigma^2 I)\)</span> to it:</p>
<p><span class="math display">\[
\pi_\theta(a|s) = \mathcal{N}(\mu_\theta(s), \sigma^2 I).
\]</span></p>
</div>
<!-- **Exercise:** Can you extend the "linear in features" policy to continuous action spaces in a similar way? -->
<p>Now that we have seen some examples of parameterized policies, we will write the total reward in terms of the parameters, overloading notation and letting <span class="math inline">\(\rho_\theta := \rho^{\pi_\theta}\)</span>:</p>
<p><span class="math display">\[
J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} R(\tau)
\]</span></p>
<p>where <span class="math inline">\(R(\tau) = \sum_{h=0}^{H-1} r(s_h, a_h)\)</span> denotes the total reward in the trajectory.</p>
<p>Now how do we maximize this function (the expected total reward) over the parameters? One simple idea would be to directly apply gradient ascent:</p>
<p><span class="math display">\[
\theta^{k+1} = \theta^k + \eta \nabla J(\theta^k).
\]</span></p>
<p>In order to apply this technique, we need to be able to evaluate the gradient <span class="math inline">\(\nabla J(\theta).\)</span> But <span class="math inline">\(J(\theta)\)</span> is very difficult, or even intractable, to compute exactly, since it involves taking an expectation over all possible trajectories <span class="math inline">\(\tau.\)</span> Can we rewrite it in a form that’s more convenient to implement?</p>
</section>
<section id="sec-importance-sampling" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="sec-importance-sampling"><span class="header-section-number">6.3.2</span> Importance Sampling</h3>
<p>There is a general trick called <strong>importance sampling</strong> for evaluating difficult expectations. Suppose we want to estimate <span class="math inline">\(\mathbb{E}_{x \sim p}[f(x)]\)</span> where <span class="math inline">\(p\)</span> is hard or expensive to sample from, but easy to evaluate the likelihood <span class="math inline">\(p(x)\)</span> of. Suppose that we <em>can</em> easily sample from a different distribution <span class="math inline">\(q\)</span>. Since an expectation is just a weighted average, we can sample <span class="math inline">\(x\)</span> from <span class="math inline">\(q\)</span>, compute <span class="math inline">\(f(x)\)</span>, and then reweight the results: if <span class="math inline">\(x\)</span> is very likely under <span class="math inline">\(p\)</span> but unlikely under <span class="math inline">\(q\)</span>, we should boost its weighting, and if it is common under <span class="math inline">\(q\)</span> but uncommon under <span class="math inline">\(p\)</span>, we should lower its weighting. The reweighting factor is exactly the <strong>likelihood ratio</strong> between the target distribution <span class="math inline">\(p\)</span> and the sampling distribution <span class="math inline">\(q\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_{x \sim p}[f(x)] = \sum_{x \in \mathcal{X}} f(x) p(x) = \sum_{x \in \mathcal{X}} f(x) \frac{p(x)}{q(x)} q(x) = \mathbb{E}_{x \sim q} \left[ \frac{p(x)}{q(x)} f(x) \right].
\]</span></p>
<p>Doesn’t this seem too good to be true? If there were no drawbacks, we could use this to estimate <em>any</em> expectation of any function on any arbitrary distribution! The drawback is that the variance may be very large due to the likelihood ratio term. If there are values of <span class="math inline">\(x\)</span> that are very rare in the sampling distribution <span class="math inline">\(q\)</span>, but common under <span class="math inline">\(p\)</span>, then the likelihood ratio <span class="math inline">\(p(x)/q(x)\)</span> will cause the variance to blow up.</p>
</section>
</section>
<section id="the-reinforce-policy-gradient" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="the-reinforce-policy-gradient"><span class="header-section-number">6.4</span> The REINFORCE policy gradient</h2>
<p>Returning to RL, suppose there is some trajectory distribution <span class="math inline">\(\rho(\tau)\)</span> that is <strong>easy to sample from,</strong> such as a database of existing trajectories. We can then rewrite <span class="math inline">\(\nabla J(\theta)\)</span>, a.k.a. the <em>policy gradient</em>, as follows. All gradients are being taken with respect to <span class="math inline">\(\theta\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla J(\theta) &amp; = \nabla \mathbb{E}_{\tau \sim \rho_\theta} [ R(\tau) ]                                                                                         \\
                     &amp; = \nabla \mathbb{E}_{\tau \sim \rho} \left[ \frac{\rho_\theta(\tau)}{\rho(\tau)} R(\tau) \right] &amp;  &amp; \text{likelihood ratio trick}             \\
                     &amp; = \mathbb{E}_{\tau \sim \rho} \left[ \frac{\nabla \rho_\theta(\tau)}{\rho(\tau)} R(\tau) \right] &amp;  &amp; \text{switching gradient and expectation}
\end{aligned}
\]</span></p>
<p>Note that for <span class="math inline">\(\rho = \rho_\theta\)</span>, the inside term becomes</p>
<p><span class="math display">\[
\nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} [ \nabla \log \rho_\theta(\tau) \cdot R(\tau)].
\]</span></p>
<p>(The order of operations is <span class="math inline">\(\nabla (\log \rho_\theta)(\tau)\)</span>.)</p>
<p>Recall that when the state transitions are Markov (i.e.&nbsp;<span class="math inline">\(s_{t}\)</span> only depends on <span class="math inline">\(s_{t-1}, a_{t-1}\)</span>) and the policy is time-homogeneous (i.e.&nbsp;<span class="math inline">\(a_h\sim \pi_\theta (s_h)\)</span>), we can write out the <em>likelihood of a trajectory</em> under the policy <span class="math inline">\(\pi_\theta\)</span> autoregressively, as in <a href="mdps.html#eq-autoregressive-trajectories" class="quarto-xref">Equation&nbsp;<span>1.1</span></a>. Taking the log of the trajectory likelihood turns it into a sum of terms:</p>
<p><span class="math display">\[
\log \rho_\theta(\tau) = \log \mu(s_0) + \sum_{h=0}^{H-1} \log \pi_\theta(a_h\mid s_h) + \log P(s_{h+1} \mid s_h, a_h)
\]</span></p>
<p>When we take the gradient with respect to the parameters <span class="math inline">\(\theta\)</span>, only the <span class="math inline">\(\pi_\theta(a_h| s_h)\)</span> terms depend on <span class="math inline">\(\theta\)</span>. This gives the following expression for the policy gradient, known as the “REINFORCE” policy gradient <span class="citation" data-cites="williams_simple_1992">Williams (<a href="references.html#ref-williams_simple_1992" role="doc-biblioref">1992</a>)</span>:</p>
<p><span id="eq-reinforce-pg"><span class="math display">\[
\begin{aligned}
    \nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} \left[ \sum_{h=0}^{H-1} \nabla_\theta \log \pi_{\theta}(a_h| s_h) R(\tau) \right]
\end{aligned}
\tag{6.2}\]</span></span></p>
<p>This expression allows us to estimate the gradient by sampling a few sample trajectories from <span class="math inline">\(\pi_\theta,\)</span> calculating the likelihoods of the chosen actions, and substituting these into the expression inside the brackets of <a href="#eq-reinforce-pg" class="quarto-xref">Equation&nbsp;<span>6.2</span></a>. Then we can update the parameters <span class="math inline">\(\theta\)</span> in this direction to perform stochastic gradient ascent.</p>
<p>The rest of this chapter investigates ways to <em>reduce the variance</em> of this estimator by subtracting off certain correlated quantities.</p>
<div id="lem-intuitive-remark" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 6.1 (Intuition behind REINFORCE)</strong></span> Intuitively speaking, we want to update the policy parameters to maximize the probability of taking <em>optimal actions</em>. That is, suppose we are in state <span class="math inline">\(s\)</span>, and <span class="math inline">\(a^\star\)</span> is an optimal action to take. Then we want to solve <span class="math inline">\(\theta = \arg\max_{\theta'} \pi_{\theta'}(a^\star \mid s)\)</span>, which would lead to the gradient ascent expression</p>
<p><span class="math display">\[
\theta \gets \theta + \nabla \pi_{\theta}(a^\star \mid s).
\]</span></p>
<p>However, we don’t know the optimal action <span class="math inline">\(a^\star\)</span> in practice. So instead, we must try many actions, and <em>increase</em> the probability of the “good” ones and <em>decrease</em> the probability of the “bad” ones. Suppose <span class="math inline">\(A(s, a)\)</span> is a measure of how good action <span class="math inline">\(a\)</span> is in state <span class="math inline">\(s\)</span>. Then we could write</p>
<p><span class="math display">\[
\theta \gets \theta + \sum_a \pi_{\theta}(a \mid s) A(s, a) \nabla \pi_{\theta}(a \mid s).
\]</span></p>
<p>But this has an issue: the size of each step doesn’t just depend on how good it is, but also how <em>often</em> the policy takes it already. This could lead to a positive feedback loop where likely actions become more and more likely, without respect to the quality of the action. So we divide by the likelihood to cancel out this factor:</p>
<p><span class="math display">\[
\theta \gets \theta + \sum_a \pi_{\theta}(a \mid s) A(s, a) \frac{\nabla \pi_{\theta}(a \mid s)}{\pi_{\theta}(a \mid s)}.
\]</span></p>
<p>But once we simplify, and sum across timesteps, this becomes <em>almost</em> exactly the gradient written above!</p>
<p><span class="math display">\[
\theta \gets \theta + \mathbb{E}_{a \sim \pi_{\theta}(\cdot \mid s)} [\sum_{h=0}^{H-1} A(s_h, a_h) \nabla \log \pi_{\theta}(a_h\mid s_h) ].
\]</span></p>
<p>We will see later on what <span class="math inline">\(A\)</span> concretely corresponds to.</p>
</div>
<div id="1ad9a848" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimate_gradient_reinforce_pseudocode(env: gym.Env, pi, theta: Float[Array, <span class="st">" D"</span>]):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Estimate the policy gradient using REINFORCE."""</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    tau <span class="op">=</span> sample_trajectory(env, pi(theta))</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    nabla_hat <span class="op">=</span> jnp.zeros_like(theta)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    total_reward <span class="op">=</span> <span class="bu">sum</span>(r <span class="cf">for</span> _s, _a, r <span class="kw">in</span> tau)</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> s, a, r <span class="kw">in</span> tau:</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> policy_log_likelihood(theta: Float[Array, <span class="st">" D"</span>]) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> log(pi(theta)(s, a))</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>        nabla_hat <span class="op">+=</span> jax.grad(policy_log_likelihood)(theta) <span class="op">*</span> total_reward</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> nabla_hat</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>latex(estimate_gradient_reinforce_pseudocode, id_to_latex<span class="op">=</span>{<span class="st">"jax.grad"</span>: <span class="vs">r"\nabla"</span>})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="5">
\begin{array}{l} \mathbf{function} \ \mathrm{estimate\_gradient\_reinforce\_pseudocode}(\mathrm{env}: \mathrm{gym}.\mathrm{Env}, \pi, \theta: \mathbb{R}^{D}) \\ \hspace{1em} \textrm{"Estimate the policy gradient using REINFORCE."} \\ \hspace{1em} \tau \gets \mathrm{sample\_trajectory} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right) \mathclose{}\right) \\ \hspace{1em} \widehat{\nabla} \gets \mathrm{jnp}.\mathrm{zeros\_like} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{1em} \mathrm{total\_reward} \gets \sum_{\mathopen{}\left( \mathrm{\_s}, \mathrm{\_a}, r \mathclose{}\right) \in \tau}^{} \mathopen{}\left({r}\mathclose{}\right) \\ \hspace{1em} \mathbf{for} \ \mathopen{}\left( s, a, r \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{2em} \mathbf{function} \ \mathrm{policy\_log\_likelihood}(\theta: \mathbb{R}^{D}) \\ \hspace{3em} \mathbf{return} \ \log \pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \widehat{\nabla} \gets \widehat{\nabla} + \nabla \mathopen{}\left(\mathrm{policy\_log\_likelihood}\mathclose{}\right) \mathopen{}\left( \theta \mathclose{}\right) \cdot \mathrm{total\_reward} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \widehat{\nabla} \\ \mathbf{end \ function} \end{array}
</div>
</div>
<p>For some intuition into how this method works, recall that we update our parameters according to</p>
<p><span class="math display">\[
\begin{aligned}
    \theta_{t+1} &amp;= \theta_t + \eta \nabla J(\theta_t) \\
    &amp;= \theta_t + \eta \mathbb{E}_{\tau \sim \rho_{\theta_t}} [\nabla \log \rho_{\theta_t}(\tau) \cdot R(\tau)].
\end{aligned}
\]</span></p>
<p>Consider the “good” trajectories where <span class="math inline">\(R(\tau)\)</span> is large. Then <span class="math inline">\(\theta\)</span> gets updated so that these trajectories become more likely. To see why, recall that <span class="math inline">\(\rho_{\theta}(\tau)\)</span> is the likelihood of the trajectory <span class="math inline">\(\tau\)</span> under the policy <span class="math inline">\(\pi_\theta,\)</span> so the gradient points in the direction that makes <span class="math inline">\(\tau\)</span> more likely.</p>
</section>
<section id="baselines-and-advantages" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="baselines-and-advantages"><span class="header-section-number">6.5</span> Baselines and advantages</h2>
<p>A central idea from supervised learning is the <strong>bias-variance decomposition</strong>, which shows that the mean squared error of an estimator is the sum of its squared bias and its variance. The REINFORCE gradient estimator <a href="#eq-reinforce-pg" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> is already <em>unbiased,</em> meaning that its expectation over trajectories is the true policy gradient. Can we find ways to reduce its <em>variance</em> as well?</p>
<p>As a first step, consider that the action taken at step <span class="math inline">\(t\)</span> does not affect the reward from previous timesteps, since they’re already in the past. You can also show rigorously that this is the case, and that we only need to consider the present and future rewards to calculate the policy gradient:</p>
<p><span class="math display">\[
\nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} \left[ \sum_{h=0}^{H-1} \nabla_\theta \log \pi_{\theta}(a_h| s_h) \sum_{h' = h}^{H-1} r(s_{h'}, a_{h'}) \right]
\]</span></p>
<p>Furthermore, by a conditioning argument, we can replace the inner sum over remaining rewards with the policy’s Q-function, evaluated at the current state:</p>
<p><span id="eq-pg-with-q"><span class="math display">\[
\nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} \left[ \sum_{h=0}^{H-1} \nabla_\theta \log \pi_{\theta}(a_h| s_h) Q^{\pi_\theta}(s_{h}, a_{h}) \right]
\tag{6.3}\]</span></span></p>
<p><strong>Exercise:</strong> Prove that this is equivalent to the previous definitions. What modification to the expression must be made for the discounted, infinite-horizon setting?</p>
<p>We can further reduce variance by subtracting a <strong>baseline function</strong> <span class="math inline">\(b_h: \mathcal{S} \to \mathbb{R}\)</span> at each timestep <span class="math inline">\(h\)</span>. This modifies the policy gradient as follows:</p>
<p><span id="eq-pg-baseline"><span class="math display">\[
\nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} \left[
    \sum_{h=0}^{H-1} \nabla \log \pi_\theta (a_h| s_h) \left(
    Q^{\pi_\theta}(s_h, a_h)
    - b_h(s_h)
    \right)
    \right].
\tag{6.4}\]</span></span></p>
<p>(Again, you should try to prove that this equality still holds.) For example, we might want <span class="math inline">\(b_h\)</span> to estimate the average reward-to-go at a given timestep:</p>
<p><span class="math display">\[
b_h^\theta = \mathbb{E}_{\tau \sim \rho_\theta} R_h(\tau).
\]</span></p>
<p>As a better baseline, we could instead choose the <em>value function.</em> Note that the random variable <span class="math inline">\(Q^\pi_h(s, a) - V^\pi_h(s),\)</span> where the randomness is taken over the actions, is centered around zero. (Recall <span class="math inline">\(V^\pi_h(s) = \mathbb{E}_{a \sim \pi} Q^\pi_h(s, a).\)</span>) This quantity matches the intuition given in <a href="#lem-intuitive-remark" class="quarto-xref">Lemma&nbsp;<span>6.1</span></a>: it is <em>positive</em> for actions that are better than average (in state <span class="math inline">\(s\)</span>), and <em>negative</em> for actions that are worse than average. In fact, this quantity has a particular name: the <strong>advantage function.</strong></p>
<div id="def-advantage" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 (Advantage function)</strong></span> <span class="math display">\[
A^\pi_h(s, a) = Q^\pi_h(s, a) - V^\pi_h(s)
\]</span></p>
</div>
<p>This measures how much better this action does than the average for that policy. (Note that for an optimal policy <span class="math inline">\(\pi^\star,\)</span> the advantage of a given state-action pair is always zero or negative.)</p>
<p>We can now express the policy gradient as follows. Note that the advantage function effectively replaces the <span class="math inline">\(Q\)</span>-function from <a href="#eq-pg-with-q" class="quarto-xref">Equation&nbsp;<span>6.3</span></a>:</p>
<p><span id="eq-pg-advantage"><span class="math display">\[
\nabla J(\theta) = \mathbb{E}_{\tau \sim \rho_\theta} \left[
        \sum_{h=0}^{H-1} \nabla \log \pi_\theta(a_h| s_h) A^{\pi_\theta}_h(s_h, a_h)
\right].
\tag{6.5}\]</span></span></p>
<div id="exm-linear-policy-grad" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 (Policy gradient for the linear-in-features parameterization)</strong></span> The gradient-log-likelihood for the linear-in-features parameterization <a href="#exm-linear-in-features" class="quarto-xref">Example&nbsp;<span>6.2</span></a> is also quite elegant:</p>
<p><span class="math display">\[
\begin{aligned}
        \nabla \log \pi_\theta(a|s) &amp;= \nabla \left( \theta^\top \phi(s, a) - \log \left( \sum_{a'} \exp(\theta^\top \phi(s, a')) \right) \right) \\
        &amp;= \phi(s, a) - \mathbb{E}_{a' \sim \pi_\theta(s)} \phi(s, a')
\end{aligned}
\]</span></p>
<p>Plugging this into our policy gradient expression, we get</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla J(\theta) &amp; = \mathbb{E}_{\tau \sim \rho_\theta} \left[
    \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_h| s_h) A_h^{\pi_\theta}
    \right]                                                                                                                    \\
                     &amp; = \mathbb{E}_{\tau \sim \rho_\theta} \left[
    \sum_{t=0}^{T-1} \left( \phi(s_h, a_h) - \mathbb{E}_{a' \sim \pi(s_h)} \phi(s_h, a') \right) A_h^{\pi_\theta}(s_h, a_h)
    \right]                                                                                                                    \\
                     &amp; = \mathbb{E}_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \phi(s_h, a_h) A_h^{\pi_\theta} (s_h, a_h) \right]
\end{aligned}
\]</span></p>
<p>Why can we drop the <span class="math inline">\(\mathbb{E}\phi(s_h, a')\)</span> term? By linearity of expectation, consider the dropped term at a single timestep: <span class="math inline">\(\mathbb{E}_{\tau \sim \rho_\theta} \left[ \left( \mathbb{E}_{a' \sim \pi(s_h)} \phi(s, a') \right) A_h^{\pi_\theta}(s_h, a_h) \right].\)</span> By Adam’s Law, we can wrap the advantage term in a conditional expectation on the state <span class="math inline">\(s_h.\)</span> Then we already know that <span class="math inline">\(\mathbb{E}_{a \sim \pi(s)} A_h^{\pi}(s, a) = 0,\)</span> and so this entire term vanishes.</p>
</div>
<p>Note that to avoid correlations between the gradient estimator and the value estimator (i.e.&nbsp;baseline), we must estimate them with independently sampled trajectories:</p>
<!-- TODO could use more explanation _why_ we want to avoid correlations -->
<div id="a23ea251" class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pg_with_learned_baseline(env: gym.Env, pi, eta: <span class="bu">float</span>, theta_init, K: <span class="bu">int</span>, N: <span class="bu">int</span>) <span class="op">-&gt;</span> Float[Array, <span class="st">" D"</span>]:</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), N)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        V_hat <span class="op">=</span> fit_value(trajectories)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        tau <span class="op">=</span> sample_trajectories(env, pi(theta), <span class="dv">1</span>)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        nabla_hat <span class="op">=</span> jnp.zeros_like(theta)  <span class="co"># gradient estimator</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> h, (s, a) <span class="kw">in</span> <span class="bu">enumerate</span>(tau):</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            <span class="kw">def</span> log_likelihood(theta_opt):</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>                <span class="cf">return</span> jnp.log(pi(theta_opt)(s, a))</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            nabla_hat <span class="op">=</span> nabla_hat <span class="op">+</span> jax.grad(log_likelihood)(theta) <span class="op">*</span> (return_to_go(tau, h) <span class="op">-</span> V_hat(s))</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> theta <span class="op">+</span> eta <span class="op">*</span> nabla_hat</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>latex(pg_with_learned_baseline)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="6">
\begin{array}{l} \mathbf{function} \ \mathrm{pg\_with\_learned\_baseline}(\mathrm{env}: \mathrm{gym}.\mathrm{Env}, \pi, \eta: \mathrm{float}, \theta_{\mathrm{init}}, K: \mathrm{int}, N: \mathrm{int}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( K \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), N \mathclose{}\right) \\ \hspace{2em} \widehat{V} \gets \mathrm{fit\_value} \mathopen{}\left( \mathrm{trajectories} \mathclose{}\right) \\ \hspace{2em} \tau \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), 1 \mathclose{}\right) \\ \hspace{2em} \widehat{\nabla} \gets \mathrm{jnp}.\mathrm{zeros\_like} \mathopen{}\left( \theta \mathclose{}\right) \\ \hspace{2em} \mathbf{for} \ \mathopen{}\left( h, \mathopen{}\left( s, a \mathclose{}\right) \mathclose{}\right) \in \mathrm{enumerate} \mathopen{}\left( \tau \mathclose{}\right) \ \mathbf{do} \\ \hspace{3em} \mathbf{function} \ \mathrm{log\_likelihood}(\theta_{\mathrm{opt}}) \\ \hspace{4em} \mathbf{return} \ \log \pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{3em} \mathbf{end \ function} \\ \hspace{3em} \widehat{\nabla} \gets \widehat{\nabla} + \nabla \mathopen{}\left(\mathrm{log\_likelihood}\mathclose{}\right) \mathopen{}\left( \theta \mathclose{}\right) \cdot \mathopen{}\left( \mathrm{return\_to\_go} \mathopen{}\left( \tau, h \mathclose{}\right) - \widehat{V} \mathopen{}\left( s \mathclose{}\right) \mathclose{}\right) \\ \hspace{2em} \mathbf{end \ for} \\ \hspace{2em} \theta \gets \theta + \eta \widehat{\nabla} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}
</div>
</div>
<p>Note that you could also generalize this by allowing the learning rate <span class="math inline">\(\eta\)</span> to vary across steps, or take multiple trajectories <span class="math inline">\(\tau\)</span> and compute the sample average of the gradient estimates.</p>
<p>The baseline estimation step <code>fit_value</code> can be done using any appropriate supervised learning algorithm. Note that the gradient estimator will be unbiased regardless of the baseline.</p>
</section>
<section id="comparing-policy-gradient-algorithms-to-policy-iteration" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="comparing-policy-gradient-algorithms-to-policy-iteration"><span class="header-section-number">6.6</span> Comparing policy gradient algorithms to policy iteration</h2>
<!-- TODO maybe restructure this part -->
<p>What advantages does the policy gradient algorithm have over the policy iteration algorithms covered in <a href="mdps.html#sec-policy-iteration" class="quarto-xref"><span>Section 1.3.7.2</span></a>?</p>
<div id="rem-policy-iteration" class="proof remark">
<p><span class="proof-title"><em>Remark 6.1</em> (Policy iteration review). </span>Recall that policy iteration is an algorithm for MDPs with unknown state transitions where we alternate between these two steps:</p>
<ul>
<li>Estimating the <span class="math inline">\(Q\)</span>-function (or advantage function) of the current policy;</li>
<li>Updating the policy to be greedy with respect to this approximate <span class="math inline">\(Q\)</span>-function (or advantage function).</li>
</ul>
</div>
<p>To analyze the difference between them, we’ll make use of the <strong>performance difference lemma</strong>, which provides an expression for comparing the difference between two value functions.</p>
<div id="thm-pdl" class="theorem">
<p><span class="theorem-title"><strong>Theorem 6.1 (Performance difference lemma)</strong></span> Suppose Alice is playing a game (an MDP). Bob is spectating, and can evaluate how good an action is compared to his own strategy. (That is, Bob can compute his <em>advantage function</em> <span class="math inline">\(A_h^{\text{Bob}}(s_h, a_h)\)</span>). The performance difference lemma says that Bob can now calculate exactly how much better or worse he is than Alice as follows:</p>
<p><span id="eq-pdl-eq"><span class="math display">\[
V_0^{\text{Alice}}(s) - V_0^{\text{Bob}}(s) = \mathbb{E}_{\tau \sim \rho_{\text{Alice}, s}} \left[ \sum_{h=0}^{H-1} A_h^{\text{Bob}} (s_h, a_h) \right]
\tag{6.6}\]</span></span></p>
<p>where <span class="math inline">\(\rho_{\text{Alice}, s}\)</span> denotes the distribution over trajectories starting in state <span class="math inline">\(s\)</span> when Alice is playing.</p>
<p>To see why, consider a specific step <span class="math inline">\(h\)</span> in the trajectory. We compute how much better actions from Bob are than the actions from Alice, on average. But this is exactly the average Bob-advantage across actions from Alice, as described in the PDL!</p>
<p>Formally, this corresponds to a nice telescoping simplification when we expand out the definition of the advantage function. Note that</p>
<p><span class="math display">\[
\begin{aligned}
A^\pi_h(s_h, a_h) &amp;= Q^\pi_h(s_h, a_h) - V^\pi_h(s_h) \\
&amp;= r_h(s_h, a_h) + \mathbb{E}_{s_{h+1} \sim P(s_h, a_h)} [V^\pi_{h+1}(s_{h+1})] - V^\pi_h(s_h)
\end{aligned}
\]</span></p>
<p>so expanding out the r.h.s. expression of <a href="#eq-pdl-eq" class="quarto-xref">Equation&nbsp;<span>6.6</span></a> and grouping terms together gives</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}_{\tau \sim \rho_{\text{Alice}, s}} \left[ \sum_{h=0}^{H-1} A_h^{\text{Bob}} (s_h, a_h) \right] &amp;= \mathbb{E}_{\tau \sim \rho_{\text{Alice}, s}} \left[ \left( \sum_{h=0}^{H-1} r_h(s_h, a_h) \right) + \left( V^{\text{Bob}}_1(s_1) + \cdots + V^{\text{Bob}}_H(s_H) \right) - \left( V^{\text{Bob}_0}(s_0) + \cdots + V^{\text{Bob}}_{H-1}(s_{H-1}) \right) \right] \\
&amp;= V^{\text{Alice}}_0(s) - V^{\text{Bob}}_0(s)
\end{aligned}
\]</span></p>
<p>as desired. (Note that the “inner” expectation from expanding the advantage function has the same distribution as the outer one, so omitting it here is valid.)</p>
</div>
<p>The PDL gives insight into why fitted approaches such as PI don’t work as well in the “full” RL setting. To see why, let’s consider a single iteration of policy iteration, where policy <span class="math inline">\(\pi\)</span> gets updated to <span class="math inline">\(\tilde \pi\)</span>. We’ll assume these policies are deterministic. Suppose the new policy <span class="math inline">\(\tilde \pi\)</span> chooses some action with a negative advantage with respect to <span class="math inline">\(\pi\)</span>. That is, when acting according to <span class="math inline">\(\pi\)</span>, taking the action from <span class="math inline">\(\tilde \pi\)</span> would perform worse than expected. Define <span class="math inline">\(\Delta_\infty\)</span> to be the most negative advantage, that is, <span class="math inline">\(\Delta_\infty = \min_{s \in \mathcal{S}} A^{\pi}_h(s, \tilde \pi(s))\)</span>. Plugging this into the <a href="#thm-pdl" class="quarto-xref">Theorem&nbsp;<span>6.1</span></a> gives</p>
<p><span class="math display">\[
\begin{aligned}
V_0^{\tilde \pi}(s) - V_0^{\pi}(s) &amp;= \mathbb{E}_{\tau \sim \rho_{\tilde \pi, s}} \left[
\sum_{h=0}^{H-1} A_h^{\pi}(s_h, a_h)
\right] \\
&amp;\ge H \Delta_\infty \\
V_0^{\tilde \pi}(s) &amp;\ge V_0^{\pi}(s) - H|\Delta_\infty|.
\end{aligned}
\]</span></p>
<p>That is, for some state <span class="math inline">\(s\)</span>, the lower bound on the performance of <span class="math inline">\(\tilde \pi\)</span> is <em>lower</em> than the performance of <span class="math inline">\(\pi\)</span>. This doesn’t state that <span class="math inline">\(\tilde \pi\)</span> <em>will</em> necessarily perform worse than <span class="math inline">\(\pi\)</span>, only suggests that it might be possible. If these worst case states do exist, though, PI does not avoid situations where the new policy often visits them; It does not enforce that the trajectory distributions <span class="math inline">\(\rho_\pi\)</span> and <span class="math inline">\(\rho_{\tilde \pi}\)</span> be close to each other. In other words, the “training distribution” that our prediction rule is fitted on, <span class="math inline">\(\rho_\pi\)</span>, may differ significantly from the “evaluation distribution” <span class="math inline">\(\rho_{\tilde \pi}\)</span>.</p>
<!-- 
This is an instance of *distributional shift*.
To begin, let's ask, where *do* fitted approaches work well?
They are commonly seen in SL,
where a prediction rule is fit using some labelled training set,
and then assessed on a test set from the same distribution.
But policy iteration isn't performed in the same scenario:
there is now _distributional shift_ between the different iterations of the policy. -->
<p>On the other hand, policy gradient methods <em>do</em>, albeit implicitly, encourage <span class="math inline">\(\rho_\pi\)</span> and <span class="math inline">\(\rho_{\tilde \pi}\)</span> to be similar. Suppose that the mapping from policy parameters to trajectory distributions is relatively smooth. Then, by adjusting the parameters only a small distance, the new policy will also have a similar trajectory distribution. But this is not very rigorous, and in practice the parameter-to-distribution mapping may not be so smooth. Can we constrain the distance between the resulting distributions more <em>explicitly</em>?</p>
<p>This brings us to the next three methods: - <strong>trust region policy optimization</strong> (TRPO), which explicitly constrains the difference between the distributions before and after each step; - the <strong>natural policy gradient</strong> (NPG), a first-order approximation of TRPO; - <strong>proximal policy optimization</strong> (PPO), a “soft relaxation” of TRPO.</p>
</section>
<section id="sec-trpo" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="sec-trpo"><span class="header-section-number">6.7</span> Trust region policy optimization</h2>
<p>We saw above that policy gradient methods are effective because they implicitly constrain how much the policy changes at each iteration. Can we design an algorithm that <em>explicitly</em> constrains the “step size”? That is, we want to <em>improve</em> the policy as much as possible, measured in terms of the r.h.s. of the <a href="#thm-pdl" class="quarto-xref">Theorem&nbsp;<span>6.1</span></a>, while ensuring that its trajectory distribution does not change too much:</p>
<p><span class="math display">\[
\begin{aligned}
\theta^{k+1} &amp;\gets \arg\max_{\theta^{\text{opt}}} \mathbb{E}_{s_0, \dots, s_{H-1} \sim \pi^{k}} \left[ \sum_{h=0}^{H-1} \mathbb{E}_{a_h\sim \pi^{\theta^\text{opt}}(s_h)} A^{\pi^{k}}(s_h, a_h) \right] \\
&amp; \text{where } \text{distance}(\rho_{\theta^{\text{opt}}}, \rho_{\theta^k}) &lt; \delta
\end{aligned}
\]</span></p>
<p>Note that we have made a small change to the r.h.s. expression: we use the <em>states</em> sampled from the old policy, and only use the <em>actions</em> from the new policy. It would be computationally infeasible to sample entire trajectories from <span class="math inline">\(\pi_\theta\)</span> as we are optimizing over <span class="math inline">\(\theta\)</span>. On the other hand, if <span class="math inline">\(\pi_\theta\)</span> returns a vector representing a probability distribution over actions, then evaluating the expected advantage with respect to this distribution only requires taking a dot product. This approximation also matches the r.h.s. of the PDL to first order in <span class="math inline">\(\theta\)</span>. (We will elaborate more on this later.)</p>
<p>How do we describe the distance between <span class="math inline">\(\rho_{\theta^{\text{opt}}}\)</span> and <span class="math inline">\(\rho_{\theta^k}\)</span>? We’ll use the <strong>Kullback-Leibler divergence (KLD)</strong>:</p>
<div id="def-kld" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 (Kullback-Leibler divergence)</strong></span> For two PDFs <span class="math inline">\(p, q\)</span>,</p>
<p><span class="math display">\[
\mathrm{KL}\left(p\parallel q\right) := \mathbb{E}_{x \sim p} \left[ \log \frac{p(x)}{q(x)} \right]
\]</span></p>
<p>This can be interpreted in many different ways, many stemming from information theory. One such interpretation is that <span class="math inline">\(\mathrm{KL}\left(p\parallel q\right)\)</span> describes my average “surprise” if I <em>think</em> data is being generated by <span class="math inline">\(q\)</span> but it’s actually generated by <span class="math inline">\(p\)</span>. (The <strong>surprise</strong> of an event with probability <span class="math inline">\(p\)</span> is <span class="math inline">\(- \log_2 p\)</span>.) Note that <span class="math inline">\(\mathrm{KL}\left(p\parallel q\right) = 0\)</span> if and only if <span class="math inline">\(p = q\)</span>. Also note that it is generally <em>not</em> symmetric.</p>
</div>
<p>Both the objective function and the KLD constraint involve a weighted average over the space of all trajectories. This is intractable in general, so we need to estimate the expectation. As before, we can do this by taking an empirical average over samples from the trajectory distribution. This gives us the following pseudocode:</p>
<div id="39274636" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> kl_div_trajectories(pi, theta_1, theta_2, trajectories):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Assume trajectories are sampled from pi(theta_1)</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    kl_div <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> tau <span class="kw">in</span> trajectories:</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> s, a, _r <span class="kw">in</span> tau:</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>            kl_div <span class="op">+=</span> jnp.log(pi(theta_1)(s, a)) <span class="op">-</span> jnp.log(pi(theta_2)(s, a))</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> kl_div <span class="op">/</span> <span class="bu">len</span>(trajectories)</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>latex(kl_div_trajectories)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="7">
\begin{array}{l} \mathbf{function} \ \mathrm{kl\_div\_trajectories}(\pi, \theta_{\mathrm{1}}, \theta_{\mathrm{2}}, \mathrm{trajectories}) \\ \hspace{1em} \mathrm{kl\_div} \gets 0 \\ \hspace{1em} \mathbf{for} \ \tau \in \mathrm{trajectories} \ \mathbf{do} \\ \hspace{2em} \mathbf{for} \ \mathopen{}\left( s, a, \mathrm{\_r} \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{3em} \mathrm{kl\_div} \gets \mathrm{kl\_div} + \log \pi \mathopen{}\left( \theta_{\mathrm{1}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) - \log \pi \mathopen{}\left( \theta_{\mathrm{2}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{2em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \frac{\mathrm{kl\_div}}{\mathrm{len} \mathopen{}\left( \mathrm{trajectories} \mathclose{}\right)} \\ \mathbf{end \ function} \end{array}
</div>
</div>
<div id="d159e48c" class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> trpo(env, δ, theta_init, n_interactions):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(K):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_interactions)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        A_hat <span class="op">=</span> fit_advantage(trajectories)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> approximate_gain(theta_opt):</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>            A_total <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tau <span class="kw">in</span> trajectories:</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> s, _a, _r <span class="kw">in</span> tau:</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>                    <span class="cf">for</span> a <span class="kw">in</span> env.action_space:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>                        A_total <span class="op">+=</span> pi(theta)(s, a) <span class="op">*</span> A_hat(s, a)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> A_total</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> constraint(theta_opt):</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> kl_div_trajectories(pi, theta, theta_opt, trajectories) <span class="op">&lt;=</span> δ</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> optimize(approximate_gain, constraint)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>latex(trpo)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="8">
\begin{array}{l} \mathbf{function} \ \mathrm{trpo}(\mathrm{env}, δ, \theta_{\mathrm{init}}, n_{\mathrm{interactions}}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( K \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), n_{\mathrm{interactions}} \mathclose{}\right) \\ \hspace{2em} \widehat{A} \gets \mathrm{fit\_advantage} \mathopen{}\left( \mathrm{trajectories} \mathclose{}\right) \\ \hspace{2em} \mathbf{function} \ \mathrm{approximate\_gain}(\theta_{\mathrm{opt}}) \\ \hspace{3em} A_{\mathrm{total}} \gets 0 \\ \hspace{3em} \mathbf{for} \ \tau \in \mathrm{trajectories} \ \mathbf{do} \\ \hspace{4em} \mathbf{for} \ \mathopen{}\left( s, \mathrm{\_a}, \mathrm{\_r} \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{5em} \mathbf{for} \ a \in \mathrm{env}.\mathrm{action\_space} \ \mathbf{do} \\ \hspace{6em} A_{\mathrm{total}} \gets A_{\mathrm{total}} + \pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \cdot \widehat{A} \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{5em} \mathbf{end \ for} \\ \hspace{4em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{return} \ A_{\mathrm{total}} \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \mathbf{function} \ \mathrm{constraint}(\theta_{\mathrm{opt}}) \\ \hspace{3em} \mathbf{return} \ \mathrm{kl\_div\_trajectories} \mathopen{}\left( \pi, \theta, \theta_{\mathrm{opt}}, \mathrm{trajectories} \mathclose{}\right) \le δ \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \theta \gets \mathrm{optimize} \mathopen{}\left( \mathrm{approximate\_gain}, \mathrm{constraint} \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}
</div>
</div>
<!--
Applying importance sampling allows us to estimate the TRPO objective as follows:

:::: {#def-trpo-implement}

#### Trust region policy optimization (implementation)

::: {prf:definitionic} TODO
Initialize $\theta^0$

Sample $N$ trajectories from $\rho^k$ to learn a value estimator $\tilde b_\hi(s) \approx V^{\pi^k}_\hi(s)$

Sample $M$ trajectories $\tau_0, \dots, \tau_{M-1} \sim \rho^k$

$$
\begin{gathered}
            \theta^{k+1} \gets \arg\max_{\theta} \frac{1}{M} \sum_{m=0}^{M-1} \sum_{h=0}^{H-1} \frac{\pi_\theta(a_\hi \mid s_\hi)}{\pi^k(a_\hi \mid s_\hi)} [ R_\hi(\tau_m) - \tilde b_\hi(s_\hi) ] \\
            \text{where } \sum_{m=0}^{M-1} \sum_{h=0}^{H-1} \log \frac{\pi_k(a_\hi^m \mid s_\hi^m)}{\pi_\theta(a_\hi^m \mid s_\hi^m)} \le \delta
        
\end{gathered}
$$
:::
:::: -->
<p>The above isn’t entirely complete: we still need to solve the actual optimization problem at each step. Unless we know additional properties of the problem, this might be an intractable optimization. Do we need to solve it exactly, though? Instead, if we assume that both the objective function and the constraint are somewhat smooth in terms of the policy parameters, we can use their <em>Taylor expansions</em> to give us a simpler optimization problem with a closed-form solution. This brings us to the <strong>natural policy gradient</strong> algorithm.</p>
</section>
<section id="natural-policy-gradient" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="natural-policy-gradient"><span class="header-section-number">6.8</span> Natural policy gradient</h2>
<p>We take a <em>linear</em> (first-order) approximation to the objective function and a <em>quadratic</em> (second-order) approximation to the KL divergence constraint about the current estimate <span class="math inline">\(\theta^k\)</span>. This results in the optimization problem</p>
<p><span id="eq-npg-optimization"><span class="math display">\[
\begin{gathered}
    \max_\theta \nabla_\theta J(\pi_{\theta^k})^\top (\theta - \theta^k) \\
    \text{where } \frac{1}{2} (\theta - \theta^k)^\top F_{\theta^k} (\theta - \theta^k) \le \delta
\end{gathered}
\tag{6.7}\]</span></span></p>
<p>where <span class="math inline">\(F_{\theta^k}\)</span> is the <strong>Fisher information matrix</strong> defined below.</p>
<div id="def-fisher-matrix" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4 (Fisher information matrix)</strong></span> Let <span class="math inline">\(p_\theta\)</span> denote a parameterized distribution. Its Fisher information matrix <span class="math inline">\(F_\theta\)</span> can be defined equivalently as:</p>
<p><span class="math display">\[
\begin{aligned}
        F_{\theta} &amp; = \mathbb{E}_{x \sim p_\theta} \left[ (\nabla_\theta \log p_\theta(x)) (\nabla_\theta \log p_\theta(x))^\top \right] &amp; \text{covariance matrix of the Fisher score}          \\
                   &amp; = \mathbb{E}_{x \sim p_{\theta}} [- \nabla_\theta^2 \log p_\theta(x)]                                                &amp; \text{average Hessian of the negative log-likelihood}
\end{aligned}
\]</span></p>
<p>Recall that the Hessian of a function describes its curvature: for a vector <span class="math inline">\(\delta \in \Theta\)</span>, the quantity <span class="math inline">\(\delta^\top F_\theta \delta\)</span> describes how rapidly the negative log-likelihood changes if we move by <span class="math inline">\(\delta\)</span>. The Fisher information matrix is precisely the Hessian of the KL divergence (with respect to either one of the parameters).</p>
<p>In particular, when <span class="math inline">\(p_\theta = \rho_{\theta}\)</span> denotes a trajectory distribution, we can further simplify the expression:</p>
<p><span id="eq-fisher-trajectory"><span class="math display">\[
F_{\theta} = \mathbb{E}_{\tau \sim \rho_\theta} \left[ \sum_{h=0}^{H-1} (\nabla \log \pi_\theta (a_h\mid s_h)) (\nabla \log \pi_\theta(a_h\mid s_h))^\top \right]
\tag{6.8}\]</span></span></p>
<p>Note that we’ve used the Markov property to cancel out the cross terms corresponding to two different time steps.</p>
</div>
<p>This is a convex optimization problem with a closed-form solution. To see why, it helps to visualize the case where <span class="math inline">\(\theta\)</span> is two-dimensional: the constraint describes the inside of an ellipse, and the objective function is linear, so we can find the extreme point on the boundary of the ellipse. We recommend <span class="citation" data-cites="boyd_convex_2004">Boyd and Vandenberghe (<a href="references.html#ref-boyd_convex_2004" role="doc-biblioref">2004</a>)</span> for a comprehensive treatment of convex optimization.</p>
<p>More generally, for a higher-dimensional <span class="math inline">\(\theta\)</span>, we can compute the global optima by setting the gradient of the Lagrangian to zero:</p>
<p><span class="math display">\[
\begin{aligned}
    \mathcal{L}(\theta, \alpha)                     &amp; = \nabla J(\pi_{\theta^k})^\top (\theta - \theta^k) - \alpha \left[ \frac{1}{2} (\theta - \theta^k)^\top F_{\theta^k} (\theta - \theta^k) - \delta \right] \\
    \nabla \mathcal{L}(\theta^{k+1}, \alpha) &amp; := 0                                                                                                                                                             \\
    \implies \nabla J(\pi_{\theta^k})        &amp; = \alpha F_{\theta^k} (\theta^{k+1} - \theta^k)                                                                                                                   \\
    \theta^{k+1}                           &amp; = \theta^k + \eta F_{\theta^k}^{-1} \nabla J(\pi_{\theta^k})                                                                                             \\
    \text{where } \eta                     &amp; = \sqrt{\frac{2 \delta}{\nabla J(\pi_{\theta^k})^\top F_{\theta^k}^{-1} \nabla J(\pi_{\theta^k})}}
\end{aligned}
\]</span></p>
<p>This gives us the closed-form update. Now the only challenge is to estimate the Fisher information matrix, since, as with the KL divergence constraint, it is an expectation over trajectories, and computing it exactly is therefore typically intractable.</p>
<div id="def-npg" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.5 (Natural policy gradient)</strong></span> How many trajectory samples do we need to accurately estimate the Fisher information matrix? As a rule of thumb, the sample complexity should scale with the dimension of the parameter space. This makes this approach intractable in the deep learning setting where we might have a very large number of parameters.</p>
</div>
<p>As you can see, the NPG is the “basic” policy gradient algorithm we saw above, but with the gradient transformed by the inverse Fisher information matrix. This matrix can be understood as accounting for the <strong>geometry of the parameter space.</strong> The typical gradient descent algorithm implicitly measures distances between parameters using the typical <em>Euclidean distance</em>. Here, where the parameters map to a <em>distribution</em>, using the natural gradient update is equivalent to optimizing over <strong>distribution space</strong> rather than parameter space, where distance between distributions is measured by the <a href="#def-kld" class="quarto-xref">Definition&nbsp;<span>6.3</span></a>.</p>
<div id="exm-natural-simple" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 (Natural gradient on a simple problem)</strong></span> Let’s step away from RL and consider the following optimization problem over Bernoulli distributions <span class="math inline">\(\pi \in \Delta(\{ 0, 1 \})\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
        J(\pi) &amp; = 100 \cdot \pi(1) + 1 \cdot \pi(0)
\end{aligned}
\]</span></p>
<p>We can think of the space of such distributions as the line between <span class="math inline">\((0, 1)\)</span> to <span class="math inline">\((1, 0)\)</span> on the Cartesian plane:</p>
<div id="4dc2d10a" class="cell" data-execution_count="9">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> jnp.linspace(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">50</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> <span class="dv">1</span> <span class="op">-</span> x</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="vs">r"$\pi(0)$"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="vs">r"$\pi(1)$"</span>)</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Space of Bernoulli distributions"</span>)</span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="pg_files/figure-html/cell-10-output-1.png" width="458" height="331" class="figure-img"></p>
<figcaption>A line from (0, 1) to (1, 0)</figcaption>
</figure>
</div>
</div>
</div>
<p>Clearly the optimal distribution is the constant one <span class="math inline">\(\pi(1) = 1\)</span>. Suppose we optimize over the parameterized family <span class="math inline">\(\pi_\theta(1) = \frac{\exp(\theta)}{1+\exp(\theta)}\)</span>. Then our optimization algorithm should set <span class="math inline">\(\theta\)</span> to be unboundedly large. Then the “vanilla” gradient is</p>
<p><span class="math display">\[
\nabla_\theta J(\pi_\theta) = \frac{99 \exp(\theta)}{(1 + \exp(\theta))^2}.
\]</span></p>
<p>Note that as <span class="math inline">\(\theta \to \infty\)</span> that the increments get closer and closer to <span class="math inline">\(0\)</span>; the rate of increase becomes exponentially slow.</p>
<p>However, if we compute the Fisher information “matrix” (which is just a scalar in this case), we can account for the geometry induced by the parameterization.</p>
<p><span class="math display">\[
\begin{aligned}
        F_\theta &amp; = \mathbb{E}_{x \sim \pi_\theta} [ (\nabla_\theta \log \pi_\theta(x))^2 ] \\
                 &amp; = \frac{\exp(\theta)}{(1 + \exp(\theta))^2}.
\end{aligned}
\]</span></p>
<p>This gives the natural gradient update</p>
<p><span class="math display">\[
\begin{aligned}
        \theta^{k+1} &amp; = \theta^k + \eta F_{\theta^k}^{-1} \nabla_ \theta J(\theta^k) \\
                     &amp; = \theta^k + 99 \eta
\end{aligned}
\]</span></p>
<p>which increases at a constant rate, i.e.&nbsp;improves the objective more quickly than “vanilla” gradient ascent.</p>
</div>
<p>Though the NPG now gives a closed-form optimization step, it requires computing the inverse Fisher information matrix, which typically scales as <span class="math inline">\(O((\dim \Theta)^3)\)</span>. This can be expensive if the parameter space is large. Can we find an algorithm that works in <em>linear time</em> with respect to the dimension of the parameter space?</p>
</section>
<section id="sec-ppo" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="sec-ppo"><span class="header-section-number">6.9</span> Proximal policy optimization</h2>
<p>We can relax the TRPO optimization problem in a different way: Rather than imposing a hard constraint on the KL distance, we can instead impose a <em>soft</em> constraint by incorporating it into the objective and penalizing parameter values that drastically change the trajectory distribution.</p>
<p><span class="math display">\[
\begin{aligned}
\theta^{k+1} &amp;\gets \arg\max_{\theta} \mathbb{E}_{s_0, \dots, s_{H-1} \sim \rho_{\pi^{k}}} \left[ \sum_{h=0}^{H-1} \mathbb{E}_{a_h\sim \pi_{\theta}(s_h)} A^{\pi^{k}}(s_h, a_h) \right] - \lambda \mathrm{KL}\left(\rho_{\theta}\parallel\rho_{\theta^k}\right)
\end{aligned}
\]</span></p>
<p>Here <span class="math inline">\(\lambda\)</span> is a <strong>regularization hyperparameter</strong> that controls the tradeoff between the two terms. This is the objective of the <strong>proximal policy optimization</strong> algorithm (<span class="citation" data-cites="schulman_proximal_2017">Schulman et al. (<a href="references.html#ref-schulman_proximal_2017" role="doc-biblioref">2017</a>)</span>).</p>
<p>How do we solve this optimization? Let us begin by simplifying the <span class="math inline">\(\mathrm{KL}\left(\rho_{\pi^k}\parallel\rho_{\pi_{\theta}}\right)\)</span> term. Expanding gives</p>
<p><span class="math display">\[
\begin{aligned}
    \mathrm{KL}\left(\rho_{\pi^k}\parallel\rho_{\pi_{\theta}}\right) &amp; = \mathbb{E}_{\tau \sim \rho_{\pi^k}} \left[\log \frac{\rho_{\pi^k}(\tau)}{\rho_{\pi_{\theta}}(\tau)}\right]                                                       \\
                                           &amp; = \mathbb{E}_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \log \frac{\pi^k(a_h\mid s_h)}{\pi_{\theta}(a_h\mid s_h)}\right] &amp; \text{state transitions cancel} \\
                                           &amp; = \mathbb{E}_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \log \frac{1}{\pi_{\theta}(a_h\mid s_h)}\right] + c
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(c\)</span> is some constant with respect to <span class="math inline">\(\theta\)</span>, and can be ignored. This gives the objective</p>
<p><span class="math display">\[
\ell^k(\theta)
=
\mathbb{E}_{s_0, \dots, s_{H-1} \sim \rho_{\pi^{k}}} \left[ \sum_{h=0}^{H-1} \mathbb{E}_{a_h\sim \pi_{\theta}(s_h)} A^{\pi^{k}}(s_h, a_h) \right] - \lambda \mathbb{E}_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \log \frac{1}{\pi_{\theta}(a_h\mid s_h)}\right]
\]</span></p>
<p>Once again, this takes an expectation over trajectories. But here we cannot directly sample trajectories from <span class="math inline">\(\pi^k\)</span>, since in the first term, the actions actually come from <span class="math inline">\(\pi_\theta\)</span>. To make this term line up with the other expectation, we would need the actions to also come from <span class="math inline">\(\pi^k\)</span>.</p>
<p>This should sound familiar: we want to estimate an expectation over one distribution by sampling from another. We can once again use <a href="#sec-importance-sampling" class="quarto-xref"><span>Section 6.3.2</span></a> to rewrite the inner expectation:</p>
<p><span class="math display">\[
\mathbb{E}_{a_h\sim \pi_{\theta}(s_h)} A^{\pi^{k}}(s_h, a_h)
=
\mathbb{E}_{a_h\sim \pi^k(s_h)} \frac{\pi_\theta(a_h\mid s_h)}{\pi^k(a_h\mid s_h)} A^{\pi^{k}}(s_h, a_h)
\]</span></p>
<p>Now we can combine the expectations together to get the objective</p>
<p><span class="math display">\[
\ell^k(\theta) = \mathbb{E}_{\tau \sim \rho_{\pi^k}} \left[ \sum_{h=0}^{H-1} \left( \frac{\pi_\theta(a_h\mid s_h)}{\pi^k(a_h\mid s_h)} A^{\pi^k}(s_h, a_h) - \lambda \log \frac{1}{\pi_\theta(a_h\mid s_h)} \right) \right]
\]</span></p>
<p>Now we can estimate this function by a sample average over trajectories from <span class="math inline">\(\pi^k\)</span>. Remember that to complete a single iteration of PPO, we execute</p>
<p><span class="math display">\[
\theta^{k+1} \gets \arg\max_{\theta} \ell^k(\theta).
\]</span></p>
<p>If <span class="math inline">\(\ell^k\)</span> is differentiable, we can optimize it by gradient ascent, completing a single iteration of PPO.</p>
<div id="c5cb9af5" class="cell" data-execution_count="10">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> typing <span class="im">import</span> TypeVar</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>State <span class="op">=</span> TypeVar(<span class="st">"State"</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>Action <span class="op">=</span> TypeVar(<span class="st">"Action"</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ppo(</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>    env,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    pi: Callable[[Float[Array, <span class="st">" D"</span>]], Callable[[State, Action], <span class="bu">float</span>]],</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    λ: <span class="bu">float</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    theta_init: Float[Array, <span class="st">" D"</span>],</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    n_iters: <span class="bu">int</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    n_fit_trajectories: <span class="bu">int</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    n_sample_trajectories: <span class="bu">int</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>):</span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> theta_init</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(n_iters):</span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>        fit_trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_fit_trajectories)</span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>        A_hat <span class="op">=</span> fit(fit_trajectories)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>        sample_trajectories <span class="op">=</span> sample_trajectories(env, pi(theta), n_sample_trajectories)</span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>        <span class="kw">def</span> objective(theta_opt):</span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>            total_objective <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> tau <span class="kw">in</span> sample_trajectories:</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a>                <span class="cf">for</span> s, a, _r <span class="kw">in</span> tau:</span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>                    total_objective <span class="op">+=</span> pi(theta_opt)(s, a) <span class="op">/</span> pi(theta)(s, a) <span class="op">*</span> A_hat(s, a) <span class="op">+</span> λ <span class="op">*</span> jnp.log(pi(theta_opt)(s, a))</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> total_objective <span class="op">/</span> n_sample_trajectories</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> optimize(objective, theta)</span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> theta</span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>latex(ppo)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="10">
\begin{array}{l} \mathbf{function} \ \mathrm{ppo}(\mathrm{env}, \pi: \mathrm{Callable}_{\mathopen{}\left[ \mathbb{R}^{D} \mathclose{}\right], \mathrm{Callable}_{\mathopen{}\left[ \mathrm{State}, \mathrm{Action} \mathclose{}\right], \mathrm{float}}}, λ: \mathrm{float}, \theta_{\mathrm{init}}: \mathbb{R}^{D}, n_{\mathrm{iters}}: \mathrm{int}, \mathrm{n\_fit\_trajectories}: \mathrm{int}, \mathrm{n\_sample\_trajectories}: \mathrm{int}) \\ \hspace{1em} \theta \gets \theta_{\mathrm{init}} \\ \hspace{1em} \mathbf{for} \ k \in \mathrm{range} \mathopen{}\left( n_{\mathrm{iters}} \mathclose{}\right) \ \mathbf{do} \\ \hspace{2em} \mathrm{fit\_trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), \mathrm{n\_fit\_trajectories} \mathclose{}\right) \\ \hspace{2em} \widehat{A} \gets \mathrm{fit} \mathopen{}\left( \mathrm{fit\_trajectories} \mathclose{}\right) \\ \hspace{2em} \mathrm{sample\_trajectories} \gets \mathrm{sample\_trajectories} \mathopen{}\left( \mathrm{env}, \pi \mathopen{}\left( \theta \mathclose{}\right), \mathrm{n\_sample\_trajectories} \mathclose{}\right) \\ \hspace{2em} \mathbf{function} \ \mathrm{objective}(\theta_{\mathrm{opt}}) \\ \hspace{3em} \mathrm{total\_objective} \gets 0 \\ \hspace{3em} \mathbf{for} \ \tau \in \mathrm{sample\_trajectories} \ \mathbf{do} \\ \hspace{4em} \mathbf{for} \ \mathopen{}\left( s, a, \mathrm{\_r} \mathclose{}\right) \in \tau \ \mathbf{do} \\ \hspace{5em} \mathrm{total\_objective} \gets \mathrm{total\_objective} + \frac{\pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right)}{\pi \mathopen{}\left( \theta \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right)} \widehat{A} \mathopen{}\left( s, a \mathclose{}\right) + λ \cdot \log \pi \mathopen{}\left( \theta_{\mathrm{opt}} \mathclose{}\right) \mathopen{}\left( s, a \mathclose{}\right) \\ \hspace{4em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{end \ for} \\ \hspace{3em} \mathbf{return} \ \frac{\mathrm{total\_objective}}{\mathrm{n\_sample\_trajectories}} \\ \hspace{2em} \mathbf{end \ function} \\ \hspace{2em} \theta \gets \mathrm{optimize} \mathopen{}\left( \mathrm{objective}, \theta \mathclose{}\right) \\ \hspace{1em} \mathbf{end \ for} \\ \hspace{1em} \mathbf{return} \ \theta \\ \mathbf{end \ function} \end{array}
</div>
</div>
</section>
<section id="summary" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="summary"><span class="header-section-number">6.10</span> Summary</h2>
<p>Policy gradient methods are a powerful family of algorithms that directly optimize the expected total reward by iteratively updating the policy parameters. Precisely, we estimate the gradient of the expected total reward (with respect to the parameters), and update the parameters in that direction. But estimating the gradient is a tricky task! We saw many ways to reduce the variance of the gradient estimator, culminating in the advantage-based expression <a href="#eq-pg-advantage" class="quarto-xref">Equation&nbsp;<span>6.5</span></a>.</p>
<p>But updating the parameters doesn’t entirely solve the problem: Sometimes, a small step in the parameters might lead to a big step in the policy. To avoid changing the policy too much at each step, we must account for the curvature in the parameter space. We first did this explicitly with <a href="#sec-trpo" class="quarto-xref"><span>Section 6.7</span></a>, and then saw ways to relax the constraint in <a href="#def-npg" class="quarto-xref">Definition&nbsp;<span>6.5</span></a> and <a href="#sec-ppo" class="quarto-xref"><span>Section 6.9</span></a>.</p>
<p>These are still popular methods to this day, especially because they efficiently integrate with <em>deep neural networks</em> for representing complex functions.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-baydin_automatic_2018" class="csl-entry" role="listitem">
Baydin, Atilim Gunes, Barak A. Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind. 2018. <span>“Automatic Differentiation in Machine Learning: A Survey.”</span> February 5, 2018. <a href="https://doi.org/10.48550/arXiv.1502.05767">https://doi.org/10.48550/arXiv.1502.05767</a>.
</div>
<div id="ref-boyd_convex_2004" class="csl-entry" role="listitem">
Boyd, Stephen, and Lieven Vandenberghe. 2004. <em>Convex <span>Optimization</span></em>. Cambridge University Press. <a href="https://web.stanford.edu/~boyd/cvxbook/">https://web.stanford.edu/~boyd/cvxbook/</a>.
</div>
<div id="ref-schulman_proximal_2017" class="csl-entry" role="listitem">
Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. <span>“Proximal <span>Policy Optimization Algorithms</span>.”</span> August 28, 2017. <a href="https://doi.org/10.48550/arXiv.1707.06347">https://doi.org/10.48550/arXiv.1707.06347</a>.
</div>
<div id="ref-williams_simple_1992" class="csl-entry" role="listitem">
Williams, Ronald J. 1992. <span>“Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.”</span> <em>Machine Learning</em> 8 (3): 229–56. <a href="https://doi.org/10.1007/BF00992696">https://doi.org/10.1007/BF00992696</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./fitted_dp.html" class="pagination-link" aria-label="Fitted Dynamic Programming Algorithms">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./imitation_learning.html" class="pagination-link" aria-label="Imitation Learning">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>