{{< include macros.qmd >}}

# Fitted Dynamic Programming Algorithms {#sec-fitted-dp}

In @sec-mdps,
we discussed how to solve Markov decision processes
where the state and action spaces $\mathcal{S}$ and $\mathcal{A}$ were finite and small.
This allowed us to use dynamic programming and the Bellman consistency equations (@thm-bellman-consistency)
to evaluate policies (@sec-eval-dp-finite-horizon)
and compute the optimal policy (@sec-finite-opt-dp).
These "small" MDPs aren't usually considered "full RL problems" because,
put simply, they're too easy!
The assumption that we know the environment
and can compute expected values over the state transitions
ends up being too strict for most real-world tasks.

In this chapter,
we will tackle the "full RL problem"
by extending DP algorithms to account for unknown environments.
We will also relax the assumption that the state space is small and finite:
the methods we will cover support large or continuous state spaces.

This will require _learning_ about the environment
by collecting data through interaction
and then applying supervised learning (see @sec-sl)
to approximate relevant functions.

| Chapter | State space | Action space | Policies | Knowledge of environment | Key algorithms |
| --- | --- | --- | --- | --- |
| @sec-mdps (MDPs) | $\{1, \dots, \vert \mathcal{S} \vert \}$ | $\{ 1, \dots, \vert \mathcal{A} \vert \}$ | deterministic | known | dynamic programming, policy iteration |
| @sec-control (Control) | $\R^{n_\st}$ | $\R^{n_\act}$ | deterministic | unknown | (iterative) linear quadratic regulator |
| @sec-bandits (MABs) | none | $\{ 1, \dots, K \}$ | deterministic or stochastic | unknown | upper confidence bound, Thompson sampling |
| This chapter (Fitted DP) | arbitrary | $\{ 1, \dots, \vert \mathcal{A} \vert \}$ | deterministic | unknown | Q-learning, fitted policy iteration |

: How this chapter extends to more general settings. {#tbl-fitted-overview}

```{python}
from utils import gym, tqdm, rand, Float, Array, NamedTuple, Callable, Optional, np

key = rand.PRNGKey(184)

class Transition(NamedTuple):
    s: int
    a: int
    r: float


Trajectory = list[Transition]


def get_num_actions(trajectories: list[Trajectory]) -> int:
    """Get the number of actions in the dataset. Assumes actions range from 0 to A-1."""
    return max(max(t.a for t in τ) for τ in trajectories) + 1


State = Float[Array, "..."]  # arbitrary shape

# assume finite `A` actions and f outputs an array of Q-values
# i.e. Q(s, a, h) is implemented as f(s, h)[a]
QFunction = Callable[[State, int], Float[Array, " A"]]


def Q_zero(A: int) -> QFunction:
    """A Q-function that always returns zero."""
    return lambda s, a: np.zeros(A)


# a deterministic time-dependent policy
Policy = Callable[[State, int], int]


def q_to_greedy(Q: QFunction) -> Policy:
    """Get the greedy policy for the given state-action value function."""
    return lambda s, h: np.argmax(Q(s, h))
```

## Fitted value iteration {#sec-fitted-vi}

In this section,
we'll see one method to compute the optimal value function
when the environment is unknown.
This method is analogous to value iteration (@sec-value-iteration),
except instead of solving the Bellman optimality equations (@eq-bellman-opt) exactly,
which is no longer assumed to be possible,
we will collect a dataset of trajectories and apply _supervised learning_
to learn the .

::: {#rem-review-vi}
#### Value iteration

It will be helpful to first review value iteration (@sec-value-iteration).
This was an algorithm we used
to compute the optimal value function $V^\star : \mathcal{S} \to \R$
in an infinite-horizon MDP.
Here, we will present the equivalent algorithm
for the optimal action-value function $Q^\star : \mathcal{S} \times \mathcal{A} \to \R$.
The optimal action-value function satisfies the Bellman optimality equations

$$
Q^\star(s, a, h) = r(s, a) + \E_{s' \sim P(\cdot \mid s, a)}[
    \max_{a' \in \mathcal{A}} Q^\star(s', a', h+1)
].
$$ {#eq-review-bellman-opt}

Now let us treat @eq-review-bellman-opt as an "operator" instead of an equation,
that is,

$$
q(s, a) \gets r(s, a) + \E_{s' \sim P(\cdot \mid s, a)}[
    \max_{a' \in \mathcal{A}} q(s', a')
].
$$ {#eq-review-bellman-opt-operator}

If we start with some guess $q : \mathcal{S} \times \mathcal{A} \to \R$,
and repeatedly apply the update step @eq-review-bellman-opt-operator,
the iterates will eventually converge to $Q^\star$
since @eq-review-bellman-opt-operator is a contraction mapping.
:::

Note that the r.h.s. of the value iteration update step (@eq-review-bellman-opt-operator)
requires computing an expectation over the state transitions.
As mentioned in the introduction,
this is intractable for most real-world tasks.
Instead, we will need to use *function approximation* methods from supervised learning
to solve for the value function in an alternative way.

Recall that supervised learning is good at learning _conditional expectations_
of the form

$$
f(x) = \E[ y \mid x ].
$$ {#eq-exm-cond-mean}

We collect a dataset of $(x, y)$ pairs,
sampled from their joint distribution,
and minimize the squared error $(f(x) - y)^2$.

Let us now rewrite @eq-review-bellman-opt-operator in this notation
to make the connection more clear.
We explicitly notate $s'$ as a random variable
and move the conditioning on $s, a$ into the brackets:

$$
q(s, a, h) \gets \E[ r(s, a) + \max_{a' \in \mathcal{A}} q(s', a', h+1) \mid s, a, h ].
$$ {#eq-q-cond-mean}

You can see that the input $x$ corresponds to $s, a, h$
and the output $y$ corresponds to $r(s, a) + \max_{a' \in \mathcal{A}} q(s', a', h+1)$.
Now we just need to sample a dataset of input-output pairs.
This is where _interaction with the environment_ comes in.

In particular,
given a policy $\pi$ (called the **data collection policy**),
we can act in the environment
to obtain a dataset of $N$ trajectories $\tau^1, \dots, \tau^N \sim \rho^{\pi}$.
Let us indicate the trajectory index in the superscript, so that

$$
\tau^n = \{ s_0^n, a_0^n, r_0^n, s_1^n, a_1^n, r_1^n, \dots, s_{\hor-1}^n, a_{\hor-1}^n, r_{\hor-1}^n \}.
$$

```{python}
def collect_data(
    env: gym.Env, N: int, H: int, key: rand.PRNGKey, π: Optional[Policy] = None
) -> list[Trajectory]:
    """Collect a dataset of trajectories from the given policy (or a random one)."""
    trajectories = []
    seeds = [rand.bits(k).item() for k in rand.split(key, N)]
    for i in tqdm(range(N)):
        τ = []
        s, _ = env.reset(seed=seeds[i])
        for h in range(H):
            # sample from a random policy
            a = π(s, h) if π else env.action_space.sample()
            s_next, r, terminated, truncated, _ = env.step(a)
            τ.append(Transition(s, a, r))
            if terminated or truncated:
                break
            s = s_next
        trajectories.append(τ)
    return trajectories
```

```{python}
env = gym.make("LunarLander-v3")
trajectories = collect_data(env, 100, 300, key)
trajectories[0][:5]  # show first five transitions from first trajectory
```

Our above dataset would give us $N \cdot \hor$ samples in the dataset:

$$
x_{i \hi} = (s_\hi^i, a_\hi^i, \hi) \qquad y_{i \hi} = r(s_\hi^i, a_\hi^i) + \max_{a'} Q^\star_{\hi + 1}(s_{\hi + 1}^i, a')
$$

```{python}
def get_X(trajectories: list[Trajectory]):
    """
    We pass the state and timestep as input to the Q-function
    and return an array of Q-values.
    """
    rows = [(τ[h].s, τ[h].a, h) for τ in trajectories for h in range(len(τ))]
    return [np.stack(ary) for ary in zip(*rows)]


def get_y(
    trajectories: list[Trajectory],
    f: Optional[QFunction] = None,
    π: Optional[Policy] = None,
):
    """
    Transform the dataset of trajectories into a dataset for supervised learning.
    If `π` is None, instead estimates the optimal Q function.
    Otherwise, estimates the Q function of π.
    """
    f = f or Q_zero(get_num_actions(trajectories))
    y = []
    for τ in trajectories:
        for h in range(len(τ) - 1):
            s, a, r = τ[h]
            Q_values = f(s, h + 1)
            y.append(r + (Q_values[π(s, h + 1)] if π else Q_values.max()))
        y.append(τ[-1].r)
    return np.array(y)
```

```{python}
s, a, h = get_X(trajectories[:1])
print("states:", s[:5])
print("actions:", a[:5])
print("timesteps:", h[:5])
```

```{python}
get_y(trajectories[:1])[:5]
```

Then we can use empirical risk minimization to find a function $\hat f$ that approximates the optimal Q-function.

```{python}
# We will see some examples of fitting methods in the next section
FittingMethod = Callable[[Float[Array, "N D"], Float[Array, " N"]], QFunction]
```

::: {#rem-bootstrap}
#### Circularity

But notice that the definition of $y_{i \hi}$ depends on the current guess of Q-function!
How can we resolve this circular dependency?
Recall that we faced the same issue when evaluating a policy in an infinite-horizon MDP (@sec-iterative-pe).
There, we iterated the @def-bellman-operator
since we knew that the policy's value function was a fixed point of the policy's Bellman operator.
We can apply the same strategy here,
using the $\hat f$ from the previous iteration to compute the labels $y_{i \hi}$,
and then using this new dataset to fit the next iterate.
:::

::: {#def-fitted-q-iteration}
#### Fitted Q-function iteration

1. Initialize some function $\hat f(s, a, h) \in \mathbb{R}$.
2. Iterate the following:
   1. Generate a supervised learning dataset $X, y$ from the trajectories and the current estimate $f$, where the labels come from the r.h.s. of the Bellman optimality operator @eq-bellman-opt-operator
   2. Set $\hat f$ to the function that minimizes the empirical risk:

$$
\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.
$$
:::

```{python}
def fitted_q_iteration(
    trajectories: list[Trajectory],
    fit: FittingMethod,
    epochs: int,
    Q_init: Optional[QFunction] = None,
) -> QFunction:
    """
    Run fitted Q-function iteration using the given dataset.
    Returns an estimate of the optimal Q-function.
    """
    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))
    X = get_X(trajectories)
    for _ in range(epochs):
        y = get_y(trajectories, Q_hat)
        Q_hat = fit(X, y)
    return Q_hat
```

## Fitted policy evaluation {#sec-fitted-pi-eval}

We can also use this fixed-point interation to *evaluate* a policy using the dataset (not necessarily the one used to generate the trajectories):

::: {#def-fitted-evaluation}
#### Fitted policy evaluation

**Input:** Policy $\pi : \mathcal{S} \times [H] \to \Delta(\mathcal{A})$ to be evaluated.

**Output:** An approximation of the value function $Q^\pi$ of the policy.

1. Initialize some function $\hat f(s, a, h) \in \mathbb{R}$.
2. Iterate the following:
   1. Generate a supervised learning dataset $X, y$ from the trajectories and the current estimate $f$, where the labels come from the r.h.s. of the @thm-bellman-consistency for the given policy.
   2. Set $\hat f$ to the function that minimizes the empirical risk:
   
$$
\hat f \gets \arg\min_f \frac{1}{N} \sum_{i=1}^N (y_i - f(x_i))^2.
$$
:::

```{python}
def fitted_evaluation(
    trajectories: list[Trajectory],
    fit: FittingMethod,
    π: Policy,
    epochs: int,
    Q_init: Optional[QFunction] = None,
) -> QFunction:
    """
    Run fitted policy evaluation using the given dataset.
    Returns an estimate of the Q-function of the given policy.
    """
    Q_hat = Q_init or Q_zero(get_num_actions(trajectories))
    X = get_X(trajectories)
    for _ in tqdm(range(epochs)):
        y = get_y(trajectories, Q_hat, π)
        Q_hat = fit(X, y)
    return Q_hat
```

::: {#exr-spot-difference}
#### Spot the difference

Spot the difference between `fitted_evaluation` and `fitted_q_iteration`. (See the definition of `get_y`.)
How would you modify this algorithm to evaluate the data collection policy?
:::

## Fitted policy iteration

We can use this policy evaluation algorithm to adapt @sec-policy-iteration to this new setting. The algorithm remains exactly the same -- repeatedly make the policy greedy w.r.t. its own value function -- except now we must evaluate the policy (i.e. compute its value function) using the iterative `fitted-evaluation` algorithm.

```{python}
def fitted_policy_iteration(
    trajectories: list[Trajectory],
    fit: FittingMethod,
    epochs: int,
    evaluation_epochs: int,
    π_init: Optional[Policy] = lambda s, h: 0,  # constant zero policy
):
    """Run fitted policy iteration using the given dataset."""
    π = π_init
    for _ in range(epochs):
        Q_hat = fitted_evaluation(trajectories, fit, π, evaluation_epochs)
        π = q_to_greedy(Q_hat)
    return π
```

## Summary



