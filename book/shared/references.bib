@book{agarwal_reinforcement_2022,
  title = {Reinforcement {{Learning}}: {{Theory}} and {{Algorithms}}},
  shorttitle = {{{AJKS}}},
  author = {Agarwal, Alekh and Jiang, Nan and Kakade, Sham M and Sun, Wen},
  year = {2022},
  month = jan,
  langid = {english},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2022/Reinforcement Learning (2022) - Agarwal, Jiang, Kakade, Sun.pdf}
}

@inproceedings{azar_minimax_2017,
  title = {Minimax {{Regret Bounds}} for {{Reinforcement Learning}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Azar, Mohammad Gheshlaghi and Osband, Ian and Munos, R{\'e}mi},
  year = {2017},
  month = jul,
  pages = {263--272},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-21},
  abstract = {We consider the problem of provably optimal exploration in reinforcement learning for finite horizon MDPs. We show that an optimistic modification to value iteration achieves a regret bound of \${\textbackslash}tilde \{O\}( {\textbackslash}sqrt\{HSAT\} + H{\textasciicircum}2S{\textasciicircum}2A+H{\textbackslash}sqrt\{T\})\$ where \$H\$ is the time horizon, \$S\$ the number of states, \$A\$ the number of actions and \$T\$ the number of time-steps. This result improves over the best previous known bound \${\textbackslash}tilde \{O\}(HS {\textbackslash}sqrt\{AT\})\$ achieved by the UCRL2 algorithm. The key significance of our new results is that when \$T{\textbackslash}geq H{\textasciicircum}3S{\textasciicircum}3A\$ and \$SA{\textbackslash}geq H\$, it leads to a regret of \${\textbackslash}tilde\{O\}({\textbackslash}sqrt\{HSAT\})\$ that matches the established lower bound of \${\textbackslash}Omega({\textbackslash}sqrt\{HSAT\})\$ up to a logarithmic factor. Our analysis contain two key insights. We use careful application of concentration inequalities to the optimal value function as a whole, rather than to the transitions probabilities (to improve scaling in \$S\$), and we define Bernstein-based ``exploration bonuses'' that use the empirical variance of the estimated values at the next states (to improve scaling in \$H\$).},
  langid = {english},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2017/Minimax Regret Bounds for Reinforcement Learning (2017) - Azar, Osband, Munos.pdf}
}

@article{degrave_magnetic_2022,
  title = {Magnetic Control of Tokamak Plasmas through Deep Reinforcement Learning},
  author = {Degrave, Jonas and Felici, Federico and Buchli, Jonas and Neunert, Michael and Tracey, Brendan and Carpanese, Francesco and Ewalds, Timo and Hafner, Roland and Abdolmaleki, Abbas and {de las Casas}, Diego and Donner, Craig and Fritz, Leslie and Galperti, Cristian and Huber, Andrea and Keeling, James and Tsimpoukelli, Maria and Kay, Jackie and Merle, Antoine and Moret, Jean-Marc and Noury, Seb and Pesamosca, Federico and Pfau, David and Sauter, Olivier and Sommariva, Cristian and Coda, Stefano and Duval, Basil and Fasoli, Ambrogio and Kohli, Pushmeet and Kavukcuoglu, Koray and Hassabis, Demis and Riedmiller, Martin},
  year = {2022},
  month = feb,
  journal = {Nature},
  volume = {602},
  number = {7897},
  pages = {414--419},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-021-04301-9},
  urldate = {2023-05-21},
  abstract = {Nuclear fusion using magnetic confinement, in particular in the tokamak configuration, is a promising path towards sustainable energy. A core challenge is to shape and maintain a high-temperature plasma within the tokamak vessel. This requires high-dimensional, high-frequency, closed-loop control using magnetic actuator coils, further complicated by the diverse requirements across a wide range of plasma configurations. In this work, we introduce a previously undescribed architecture for tokamak magnetic controller design that autonomously learns to command the full set of control coils. This architecture meets control objectives specified at a high level, at the same time satisfying physical and operational constraints. This approach has unprecedented flexibility and generality in problem specification and yields a notable reduction in design effort to produce new plasma configurations. We successfully produce and control a diverse set of plasma configurations on the Tokamak {\`a} Configuration Variable1,2, including elongated, conventional shapes, as well as advanced configurations, such as negative triangularity and `snowflake' configurations. Our approach achieves accurate tracking of the location, current and shape for these configurations. We also demonstrate sustained `droplets' on TCV, in which two separate plasmas are maintained simultaneously within the vessel. This represents a notable advance for tokamak feedback control, showing the potential of reinforcement learning to accelerate research in the fusion domain, and is one of the most challenging real-world systems to which reinforcement learning has been applied.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Magnetically confined plasmas,Nuclear fusion and fission},
  annotation = {230 citations (Semantic Scholar/DOI) [2023-05-21]},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2022/Magnetic control of tokamak plasmas through deep reinforcement learning (2022) - Degrave et al.pdf}
}

@misc{hausknecht_deep_2017,
  title = {Deep {{Recurrent Q-Learning}} for {{Partially Observable MDPs}}},
  author = {Hausknecht, Matthew and Stone, Peter},
  year = {2017},
  month = jan,
  number = {arXiv:1507.06527},
  eprint = {1507.06527},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1507.06527},
  urldate = {2023-06-04},
  abstract = {Deep Reinforcement Learning has yielded proficient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the first post-convolutional fully-connected layer with a recurrent LSTM. The resulting {\textbackslash}textit\{Deep Recurrent Q-Network\} (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN's performance on standard Atari games and partially observed equivalents featuring flickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN's performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN's performance degrades less than DQN's. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN's input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  annotation = {1274 citations (Semantic Scholar/arXiv) [2023-06-04]},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2017/Deep Recurrent Q-Learning for Partially Observable MDPs (2017) - Hausknecht, Stone.pdf}
}

@article{lai_asymptotically_1985,
  title = {Asymptotically Efficient Adaptive Allocation Rules},
  author = {Lai, T. L and Robbins, Herbert},
  year = {1985},
  month = mar,
  journal = {Advances in Applied Mathematics},
  volume = {6},
  number = {1},
  pages = {4--22},
  issn = {0196-8858},
  doi = {10.1016/0196-8858(85)90002-8},
  urldate = {2023-10-23},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/1985/Asymptotically efficient adaptive allocation rules (1985) - Lai, Robbins.pdf}
}

@article{mnih_playing_2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin A.},
  year = {2013},
  journal = {CoRR},
  volume = {abs/1312.5602},
  eprint = {1312.5602},
  urldate = {2024-06-21},
  archiveprefix = {arXiv},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2013/Playing Atari with Deep Reinforcement Learning (2013) - Mnih et al.pdf}
}

@book{nielsen_neural_2015,
  title = {Neural {{Networks}} and {{Deep Learning}}},
  author = {Nielsen, Michael A.},
  year = {2015},
  publisher = {Determination Press},
  urldate = {2024-03-10}
}

@book{sutton_reinforcement_2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  lccn = {Q325.6 .R45 2018},
  keywords = {Reinforcement learning},
  file = {/Users/alexandercai/Library/CloudStorage/GoogleDrive-alexcai@college.harvard.edu/My Drive/Vault/papers/assets/2018/Reinforcement learning (2018) - Sutton, Barto.pdf}
}
