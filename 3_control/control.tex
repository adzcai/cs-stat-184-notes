\documentclass[../main/main]{subfiles}

\setcounter{chapter}{2} % one before

\begin{document}
    
\chapter{Linear Quadratic Regulators}


\section{Motivation}

Have you ever tried balancing a pen upright on your palm?
If not, try it! It's a lot harder than seems.
Unlike the cases we studied the previous chapter,
the state and action spaces are not finite or even discrete.
Instead, they are continuous and uncountably infinite.
In addition, the state transitions governing the system -- that is, the laws of physics --
are nonlinear and complex.
Swapping out a pen in your palm for a pole on a cart,
we have the following classic \emph{control problem:}

\begin{example}{CartPole}{cart_pole}
    Consider a pole balanced on a cart.
    The state $s$ consists of just four continuous values:

    \begin{enumerate}
        \item The position of the cart;
        \item The velocity of the cart;
        \item The angle of the pole;
        \item The angular velocity of the pole.
    \end{enumerate}

    \noindent
    We can \emph{control} the cart by applying a horizontal force $a$.
    \footnote{\emph{Controls} are the continuous analogue to \emph{actions} in the discrete setting. In control theory, the state and controls are typically denoted as
    $x$ and $u,$ but we'll stick with the $s$ and $a$ notation to highlight the
    similarity with the discrete case.}

    \noindent
    \textbf{Goal:} Stabilize the cart around an ideal state $s^\star$.
\end{example}

Beyond this simple scenario, there are many real-world examples that involve continuous control:
\begin{itemize}
    \item \textbf{Robotics.} Autonomous driving; Controlling a drone's position; Automation in warehouses and manufacturing; Humanoid robots with joints.
    \item \textbf{Temperature.} Controlling the temperature in a room; Keeping greenhouses warm; Understanding weather patterns.
    \item \textbf{Games.} Sports; MMORPGs (Massively Multiplayer Online Role-Playing Games); Board games.
    \item \textbf{Finance.} Stock trading; Portfolio management; Risk management.
\end{itemize}
How can we teach computers to solve these kinds of problems?

In the last chapter, we developed efficient dynamic programming algorithms (\emph{value iteration} and \emph{policy iteration})
for calculating $V^\star$ and $\pi^\star$ in the finite setting.
In this chapter, we'll derive similar results in the continuous case by imposing
some additional structure on the problem.

Note that we're still assuming that the entire environment is \emph{known} --
that is, we understand `how the world works'. (Hang tight -- we'll get to the
unknown case soon!)

\section{Optimal control}

Recall that an MDP was defined by its state space $\cS$, action space $\cA$, state transitions $P$, reward function $r$, and discount factor $\gamma$ or time horizon $T$. What are the equivalents in the control setting?

\begin{itemize}
    \item The state and action spaces are \emph{continuous} rather than finite. That is, $\cS = \R^{n_s}$ and $\cA = \R^{n_a},$ where $n_s$ and $n_a$ are the number of coordinates to specify a single state or action respectively.
    \item We call the state transitions the \textbf{dynamics} of the system. In the most general case, these might change across timesteps and also include some stochastic \textbf{noise} $w_t$ at each timestep from some distribution $p_{w_t}$. We denote these dynamics as the function $s_{t+1} = f_t(s_t, a_t, w_t).$ Of course, we can simplify to cases where the dynamics are \emph{deterministic/noise-free} (no $w_t$ term) or are \emph{stationary/time-homogenous} (just $f$ without the $t$ subscript).
    \item Instead of a reward function, it's more intuitive to consider a \textbf{cost function} $c_t: \cS \times \cA \to \R$ that describes \emph{how far away} we are from our goal state/action. Like the dynamics, a special case is when this function is time-homogenous (and we drop the $t$ subscript).
    \item We consider the \emph{undiscounted} case with a \emph{time horizon} $T$. Note that we end an episode at $s_T$ -- there is no $a_T,$ and so we denote the cost for the final state as $c_T(s_T).$
\end{itemize}

With all of these components, we can now formulate the \textbf{optimal control problem:} \emph{find a policy (time-dependent) to minimize the expected total cost over $T$ timesteps (undiscounted).}

\begin{definition}{Optimal control problem}{optimal_control}
    \begin{equation}
        \begin{split}
            \min_{\pi_0, \dots, \pi_{T-1} : \cS \to \cA} \quad & \E \left[
                \left( \sum_{t=0}^{T-1} c_t(s_t, a_t) \right) + c_T(s_T)
            \right] \\
            \text{where} \quad & s_{t+1} = f_t(s_t, a_t, w_t), \\
            & a_t = \pi_t(s_t) \\
            & s_0 \sim \mu_0 \\
            & w_t \sim p_{w_t}
        \end{split}
    \end{equation}
\end{definition}

% Note that this is a pretty hard problem the way it's written right now!
% We have some pretty strict constraints in the form of the state transitions.

How does this relate to the finite horizon case?
If $s_t$ and $a_t$ were discrete, then we'd be able to work backwards using
the DP algorithms we saw before.
% In this case, you can also think of $f_t$ as an (uncountably infinite-dimensional matrix.
As a matter of fact, let's consider what happens if we \emph{discretize} the
problem. For intuition, suppose $n_s = n_a = 1$ (that is, states and actions are real numbers).
To make $\cS$ and $\cA$ discrete, let's choose some small positive $\epsilon,$
and simply round states and actions to the nearest multiple of
$\epsilon.$ For example, if $\epsilon = 0.01$, then we're just rounding $s$ and $a$ to two decimal spaces. \footnote{Formally, we can consider an $\epsilon$-net over the original continuous space. Let $V$ be some normed space. A subset $V_\epsilon \subseteq V$ is called an $\epsilon$-net if for all $v \in V,$ there exists a $v_\epsilon \in V_\epsilon$ such that $\|v - v_\epsilon\| \le \epsilon.$ The rounding example given is technically a $0.005$-net.}

If both these state and action spaces can be bounded, then the resulting
sets are actually finite, so now we can use our previous tools for MDPs!
But is this actually a feasible solution? Even if our $\cS$ and $\cA$ are
finite, the existing algorithms might take unfeasibly long to complete.
Suppose our state and action spaces
are bounded by some constants $\max_{s \in \cS} \|s\| \le B_s$ and
$\max_{a \in \cA} \|a\| \le B_a$.
Then using our rounding method, we must divide each dimension into intervals of length $\varepsilon$,
resulting in $(B_s/\varepsilon)^{n_s}$ and $(B_a/\varepsilon)^{n_a}$ points!

To get a sense of how quickly this grows, let's consider $\varepsilon = 0.01, d = k = 10$. Then the number of elements in our transition matrix of size $|\cS|^2 |\cA|$ is on the order of $(100^{10})^2 (100^{10}) = 10^{60}$!
Try finding a computer that'll fit that in memory! (For reference, 32 GB of memory can store $10^9$ 32-bit floating point numbers!)
So as we've seen, discretizing the problem isn't a feasible solution as soon as
our action and state spaces are even moderately high-dimensional.
How can we do better?

Note that in order for discretization to yield and accurate solution,
we implicitly relied assume that rounding each value by some tiny amount $\varepsilon$
wouldn't change the behavior much; namely, that the functions involved are relatively \emph{continuous}.
Can we use this structure in other ways? This brings us to the topic of \textbf{Linear Quadratic Regulators.}

\section{The Linear Quadratic Regulator Problem} \label{sec:lqr}

The optimal control problem stated above seems very difficult to solve!
The cost function might not be convex, and the state transitions might be very complex, making it difficult to satisfy the constraints.
Is there a relevant simplification that we can analyze?

A natural structure to impose is \emph{linear dynamics} and a \emph{quadratic cost function} (in both arguments).
Why is this a useful assumption to make?
Note that given any continuous function, we can locally approximate it using a \emph{Taylor approximation.}
The simplest non-trivial case for the dynamics is a first-order linear approximation,
whereas for the cost function it must be quadratic, since we want it to have an optimum.
This means that even given \emph{nonlinear} dynamics and a \emph{nonlinear} cost function,
as long as they're continuous,
we can write out their first and second order approximations respectively
and use the results that we're about to derive!

These assumptions result in the extremely popular \textbf{linear quadratic regulator} model.
In fact, some people even design systems to be linear in order to use results from LQR!
We'll see later on that even for more complex setups,
we can generalize these simple ideas to get surprisingly good solutions.

% TODO: maybe insert an example of another cost function?


\begin{definition}{The linear quadratic regulator}{lqr}
    \textbf{Linear dynamics} (time-homogeneous): \[
        s_{t+1} = f(s_t, a_t, w_t) = A s_t + B a_t + w_t
    \]

    \noindent \textbf{Quadratic cost function} (time-homogeneous):
    % TODO Link to that video in the multivariable calculus series
    \footnote{For some intuition into this expression, consider the simple case where $a_t$ and $s_t$ are scalars (and so are $Q$ and $R$), so $c(s_t, a_t) = Q s_t^2 + R a_t^2$. If this notation is unfamiliar to you, we recommend this tutorial from Khan Academy!}
    \[
        c(s_t, a_t) = \begin{cases}
            s_t^\top Q s_t + a_t^\top R a_t & t < T \\
            s_T^\top Q s_T & t = T
        \end{cases}
    \]
    We want $c$ to be a convex function, so $Q$ and $R$ must both be positive definite.

    \noindent \textbf{Isotropic Gaussian noise:} \[ w_t \sim \mathcal{N}(0, \sigma^2 I) \]

    \noindent Putting everything together, the optimization problem we want to solve is:
    \begin{align*}
        \min_{\pi_0, \dots, \pi_{T-1} : \cS \to \cA} \quad & \E \left[ \left( \sum_{t=0}^{T-1} s_t^\top Q s_t + a_t^\top R a_t \right) + s_T^\top Q s_T \right] \\
        \textrm{where} \quad & s_{t+1} = A s_t + B a_t + w_t \\
        & a_t = \pi_t (s_t) \\
        & w_t \sim \mathcal{N}(0, \sigma^2 I) \\
        & s_0 \sim \mu_0
    \end{align*}
\end{definition}

\begin{example}{Driving down a road}{road_lqr}
    Suppose we're driving down a road. At each time step, we can choose an
    action $a_t$: either we accelerate and apply a force forward ($a_t > 0$),
    or reverse and apply a force backward ($a_t < 0$).
    Suppose we can choose an action every $\delta$ seconds, and that our car
    has mass $m$.

    Recall that Newtonian mechanics says that $\text{force} = \text{mass} \times \text{acceleration}$. We can write the acceleration as the change in velocity
    over time, and write the velocity as the change in position over time:

    \begin{align*}
        \text{acceleration}_t &= \frac{v_t - v_{t-1}}{\delta} \\
        v_t &= \frac{p_t - p_{t-1}}{\delta} \\
    \end{align*}

    How should we construct our state? We want to express everything in terms
    of these linear dynamics, and we also want our state to be Markov, so that
    we can apply dynamic programming like before.
    Then if we write our state as consisting of the position and velocity, then
    we can write

    \begin{align*}
        p_{t+1} &= p_t  + \delta v_t \\
        v_{t+1} &= v_t + \frac{\delta}{m} a_t
    \end{align*}

    Writing everything out in matrix notation, we get: \[
        s_{t+1} = \begin{bmatrix}
            1 & \delta \\
            0 & 1
        \end{bmatrix} \begin{bmatrix}
            p_t \\ v_t
        \end{bmatrix}
        +
        \begin{bmatrix}
            0 \\ \frac{\delta}{m}
        \end{bmatrix}
        a_t
    \]
\end{example}


Let's derive a more compressed form for the state at time $t$ as a summation
over past time steps. Note that

\begin{align*}
    s_t &= A s_{t-1} + B a_{t-1} + w_{t-1} \\
    &= A (As_{t-2} + B a_{t-2} + w_{t-2}) + B a_{t-1} + w_{t-1} \\
    &= \cdots \\
    &= A^t s_0 + \sum_{i=0}^{t-1} A^i (B a_{t-i-1} + w_{t-i-1})
\end{align*}

Let's consider the expected value of the state at this time.
Since we assume that $\E w_t = 0$ (this is the zero vector in $d$ dimensions),
by linearity of expectation, the $w_t$ term vanishes, and so we're left with \[
    \E [s_t \mid s_{0:(t-1)}, a_{0:(t-1)}] = A^t s_0 + \sum_{i=0}^{t-1} A^i B a_{t-i-1}.
\]

So now we have a good overview of the LQR setting.
How can we define an optimal time-dependent policy in this setting?

It turns out that the optimal policy is one that is deterministic and \emph{linear} at each time
step! That is, \[ \pi_t^\star (s_t) = - K_t s_t. \] We'll prove this more
formally in \autoref{th:optimal_policy_lqr_linear}. This should remind you somewhat of the way in which the optimal policy in
the previous MDP setting was stationary and deterministic. In both cases,
it turns out that the optimal policy has special structure!

Note that the average state at time $t$ for the optimal policy is then \[
    \E [s_t \mid s_0, a_t = - K_t s_t] = \left( \prod_{i=0}^{t-1} (A - B K_i) \right) s_0.
\]
This introdces the quantity $A - B K_i,$ which will show up frequently in
discussions of LQR!
For example, one important question is: will $s_t$ remain bounded,
or will it go to infinity as time goes on?
We can answer this by analyzing this quantity $A - B K_i,$ in particular its
largest eigenvalue.
Intuitively, if we imagine that these $K_i$s are equal (call this matrix $K$),
then this expression looks like $(A-BK)^t s_0.$
Now consider the maximum eigenvalue of $A - BK,$ which we denote as $\lambda_{\max}.$
If $\lambda_{\max} > 1,$ then there's some initial state $s_0^\star$
for which \[ (A - BK)^t s_0^\star = \lambda_{\max}^t s_0^\star \xrightarrow{t \to \infty} \infty. \]

\begin{definition}{Value functions for LQR}{value_lqr}
    Given a policy $\mathbf{\pi} = (\pi_0, \dots, \pi_{t-1}),$ we can define the
    value function $V^\pi_t : \cS \to \R$ as \[ \begin{split}
        V^\pi_t (s) &= \E \left[ \sum_{i=t}^{T} c(s_i, a_i) \right] \\
        &= \E \left[ \left( \sum_{i=t}^{T-1} s_i^\top Q s_i + a_i^\top R a_i \right) + s_T^\top Q s_T \right] \\
        \textrm{where} \quad s_t &= s \\
        a_i &= \pi_i(s_i) \quad \forall i \ge t.
    \end{split} \]
    We call this expression inside the equation the \textbf{cost-to-go},
    since it's just the total cost starting from timestep $t$.

    Similarly, the $Q$ function just additionally conditions on the first
    action we take: \[ \begin{split}
        Q^\pi_t (s, a) &= \E \left[ \sum_{i=t}^{T} c(s_i, a_i) \right] \\
        &= \E \left[ \left( \sum_{i=t}^{T-1} s_i^\top Q s_i + a_i^\top R a_i \right) + s_T^\top Q s_T \right] \\
        \textrm{where} \quad (s_t, a_t) &= (s, a) \\
        \quad a_i &= \pi_i(s_i) \quad \forall i > t
    \end{split} \]
\end{definition}

As it turns out, we can now solve for the optimal policy $\pi$ via dynamic
programming in terms of these value and action-value functions.

\section{Optimality for LQR}

\begin{definition}{Optimal value functions for LQR}{optimal_value_lqr}
    The optimal value function is the one that, in all states and across all timesteps,
    achieves \emph{lowest cost} across all policies: \[
        \begin{split}
            V^\star_t(s) &= \min_\pi V^\pi_t(s) \\
            &= \min_{\pi_{t:T-1}} \E \left[ \left( \sum_{i=t}^{T-1} s_t^\top Q s_t + a_t^\top R a_t \right) + s_T^\top Q s_T \right] \\
            \textrm{where} \quad a_i &= \pi_i(s_i) \quad \forall i \ge t \\
            s_t &= s
        \end{split}
    \]
    Additionally, we'll show theorems \ref{th:optimal_value_lqr_quadratic} and \ref{th:optimal_policy_lqr_linear} below,
    showing that the $V^\star_t$ is quadratic and that $\pi^\star_t$ is linear.
    Then, we'll show how to calculate the actual coefficients that specify these
    functions.
\end{definition}


\begin{theorem}{$V^\star_t$ in LQR is a quadratic function}{optimal_value_lqr_quadratic}
    Formally, we claim that \[
        V^\star_t(s) = s^\top P_t s + p_t
    \]
    for some $P_t \in \R^{d \times d}$ and $p_t \in \R^d$ where $P_t$ is positive-definite.
    Note that this doesn't have a linear term,
    just a quadratic term plus a constant.
\end{theorem}


\begin{theorem}{Optimal policy in LQR is linear}{optimal_policy_lqr_linear}
    That is, \[
        \pi^\star_t (s) = - K_t s
    \]
    for some $K_t \in \R^{k \times d}.$ (The negative is just there by convention.)
\end{theorem}


We'll derive these theorems by induction, starting from the last timestep and
working backwards in time. Note that induction has a very fundamental connection
with dynamic programming: our inductive proof will naturally lend itself to a DP
algorithm that allows us to calculate the optimal value and policy!

\textbf{Base case:} $V^\star_T(s)$ is quadratic.

\textbf{Inductive hypothesis:} Show that if $V^\star_{t+1}(s)$ is quadratic, then:
\begin{enumerate}
    \item $Q^\star_t(s, a)$ is quadratic (in both $s$ and $a$)
    \item Derive the optimal policy $\pi^\star_t(s) = \arg \min_a Q^\star_t(s, a)$, and show that it's linear.
    \item Show $V^\star_t(s)$ is quadratic.
\end{enumerate}

Finally, this will have shown that $V^\star_t(s)$ is quadratic and $\pi^\star_t(s)$ is linear.

This is essentially the same proof as a finite-horizon MDP,
except that now the state and action are \emph{continuous} instead of finite.

\textbf{Base case.} Let's start by considering the final timestep $V^\pi_T,$ for some policy $\pi.$
Then the only expression is \[
    V^\star_T(s) = s^\top Q s,
\]
which is quadratic, as we desired. Pattern-matching to the expression from earlier,
we see that $P_T = Q$ and $p_t = 0.$


\textbf{Inductive step.} Assume $V^\star_{t+1}(s) = s^\top P_{t+1} s + p_{t+1}$
for all states $s.$ We'll start off by demonstrating that $Q^\star_t(s)$ is quadratic.
Recall that the definition of $Q^\star_t : \cS \times \cA \to \R$ is \[
    Q^\star_t(s, a) = c(s, a) + \E_{s' \sim f(s, a, w_{t+1})} V^\star_{t+1}(s').
\]
We know $c(s, a) := s^\top Q s + a^\top R a.$ Let's consider the average value
over the next timestep. The only randomness in the dynamics comes from the noise
$w_{t+1}$, so we can write out this expected value as:
\begin{align*}
    \E_{s' \sim f(s, a, w_{t+1})} V^\star_{t+1}(s') &= \E_{w_{t+1} \sim \cN(0, \sigma^2 I)} V^\star_{t+1}(As + Ba + w_{t+1}) \\
    &= \E_{w_{t+1}} [ (As + Ba + w_{t+1})^\top P_{t+1} (As + Ba + w_{t+1}) + p_{t+1} ].
\end{align*}
Summing these two expressions and combining like terms, we get \begin{align*}
    Q^\star_t(s, a) &= s^\top Q s + a^\top R a + \E_{w_{t+1}} [(As + Ba + w_{t+1})^\top P_{t+1} (As + Ba + w_{t+1}) + p_{t+1}] \\
    &= s^\top (Q + A^\top P_{t+1} A)s + a^\top (R + B^\top P_{t+1} B) a + 2 s^\top A^\top P_{t+1} B a + p_{t+1} \\
    &\qquad \E_{w_{t+1}} w_{t+1}^\top P_{t+1} w_{t+1}.
\end{align*}
Now consider this last term. By writing out the product and using linearity of
expectation, we can write this out as \begin{align*}
    \E_{w_{t+1}} w_{t+1}^\top P_{t+1} w_{t+1} &= \sum_{i=1}^d \sum_{j=1}^d (P_{t+1})_{i, j} \E_{w_{t+1}} [(w_{t+1})_i (w_{t+1})_j].
\end{align*}
When dealing with these quadratic forms, it's often helpful to consider the terms
on the diagonal separately from those off the diagonal. On the diagonal, the
expectation becomes $\E (w_{t+1})_i^2 = \var \big((w_{t+1})_i \big) = \sigma^2.$
Off the diagonal, since the elements of $w_{t+1}$ are independent,
the expectation factors into $\E (w_{t+1})_i \E (w_{t+1})_j = 0.$
Thus, the only terms left are the ones on the diagonal, so the sum of these can
be expressed as the trace of $\sigma^2 P_{t+1}$: \[
    \E_{w_{t+1}} w_{t+1}^\top P_{t+1} w_{t+1} = \tr(\sigma^2 P_{t+1}).
\]
Substituting this back into the expression for $Q^\star_t,$ we have:

\begin{theorem}{Optimal Q-Function in LQR}{q_star_lqr}
    \[
        Q^\star_t(s, a) = s^\top (Q + A^\top P_{t+1} A) s + a^\top (R + B^\top P_{t+1} B) a
        + 2s^\top A^\top P_{t+1} B a + \tr(\sigma^2 P_{t+1}) + p_{t+1}.
    \]
\end{theorem}

As we'd hoped, this expression is quadratic in $s$ and $a$! (Phew!)

Now let's move on to the next part of the next part of proving the inductive hypothesis:
showing that $\pi^\star_t(s) = \arg \min_a Q^\star_t(s, a)$ is linear.
This becomes easy if $Q^\star_t$ is convex w.r.t. $a\dots$ Which it is!

\begin{theorem}{$Q^\star_t$ is convex in $a$}{optimal_q_convex}
    Consider the part of \autoref{th:q_star_lqr} that is quadratic in $a,$ namely
    $a^\top (R + B^\top P_{t+1} B) a.$ Then $Q^\star_t$ is convex w.r.t. $a$
    if $R + B^\top P_{t+1} B$ is positive definite.
    
    To show this, recall that in our definition of LQR,
    we assumed that $R$ is positive definite (see \autoref{df:lqr}).
    Also note that $B^\top P_{t+1} B$ is symmetric, and therefore positive definite.
    Since the sum of two positive-definite matrices is also positive-definite,
    we have that $R + B^\top P_{t+1} B$ is positive-definite,
    and so $Q^\star_t$ is convex w.r.t. $a.$
\end{theorem}

This means that finding the minimum is easy:
we can just take the gradient w.r.t. $a$ and set it to zero!
First, we calculate the gradient:
\begin{align*}
    \nabla_a Q^\star_t(s, a) &= \nabla_a [ a^\top (R + B^\top P_{t+1} B) a + 2 s^\top A^\top P_{t+1} B a ] \\
    &= 2 (R + B^\top P_{t+1} B) a + (2 s^\top A^\top P_{t+1} B)^\top
\end{align*}
Setting this to zero, we get \begin{align}
    0 &= (R + B^\top P_{t+1} B) a + B^\top P_{t+1} A s \nonumber \\
    \pi^\star_t(s) := a &= -(R + B^\top P_{t+1} B)^{-1} B^\top P_{t+1} A s \nonumber \\
    &= - K_t s,
\end{align}
where $K_t = (R + B^\top P_{t+1} B)^{-1} B^\top P_{t+1} A.$

We're now almost there! To complete our inductive proof, we must show that the
inductive hypothesis is true at time $t$; that is, we must prove that
$V^\star_t(s)$ is quadratic.
Using the identity $V^\star_t(s) = Q^\star_t(s, \pi^\star(s)),$ we have: \begin{align*}
    V^\star_t(s) &= Q^\star_t(s, \pi^\star(s)) \\
    &= s^\top (Q + A^\top P_{t+1} A) s + (-K_t s)^\top (R + B^\top P_{t+1} B) (-K_t s)
    + 2s^\top A^\top P_{t+1} B (-K_t s) \\
        &\qquad + \tr(\sigma^2 P_{t+1}) + p_{t+1}
\end{align*}
Note that w.r.t. $s,$ this is the sum of a quadratic term and a constant,
which is exactly what we were aiming for!
The constant term is clearly $p_t = \tr(\sigma^2 P_{t+1}) + p_{t+1}.$
We can simplify the quadratic term by substituting in $K_t.$
Notice that when we do this, the $(R+B^\top P_{t+1} B)$ term in
the expression is cancelled out by its inverse, and the remaining terms combine to give
what is known as the \emph{Ricatti equation:}
\begin{theorem}{Ricatti equation}{ricatti}
    \[
        P_t = Q + A^\top P_{t+1} A - A^\top P_{t+1} B (R + B^\top P_{t+1} B)^{-1} B^\top P_{t+1} A.
    \]
\end{theorem}
There are several nice things to note about this expression:
\begin{enumerate}
    \item It's defined recursively; Given $P_T, A, B,$ and the state coefficients $Q,$ we can
    recursively calculate all values of $P_t$ across timesteps.
    \item It appears frequently in calculations surrounding optimality,
    such as in $V^\star$ and $Q^\star$.
    \item Together with $A, B,$ and the action coefficients $R,$ it fully defines the optimal policy.
\end{enumerate}
The optimal policy also has some interesting properties: in addition to being
independent of the starting distribution $\mu_0,$ which also happened for our
finite-horizon MDP solution, it's fully deterministic and doesn't depend on any noise!
(Compare this with the discrete MDP case, where calculating our optimal policy
required taking an expectation over the state transitions.)


\section{Time-dependent dynamics} \label{sec:time_dep}


We've closely studied the standard case of LQR, where the state and action spaces are both
continuous, the dynamics are linear and time-homogeneous, and the cost is quadratic.
We can also consider situations where some, or all, of these assumptions are relaxed.

So far, we've considered the \emph{time-homogeneous} case, where the dynamics are the same at every timestep. We can also loosen this restriction, and consider the case where the dynamics are \emph{time-dependent.} Our analysis
remains almost the same, except we just add a time index to each of the relevant matrices.

The problem is now defined as follows:
\begin{align*}
    \arg \min_{\pi_{0:T-1} : \cS \to \cA} \quad & \E \left[ \left( \sum_{t=0}^{T-1} (s_t^\top Q_t s_t) + a_t^\top R_t a_t \right) + s_T^\top Q_T s_T \right] \\
    \textrm{where} \quad s_{t+1} = f_t(s_t, a_t, w_t) &= A_t s_t + B_t a_t + w_t \\
    s_0 &\sum \mu_0 \\
    a_t &= \pi_t (s_t) \\
    w_t &\sim \mathcal{N}(0, \sigma^2 I).
\end{align*}
The derivation remains almost exactly the same, and so the Ricatti equation then
becomes the \emph{time-dependent Ricatti equation:}

\begin{theorem}{Time-dependent Ricatti Equation}{ricatti_time_dependent}
    \[
        P_t = Q_t + A_t^\top P_{t+1} A_t - A_t^\top P_{t+1} B_t (R_t + B_t^\top P_{t+1} B_t)^{-1} B_t^\top P_{t+1} A_t.
    \]

    Note that this is just \autoref{th:ricatti}, but with the
    time index added to each of the matrices.
\end{theorem}

Additionally, by allowing the dynamics to vary across time,
this gives us a \emph{locally linear} approximation of nonlinear dynamics at each timestep!
We'll discuss this later in the chapter.

\section{More general quadratic cost functions}

Our original cost function had only second-order terms w.r.t. the state and action. We can also consider more general quadratic cost functions that also have first-order terms and a constant term. Combining this with
time-dependent dynamics, we get the following expression
\[
    c_t(s_t, a_t) = ( s_t^\top Q_t s_t + s_t^\top M_t a_t + a_t^\top R_t a_t ) + (s_t^\top q_t + a_t^\top r_t) + c_t.
\]
We can also include a constant term $v_t \in \R^d$ in the dynamics:
\[
    s_{t+1} = f_t(s_t, a_t, w_t) = A_t s_t + B_t a_t + v_t + w_t.
\]


\section{Tracking a predefined trajectory}

So far, we've been trying to get the robot to stay as close as possible to
a single point $(s^\star, a^\star).$ We can also consider the case where we
want the robot to follow a predefined trajectory, which is a sequence of
states and actions $(s_t^\star, a_t^\star)_{t = 1}^T$. In this case, we can use the following cost function:
\[
    c_t(s_t, a_t) = (s_t - s^\star_t)^\top Q (s_t - s^\star_t) + (a_t - a^\star_t)^\top R (a_t - a^\star_t).
\]
By expanding out these multiplication, we can see that this is actually a special case of the more general quadratic cost function we discussed
above: \[
    M_t = 0, \qquad q_t = -2Q s^\star_t, \qquad r_t = -2R a^\star_t, \qquad c_t = (s^\star_t)^\top Q (s^\star_t) + (a^\star_t)^\top R (a^\star_t).
\]


\section{Infinite-horizon}

So far, we've been considering finite-horizon problems, where we have a fixed number of timesteps $T$. We can also consider the case where we have an infinite horizon, and so we want to minimize the expected cost over all future timesteps.
Additionally, we'll compare this \emph{undiscounted finite-horizon} case to the \emph{discounted infinite-horizon} case, where we have a discount factor $\gamma \in [0, 1]$ that discounts future rewards.

Instead of considering the \emph{total} expected cost, which is going to diverge as time goes on,
we'll consider the average cost per timestep. That is, we'll divide our objective
function by $T,$ the number of timesteps.
What happens is that the recursion in our Ricatti equation (the recursive equation
for $P_t$) converges to a fixed value $P$.

From an intuitive perspective of why this happens, let's suppose you have an upcoming project deadline. When it's still a few months away, you might not pay
much attention to it. But as the deadline gets closer and closer, suddenly the
horizon becomes clear, and you'll spend more time thinking about it.
Now the infinite-horizon case is just where the deadline is infinitely far away, so you can just ``behave'' like normal!

This ties in exactly with our analysis of value iteration, which gave us a sequence of policies,
and then we just took the final policy.
We're doing the same thing in the large-horizon setting, but note that here there's
no discount term $\gamma$; the structure of the problem allows us to analyze it even without one.

Note that several parts of this proof are very similar to our derivations of
value iteration and policy iteration in the previous chapter.
Let's spend some time on formalizing this connection.

In the discounted case, instead of considering the limit as $T \to \infty,$ we consider the limit as $\gamma \to 1.$ This is because as the discount factor $\gamma$ approaches $1,$ time discounting becomes less and less important, and we're left with the undiscounted case. Just like above, this means we need to
multiply our cost function by $1 - \gamma,$ and then we can use the same analysis as before.

Let's consider value iteration, which uses the Bellman operator to update $V$: \[
    V_{t+1}(s) = \max_a \left( r(s, a) + \gamma \E_{s' \sim P(s, a)} V_t(s') \right).
\]
The analogue of this in the undiscounted finite-horizon case is the \emph{Ricatti equation} (\autoref{th:ricatti}).
Instead of thinking of $P_{t+1}$ as defining the value function for the \emph{next timestep,} though, let's think of it as the \emph{next version} of the value function.

% TODO finish this up

\begin{figure}[h]
    \centering
    \begin{tabular}{c|c}
        \textbf{Control} & \textbf{MDP} \\
        \hline
        Finite-horizon & Infinite-horizon \\
        Continuous states and actions & Finite states and actions \\
        Total undiscounted cost & Total discounted reward \\
        $\displaystyle \lim_{T \to \infty} \frac{1}{T} \E \left[ \left( \sum_{t=0}^{T-1} c(s_t, a_t) \right) + c(s_T) \right]$ & $\lim_{\gamma \to 1} \displaystyle (1 - \gamma) \E \left[ \sum_{t=0}^{\infty} \gamma^t c(s_t, a_t) \right]$ \\
        Ricatti equations & Value iteration \\
        $\displaystyle P \gets Q + A^\top P A - A^\top P B (R + B^\top P B)^{-1} B^\top P A$ & $\displaystyle V(s) \gets \max_a \left[ r(s, a) + \gamma \E_{s' \sim P(s, a)} V(s') \right]$
    \end{tabular}
    \caption{Comparison of control and finite MDPs.}
    \label{fig:control_mdp}
\end{figure}


\section{Approximating nonlinear dynamics with LQR}

Let's return to the CartPole example (\ref{eg:cart_pole}) from the start of the chapter. We want to stabilize the pole around some
optimal state and action $(s^\star, a^\star).$
The `Q' in LQR stands for ``quadratic cost,'' so previously
we've modelled the cost as $c(s, a) = (s - s^\star)^\top Q (s - s^\star) + (a - a^\star)^\top R (a - a^\star).$
Here, let's relax those constraints and consider some more
general measure of distance to the optimal state and action: \[
    c(s, a) = d(s, s^\star) + d(a, a^\star).
\]
We'll consider the noise-free setting, since as we previously saw,
the noise doesn't actually affect the optimal policy.

This is essentially just a general control problem (see \ref{df:optimal_control}). We can use LQR to solve it, but we'll need to approximate the dynamics of the system.
Here, we don't know the dynamics $f$ or the cost function $c$,
but we suppose that we're able to \emph{query/sample/simulate} them
to get their values at a given state and action.

We also assume that $f$ is differentiable and that $c$ is twice-differentiable. This makes sense since we want to approximate
$f$ as linear and $c$ as quadratic so that we can apply LQR.
For our approximation to be accurate, we need to make sure that all states are close to the optimal state $s^\star,$
and we can stay close using actions that are close to $a^\star.$

% TODO: insert figure here for visual intuition, in case people aren't experienced with the calculus



\end{document}


