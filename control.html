<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2&nbsp; Linear Quadratic Regulators – CS 1840: Introduction to Reinforcement Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./bandits.html" rel="next">
<link href="./mdps.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./control.html"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CS 1840: Introduction to Reinforcement Learning</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./mdps.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./control.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./bandits.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./supervised_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Supervised learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./fitted_dp.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Fitted Dynamic Programming Algorithms</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./pg.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Policy Gradient Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./imitation_learning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Imitation Learning</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./planning.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Tree Search Methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./exploration.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Exploration in MDPs</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./background.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Appendix: Background</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">2.1</span> Introduction</a></li>
  <li><a href="#optimal-control" id="toc-optimal-control" class="nav-link" data-scroll-target="#optimal-control"><span class="header-section-number">2.2</span> Optimal control</a>
  <ul class="collapse">
  <li><a href="#a-first-attempt-discretization" id="toc-a-first-attempt-discretization" class="nav-link" data-scroll-target="#a-first-attempt-discretization"><span class="header-section-number">2.2.1</span> A first attempt: Discretization</a></li>
  </ul></li>
  <li><a href="#sec-lqr" id="toc-sec-lqr" class="nav-link" data-scroll-target="#sec-lqr"><span class="header-section-number">2.3</span> The Linear Quadratic Regulator</a></li>
  <li><a href="#sec-optimal-lqr" id="toc-sec-optimal-lqr" class="nav-link" data-scroll-target="#sec-optimal-lqr"><span class="header-section-number">2.4</span> Optimality and the Riccati Equation</a>
  <ul class="collapse">
  <li><a href="#expected-state-at-time-h" id="toc-expected-state-at-time-h" class="nav-link" data-scroll-target="#expected-state-at-time-h"><span class="header-section-number">2.4.1</span> Expected state at time <span class="math inline">\(h\)</span></a></li>
  </ul></li>
  <li><a href="#extensions" id="toc-extensions" class="nav-link" data-scroll-target="#extensions"><span class="header-section-number">2.5</span> Extensions</a>
  <ul class="collapse">
  <li><a href="#sec-time-dep-lqr" id="toc-sec-time-dep-lqr" class="nav-link" data-scroll-target="#sec-time-dep-lqr"><span class="header-section-number">2.5.1</span> Time-dependent dynamics and cost function</a></li>
  <li><a href="#more-general-quadratic-cost-functions" id="toc-more-general-quadratic-cost-functions" class="nav-link" data-scroll-target="#more-general-quadratic-cost-functions"><span class="header-section-number">2.5.2</span> More general quadratic cost functions</a></li>
  <li><a href="#tracking-a-predefined-trajectory" id="toc-tracking-a-predefined-trajectory" class="nav-link" data-scroll-target="#tracking-a-predefined-trajectory"><span class="header-section-number">2.5.3</span> Tracking a predefined trajectory</a></li>
  </ul></li>
  <li><a href="#sec-approx-nonlinear" id="toc-sec-approx-nonlinear" class="nav-link" data-scroll-target="#sec-approx-nonlinear"><span class="header-section-number">2.6</span> Approximating nonlinear dynamics</a>
  <ul class="collapse">
  <li><a href="#local-linearization" id="toc-local-linearization" class="nav-link" data-scroll-target="#local-linearization"><span class="header-section-number">2.6.1</span> Local linearization</a></li>
  <li><a href="#finite-differencing" id="toc-finite-differencing" class="nav-link" data-scroll-target="#finite-differencing"><span class="header-section-number">2.6.2</span> Finite differencing</a></li>
  <li><a href="#local-convexification" id="toc-local-convexification" class="nav-link" data-scroll-target="#local-convexification"><span class="header-section-number">2.6.3</span> Local convexification</a></li>
  <li><a href="#sec-iterative-lqr" id="toc-sec-iterative-lqr" class="nav-link" data-scroll-target="#sec-iterative-lqr"><span class="header-section-number">2.6.4</span> Iterative LQR</a></li>
  </ul></li>
  <li><a href="#summary" id="toc-summary" class="nav-link" data-scroll-target="#summary"><span class="header-section-number">2.7</span> Summary</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-control" class="quarto-section-identifier"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Linear Quadratic Regulators</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">

</div>
<div class="hidden">

</div>
<section id="introduction" class="level2" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">2.1</span> Introduction</h2>
<p>Up to this point, we have considered decision problems with finitely many states and actions. However, in many applications, states and actions may take on continuous values. For example, consider autonomous driving, controlling a robot’s joints, and automated manufacturing. How can we teach computers to solve these kinds of problems? This is the task of <strong>continuous control</strong>.</p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/rubiks_cube.jpg" class="img-fluid figure-img"></p>
<figcaption>Solving a Rubik’s Cube with a robot hand</figcaption>
</figure>
</div>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/boston_dynamics.jpg" class="img-fluid figure-img"></p>
<figcaption>Boston Dynamics’s Spot robot</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Aside from the change in the state and action spaces, the general problem setup remains the same: we seek to construct an <em>optimal policy</em> that outputs actions to solve the desired task. We will see that many key ideas and algorithms, in particular dynamic programming algorithms, carry over to this new setting.</p>
<p>This chapter introduces a fundamental tool to solve a simple class of continuous control problems: the <strong>linear quadratic regulator</strong>. We will then extend this basic method to more complex settings.</p>
<div id="exm-cart-pole" class="theorem example">
<p><span class="theorem-title"><strong>Example 2.1 (CartPole)</strong></span> Try to balance a pencil on its point on a flat surface. It’s much more difficult than it may first seem: the position of the pencil varies continuously, and the state transitions governing the system, i.e.&nbsp;the laws of physics, are highly complex. This task is equivalent to the classic control problem known as <em>CartPole</em>:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/cart_pole.png" class="img-fluid figure-img" width="200"></p>
<figcaption>Cart pole</figcaption>
</figure>
</div>
<p>The state <span class="math inline">\(s\in \mathbb{R}^4\)</span> can be described by:</p>
<ol type="1">
<li><p>the position of the cart;</p></li>
<li><p>the velocity of the cart;</p></li>
<li><p>the angle of the pole;</p></li>
<li><p>the angular velocity of the pole.</p></li>
</ol>
<p>We can <em>control</em> the cart by applying a horizontal force <span class="math inline">\(a\in \mathbb{R}\)</span>.</p>
<p><strong>Goal:</strong> Stabilize the cart around an ideal state and action <span class="math inline">\((s^\star, a^\star)\)</span>.</p>
</div>
</section>
<section id="optimal-control" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="optimal-control"><span class="header-section-number">2.2</span> Optimal control</h2>
<p>Recall that an MDP is defined by its state space <span class="math inline">\(\mathcal{S}\)</span>, action space <span class="math inline">\(\mathcal{A}\)</span>, state transitions <span class="math inline">\(P\)</span>, reward function <span class="math inline">\(r\)</span>, and discount factor <span class="math inline">\(\gamma\)</span> or time horizon <span class="math inline">\(H\)</span>. These have equivalents in the control setting:</p>
<ul>
<li><p>The state and action spaces are <em>continuous</em> rather than finite. That is, <span class="math inline">\(\mathcal{S} \subseteq \mathbb{R}^{n_s}\)</span> and <span class="math inline">\(\mathcal{A} \subseteq \mathbb{R}^{n_a}\)</span>, where <span class="math inline">\(n_s\)</span> and <span class="math inline">\(n_a\)</span> are the corresponding dimensions of these spaces, i.e.&nbsp;the number of coordinates to specify a single state or action respectively.</p></li>
<li><p>We call the state transitions the <strong>dynamics</strong> of the system. In the most general case, these might change across timesteps and also include some stochastic <strong>noise</strong> <span class="math inline">\(w_h\)</span> at each timestep. We denote these dynamics as the function <span class="math inline">\(f_h\)</span> such that <span class="math inline">\(s_{h+1} = f_h(s_h, a_h, w_h)\)</span>. Of course, we can simplify to cases where the dynamics are <em>deterministic/noise-free</em> (no <span class="math inline">\(w_h\)</span> term) and/or <em>time-homogeneous</em> (the same function <span class="math inline">\(f\)</span> across timesteps).</p></li>
<li><p>Instead of maximizing the reward function, we seek to minimize the <strong>cost function</strong> <span class="math inline">\(c_h: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span>. Often, the cost function describes <em>how far away</em> we are from a <strong>target state-action pair</strong> <span class="math inline">\((s^\star, a^\star)\)</span>. An important special case is when the cost is <em>time-homogeneous</em>; that is, it remains the same function <span class="math inline">\(c\)</span> at each timestep <span class="math inline">\(h\)</span>.</p></li>
<li><p>We seek to minimize the <em>undiscounted</em> cost within a <em>finite time horizon</em> <span class="math inline">\(H\)</span>. Note that we end an episode at the final state <span class="math inline">\(s_H\)</span> – there is no <span class="math inline">\(a_H\)</span>, and so we denote the cost for the final state as <span class="math inline">\(c_H(s_H)\)</span>.</p></li>
</ul>
<p>With all of these components, we can now formulate the <strong>optimal control problem:</strong> <em>compute a policy to minimize the expected undiscounted cost over <span class="math inline">\(H\)</span> timesteps.</em> In this chapter, we will only consider <em>deterministic, time-dependent</em> policies <span class="math inline">\(\pi = (\pi_0, \dots, \pi_{H-1})\)</span> where <span class="math inline">\(\pi_h : \mathcal{S} \to \mathcal{A}\)</span> for each <span class="math inline">\(h\in [H]\)</span>.</p>
<div id="def-optimal-control" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.1 (General optimal control problem)</strong></span> <span class="math display">\[
\begin{split}
    \min_{\pi_0, \dots, \pi_{H-1} : \mathcal{S} \to \mathcal{A}} \quad &amp; \mathbb{E}\left[
        \left( \sum_{h=0}^{H-1} c_h(s_h, a_h) \right) + c_H(s_H)
        \right] \\
    \text{where} \quad &amp; s_{h+1} = f_h(s_h, a_h, w_h), \\
    &amp; a_h= \pi_h(s_h) \\
    &amp; s_0 \sim \mu_0 \\
    &amp; w_h\sim \text{noise}
\end{split}
\]</span></p>
</div>
<section id="a-first-attempt-discretization" class="level3" data-number="2.2.1">
<h3 data-number="2.2.1" class="anchored" data-anchor-id="a-first-attempt-discretization"><span class="header-section-number">2.2.1</span> A first attempt: Discretization</h3>
<p>Can we solve this problem using tools from the finite MDP setting? If <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> were finite, then we’d be able to work backwards using the DP algorithm for computing the optimal policy in an MDP (<a href="mdps.html#def-pi-star-dp" class="quarto-xref">Definition&nbsp;<span>1.11</span></a>). This inspires us to try <em>discretizing</em> the problem.</p>
<p>Suppose <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> are bounded, that is, <span class="math inline">\(\max_{s\in \mathcal{S}} \|s\| \le B_s\)</span> and <span class="math inline">\(\max_{a\in \mathcal{A}} \|a\| \le B_a\)</span>. To make <span class="math inline">\(\mathcal{S}\)</span> and <span class="math inline">\(\mathcal{A}\)</span> finite, let’s choose some small positive <span class="math inline">\(\epsilon\)</span>, and simply round each coordinate to the nearest multiple of <span class="math inline">\(\epsilon\)</span>. For example, if <span class="math inline">\(\epsilon = 0.01\)</span>, then we round each element of <span class="math inline">\(s\)</span> and <span class="math inline">\(a\)</span> to two decimal spaces.</p>
<p>However, the discretized <span class="math inline">\(\widetilde{\mathcal{S}}\)</span> and <span class="math inline">\(\widetilde{\mathcal{A}}\)</span> may be finite, but they may be infeasibly large: we must divide <em>each dimension</em> into intervals of length <span class="math inline">\(\varepsilon\)</span>, resulting in <span class="math inline">\(|\widetilde{\mathcal{S}}| = (B_s/\varepsilon)^{n_s}\)</span> and <span class="math inline">\(|\widetilde{\mathcal{A}}| = (B_a/\varepsilon)^{n_a}\)</span>. To get a sense of how quickly this grows, consider <span class="math inline">\(\varepsilon = 0.01, n_s= n_a= 10\)</span>. Then the number of elements in the transition matrix would be <span class="math inline">\(|\widetilde{\mathcal{S}}|^2 |\widetilde{\mathcal{A}}| = (100^{10})^2 (100^{10}) = 10^{60}\)</span>! (That’s a trillion trillion trillion trillion trillion.)</p>
<p>What properties of the problem could we instead make use of? Note that by discretizing the state and action spaces, we implicitly assumed that rounding each state or action vector by some tiny amount <span class="math inline">\(\varepsilon\)</span> wouldn’t change the behavior of the system by much; namely, that the cost and dynamics were relatively <em>continuous</em>. Can we use this continuous structure in other ways? This leads us to the <strong>linear quadratic regulator</strong>.</p>
</section>
</section>
<section id="sec-lqr" class="level2" data-number="2.3">
<h2 data-number="2.3" class="anchored" data-anchor-id="sec-lqr"><span class="header-section-number">2.3</span> The Linear Quadratic Regulator</h2>
<p>The optimal control problem <a href="#def-optimal-control" class="quarto-xref">Definition&nbsp;<span>2.1</span></a> seems highly complex in general. Is there a relevant simplification that we can analyze? The <strong>linear quadratic regulator</strong> (LQR) is a solvable case and a fundamental tool in control theory.</p>
<div id="def-lqr-definition" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.2 (The linear quadratic regulator)</strong></span> The LQR problem is a special case of the <a href="#def-optimal-control" class="quarto-xref">Definition&nbsp;<span>2.1</span></a> with <em>linear dynamics</em> and an <em>upward-curved quadratic cost function</em>. Solving the LQR problem will additionally enable us to <em>locally approximate</em> more complex setups using <em>Taylor approximations</em>.</p>
<p><strong>Linear, time-homogeneous dynamics</strong>: for each timestep <span class="math inline">\(h\in [H]\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
    s_{h+1} &amp;= f(s_h, a_h, w_h) = A s_h+ B a_h+ w_h\\
    \text{where } w_h&amp;\sim \mathcal{N}(0, \sigma^2 I).
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(w_h\)</span> is a spherical Gaussian <strong>noise term</strong> that makes the dynamics random. Setting <span class="math inline">\(\sigma = 0\)</span> gives us <strong>deterministic</strong> state transitions. We will find that the optimal policy actually <em>does not depend on the noise</em>, although the optimal value function and Q-function do.</p>
<p><strong>Upward-curved quadratic, time-homogeneous cost function</strong>:</p>
<p><span class="math display">\[
c(s_h, a_h) = \begin{cases}
    s_h^\top Q s_h+ a_h^\top R a_h&amp; h&lt; H\\
    s_h^\top Q s_h&amp; h= H
\end{cases}.
\]</span></p>
<p>This cost function attempts to stabilize the state and action about <span class="math inline">\((s^\star, a^\star) = (0, 0)\)</span>. We require <span class="math inline">\(Q \in \mathbb{R}^{n_s\times n_s}\)</span> and <span class="math inline">\(R \in \mathbb{R}^{n_a\times n_a}\)</span> to both be <em>positive definite</em> matrices so that <span class="math inline">\(c\)</span> has a well-defined unique minimum. We can furthermore assume without loss of generality that they are both <em>symmetric</em> (see exercise below).</p>
<p>This results in the LQR optimization problem:</p>
<p><span class="math display">\[
\begin{aligned}
        \min_{\pi_0, \dots, \pi_{H-1} : \mathcal{S} \to \mathcal{A}} \quad &amp; \mathbb{E}\left[ \left( \sum_{h=0}^{H-1} s_h^\top Q s_h+ a_h^\top R a_h\right) + s_H^\top Q s_H\right] \\
        \textrm{where} \quad                                &amp; s_{h+1} = A s_h+ B a_h+ w_h\\
                                                            &amp; a_h= \pi_h(s_h)                                                                                                        \\
                                                            &amp; w_h\sim \mathcal{N}(0, \sigma^2 I)                                                                                               \\
                                                            &amp; s_0 \sim \mu_0.
\end{aligned}
\]</span></p>
</div>
<div id="exr-symmetric-q-r" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.1 (Symmetric <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>)</strong></span> Here we’ll show that we don’t lose generality by assuming that <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> are symmetric. Show that replacing <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> with <span class="math inline">\((Q + Q^\top) / 2\)</span> and <span class="math inline">\((R + R^\top) / 2\)</span> (which are symmetric) yields the same cost function.</p>
</div>
<p>We will henceforth abbreviate “symmetric positive definite” as s.p.d. and “positive definite” as p.d.</p>
<p>It will be helpful to reintroduce the <em>value function</em> notation for a policy to denote the average cost it incurs. These will be instrumental in constructing the optimal policy via <strong>dynamic programming,</strong> as we did in <a href="mdps.html#sec-opt-dynamic-programming" class="quarto-xref"><span>Section 1.2.7</span></a> for MDPs.</p>
<div id="def-value-lqr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.3 (Value functions for LQR)</strong></span> Given a policy <span class="math inline">\(\mathbf{\pi} = (\pi_0, \dots, \pi_{H-1})\)</span>, we can define its value function <span class="math inline">\(V^\pi_h: \mathcal{S} \to \mathbb{R}\)</span> at time <span class="math inline">\(h\in [H]\)</span> as the average <strong>cost-to-go</strong> incurred by that policy:</p>
<p><span class="math display">\[
\begin{split}
    V^\pi_h(s) &amp;= \mathbb{E}\left[ \left( \sum_{i=h}^{H-1} c(s_i, a_i) \right) + c(s_H) \mid s_h= s,  a_i = \pi_i(s_i) \quad \forall h\le i &lt; H \right] \\
    &amp;= \mathbb{E}\left[ \left( \sum_{i=h}^{H-1} s_i^\top Q s_i + a_i^\top R a_i \right) + s_H^\top Q s_H\mid s_h= s, a_i = \pi_i(s_i) \quad \forall h\le i &lt; H \right] \\
\end{split}
\]</span></p>
<p>The Q-function additionally conditions on the first action we take:</p>
<p><span class="math display">\[
\begin{split}
    Q^\pi_h(s, a) &amp;= \mathbb{E}\bigg[ \left( \sum_{i=h}^{H-1} c(s_i, a_i) \right) + c(s_H) \\
        &amp;\qquad\qquad \mid  (s_h, a_h) = (s, a), a_i = \pi_i(s_i) \quad \forall h\le i &lt; H \bigg] \\
    &amp;= \mathbb{E}\bigg[ \left( \sum_{i=h}^{H-1} s_i^\top Q s_i + a_i^\top R a_i \right) + s_H^\top Q s_H\\
        &amp;\qquad\qquad \mid (s_h, a_h) = (s, a), a_i = \pi_i(s_i) \quad \forall h\le i &lt; H \bigg] \\
\end{split}
\]</span></p>
<p>Note that since we use <em>cost</em> instead of <em>reward,</em> the best policies are the ones with <em>smaller</em> values of the value function.</p>
</div>
</section>
<section id="sec-optimal-lqr" class="level2" data-number="2.4">
<h2 data-number="2.4" class="anchored" data-anchor-id="sec-optimal-lqr"><span class="header-section-number">2.4</span> Optimality and the Riccati Equation</h2>
<p>In this section, we’ll compute the optimal value function <span class="math inline">\(V^\star_h\)</span>, Q-function <span class="math inline">\(Q^\star_h\)</span>, and policy <span class="math inline">\(\pi^\star_h\)</span> in <a href="#def-lqr-definition" class="quarto-xref">Definition&nbsp;<span>2.2</span></a> using <strong>dynamic programming</strong> in a very similar way to the DP algorithms in <a href="mdps.html#sec-eval-dp" class="quarto-xref"><span>Section 1.2.5</span></a>. Recall the definition of the optimal value function:</p>
<div id="def-optimal-value-lqr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.4 (Optimal value function in LQR)</strong></span> The <strong>optimal value function</strong> is the one that, at any time and in any state, achieves <em>minimum cost</em> across <em>all policies</em>:</p>
<p><span class="math display">\[
\begin{split}
    V^\star_h(s) &amp;= \min_{\pi_h, \dots, \pi_{H-1}} V^\pi_h(s) \\
    &amp;= \min_{\pi_{h}, \dots, \pi_{H-1}} \mathbb{E}\bigg[ \left( \sum_{i=h}^{H-1} s_h^\top Q s_h+ a_h^\top R a_h\right) + s_H^\top Q s_H\\
        &amp;\hspace{8em} \mid s_h= s, a_i = \pi_i(s_i) \quad \forall h\le i &lt; H \bigg] \\
\end{split}
\]</span></p>
<p>The optimal Q-function is defined similarly, conditioned on the starting action as well:</p>
<p><span class="math display">\[
\begin{split}
    Q^\star_h(s, a) &amp;= \min_{\pi_h, \dots, \pi_{H-1}} Q^\pi_h(s, a) \\
    &amp;= \min_{\pi_{h}, \dots, \pi_{H-1}} \mathbb{E}\bigg[ \left( \sum_{i=h}^{H-1} s_h^\top Q s_h+ a_h^\top R a_h\right) + s_H^\top Q s_H\\
        &amp;\hspace{8em} \mid s_h= s, a_h= a, a_i = \pi_i(s_i) \quad \forall h&lt; i &lt; H \bigg] \\
\end{split}
\]</span></p>
<p>Both of the definitions above assume <em>deterministic</em> policies. Otherwise we would have to take an <em>expectation</em> over actions drawn from the policy, i.e.&nbsp;<span class="math inline">\(a_h\sim \pi_h(s_h)\)</span>.</p>
</div>
<p>We will prove the striking fact that the solution has very simple structure: <span class="math inline">\(V_h^\star\)</span> and <span class="math inline">\(Q^\star_h\)</span> are <em>upward-curved quadratics</em> and <span class="math inline">\(\pi_h^\star\)</span> is <em>linear</em> and furthermore does not depend on the noise!</p>
<div id="thm-optimal-value-lqr-quadratic" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.1 (Optimal value function in LQR is an upward-curved quadratic)</strong></span> At each timestep <span class="math inline">\(h\in [H]\)</span>,</p>
<p><span class="math display">\[
V^\star_h(s) = s^\top P_hs+ p_h
\]</span></p>
<p>for some s.p.d. matrix <span class="math inline">\(P_h\in \mathbb{R}^{n_s\times n_s}\)</span> and scalar <span class="math inline">\(p_h\in \mathbb{R}\)</span>.</p>
</div>
<div id="thm-optimal-policy-lqr-linear" class="theorem">
<p><span class="theorem-title"><strong>Theorem 2.2 (Optimal policy in LQR is linear)</strong></span> At each timestep <span class="math inline">\(h\in [H]\)</span>,</p>
<p><span class="math display">\[
\pi^\star_h(s) = - K_hs
\]</span></p>
<p>for some <span class="math inline">\(K_h\in \mathbb{R}^{n_a\times n_s}\)</span>. (The negative is due to convention.)</p>
</div>
<p>The construction (and inductive proof) proceeds similarly to the one in the MDP setting (<a href="mdps.html#sec-eval-dp" class="quarto-xref"><span>Section 1.2.5</span></a>).</p>
<ol type="1">
<li>We’ll compute <span class="math inline">\(V_H^\star\)</span> (at the end of the horizon) as our base case.</li>
<li>Then we’ll work step-by-step backwards in time, using <span class="math inline">\(V_{h+1}^\star\)</span> to compute <span class="math inline">\(Q_h^\star\)</span>, <span class="math inline">\(\pi_{h}^\star\)</span>, and <span class="math inline">\(V_h^\star\)</span>.</li>
</ol>
<!-- TODO insert reference for proof by induction -->
<p><strong>Base case:</strong> At the final timestep, there are no possible actions to take, and so <span class="math inline">\(V^\star_H(s) = c(s) = s^\top Q s\)</span>. Thus <span class="math inline">\(V_H^\star(s) = s^\top P_Hs+ p_H\)</span> where <span class="math inline">\(P_H= Q\)</span> and <span class="math inline">\(p_H= 0\)</span>.</p>
<p><strong>Inductive hypothesis:</strong> We seek to show that the inductive step holds for both theorems: If <span class="math inline">\(V^\star_{h+1}(s)\)</span> is an upward-curved quadratic, then <span class="math inline">\(V^\star_h(s)\)</span> must also be an upward-curved quadratic, and <span class="math inline">\(\pi^\star_h(s)\)</span> must be linear. We’ll break this down into the following steps:</p>
<ol type="1">
<li>Show that <span class="math inline">\(Q^\star_h(s, a)\)</span> is an upward-curved quadratic (in both <span class="math inline">\(s\)</span> and <span class="math inline">\(a\)</span>).</li>
<li>Derive the optimal policy <span class="math inline">\(\pi^\star_h(s) = \arg \min_aQ^\star_h(s, a)\)</span> and show that it’s linear.</li>
<li>Show that <span class="math inline">\(V^\star_h(s)\)</span> is an upward-curved quadratic.</li>
</ol>
<p>We first assume the inductive hypothesis that our theorems are true at time <span class="math inline">\(h+1\)</span>. That is,</p>
<p><span class="math display">\[
V^\star_{h+1}(s) = s^\top P_{h+1} s+ p_{h+1} \quad \forall s\in \mathcal{S}.
\]</span></p>
<div class="{lem-q-upward-quadratic}">
<section id="qstar_hs-a-is-an-upward-curved-quadratic" class="level4" data-number="2.4.0.1">
<h4 data-number="2.4.0.1" class="anchored" data-anchor-id="qstar_hs-a-is-an-upward-curved-quadratic"><span class="header-section-number">2.4.0.1</span> <span class="math inline">\(Q^\star_h(s, a)\)</span> is an upward-curved quadratic</h4>
<p>Let us decompose <span class="math inline">\(Q^\star_h: \mathcal{S} \times \mathcal{A} \to \mathbb{R}\)</span> into the immediate reward plus the expected cost-to-go:</p>
<p><span class="math display">\[
Q^\star_h(s, a) = c(s, a) + \mathbb{E}_{s' \sim f(s, a, w_{h+1})} [V^\star_{h+1}(s')].
\]</span></p>
<p>Recall <span class="math inline">\(c(s, a) := s^\top Q s+ a^\top R a\)</span>. Let’s consider the expectation over the next timestep. The only randomness in the dynamics comes from the noise <span class="math inline">\(w_{h+1} \sim \mathcal{N}(0, \sigma^2 I)\)</span>, so we can expand the expectation as:</p>
<p><span class="math display">\[
\begin{aligned}
            &amp; \mathbb{E}_{s'} [V^\star_{h+1}(s')]                                                                                                         \\
    {} = {} &amp; \mathbb{E}_{w_{h+1}} [V^\star_{h+1}(A s+ B a+ w_{h+1})]                                             &amp;  &amp; \text{definition of } f     \\
    {} = {} &amp; \mathbb{E}_{w_{h+1}} [ (A s+ B a+ w_{h+1})^\top P_{h+1} (A s+ B a+ w_{h+1}) + p_{h+1} ]. &amp;  &amp; \text{inductive hypothesis}
\end{aligned}
\]</span></p>
<p>Summing and combining like terms, we get</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\star_h(s, a) &amp; = s^\top Q s+ a^\top R a+ \mathbb{E}_{w_{h+1}} [(A s+ B a+ w_{h+1})^\top P_{h+1} (A s+ B a+ w_{h+1}) + p_{h+1}] \\
                           &amp; = s^\top (Q + A^\top P_{h+1} A)s+ a^\top (R + B^\top P_{h+1} B) a+ 2 s^\top A^\top P_{h+1} B a\\
                           &amp; \qquad + \mathbb{E}_{w_{h+1}} [w_{h+1}^\top P_{h+1} w_{h+1}] + p_{h+1}.
\end{aligned}
\]</span></p>
<p>Note that the terms that are linear in <span class="math inline">\(w_h\)</span> have mean zero and vanish. Now consider the remaining expectation over the noise. By expanding out the product and using linearity of expectation, we can write this out as</p>
<p><span class="math display">\[
\begin{aligned}
    \mathbb{E}_{w_{h+1}} [w_{h+1}^\top P_{h+1} w_{h+1}] &amp; = \sum_{i=1}^d \sum_{j=1}^d (P_{h+1})_{ij} \mathbb{E}_{w_{h+1}} [(w_{h+1})_i (w_{h+1})_j] \\
    &amp; = \sigma^2 \mathrm{Tr}(P_{h+ 1})
\end{aligned}
\]</span></p>
</section>
<section id="quadratic-forms" class="level4 {rem-quadratic-forms}" data-number="2.4.0.2">
<h4 data-number="2.4.0.2" class="anchored" data-anchor-id="quadratic-forms"><span class="header-section-number">2.4.0.2</span> Quadratic forms</h4>
<p>When solving <em>quadratic forms</em>, i.e.&nbsp;expressions of the form <span class="math inline">\(x^\top A x\)</span>, it’s often helpful to consider the terms on the diagonal (<span class="math inline">\(i = j\)</span>) separately from those off the diagonal.</p>
<p>In this case, the expectation of each diagonal term becomes</p>
<p><span class="math display">\[
(P_{h+1})_{ii} \mathbb{E}(w_{h+1})_i^2 = \sigma^2 (P_{h+1})_{ii}.
\]</span></p>
<p>Off the diagonal, since the elements of <span class="math inline">\(w_{h+1}\)</span> are independent, the expectation factors, and since each element has mean zero, the term vanishes:</p>
<p><span class="math display">\[
(P_{h+1})_{ij} \mathbb{E}[(w_{h+1})_i] \mathbb{E}[(w_{h+1})_j] = 0.
\]</span></p>
<p>Thus, the only terms left are the ones on the diagonal, so the sum of these can be expressed as the trace of <span class="math inline">\(\sigma^2 P_{h+1}\)</span>:</p>
<p><span class="math display">\[
\mathbb{E}_{w_{h+1}} [w_{h+1}^\top P_{h+1} w_{h+1}] = \sigma^2 \mathrm{Tr}(P_{h+1}).
\]</span></p>
</section>
<p>Substituting this back into the expression for <span class="math inline">\(Q^\star_h\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
    Q^\star_h(s, a) &amp; = s^\top (Q + A^\top P_{h+1} A) s+ a^\top (R + B^\top P_{h+1} B) a
    + 2s^\top A^\top P_{h+1} B a\\
                            &amp; \qquad + \sigma^2 \mathrm{Tr}(P_{h+1}) + p_{h+1}.
\end{aligned}
\]</span></p>
<p>As we hoped, this expression is quadratic in <span class="math inline">\(s\)</span> and <span class="math inline">\(a\)</span>. Furthermore, we’d like to show that it also <em>curves upwards</em> with respect to <span class="math inline">\(a\)</span> so that its minimum with respect to <span class="math inline">\(a\)</span> is well-defined. We can do this by noting that the <strong>Hessian matrix</strong> of second derivatives is positive definite:</p>
<p><span class="math display">\[
\nabla_{aa} Q_h^\star(s, a) = R + B^\top P_{h+1} B
\]</span></p>
<p>Since <span class="math inline">\(R\)</span> is s.p.d. (<a href="#def-lqr-definition" class="quarto-xref">Definition&nbsp;<span>2.2</span></a>), and <span class="math inline">\(P_{h+1}\)</span> is s.p.d. (by the inductive hypothesis), this sum must also be s.p.d., and so <span class="math inline">\(Q^\star_h\)</span> is indeed an upward-curved quadratic with respect to <span class="math inline">\(a\)</span>. (If this isn’t clear, try proving it as an exercise.) The proof of its upward curvature with respect to <span class="math inline">\(s\)</span> is equivalent.</p>
</div>
<div id="lem-pi-linear" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.1 (<span class="math inline">\(\pi^\star_h\)</span> is linear)</strong></span> Since <span class="math inline">\(Q^\star_h\)</span> is an upward-curved quadratic, finding its minimum over <span class="math inline">\(a\)</span> is easy: we simply set the gradient with respect to <span class="math inline">\(a\)</span> equal to zero and solve for <span class="math inline">\(a\)</span>. First, we calculate the gradient:</p>
<p><span class="math display">\[
\begin{aligned}
    \nabla_aQ^\star_h(s, a) &amp; = \nabla_a[ a^\top (R + B^\top P_{h+1} B) a+ 2 s^\top A^\top P_{h+1} B a] \\
                                       &amp; = 2 (R + B^\top P_{h+1} B) a+ 2 (s^\top A^\top P_{h+1} B)^\top
\end{aligned}
\]</span></p>
<p>Setting this to zero, we get</p>
<p><span class="math display">\[
\begin{aligned}
    0                  &amp; = (R + B^\top P_{h+1} B) \pi^\star_h(s) + B^\top P_{h+1} A s\nonumber \\
    \pi^\star_h(s) &amp; = (R + B^\top P_{h+1} B)^{-1} (-B^\top P_{h+1} A s) \nonumber              \\
                       &amp; = - K_hs,
\end{aligned}
\]</span></p>
<p>where</p>
<p><span id="eq-k-pi"><span class="math display">\[
K_h= (R + B^\top P_{h+1} B)^{-1} B^\top P_{h+1} A.
\tag{2.1}\]</span></span></p>
<p>Note that this optimal policy doesn’t depend on the starting distribution <span class="math inline">\(\mu_0\)</span>. It’s also fully <strong>deterministic</strong> and isn’t affected by the noise terms <span class="math inline">\(w_0, \dots, w_{H-1}\)</span>.</p>
</div>
<div id="lem-upward-curved" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.3 (The value function is an upward-curved quadratic)</strong></span> Using the identity <span class="math inline">\(V^\star_h(s) = Q^\star_h(s, \pi^\star(s))\)</span>, we have:</p>
<p><span class="math display">\[
\begin{aligned}
    V^\star_h(s) &amp; = Q^\star_h(s, \pi^\star(s))                                                                \\
                     &amp; = s^\top (Q + A^\top P_{h+1} A) s+ (-K_hs)^\top (R + B^\top P_{h+1} B) (-K_hs)
    + 2s^\top A^\top P_{h+1} B (-K_hs)                                                                          \\
                     &amp; \qquad + \mathrm{Tr}(\sigma^2 P_{h+1}) + p_{h+1}
\end{aligned}
\]</span></p>
<p>Note that with respect to <span class="math inline">\(s\)</span>, this is the sum of a quadratic term and a constant, which is exactly what we were aiming for! The scalar term is clearly</p>
<p><span class="math display">\[
p_h= \mathrm{Tr}(\sigma^2 P_{h+1}) + p_{h+1}.
\]</span></p>
<p>We can simplify the quadratic term by substituting in <span class="math inline">\(K_h\)</span> from <a href="#eq-k-pi" class="quarto-xref">Equation&nbsp;<span>2.1</span></a>. Notice that when we do this, the <span class="math inline">\((R+B^\top P_{h+1} B)\)</span> term in the expression is cancelled out by its inverse, and the remaining terms combine to give the <strong>Riccati equation</strong>:</p>
<div id="def-riccati" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.5 (Riccati equation)</strong></span> <span class="math display">\[
P_h= Q + A^\top P_{h+1} A - A^\top P_{h+1} B (R + B^\top P_{h+1} B)^{-1} B^\top P_{h+1} A.
\]</span></p>
</div>
<p>There are several nice properties to note about the Riccati equation:</p>
<ol type="1">
<li>It’s defined <strong>recursively.</strong> Given the dynamics defined by <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, and the state cost matrix <span class="math inline">\(Q\)</span>, we can recursively calculate <span class="math inline">\(P_h\)</span> across all timesteps starting from <span class="math inline">\(P_H= Q\)</span>.</li>
<li><span class="math inline">\(P_h\)</span> often appears in calculations surrounding optimality, such as <span class="math inline">\(V^\star_h, Q^\star_h\)</span>, and <span class="math inline">\(\pi^\star_h\)</span>.</li>
<li>Together with the dynamics given by <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, and the action coefficients <span class="math inline">\(R\)</span> in the lost function, it fully defines the optimal policy <a href="#lem-pi-linear" class="quarto-xref">Lemma&nbsp;<span>2.1</span></a>.</li>
</ol>
<p>It remains to prove that <span class="math inline">\(V^\star_h\)</span> <em>curves upwards,</em> that is, that <span class="math inline">\(P_h\)</span> is s.p.d. We will use the following fact about <strong>Schur complements:</strong></p>
<div id="lem-lemma-schur" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 2.2 (Positive definiteness of Schur complements)</strong></span> Let</p>
<p><span class="math display">\[
D = \begin{pmatrix}
A &amp; B \\
B^\top &amp; C
\end{pmatrix}
\]</span></p>
<p>be a symmetric <span class="math inline">\((m+n) \times (m+n)\)</span> block matrix, where <span class="math inline">\(A \in \mathbb{R}^{m \times m}, B \in \mathbb{R}^{m \times n}, C \in \mathbb{R}^{n \times n}\)</span>. The <strong>Schur complement</strong> of <span class="math inline">\(A\)</span> is denoted</p>
<p><span class="math display">\[
D/A = C - B^\top A^{-1} B.
\]</span></p>
<p>Schur complements have various uses in linear algebra and numerical computation.</p>
<p>A useful fact for us is that if <span class="math inline">\(A\)</span> is positive <em>definite,</em> then <span class="math inline">\(D\)</span> is positive <em>semidefinite</em> if and only if <span class="math inline">\(D/A\)</span> is positive <em>semidefinite</em>.</p>
</div>
<p>Let <span class="math inline">\(P\)</span> denote <span class="math inline">\(P_{h+ 1}\)</span> for brevity. We already know <span class="math inline">\(Q\)</span> is p.d., so it suffices to show that</p>
<p><span class="math display">\[
S = P - P B (R + B^\top P B)^{-1} B^\top P
\]</span></p>
<p>is p.s.d. (positive semidefinite), since left- and right- multiplying by <span class="math inline">\(A^\top\)</span> and <span class="math inline">\(A\)</span> respectively preserves p.s.d. We note that <span class="math inline">\(S\)</span> is the Schur complement <span class="math inline">\(D/(R + B^\top P B)\)</span>, where</p>
<p><span class="math display">\[
D = \begin{pmatrix}
R + B^\top P B &amp; B^\top P \\
P B &amp; P
\end{pmatrix}.
\]</span></p>
<p>Thus we must show that <span class="math inline">\(D\)</span> is p.s.d.. This can be seen by computing</p>
<p><span class="math display">\[
\begin{aligned}
\begin{pmatrix}
y^\top &amp; z^\top
\end{pmatrix}
D
\begin{pmatrix}
y \\ z
\end{pmatrix}
&amp;= y^\top R y + y^\top B^\top P B y + 2 y^\top B^\top P z + z^\top P z \\
&amp;= y^\top R y + (By + z)^\top P (By + z) \\
&amp;&gt; 0.
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(R + B^\top P B\)</span> is p.d. and <span class="math inline">\(D\)</span> is p.s.d., then <span class="math inline">\(S = D / (R + B^\top P B)\)</span> must be p.s.d., and <span class="math inline">\(P_h= Q + A S A^\top\)</span> must be p.d.</p>
</div>
<p>Now we’ve shown that <span class="math inline">\(V^\star_h(s) = s^\top P_hs+ p_h\)</span>, where <span class="math inline">\(P_h\)</span> is s.p.d., proving the inductive hypothesis and completing the proof of <a href="#thm-optimal-policy-lqr-linear" class="quarto-xref">Theorem&nbsp;<span>2.2</span></a> and <a href="#thm-optimal-value-lqr-quadratic" class="quarto-xref">Theorem&nbsp;<span>2.1</span></a>.</p>
<p>In summary, we just demonstrated that at each timestep <span class="math inline">\(h\in [H]\)</span>, the optimal value function <span class="math inline">\(V^\star_h\)</span> and optimal Q-function <span class="math inline">\(Q^\star_h\)</span> are both upward-curved quadratics and the optimal policy <span class="math inline">\(\pi^\star_h\)</span> is linear. We also showed that all of these quantities can be calculated using a sequence of s.p.d. matrices <span class="math inline">\(P_0, \dots, P_H\)</span> that can be defined recursively using <a href="#def-riccati" class="quarto-xref">Definition&nbsp;<span>2.5</span></a>.</p>
<p>Before we move on to some extensions of LQR, let’s consider how the state at time <span class="math inline">\(h\)</span> behaves when we act according to this optimal policy.</p>
<section id="expected-state-at-time-h" class="level3" data-number="2.4.1">
<h3 data-number="2.4.1" class="anchored" data-anchor-id="expected-state-at-time-h"><span class="header-section-number">2.4.1</span> Expected state at time <span class="math inline">\(h\)</span></h3>
<p>How can we compute the expected state at time <span class="math inline">\(h\)</span> when acting according to the optimal policy? Let’s first express <span class="math inline">\(s_h\)</span> in a cleaner way in terms of the history. Note that having linear dynamics makes it easy to expand terms backwards in time:</p>
<p><span class="math display">\[
\begin{aligned}
    s_h&amp; = A s_{h-1} + B a_{h-1} + w_{h-1}                                 \\
            &amp; = A (As_{h-2} + B a_{h-2} + w_{h-2}) + B a_{h-1} + w_{h-1} \\
            &amp; = \cdots                                                                     \\
            &amp; = A^hs_0 + \sum_{i=0}^{h-1} A^i (B a_{h-i-1} + w_{h-i-1}).
\end{aligned}
\]</span></p>
<p>Let’s consider the <em>average state</em> at this time, given all the past states and actions. Since we assume that <span class="math inline">\(\mathbb{E}[w_h] = 0\)</span> (this is the zero vector in <span class="math inline">\(d\)</span> dimensions), when we take an expectation, the <span class="math inline">\(w_h\)</span> term vanishes due to linearity, and so we’re left with</p>
<p><span id="eq-expected-state"><span class="math display">\[
\mathbb{E}[s_h\mid s_{0:(h-1)}, a_{0:(h-1)}] = A^hs_0 + \sum_{i=0}^{h-1} A^i B a_{h-i-1}.
\tag{2.2}\]</span></span></p>
<div id="exr-expected-state" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.2 (Expected state)</strong></span> Show that if we choose actions according to the optimal policy <a href="#lem-pi-linear" class="quarto-xref">Lemma&nbsp;<span>2.1</span></a>, <a href="#eq-expected-state" class="quarto-xref">Equation&nbsp;<span>2.2</span></a> becomes</p>
<p><span class="math display">\[
\mathbb{E}[s_h\mid s_0, a_i = \pi^\star_i(s_i)\quad \forall i \le h] = \left( \prod_{i=0}^{h-1} (A - B K_i) \right) s_0.
\]</span></p>
</div>
<p>This introdces the quantity <span class="math inline">\(A - B K_i\)</span>, which shows up frequently in control theory. For example, one important question is: will <span class="math inline">\(s_h\)</span> remain bounded, or will it go to infinity as time goes on? To answer this, let’s imagine for simplicity that these <span class="math inline">\(K_i\)</span>s are equal (call this matrix <span class="math inline">\(K\)</span>). Then the expression above becomes <span class="math inline">\((A-BK)^hs_0\)</span>. Now consider the maximum eigenvalue <span class="math inline">\(\lambda_{\max}\)</span> of <span class="math inline">\(A - BK\)</span>. If <span class="math inline">\(|\lambda_{\max}| &gt; 1\)</span>, then there’s some nonzero initial state <span class="math inline">\(\bar s_0\)</span>, the corresponding eigenvector, for which</p>
<p><span class="math display">\[
\lim_{h\to \infty} (A - BK)^h\bar s_0
    = \lim_{h\to \infty} \lambda_{\max}^h\bar s_0
    = \infty.
\]</span></p>
<p>Otherwise, if <span class="math inline">\(|\lambda_{\max}| &lt; 1\)</span>, then it’s impossible for your original state to explode as dramatically.</p>
</section>
</section>
<section id="extensions" class="level2" data-number="2.5">
<h2 data-number="2.5" class="anchored" data-anchor-id="extensions"><span class="header-section-number">2.5</span> Extensions</h2>
<p>We’ve now formulated an optimal solution for the time-homogeneous LQR and computed the expected state under the optimal policy. However, real world tasks rarely have such simple dynamics, and we may wish to design more complex cost functions. In this section, we’ll consider more general extensions of LQR where some of the assumptions we made above are relaxed. Specifically, we’ll consider:</p>
<ol type="1">
<li><p><strong>Time-dependency</strong>, where the dynamics and cost function might change depending on the timestep.</p></li>
<li><p><strong>General quadratic cost</strong>, where we allow for linear terms and a constant term.</p></li>
<li><p><strong>Tracking a goal trajectory</strong> rather than aiming for a single goal state-action pair.</p></li>
</ol>
<p>Combining these will allow us to use the LQR solution to solve more complex setups by taking <em>Taylor approximations</em> of the dynamics and cost functions.</p>
<section id="sec-time-dep-lqr" class="level3" data-number="2.5.1">
<h3 data-number="2.5.1" class="anchored" data-anchor-id="sec-time-dep-lqr"><span class="header-section-number">2.5.1</span> Time-dependent dynamics and cost function</h3>
<p>So far, we’ve considered the <em>time-homogeneous</em> case, where the dynamics and cost function stay the same at every timestep. However, this might not always be the case. As an example, in many sports, the rules and scoring system might change during an overtime period. To address these sorts of problems, we can loosen the time-homogeneous restriction, and consider the case where the dynamics and cost function are <em>time-dependent.</em> Our analysis remains almost identical; in fact, we can simply add a time index to the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> that determine the dynamics and the matrices <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> that determine the cost.</p>
<p>The modified problem is now defined as follows:</p>
<div id="def-time-dependent-lqr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.6 (Time-dependent LQR)</strong></span> <span class="math display">\[
\begin{aligned}
        \min_{\pi_{0}, \dots, \pi_{H-1}} \quad &amp; \mathbb{E}\left[ \left( \sum_{h=0}^{H-1} (s_h^\top Q_hs_h) + a_h^\top R_ha_h\right) + s_H^\top Q_Hs_H\right] \\
        \textrm{where} \quad                      &amp; s_{h+1} = f_h(s_h, a_h, w_h) = A_hs_h+ B_ha_h+ w_h\\
                                                  &amp; s_0 \sim \mu_0                                                                                                                                   \\
                                                  &amp; a_h= \pi_h(s_h)                                                                                                                       \\
                                                  &amp; w_h\sim \mathcal{N}(0, \sigma^2 I).
\end{aligned}
\]</span></p>
</div>
<p>The derivation of the optimal value functions and the optimal policy remains almost exactly the same, and we can modify the Riccati equation accordingly:</p>
<div id="def-riccati-time-dependent" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.7 (Time-dependent Riccati Equation)</strong></span> <span class="math display">\[
P_h= Q_h+ A_h^\top P_{h+1} A_h- A_h^\top P_{h+1} B_h(R_h+ B_h^\top P_{h+1} B_h)^{-1} B_h^\top P_{h+1} A_h.
\]</span></p>
<p>Note that this is just the time-homogeneous Riccati equation (<a href="#def-riccati" class="quarto-xref">Definition&nbsp;<span>2.5</span></a>), but with the time index added to each of the relevant matrices.</p>
</div>
<div id="exr-time-dependent" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.3 (Time dependent LQR proof)</strong></span> Walk through the proof in <a href="#sec-optimal-lqr" class="quarto-xref"><span>Section 2.4</span></a> to verify that we can simply add <span class="math inline">\(h\)</span> for the time-dependent case.</p>
</div>
<p>Additionally, by allowing the dynamics to vary across time, we gain the ability to <em>locally approximate</em> nonlinear dynamics at each timestep. We’ll discuss this later in the chapter.</p>
</section>
<section id="more-general-quadratic-cost-functions" class="level3" data-number="2.5.2">
<h3 data-number="2.5.2" class="anchored" data-anchor-id="more-general-quadratic-cost-functions"><span class="header-section-number">2.5.2</span> More general quadratic cost functions</h3>
<p>Our original cost function had only second-order terms with respect to the state and action, incentivizing staying as close as possible to <span class="math inline">\((s^\star, a^\star) = (0, 0)\)</span>. We can also consider more general quadratic cost functions that also have first-order terms and a constant term. Combining this with time-dependent dynamics results in the following expression, where we introduce a new matrix <span class="math inline">\(M_h\)</span> for the cross term, linear coefficients <span class="math inline">\(q_h\)</span> and <span class="math inline">\(r_h\)</span> for the state and action respectively, and a constant term <span class="math inline">\(c_h\)</span>:</p>
<p><span id="eq-general-quadratic-cost"><span class="math display">\[
c_h(s_h, a_h) = ( s_h^\top Q_hs_h+ s_h^\top M_ha_h+ a_h^\top R_ha_h) + (s_h^\top q_h+ a_h^\top r_h) + c_h.
\tag{2.3}\]</span></span></p>
<p>Similarly, we can also include a constant term <span class="math inline">\(v_h\in \mathbb{R}^{n_s}\)</span> in the dynamics (note that this is <em>deterministic</em> at each timestep, unlike the stochastic noise <span class="math inline">\(w_h\)</span>):</p>
<p><span class="math display">\[
s_{h+1} = f_h(s_h, a_h, w_h) = A_hs_h+ B_ha_h+ v_h+ w_h.
\]</span></p>
<div id="exr-general-lqr" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 2.4 (General cost function)</strong></span> Derive the optimal solution. You will need to slightly modify the proof in <a href="#sec-optimal-lqr" class="quarto-xref"><span>Section 2.4</span></a>.</p>
</div>
</section>
<section id="tracking-a-predefined-trajectory" class="level3" data-number="2.5.3">
<h3 data-number="2.5.3" class="anchored" data-anchor-id="tracking-a-predefined-trajectory"><span class="header-section-number">2.5.3</span> Tracking a predefined trajectory</h3>
<p>Consider applying LQR to a task like autonomous driving, where the target state-action pair changes over time. We might want the vehicle to follow a predefined <em>trajectory</em> of states and actions <span class="math inline">\((s_h^\star, a_h^\star)_{h=0}^{H-1}\)</span>. To express this as a control problem, we’ll need a corresponding time-dependent cost function:</p>
<p><span class="math display">\[
c_h(s_h, a_h) = (s_h- s^\star_h)^\top Q (s_h- s^\star_h) + (a_h- a^\star_h)^\top R (a_h- a^\star_h).
\]</span></p>
<p>Note that this punishes states and actions that are far from the intended trajectory. By expanding out these multiplications, we can see that this is actually a special case of the more general quadratic cost function above <a href="#eq-general-quadratic-cost" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>:</p>
<p><span class="math display">\[
M_h= 0, \qquad q_h= -2Q s^\star_h, \qquad r_h= -2R a^\star_h, \qquad c_h= (s^\star_h)^\top Q (s^\star_h) + (a^\star_h)^\top R (a^\star_h).
\]</span></p>
</section>
</section>
<section id="sec-approx-nonlinear" class="level2" data-number="2.6">
<h2 data-number="2.6" class="anchored" data-anchor-id="sec-approx-nonlinear"><span class="header-section-number">2.6</span> Approximating nonlinear dynamics</h2>
<p>The LQR algorithm solves for the optimal policy when the dynamics are <em>linear</em> and the cost function is an <em>upward-curved quadratic</em>. However, real settings are rarely this simple! Let’s return to the CartPole example from the start of the chapter (<a href="#exm-cart-pole" class="quarto-xref">Example&nbsp;<span>2.1</span></a>). The dynamics (physics) aren’t linear. How can we approximate this by an LQR problem?</p>
<p>Concretely, let’s consider a <em>noise-free</em> problem since, as we saw, the noise doesn’t factor into the optimal policy. Let’s assume the dynamics and cost function are stationary, and ignore the terminal state for simplicity:</p>
<div id="def-nonlinear-control" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.8 (Nonlinear control problem)</strong></span> <span class="math display">\[
\begin{aligned}
        \min_{\pi_0, \dots, \pi_{H-1} : \mathcal{S} \to \mathcal{A}} \quad &amp; \mathbb{E}_{s_0} \left[ \sum_{h=0}^{H-1} c(s_h, a_h) \right] \\
        \text{where} \quad                                  &amp; s_{h+1} = f(s_h, a_h)                                   \\
                                                            &amp; a_h= \pi_h(s_h)                                          \\
                                                            &amp; s_0 \sim \mu_0                                                     \\
                                                            &amp; c(s, a) = d(s, s^\star) + d(a, a^\star).
\end{aligned}
\]</span></p>
<p>Here, <span class="math inline">\(d\)</span> denotes a function that measures the “distance” between its two arguments.</p>
</div>
<p>This is now only slightly simplified from the general optimal control problem (see <a href="#def-optimal-control" class="quarto-xref">Definition&nbsp;<span>2.1</span></a>). Here, we don’t know an analytical form for the dynamics <span class="math inline">\(f\)</span> or the cost function <span class="math inline">\(c\)</span>, but we assume that we’re able to <em>query/sample/simulate</em> them to get their values at a given state and action. To clarify, consider the case where the dynamics are given by real world physics. We can’t (yet) write down an expression for the dynamics that we can differentiate or integrate analytically. However, we can still <em>simulate</em> the dynamics and cost function by running a real-world experiment and measuring the resulting states and costs. How can we adapt LQR to this more general nonlinear case?</p>
<section id="local-linearization" class="level3" data-number="2.6.1">
<h3 data-number="2.6.1" class="anchored" data-anchor-id="local-linearization"><span class="header-section-number">2.6.1</span> Local linearization</h3>
<p>How can we apply LQR when the dynamics are nonlinear or the cost function is more complex? We’ll exploit the useful fact that we can take a function that’s <em>locally continuous</em> around <span class="math inline">\((s^\star, a^\star)\)</span> and approximate it nearby with low-order polynomials (i.e.&nbsp;its Taylor approximation). In particular, as long as the dynamics <span class="math inline">\(f\)</span> are differentiable around <span class="math inline">\((s^\star, a^\star)\)</span> and the cost function <span class="math inline">\(c\)</span> is twice differentiable at <span class="math inline">\((s^\star, a^\star)\)</span>, we can take a linear approximation of <span class="math inline">\(f\)</span> and a quadratic approximation of <span class="math inline">\(c\)</span> to bring us back to the regime of LQR.</p>
<p>Linearizing the dynamics around <span class="math inline">\((s^\star, a^\star)\)</span> gives:</p>
<p><span class="math display">\[
\begin{gathered}
    f(s, a) \approx f(s^\star, a^\star) + \nabla_sf(s^\star, a^\star) (s- s^\star) + \nabla_af(s^\star, a^\star) (a- a^\star) \\
    (\nabla_sf(s, a))_{ij} = \frac{d f_i(s, a)}{d s_j}, \quad i, j \le n_s\qquad (\nabla_af(s, a))_{ij} = \frac{d f_i(s, a)}{d a_j}, \quad i \le n_s, j \le n_a
\end{gathered}
\]</span></p>
<p>and quadratizing the cost function around <span class="math inline">\((s^\star, a^\star)\)</span> gives:</p>
<p><span class="math display">\[
\begin{aligned}
    c(s, a) &amp; \approx c(s^\star, a^\star) \quad \text{constant term}                                                                                      \\
                 &amp; \qquad + \nabla_sc(s^\star, a^\star) (s- s^\star) + \nabla_ac(s^\star, a^\star) (a - a^\star) \quad \text{linear terms} \\
                 &amp; \left. \begin{aligned}
                               &amp; \qquad + \frac{1}{2} (s- s^\star)^\top \nabla_{ss} c(s^\star, a^\star) (s- s^\star)       \\
                               &amp; \qquad + \frac{1}{2} (a- a^\star)^\top \nabla_{aa} c(s^\star, a^\star) (a- a^\star) \\
                               &amp; \qquad + (s- s^\star)^\top \nabla_{sa} c(s^\star, a^\star) (a- a^\star)
                          \end{aligned} \right\} \text{quadratic terms}
\end{aligned}
\]</span></p>
<p>where the gradients and Hessians are defined as</p>
<p><span class="math display">\[
\begin{aligned}
    (\nabla_sc(s, a))_{i}         &amp; = \frac{d c(s, a)}{d s_i}, \quad i \le n_s
                                          &amp; (\nabla_ac(s, a))_{i}                                               &amp; = \frac{d c(s, a)}{d a_i}, \quad i \le n_a\\
    (\nabla_{ss} c(s, a))_{ij}  &amp; = \frac{d^2 c(s, a)}{d s_i d s_j}, \quad i, j \le n_s
                                          &amp; (\nabla_{aa} c(s, a))_{ij}                                       &amp; = \frac{d^2 c(s, a)}{d a_i d a_j}, \quad i, j \le n_a\\
    (\nabla_{sa} c(s, a))_{ij} &amp; = \frac{d^2 c(s, a)}{d s_i d a_j}. \quad i \le n_s, j \le n_a
\end{aligned}
\]</span></p>
<p><strong>Exercise:</strong> Note that this cost can be expressed in the general quadratic form seen in <a href="#eq-general-quadratic-cost" class="quarto-xref">Equation&nbsp;<span>2.3</span></a>. Derive the corresponding quantities <span class="math inline">\(Q, R, M, q, r, c\)</span>.</p>
</section>
<section id="finite-differencing" class="level3" data-number="2.6.2">
<h3 data-number="2.6.2" class="anchored" data-anchor-id="finite-differencing"><span class="header-section-number">2.6.2</span> Finite differencing</h3>
<p>To calculate these gradients and Hessians in practice, we use a method known as <strong>finite differencing</strong> for numerically computing derivatives. Namely, we can simply use the limit definition of the derivative, and see how the function changes as we add or subtract a tiny <span class="math inline">\(\delta\)</span> to the input.</p>
<p><span class="math display">\[
\frac{d}{dx} f(x) = \lim_{\delta \to 0} \frac{f(x + \delta) - f(x)}{\delta}
\]</span></p>
<p>Note that this only requires us to be able to <em>query</em> the function, not to have an analytical expression for it, which is why it’s so useful in practice.</p>
</section>
<section id="local-convexification" class="level3" data-number="2.6.3">
<h3 data-number="2.6.3" class="anchored" data-anchor-id="local-convexification"><span class="header-section-number">2.6.3</span> Local convexification</h3>
<p>However, simply taking the second-order approximation of the cost function is insufficient, since for the LQR setup we required that the <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> matrices were positive definite, i.e.&nbsp;that all of their eigenvalues were positive.</p>
<p>One way to naively <em>force</em> some symmetric matrix <span class="math inline">\(D\)</span> to be positive definite is to set any non-positive eigenvalues to some small positive value <span class="math inline">\(\varepsilon &gt; 0\)</span>. Recall that any real symmetric matrix <span class="math inline">\(D \in \mathbb{R}^{n \times n}\)</span> has an basis of eigenvectors <span class="math inline">\(u_1, \dots, u_n\)</span> with corresponding eigenvalues <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span> such that <span class="math inline">\(D u_i = \lambda_i u_i\)</span>. Then we can construct the positive definite approximation by</p>
<p><span class="math display">\[
\widetilde{D} = \left( \sum_{i=1, \dots, n \mid \lambda_i &gt; 0} \lambda_i u_i u_i^\top \right) + \varepsilon I.
\]</span></p>
<p><strong>Exercise:</strong> Convince yourself that <span class="math inline">\(\widetilde{D}\)</span> is indeed positive definite.</p>
<p>Note that Hessian matrices are generally symmetric, so we can apply this process to <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span> to obtain the positive definite approximations <span class="math inline">\(\widetilde{Q}\)</span> and <span class="math inline">\(\widetilde{R}\)</span>. Now that we have an upward-curved quadratic approximation to the cost function, and a linear approximation to the state transitions, we can simply apply the time-homogenous LQR methods from <a href="#sec-optimal-lqr" class="quarto-xref"><span>Section 2.4</span></a>.</p>
<p>But what happens when we enter states far away from <span class="math inline">\(s^\star\)</span> or want to use actions far from <span class="math inline">\(a^\star\)</span>? A Taylor approximation is only accurate in a <em>local</em> region around the point of linearization, so the performance of our LQR controller will degrade as we move further away. We’ll see how to address this in the next section using the <strong>iterative LQR</strong> algorithm.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="shared/log_taylor.png" class="img-fluid figure-img"></p>
<figcaption>Local linearization might only be accurate in a small region around the point of linearization</figcaption>
</figure>
</div>
</section>
<section id="sec-iterative-lqr" class="level3" data-number="2.6.4">
<h3 data-number="2.6.4" class="anchored" data-anchor-id="sec-iterative-lqr"><span class="header-section-number">2.6.4</span> Iterative LQR</h3>
<p>To address these issues with local linearization, we’ll use an iterative approach, where we repeatedly linearize around different points to create a <em>time-dependent</em> approximation of the dynamics, and then solve the resulting time-dependent LQR problem to obtain a better policy. This is known as <strong>iterative LQR</strong> or <strong>iLQR</strong>:</p>
<div id="def-ilqr" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 2.9 (Iterative LQR)</strong></span> For each iteration of the algorithm:</p>
<ol type="1">
<li>Form a time-dependent LQR problem around the current candidate trajectory using local linearization.</li>
<li>Compute the optimal policy using <a href="#sec-time-dep-lqr" class="quarto-xref"><span>Section 2.5.1</span></a>.</li>
<li>Generate a new series of actions using this policy.</li>
<li>Compute a better candidate trajectory by interpolating between the current and proposed actions.</li>
</ol>
</div>
<p>Now let’s go through the details of each step. We’ll use superscripts to denote the iteration of the algorithm. We’ll also denote <span class="math inline">\(\bar s_0 = \mathbb{E}_{s_0 \sim \mu_0} [s_0]\)</span> as the expected initial state.</p>
<p>At iteration <span class="math inline">\(i\)</span> of the algorithm, we begin with a <strong>candidate</strong> trajectory <span class="math inline">\(\bar \tau^i = (\bar s^i_0, \bar a^i_0, \dots, \bar s^i_{H-1}, \bar a^i_{H-1})\)</span>.</p>
<p><strong>Step 1: Form a time-dependent LQR problem.</strong> At each timestep <span class="math inline">\(h\in [H]\)</span>, we use the techniques from <a href="#approx_nonlinear"></a> to linearize the dynamics and quadratize the cost function around <span class="math inline">\((\bar s^i_h, \bar a^i_h)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
    f_h(s, a) &amp; \approx f(\bar {s}^i_h, \bar {a}^i_h) + \nabla_{s} f(\bar {s}^i_h, \bar {a}^i_h)(s- \bar {s}^i_h) + \nabla_{a} f(\bar {s}^i_h, \bar {a}^i_h)(a- \bar {a}^i_h)                         \\
    c_h(s, a) &amp; \approx c(\bar {s}^i_h, \bar {a}^i_h) + \begin{bmatrix}
                                                              s- \bar {s}^i_h&amp; a- \bar {a}^i_h
                                                          \end{bmatrix} \begin{bmatrix}
                                                                            \nabla_{s} c(\bar {s}^i_h, \bar {a}^i_h)\\
                                                                            \nabla_{a} c(\bar {s}^i_h, \bar {a}^i_h)
                                                                        \end{bmatrix}                                                      \\
                     &amp; \qquad + \frac{1}{2} \begin{bmatrix}
                                                s- \bar {s}^i_h&amp; a- \bar {a}^i_h
                                            \end{bmatrix} \begin{bmatrix}
                                                              \nabla_{ss} c(\bar {s}^i_h, \bar {a}^i_h)  &amp; \nabla_{sa} c(\bar {s}^i_h, \bar {a}^i_h)  \\
                                                              \nabla_{as} c(\bar {s}^i_h, \bar {a}^i_h) &amp; \nabla_{aa} c(\bar {s}^i_h, \bar {a}^i_h)
                                                          \end{bmatrix}
    \begin{bmatrix}
        s- \bar {s}^i_h\\
        a- \bar {a}^i_h
    \end{bmatrix}.
\end{aligned}
\]</span></p>
<p><strong>Step 2: Compute the optimal policy.</strong> We can now solve the time-dependent LQR problem using the Riccati equation from <a href="#sec-time-dep-lqr" class="quarto-xref"><span>Section 2.5.1</span></a> to compute the optimal policy <span class="math inline">\(\pi^i_0, \dots, \pi^i_{H-1}\)</span>.</p>
<p><strong>Step 3: Generate a new series of actions.</strong> We can then generate a new sample trajectory by taking actions according to this optimal policy:</p>
<p><span class="math display">\[
\bar s^{i+1}_0 = \bar s_0, \qquad \widetilde a_h= \pi^i_h(\bar s^{i+1}_h), \qquad \bar s^{i+1}_{h+1} = f(\bar s^{i+1}_h, \widetilde a_h).
\]</span></p>
<p>Note that the states are sampled according to the <em>true</em> dynamics, which we assume we have query access to.</p>
<p><strong>Step 4: Compute a better candidate trajectory.</strong>, Note that we’ve denoted these actions as <span class="math inline">\(\widetilde a_h\)</span> and aren’t directly using them for the next iteration <span class="math inline">\(\bar a^{i+1}_h\)</span>. Rather, we want to <em>interpolate</em> between them and the actions from the previous iteration <span class="math inline">\(\bar a^i_0, \dots, \bar a^i_{H-1}\)</span>. This is so that the cost will <em>increase monotonically,</em> since if the new policy turns out to actually be worse, we can stay closer to the previous trajectory. (Can you think of an intuitive example where this might happen?)</p>
<p>Formally, we want to find <span class="math inline">\(\alpha \in [0, 1]\)</span> to generate the next iteration of actions <span class="math inline">\(\bar a^{i+1}_0, \dots, \bar a^{i+1}_{H-1}\)</span> such that the cost is minimized:</p>
<p><span class="math display">\[
\begin{aligned}
    \min_{\alpha \in [0, 1]} \quad &amp; \sum_{h=0}^{H-1} c(s_h, \bar a^{i+1}_h)                     \\
    \text{where} \quad             &amp; s_{h+1} = f(s_h, \bar a^{i+1}_h)                             \\
                                   &amp; \bar a^{i+1}_h= \alpha \bar a^i_h+ (1-\alpha) \widetilde a_h\\
                                   &amp; s_0 = \bar s_0.
\end{aligned}
\]</span></p>
<p>Note that this optimizes over the closed interval <span class="math inline">\([0, 1]\)</span>, so by the Extreme Value Theorem, it’s guaranteed to have a global maximum.</p>
<p>The final output of this algorithm is a policy <span class="math inline">\(\pi^{n_\text{steps}}\)</span> derived after <span class="math inline">\(n_\text{steps}\)</span> of the algorithm. Though the proof is somewhat complex, one can show that for many nonlinear control problems, this solution converges to a locally optimal solution (in the policy space).</p>
</section>
</section>
<section id="summary" class="level2" data-number="2.7">
<h2 data-number="2.7" class="anchored" data-anchor-id="summary"><span class="header-section-number">2.7</span> Summary</h2>
<p>This chapter introduced some approaches to solving different variants of the optimal control problem <a href="#def-optimal-control" class="quarto-xref">Definition&nbsp;<span>2.1</span></a>. We began with the simple case of linear dynamics and an upward-curved quadratic cost. This model is called the LQR and we solved for the optimal policy using dynamic programming. We then extended these results to the more general nonlinear case via local linearization. We finally saw the iterative LQR algorithm for solving nonlinear control problems.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./mdps.html" class="pagination-link" aria-label="Markov Decision Processes">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Markov Decision Processes</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./bandits.html" class="pagination-link" aria-label="Multi-Armed Bandits">
        <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Multi-Armed Bandits</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>