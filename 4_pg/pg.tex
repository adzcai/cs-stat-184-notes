\documentclass[../main/main]{subfiles}

\setcounter{chapter}{3}

\begin{document}
    
% \begingroup
% \renewcommand{\clearpage}{\relax}
% \tableofcontents
% \endgroup

\chapter{Policy Gradients}

\section{Motivation}

The scope of our problem has been gradually expanding.

\begin{enumerate}
    \item  In the first chapter, we considered \emph{bandits} with a finite number of arms, where the only stochasticity involved was their rewards.
    \item In the second chapter, we considered \emph{MDPs} more generally, involving a finite number of states and actions, where the state transitions are Markovian.
    \item In the third chapter, we considered \emph{continuous} state and action spaces and developed the \emph{Linear Quadratic Regulator.} We then showed how to use it to find \emph{locally optimal solutions} to problems with nonlinear dynamics and non-quadratic cost functions.
\end{enumerate}

Now, we'll continue to investigate the case of finding optimal policies in large MDPs using the self-explanatory approach of \emph{policy optimization.} This is a general term encompassing many specific algorithms we've already seen:

\begin{itemize}
    \item \emph{Policy iteration} for finite MDPs,
    \item \emph{Iterative LQR} for locally optimal policies in continuous control.
\end{itemize}

Here we'll see some general algorithms that allow us to optimize policies for general kinds of problems. These algorithms have been used in many groundbreaking applications, including AlphaGo, OpenAI Five. These methods also bring us into the domain where we can use \emph{deep learning} to approximate complex, nonlinear functions.

% TODO insert "map diagram"

\section{(Stochastic) Policy Gradient Ascent}

Let's suppose our policy can be \emph{parameterized} by some parameters $\theta.$ For example, these might be a preferences over state-action pairs, or in a high-dimensional case, the weights and biases of a deep neural network. We'll talk more about possible parameterizations in \autoref{sec:parameterizations}

Remember that in reinforcement learning, the goal is to \emph{maximize reward.} Specifically, we seek the parameters that maximize the expected total reward, which we can express concisely using the value function we defined earlier: \begin{equation} \begin{split}
    J(\theta) := \E_{s_0 \sim \mu_0} V^{\pi_\theta} (s_0) = & \E \sum_{t=0}^{T-1} r_t \\
    \text{where} \quad & s_0 \sim \mu_0 \\
    & s_{t+1} \sim P(s_t, a_t), \\
    & a_h = \pi_\theta(s_h) \\
    & r_h = r(s_h, a_h).
\end{split} \label{eq:objective_fn} \end{equation}
We call a sequence of states, actions, and rewards a \textbf{trajectory} $\tau = (s_i, a_i, r_i)_{i=0}^{T-1},$ and the total time-discounted reward is also often called the \textbf{return} $R(\tau)$ of a trajectory.
Note that the above is the \emph{undiscounted, finite-horizon case,} which we'll continue to use throughout the chapter, but analogous results hold for the \emph{discounted, infinite-horizon case.}

Note that when the state transitions are Markov (i.e. $s_{t}$ only depends on $s_{t-1}, a_{t-1}$) and the policy is stationary (i.e. $a_t \sim \pi_\theta (s_t)$), we can write out the \emph{likelihood of a trajectory} under the policy $\pi_\theta$:
\begin{equation}
    \begin{split}
        \rho_\theta(\tau) &= \mu(s_0) \pi_\theta(a_0 | s_0) \\
        &\qquad \times P(s_1 | s_0, a_0) \pi_\theta(a_1 | s_1) \\
        &\qquad \times \cdots \\
        &\qquad \times P(s_{H-1} | s_{H-2}, a_{H-2}) \pi_\theta(a_{H-1} | s_{H-1}).
    \end{split}
    \label{eq:trajectory_likelihood}
\end{equation}
This lets us rewrite $J(\theta) = \E_{\tau \sim \rho_\theta} R(\tau).$

Now how do we optimize for this function?
One very general optimization technique is \emph{gradient ascent.} Namely, the \textbf{gradient} of a function at a given point answers: At this point, which direction should we move to increase the function the most? By repeatedly moving in this direction, we can keep moving up on the graph of this function. Expressing this iteratively, we have: \[
    \theta_{t+1} = \theta_t + \eta \nabla_\theta J(\pi_\theta) \Big|_{\theta = \theta_t},
\]

Where $\eta$ is a \emph{hyperparameter} that says how big of a step to take each time.

In order to apply this technique, we need to be able to evaluate the gradient $\nabla_\theta J(\pi_\theta).$ How can we do this?

In practice, it's often impractical to evaluate the gradient directly. For example, in supervised learning, $J(\theta)$ might be the sum of squared prediction errors across an entire \textbf{training dataset.} However, if our dataset is very large, we might not be able to fit it into our computer's memory!

Instead, we can \emph{estimate} a gradient step using some estimator $\tilde \nabla J(\theta).$ This is called \textbf{\emph{stochastic} gradient descent} (SGD). Ideally, we want this estimator to be \textbf{unbiased,} that is, on average, it matches a single true gradient step:
\[ \E [\tilde \nabla J(\theta)] = \nabla J(\theta). \]
If $J$ is defined in terms of some training dataset, we might randomly choose a \emph{minibatch} of samples and use them to estimate the prediction error across the \emph{whole} dataset. (This approach is known as \textbf{\emph{minibatch} SGD}.)

Notice that our parameters will stop changing once $\nabla J(\theta) = 0.$ This implies that our current parameters are `locally optimal' in some sense; it's impossible to increase the function by moving in any direction. If $J$ is convex, then the only point where this happens is at the \emph{global optimum.} Otherwise, if $J$ is nonconvex, the best we can hope for is a \emph{local optimum.}

We can actually show that in a finite number of steps, SGD will find a $\theta$ that is ``close'' to a local optimum. More formally, suppose we run SGD for $T$ steps, using an unbiased gradient estimator. Let the step size $\eta_t$ scale as $O(1/ \sqrt{t}).$ Then if $J$ is bounded and $\beta$-smooth, and the norm of the gradient estimator has a finite variance, then after $T$ steps: \[
    \|\nabla_\theta J(\theta)\|^2 \le O \left( M \beta \sigma^2 / T\right).
\]
In another perspective, the local ``landscape'' of $J$ around $\theta$ becomes flatter and flatter the longer we run SGD.

\section{REINFORCE and Importance Sampling}

Note that the objective function above, $J(\theta) = \E_{\tau \sim \rho_\theta}R(\tau),$ is very difficult to compute! It requires playing out every possible trajectory,
which is clearly infeasible for slightly complex state and action spaces.
Can we rewrite this in a form that's more convenient to implement?
Specifically, suppose there is some distribution, given by a likelihood $\rho(\tau),$ that's easy to sample from (e.g. a database of existing trajectories).
We can then rewrite the objective function as follows (all gradients are being taken w.r.t. $\theta$):
\begin{align*}
    \nabla J(\theta) &= \nabla \E_{\tau \sim \rho_\theta} R(\tau) \\
    &= \nabla \E_{\tau \sim \rho} \frac{\rho_\theta(\tau)}{\rho(\tau)} R(\tau) && \text{likelihood ratio trick} \\
    &= \E_{\tau \sim \rho} \frac{\nabla \rho_\theta(\tau)}{\rho(\tau)} R(\tau) && \text{switching gradient and expectation}
\end{align*}
Note that setting $\rho = \rho_\theta$ gives us an alternative form of $J$ that's easier to implement. (Notice the swapped order of $\nabla$ and $\E$!)
\begin{align*}
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} [ \nabla \log \rho_\theta(\tau) \cdot R(\tau)].
\end{align*}
Consider expanding out $\rho_\theta.$ Note that taking its $\log$ turns it into a sum of $\log$ terms, of which only the $\pi_\theta(a_t | s_t)$ terms depend on $\theta,$ so we can simplify even further to obtain
\begin{align*}
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) R(\tau) \right]
\end{align*}
In fact, we can perform one more simplification. Intuitively, the action at step $t$ does not affect the reward at previous timesteps. You can also show rigorously that this is the case, and that we only need to consider the present and future rewards to calculate the policy gradient: \begin{equation}
    \begin{split}
    \nabla J(\theta) &= \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) \sum_{t' = t}^{T-1} r(s_{t'}, a_{t'}) \right] \\
    &= \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \nabla_\theta \log \pi_{\theta}(a_t | s_t) Q^{\pi_\theta}(s_{t}, a_{t}) \right]
    \end{split}
    \label{eq:pg_with_q}
\end{equation}
Note that in the discounted case, the $Q^{\pi_\theta}$ term must becomes $\lambda^t Q^{\pi_\theta}.$ (Make sure this makes sense!)
\textbf{Exercise:} Prove that this is equivalent to the previous definitions. Also show that this works in the undiscounted case and for infinite horizon.

This expression allows us to estimate the gradient by sampling a few sample trajectories from $\pi_\theta,$ calculating the likelihoods of the chosen actions, and substituting these into the expression above.

For some intuition into how this method works, recall that we update our parameters according to \[
    \begin{split}
        \theta_{t+1} &= \theta_t + \nabla J(\theta_t) \\
        &= \theta_t + \E_{\tau \sim \rho_{\theta_t}} \nabla \log \rho_{\theta_t}(\tau) \cdot R(\tau).
    \end{split}
\]
Consider the ``good'' trajectories where $R(\tau)$ is large. Then $\theta$ gets updated so that these trajectories become more likely. To see why, recall that $\rho_{\theta}(\tau)$ is the likelihood of the trajectory $\tau$ under the policy $\pi_\theta,$ so evaluating the gradient points in the direction that makes $\tau$ more likely.

This is an example of \textbf{importance sampling:} updating a distribution to put more density on ``more important'' samples (in this case trajectories).

\section{Baselines and advantages}

% TODO: Insert better intuition.

A central idea from supervised learning is the bias-variance tradeoff. So far, our method is \emph{unbiased,} meaning that its average is the true policy gradient. Can we find ways to reduce the variance of our estimator as well?

We can instead subtract a \textbf{baseline function} $b_t : \S \to \R$ at each timestep $t.$ This modifies the policy gradient as follows: \[
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{H-1} \nabla \log \pi_\theta (a_t | s_t) \left(
            \left(
                \sum_{t' = t}^{H-1} r_t
            \right)
            - b_t(s_t)
        \right)
    \right].
\]

For example, we might want $b_t$ to estimate the average reward-to-go at a given timestep: $b_t^\theta = \E_{\tau \sim \rho_\theta} R_t(\tau).$ This way,
the random variable $R_t(\tau) - b_t^\theta$ is centered around zero,
making certain algorithms more stable.

As a better baseline, we could instead choose the \emph{value function.} Note that the random variable $Q^\pi_t(s, a) - V^\pi_t(s),$ where the randomness is taken over the actions,
is also centered around zero. (Recall $V^\pi_t(s) = \E_{a \sim \pi} Q^\pi_t(s, a).$) In fact, this quantity has a particular name: the \textbf{advantage function.} In a sense, it measures how much better this action does than the average for that policy.
We can now alternatively and concisely express the policy gradient as follows. Note that the advantage function effectively replaces the $Q$-function from \autoref{eq:pg_with_q}: \[
    \nabla J(\theta) = \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_t | s_t) A^{\pi_\theta}_t (s_t, a_t)
    \right]
\]
Additionally, note that for an optimal policy $\pi^\star,$ the advantage of a given state-action pair is always nonpositive. (Why?)


\section{Policy parameterizations} \label{sec:parameterizations}

What are some different ways we could parameterize our policy?

If both the state and action spaces are finite, perhaps we could simply learn a preference value $\theta_{s,a}$ for each state-action pair. Then to turn this into a valid distribution, we exponentiate each of them, and divide by the total: \[
    \pi^\text{softmax}_\theta(a | s) = \frac{\exp(\theta_{s,a})}{\sum_{s,a'} \exp (\theta_{s,a'})}.
\]
However, this doesn't preserve any structure in the states or actions. While this is flexible, it is also prone to overfitting.

\subsection{Linear in features}

Instead, what if we map each state-action pair into some \textbf{feature space} $\phi(s, a) \in \mathbb{R}^p$? Then, to map a feature vector to a probability,
we take a linear combination $\theta \in \mathbb{R}^p$ of the features and take a softmax: \[
    \pi^\text{linear in features}_{\theta}(a|s) = \frac{\exp(\theta^\top \phi(s, a))}{\sum_{a'} \exp(\theta^\top \phi(s, a'))}.
\]
Another interpretation is that $\theta$ represents the feature vector of the ``ideal'' state-action pair, as state-action pairs whose features align closely with $\theta$ are given higher probability.

The score for this parameterization is also quite elegant: \begin{equation*}
    \begin{split}
        \nabla \log \pi_\theta(a|s) &= \nabla \left( \theta^\top \phi(s, a) - \log \left( \sum_{a'} \exp(\theta^\top \phi(s, a')) \right) \right) \\
        &= \phi(s, a) - \E_{a' \sim \pi_\theta(s)} \phi(s, a')
    \end{split}
\end{equation*}
Plugging this into our policy gradient expression, we get \begin{align*}
    \nabla J(\theta) &= \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{T-1} \nabla \log \pi_\theta(a_t | s_t) A_t^{\pi_\theta}
    \right] \\
    &= \E_{\tau \sim \rho_\theta} \left[
        \sum_{t=0}^{T-1} \left( \phi(s_t, a_t) - \E_{a' \sim \pi(s_t)} \phi(s_t, a') \right) A_t^{\pi_\theta}(s_t, a_t)
    \right] \\
    &= \E_{\tau \sim \rho_\theta} \left[ \sum_{t=0}^{T-1} \phi(s_t, a_t) A_t^{\pi_\theta} (s_t, a_t) \right]
\end{align*}
Why can we drop the $\E \phi(s_t, a')$ term? By linearity of expectation, consider the dropped term at a single timestep: $\E_{\tau \sim \rho_\theta} \left[ \left( \E_{a' \sim \pi(s_t)} \phi(s, a') \right) A_t^{\pi_\theta}(s_t, a_t) \right].$ By Adam's Law, we can wrap the advantage term in a conditional expectation on the state $s_t.$ Then we already know that $\E_{a \sim \pi(s)} A_t^{\pi}(s, a) = 0,$ and so this entire term vanishes.

\subsection{Neural policies}

More generally, we could map states and actions to unnormalized scores via some parameterized function $f_\theta : \S \times \A \to \R,$ such as a neural network, and choose actions according to a softmax: \[
    \pi^\text{general}_\theta(a|s) = \frac{\exp(f_{\theta}(s,a))}{\sum_{a'} \exp(f_{\theta}(s,a'))}.
\]

The score can then be written as \[
    \nabla \log \pi_\theta(a|s) = \nabla f_\theta(s, a) - \E_{a \sim \pi_\theta(s)} \nabla f_\theta (s, a')
\]


\end{document}