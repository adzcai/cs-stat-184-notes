{"version":"1","myst":"1.3.17","references":[{"kind":"page","data":"/index.json","url":"/"},{"identifier":"prerequisites","kind":"heading","data":"/index.json","url":"/","implicit":true},{"identifier":"reinforcement-learning-in-a-nutshell","kind":"heading","data":"/index.json","url":"/","implicit":true},{"identifier":"core-tasks-of-reinforcement-learning","kind":"heading","data":"/index.json","url":"/","implicit":true},{"identifier":"course-overview","kind":"heading","data":"/index.json","url":"/","implicit":true},{"identifier":"notation","kind":"heading","data":"/index.json","url":"/","implicit":true},{"identifier":"programming","kind":"heading","data":"/index.json","url":"/"},{"kind":"page","data":"/mdps.json","url":"/mdps"},{"identifier":"introduction","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"markov","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"finite-horizon-mdps","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"definition","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"finite_horizon_mdp","html_id":"finite-horizon-mdp","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"tidy_mdp","html_id":"tidy-mdp","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"policies","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"policy","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"tidy_policy","html_id":"tidy-policy","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"trajectories","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"trajectory","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"tidy_traj","html_id":"tidy-traj","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"autoregressive_trajectories","html_id":"autoregressive-trajectories","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"value-functions","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"value","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"action_value","html_id":"action-value","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"relating-the-value-function-and-action-value-function","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"greedy-policies","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"the-one-step-bellman-consistency-equation","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"bellman_consistency","html_id":"bellman-consistency","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_consistency_action","html_id":"bellman-consistency-action","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_det","html_id":"bellman-det","kind":"proof:remark","data":"/mdps.json","url":"/mdps"},{"identifier":"the-one-step-bellman-operator","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"bellman_operator","html_id":"bellman-operator","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"finite_horizon_mdps","html_id":"finite-horizon-mdps-1","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"eval_dp","html_id":"eval-dp","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"tidy_eval_finite","html_id":"tidy-eval-finite","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"opt_dynamic_programming","html_id":"opt-dynamic-programming","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"optimal_policy_finite","html_id":"optimal-policy-finite","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"optimal_greedy","html_id":"optimal-greedy","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_consistency_optimal","html_id":"bellman-consistency-optimal","kind":"proof:corollary","data":"/mdps.json","url":"/mdps"},{"identifier":"pi_star_dp","html_id":"pi-star-dp","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"infinite_horizon_mdps","html_id":"infinite-horizon-mdps","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"discounted-rewards","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"stationary-policies","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"value-functions-and-bellman-consistency","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"bellman_consistency_infinite","html_id":"bellman-consistency-infinite","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"solving-infinite-horizon-mdps","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"the-bellman-operator-is-a-contraction-mapping","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"contraction","kind":"proof:definition","data":"/mdps.json","url":"/mdps"},{"identifier":"contraction_convergence","html_id":"contraction-convergence","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_convergence","html_id":"bellman-convergence","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_contraction","html_id":"bellman-contraction","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"policy-evaluation-in-infinite-horizon-mdps","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"matrix-inversion-for-deterministic-policies","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"tidy_tabular","html_id":"tidy-tabular","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"matrix_inversion_pe","html_id":"matrix-inversion-pe","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"tidy_eval_infinite","html_id":"tidy-eval-infinite","kind":"proof:example","data":"/mdps.json","url":"/mdps"},{"identifier":"iterative_pe","html_id":"iterative-pe","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"iterations_vi","html_id":"iterations-vi","kind":"proof:remark","data":"/mdps.json","url":"/mdps"},{"identifier":"optimal-policies-in-infinite-horizon-mdps","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"identifier":"optimal_policy_infinite","html_id":"optimal-policy-infinite","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_optimality","html_id":"bellman-optimality","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"bellman_optimality_operator","html_id":"bellman-optimality-operator","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"value_iteration","html_id":"value-iteration","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"greedy_worsen","html_id":"greedy-worsen","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"policy_iteration","html_id":"policy-iteration","kind":"heading","data":"/mdps.json","url":"/mdps"},{"identifier":"pi_iter_analysis","html_id":"pi-iter-analysis","kind":"proof:theorem","data":"/mdps.json","url":"/mdps"},{"identifier":"pi_iter_proof","html_id":"pi-iter-proof","kind":"equation","data":"/mdps.json","url":"/mdps"},{"identifier":"summary","kind":"heading","data":"/mdps.json","url":"/mdps","implicit":true},{"kind":"page","data":"/control.json","url":"/control"},{"identifier":"introduction","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"control_examples","html_id":"control-examples","kind":"figure","data":"/control.json","url":"/control"},{"identifier":"robot_hand","html_id":"robot-hand","kind":"figure","data":"/control.json","url":"/control"},{"identifier":"cart_pole","html_id":"cart-pole","kind":"proof:example","data":"/control.json","url":"/control"},{"identifier":"optimal-control","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"optimal_control","html_id":"optimal-control","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"a-first-attempt-discretization","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"lqr","kind":"heading","data":"/control.json","url":"/control"},{"identifier":"lqr_definition","html_id":"lqr-definition","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"value_lqr","html_id":"value-lqr","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"optimal_lqr","html_id":"optimal-lqr","kind":"heading","data":"/control.json","url":"/control"},{"identifier":"optimal_value_lqr","html_id":"optimal-value-lqr","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"optimal_value_lqr_quadratic","html_id":"optimal-value-lqr-quadratic","kind":"proof:theorem","data":"/control.json","url":"/control"},{"identifier":"optimal_policy_lqr_linear","html_id":"optimal-policy-lqr-linear","kind":"proof:theorem","data":"/control.json","url":"/control"},{"identifier":"lemma_pi_linear","html_id":"lemma-pi-linear","kind":"proof:lemma","data":"/control.json","url":"/control"},{"identifier":"k_pi","html_id":"k-pi","kind":"equation","data":"/control.json","url":"/control"},{"identifier":"riccati","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"lemma_schur","html_id":"lemma-schur","kind":"proof:lemma","data":"/control.json","url":"/control"},{"identifier":"expected-state-at-time-hi","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"expected_state","html_id":"expected-state","kind":"equation","data":"/control.json","url":"/control"},{"identifier":"extensions","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"time_dep_lqr","html_id":"time-dep-lqr","kind":"heading","data":"/control.json","url":"/control"},{"identifier":"time_dependent_lqr","html_id":"time-dependent-lqr","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"riccati_time_dependent","html_id":"riccati-time-dependent","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"more-general-quadratic-cost-functions","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"general_quadratic_cost","html_id":"general-quadratic-cost","kind":"equation","data":"/control.json","url":"/control"},{"identifier":"tracking-a-predefined-trajectory","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"approx_nonlinear","html_id":"approx-nonlinear","kind":"heading","data":"/control.json","url":"/control"},{"identifier":"nonlinear_control","html_id":"nonlinear-control","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"local-linearization","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"finite-differencing","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"local-convexification","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"identifier":"local_linearization","html_id":"local-linearization","kind":"figure","data":"/control.json","url":"/control"},{"identifier":"iterative_lqr","html_id":"iterative-lqr","kind":"heading","data":"/control.json","url":"/control"},{"identifier":"ilqr","kind":"proof:definition","data":"/control.json","url":"/control"},{"identifier":"summary","kind":"heading","data":"/control.json","url":"/control","implicit":true},{"kind":"page","data":"/bandits.json","url":"/bandits"},{"identifier":"introduction","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"advertising","kind":"proof:example","data":"/bandits.json","url":"/bandits"},{"identifier":"clinical_trials","html_id":"clinical-trials","kind":"proof:example","data":"/bandits.json","url":"/bandits"},{"identifier":"multi-armed","kind":"proof:remark","data":"/bandits.json","url":"/bandits"},{"identifier":"regret","kind":"proof:definition","data":"/bandits.json","url":"/bandits"},{"identifier":"pure-exploration-random-guessing","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"pure_exploration","html_id":"pure-exploration","kind":"block:notebook-code","data":"/bandits.json","url":"/bandits"},{"identifier":"pure_exploration-code","html_id":"pure-exploration-code","kind":"code","data":"/bandits.json","url":"/bandits"},{"identifier":"pure_exploration-output","html_id":"pure-exploration-output","kind":"output","data":"/bandits.json","url":"/bandits"},{"identifier":"pure-greedy","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"pure_greedy","html_id":"pure-greedy","kind":"block:notebook-code","data":"/bandits.json","url":"/bandits"},{"identifier":"pure_greedy-code","html_id":"pure-greedy-code","kind":"code","data":"/bandits.json","url":"/bandits"},{"identifier":"pure_greedy-output","html_id":"pure-greedy-output","kind":"output","data":"/bandits.json","url":"/bandits"},{"identifier":"etc","kind":"heading","data":"/bandits.json","url":"/bandits"},{"identifier":"etc-regret-analysis","kind":"heading","data":"/bandits.json","url":"/bandits"},{"identifier":"exploration-phase","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"exploitation-phase","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"hoeffding","kind":"proof:theorem","data":"/bandits.json","url":"/bandits"},{"identifier":"hoeffding-etc","kind":"equation","data":"/bandits.json","url":"/bandits"},{"identifier":"union_bound","html_id":"union-bound","kind":"proof:theorem","data":"/bandits.json","url":"/bandits"},{"identifier":"epsilon-greedy","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"ucb","kind":"heading","data":"/bandits.json","url":"/bandits"},{"identifier":"ucb-regret-analysis","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"lower-bound-on-regret-intuition","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"thompson_sampling","html_id":"thompson-sampling","kind":"heading","data":"/bandits.json","url":"/bandits"},{"identifier":"bayesian_bernoulli","html_id":"bayesian-bernoulli","kind":"proof:example","data":"/bandits.json","url":"/bandits"},{"identifier":"contextual-bandits","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"identifier":"contextual_bandit","html_id":"contextual-bandit","kind":"proof:definition","data":"/bandits.json","url":"/bandits"},{"identifier":"lin_ucb","html_id":"lin-ucb","kind":"heading","data":"/bandits.json","url":"/bandits"},{"identifier":"ols_bandit","html_id":"ols-bandit","kind":"equation","data":"/bandits.json","url":"/bandits"},{"identifier":"chebyshev","kind":"proof:theorem","data":"/bandits.json","url":"/bandits"},{"identifier":"summary","kind":"heading","data":"/bandits.json","url":"/bandits","implicit":true},{"kind":"page","data":"/supervised-learning.json","url":"/supervised-learning"},{"identifier":"introduction","kind":"heading","data":"/supervised-learning.json","url":"/supervised-learning","implicit":true},{"identifier":"parameterized_empirical_risk_minimization","html_id":"parameterized-empirical-risk-minimization","kind":"proof:definition","data":"/supervised-learning.json","url":"/supervised-learning"},{"identifier":"gd_def","html_id":"gd-def","kind":"proof:definition","data":"/supervised-learning.json","url":"/supervised-learning"},{"identifier":"linear-regression","kind":"heading","data":"/supervised-learning.json","url":"/supervised-learning","implicit":true},{"identifier":"neural-networks","kind":"heading","data":"/supervised-learning.json","url":"/supervised-learning","implicit":true},{"kind":"page","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"introduction","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp","implicit":true},{"identifier":"erm","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"conditional_expectation_minimizes_mse","html_id":"conditional-expectation-minimizes-mse","kind":"proof:theorem","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"empirical_risk_minimization","html_id":"empirical-risk-minimization","kind":"proof:definition","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"fitted-value-iteration","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp","implicit":true},{"identifier":"fitted_q_iteration","html_id":"fitted-q-iteration","kind":"proof:definition","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"fitted-pi-eval","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"fitted_evaluation","html_id":"fitted-evaluation","kind":"proof:definition","data":"/fitted-dp.json","url":"/fitted-dp"},{"identifier":"fitted-policy-iteration","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp","implicit":true},{"identifier":"summary","kind":"heading","data":"/fitted-dp.json","url":"/fitted-dp","implicit":true},{"kind":"page","data":"/pg.json","url":"/pg"},{"identifier":"introduction","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"gradient-ascent","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"stochastic-gradient-ascent","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"policy-stochastic-gradient-ascent","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"objective_fn","html_id":"objective-fn","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"parameterizations","kind":"heading","data":"/pg.json","url":"/pg"},{"identifier":"importance_sampling","html_id":"importance-sampling","kind":"heading","data":"/pg.json","url":"/pg"},{"identifier":"the-reinforce-policy-gradient","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"reinforce_pg","html_id":"reinforce-pg","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"intuitive-remark","kind":"admonition:note","data":"/pg.json","url":"/pg"},{"identifier":"baselines-and-advantages","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"pg_with_q","html_id":"pg-with-q","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"eq:pg_baseline","html_id":"eq-pg-baseline","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"advantage","kind":"proof:definition","data":"/pg.json","url":"/pg"},{"identifier":"pg_advantage","html_id":"pg-advantage","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"comparing-policy-gradient-algorithms-to-policy-iteration","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"pdl","kind":"proof:theorem","data":"/pg.json","url":"/pg"},{"identifier":"pdl_eq","html_id":"pdl-eq","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"trust-region-policy-optimization","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"kld","kind":"proof:definition","data":"/pg.json","url":"/pg"},{"identifier":"trpo","kind":"proof:definition","data":"/pg.json","url":"/pg"},{"identifier":"natural-policy-gradient","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"npg_optimization","html_id":"npg-optimization","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"fisher_matrix","html_id":"fisher-matrix","kind":"proof:definition","data":"/pg.json","url":"/pg"},{"identifier":"fisher_trajectory","html_id":"fisher-trajectory","kind":"equation","data":"/pg.json","url":"/pg"},{"identifier":"npg","kind":"proof:definition","data":"/pg.json","url":"/pg"},{"identifier":"natural_simple","html_id":"natural-simple","kind":"proof:example","data":"/pg.json","url":"/pg"},{"identifier":"proximal-policy-optimization","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"identifier":"summary","kind":"heading","data":"/pg.json","url":"/pg","implicit":true},{"kind":"page","data":"/imitation-learning.json","url":"/imitation-learning"},{"identifier":"introduction","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"identifier":"behavioral-cloning","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"identifier":"behavioral_cloning","html_id":"behavioral-cloning","kind":"proof:definition","data":"/imitation-learning.json","url":"/imitation-learning"},{"identifier":"performance-of-behavioral-cloning","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"identifier":"eq:pdl-rhs","html_id":"eq-pdl-rhs","kind":"equation","data":"/imitation-learning.json","url":"/imitation-learning"},{"identifier":"distribution-shift","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"identifier":"dataset-aggregation-dagger","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"identifier":"summary","kind":"heading","data":"/imitation-learning.json","url":"/imitation-learning","implicit":true},{"kind":"page","data":"/planning.json","url":"/planning"},{"identifier":"introduction","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"deterministic-zero-sum-fully-observable-two-player-games","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"notation","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"tic-tac-toe","kind":"proof:example","data":"/planning.json","url":"/planning"},{"identifier":"min-max-search","kind":"heading","data":"/planning.json","url":"/planning"},{"identifier":"min-max-value","kind":"proof:definition","data":"/planning.json","url":"/planning"},{"identifier":"min-max-example","kind":"proof:example","data":"/planning.json","url":"/planning"},{"identifier":"complexity-of-min-max-search","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"alpha-beta-search","kind":"heading","data":"/planning.json","url":"/planning"},{"identifier":"alpha-beta-example","kind":"proof:example","data":"/planning.json","url":"/planning"},{"identifier":"monte-carlo-tree-search","kind":"heading","data":"/planning.json","url":"/planning"},{"identifier":"mcts-algorithm","kind":"proof:algorithm","data":"/planning.json","url":"/planning"},{"identifier":"ucb-tree","kind":"equation","data":"/planning.json","url":"/planning"},{"identifier":"incorporating-value-functions-and-policies","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"mcts-policy-value","kind":"proof:algorithm","data":"/planning.json","url":"/planning"},{"identifier":"ucb-tree-policy","kind":"equation","data":"/planning.json","url":"/planning"},{"identifier":"self-play","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"mcts-self-play","kind":"proof:algorithm","data":"/planning.json","url":"/planning"},{"identifier":"summary","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"identifier":"references","kind":"heading","data":"/planning.json","url":"/planning","implicit":true},{"kind":"page","data":"/exploration.json","url":"/exploration"},{"identifier":"introduction","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"per_episode_regret","html_id":"per-episode-regret","kind":"proof:definition","data":"/exploration.json","url":"/exploration"},{"identifier":"sparse-reward","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"sparse_reward_mdp","html_id":"sparse-reward-mdp","kind":"proof:example","data":"/exploration.json","url":"/exploration"},{"identifier":"exploration-in-deterministic-mdps","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"explore_then_exploit","html_id":"explore-then-exploit","kind":"proof:definition","data":"/exploration.json","url":"/exploration"},{"identifier":"explore_then_exploit_performance","html_id":"explore-then-exploit-performance","kind":"proof:theorem","data":"/exploration.json","url":"/exploration"},{"identifier":"mdp_mab","html_id":"mdp-mab","kind":"heading","data":"/exploration.json","url":"/exploration"},{"identifier":"mdp_as_mab","html_id":"mdp-as-mab","kind":"equation","data":"/exploration.json","url":"/exploration"},{"identifier":"ineffective_mdp","html_id":"ineffective-mdp","kind":"proof:example","data":"/exploration.json","url":"/exploration"},{"identifier":"ucb-vi","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"modelling-the-transitions","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"reward-bonus","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"eq:ucb_vi_bonus","html_id":"eq-ucb-vi-bonus","kind":"equation","data":"/exploration.json","url":"/exploration"},{"identifier":"ucb_vi_bonus","html_id":"ucb-vi-bonus","kind":"proof:remark","data":"/exploration.json","url":"/exploration"},{"identifier":"err","kind":"equation","data":"/exploration.json","url":"/exploration"},{"identifier":"definition","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"ucb-vi-alg","kind":"equation","data":"/exploration.json","url":"/exploration"},{"identifier":"performance-of-ucb-vi","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"ucb_vi_regret","html_id":"ucb-vi-regret","kind":"proof:theorem","data":"/exploration.json","url":"/exploration"},{"identifier":"linear-mdps","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"linear_mdp","html_id":"linear-mdp","kind":"proof:definition","data":"/exploration.json","url":"/exploration"},{"identifier":"planning-in-a-linear-mdp","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"lin_ucb_vi","html_id":"lin-ucb-vi","kind":"heading","data":"/exploration.json","url":"/exploration"},{"identifier":"performance","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"identifier":"lin_ucb_vi_regret","html_id":"lin-ucb-vi-regret","kind":"proof:theorem","data":"/exploration.json","url":"/exploration"},{"identifier":"summary","kind":"heading","data":"/exploration.json","url":"/exploration","implicit":true},{"kind":"page","data":"/background.json","url":"/background"},{"identifier":"o-notation","kind":"heading","data":"/background.json","url":"/background","implicit":true},{"identifier":"python","kind":"heading","data":"/background.json","url":"/background","implicit":true}]}