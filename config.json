{"options":{"logo":"/184-5f23de62f8c37b6a5894408ebcf81c79.png"},"myst":"1.3.17","nav":[],"actions":[],"projects":[{"numbering":{"all":{"enabled":true}},"bibliography":["/Users/adzcai/Developer/rlbook/book/shared/references.bib"],"math":{"\\E":{"macro":"\\mathop{\\mathbb{E}}"},"\\pr":{"macro":"\\mathop{\\mathbb{P}}"},"\\kl":{"macro":"\\mathrm{KL}\\left(#1\\parallel#2\\right)"},"\\ind":{"macro":"\\mathbf{1}\\left\\{#1\\right\\}"},"\\hi":{"macro":"h"},"\\hor":{"macro":"H"},"\\st":{"macro":"s"},"\\act":{"macro":"a"}},"exports":[],"title":"CS/STAT 184: Introduction to Reinforcement Learning","authors":[{"nameParsed":{"literal":"Fall 2024","given":"Fall","family":"2024"},"name":"Fall 2024","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/adzcai/cs-stat-184-notes","toc":[{"file":"index.md"},{"file":"mdps.md"},{"file":"control.md"},{"file":"bandits.md"},{"file":"supervised_learning.md"},{"file":"fitted_dp.md"},{"file":"pg.md"},{"file":"imitation_learning.md"},{"file":"planning.md"},{"file":"exploration.md"},{"file":"background.md"}],"index":"index","pages":[{"slug":"mdps","title":"1 Markov Decision Processes","description":"","date":"","thumbnail":"/deterministic_policy-812169be9756946f7e3c7d574430be33.png","thumbnailOptimized":"/deterministic_policy-812169be9756946f7e3c7d574430be33.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"control","title":"2 Linear Quadratic Regulators","description":"","date":"","thumbnail":"/rubiks_cube-736e8f94de8f8c4e720bad966191ed3f.jpg","thumbnailOptimized":"/rubiks_cube-736e8f94de8f8c4e720bad966191ed3f.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"bandits","title":"3 Multi-Armed Bandits","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"supervised-learning","title":"4 Supervised learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"fitted-dp","title":"5 Fitted Dynamic Programming Algorithms","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"pg","title":"6  Policy Gradient Methods","description":"","date":"","thumbnail":"/npg_line-d866eed4d36f0f17d1b36b6fa0a76770.png","thumbnailOptimized":"/npg_line-d866eed4d36f0f17d1b36b6fa0a76770.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"imitation-learning","title":"7 Imitation Learning","description":"","date":"","thumbnail":"/robot-imitation-lear-3959133f2c440537594fb904c12725f5.jpg","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"planning","title":"8 Tree Search Methods","description":"","date":"","thumbnail":"/tic_tac_toe-b48eec3dc0012079611b6294983f5b74.png","thumbnailOptimized":"/tic_tac_toe-b48eec3dc0012079611b6294983f5b74.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"exploration","title":"9 Exploration in MDPs","description":"","date":"","thumbnail":"/sparse_reward_mdp-78fc15b37994af884414c2a05bc09dc2.png","thumbnailOptimized":"/sparse_reward_mdp-78fc15b37994af884414c2a05bc09dc2.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"background","title":"Appendix: Background","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]}