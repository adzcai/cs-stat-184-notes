{"options":{"logo":"/184-10fe069484708f6514e3854e25d06608.png"},"myst":"1.3.7","nav":[],"actions":[],"projects":[{"numbering":{"all":{"enabled":true}},"bibliography":["/Users/adzcai/Developer/cs-stat-184-notes/book/shared/references.bib"],"math":{"\\E":{"macro":"\\mathop{\\mathbb{E}}"},"\\pr":{"macro":"\\mathop{\\mathbb{P}}"},"\\kl":{"macro":"\\mathrm{KL}\\left(#1\\parallel#2\\right)"},"\\ind":{"macro":"\\mathbf{1}\\left\\{#1\\right\\}"},"\\hi":{"macro":"h"},"\\hor":{"macro":"H"},"\\st":{"macro":"s"},"\\act":{"macro":"a"}},"exports":[],"title":"CS/STAT 184: Introduction to Reinforcement Learning","authors":[{"nameParsed":{"literal":"Fall 2024","given":"Fall","family":"2024"},"name":"Fall 2024","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/adzcai/cs-stat-184-notes","toc":[{"file":"index.md"},{"file":"mdps.md"},{"file":"control.md"},{"file":"bandits.md"},{"file":"supervised_learning.md"},{"file":"fitted_dp.md"},{"file":"pg.md"},{"file":"imitation_learning.md"},{"file":"planning.md"},{"file":"exploration.md"},{"file":"background.md"}],"index":"index","pages":[{"slug":"mdps","title":"1 Markov Decision Processes","description":"","date":"","thumbnail":"/deterministic_policy-9d0b50d69541007293ead345d987b682.png","thumbnailOptimized":"/deterministic_policy-9d0b50d69541007293ead345d987b682.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"control","title":"2 Linear Quadratic Regulators","description":"","date":"","thumbnail":"/rubiks_cube-5d86d5b19a044eede0a3801e51b37815.jpg","thumbnailOptimized":"/rubiks_cube-5d86d5b19a044eede0a3801e51b37815.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"bandits","title":"3 Multi-Armed Bandits","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"supervised-learning","title":"4 Supervised learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"fitted-dp","title":"5 Fitted Dynamic Programming Algorithms","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"pg","title":"6  Policy Optimization","description":"","date":"","thumbnail":"/npg_line-18dfc6d5286c25a94643b5e115d15484.png","thumbnailOptimized":"/npg_line-18dfc6d5286c25a94643b5e115d15484.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"imitation-learning","title":"7 Imitation Learning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"planning","title":"8 Planning","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"exploration","title":"9 Exploration in MDPs","description":"","date":"","thumbnail":"/sparse_reward_mdp-d4beda7e57ed42a0bbe96cfa6c5ecbbe.png","thumbnailOptimized":"/sparse_reward_mdp-d4beda7e57ed42a0bbe96cfa6c5ecbbe.webp","banner":"","bannerOptimized":"","tags":[],"level":1},{"slug":"background","title":"Appendix: Background","description":"","date":"","thumbnail":"","thumbnailOptimized":"","banner":"","bannerOptimized":"","tags":[],"level":1}]}]}